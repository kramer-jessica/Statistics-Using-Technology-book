[
  {
    "objectID": "Numerical description of Data.html",
    "href": "Numerical description of Data.html",
    "title": "4  Numerical Description of Data",
    "section": "",
    "text": "4.1 Measures of Center\nChapter 1 discussed what a population, sample, parameter, and statistic are, and how to take different types of samples. Chapter 2 discussed ways to graphically display data. There was also a discussion of important characteristics: center, variations, distribution, outliers, and changing characteristics of the data over time. Distributions and outliers can be answered using graphical means. Finding the center and variation can be done using numerical methods that will be discussed in this chapter. Both graphical and numerical methods are part of a branch of statistics known as descriptive statistics. Later descriptive statistics will be used to make decisions and/or estimate population parameters using methods that are part of the branch called inferential statistics.\nThis section focuses on measures of central tendency. Many times you are asking what to expect on average. Such as when you pick a major, you would probably ask how much you expect to earn in that field. If you are thinking of relocating to a new town, you might ask how much you can expect to pay for housing. If you are planting vegetables in the spring, you might want to know how long it will be until you can harvest. These questions, and many more, can be answered by knowing the center of the data set. There are three measures of the “center” of the data. They are the mode, median, and mean. Any of the values can be referred to as the “average.”\nThe mode is the data value that occurs the most frequently in the data. To find it, you count how often each data value occurs, and then determine which data value occurs most often. The mode is not the most useful measure of center. This is because, a data set can have more than one mode. If there is a tie between two values for the most number of times then both values are the mode and the data is called bimodal (two modes). If every data point occurs the same number of times, there is no mode. If there are more than two numbers that appear the most times, then usually there is no mode.\nThe median is the data value in the middle of a sorted list of data. To find it, you put the data in order, and then determine which data value is in the middle of the data set.\nThe mean is the arithmetic average of the numbers. This is the center that most people call the average, though all three – mean, median, and mode – really are averages.\nThere are no symbols for the mode and the median, but the mean is used a great deal, and statisticians gave it a symbol. There are actually two symbols, one for the population parameter and one for the sample statistic. In most cases you cannot find the population parameter, so you use the sample statistic to estimate the population parameter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical description of Data.html#measures-of-center",
    "href": "Numerical description of Data.html#measures-of-center",
    "title": "4  Numerical Description of Data",
    "section": "",
    "text": "4.1.1 Population Mean\n\\(\\mu=\\frac{\\sum{x}}{N}\\), pronounced mu\nN is the size of the population.\nx represents a data value.\n\\(\\sum{x}\\) means to add up all of the data values.\n\n\n4.1.2 Sample Mean:\n\\(\\bar{x}=\\frac{\\sum{x}}{n}\\), pronounced x bar.\nn is the size of the sample.\nx represents a data value.\n\\(\\sum{x}\\) means to add up all of the data values.\nThe value for \\(\\bar{x}\\) is used to estimate \\(\\mu\\) since \\(\\mu\\) can’t be calculated in most situations.\n\n\n4.1.3 Example: Finding the Mean and Median using Rguroo\nSuppose a vet wants to find the average weight of cats. The weights (in kg) of cats are in Table 4.1.\n\n\n\n\nTable 4.1: Head of Cats\n\n\n\n\n\n\nSex\nBwt\nHwt\n\n\n\n\nF\n2.0\n7.0\n\n\nF\n2.0\n7.4\n\n\nF\n2.0\n9.5\n\n\nF\n2.1\n7.2\n\n\nF\n2.1\n7.3\n\n\nF\n2.1\n7.6\n\n\n\n\n\n\n\n\n\nClick to expand the box below to see instructions to import and view the cats dataset in your Data Toolbox in Rguroo.\n\n\n\n\n\n\n Importing cats to Data Toolbox\n\n\n\n\n\n\nGo to the Data toolbox.\nFrom the Data Import dropdown, select Dataset Repository.\nIn the top search box, type MASS, then select the MASS repository.\nIn the middle search box, type cats, and select the cats dataset that appears in the lower panel.\n\nClick the Import button. The dataset will be imported into your Rguroo account.\nClick the Close button to close the Rguroo dialog.\nTo view the dataset, double-click cats under the Data toolbox list.\n\n\n\n\nFind the mean and median of the weight of a cat.\n\n4.1.3.1 Solution\nBefore starting any mathematics problem, it is always a good idea to define the unknown in the problem. In statistics, you want to define the variable. The symbol for the variable is *x*.\nThe variable is x = weight of a cat\nClick to expand the box below see how to calculate the mean and median in Rguroo.\n\n\n\n\n\n\n Calculating the Mean and Median Weight of Cats\n\n\n\n\n\nBefore you begin: Make sure you have already imported the cats dataset into your Data Toolbox, as was shown here.\n\nOpen the Data toolbox.\n\nClick on the Functions dropdown, and select Summary Statistic. This opens the Basic Summary Statistic dialog.\n\nFrom the Dataset dropdown, select the cats dataset.\n\nFrom the Numerical dropdown, select the Bwt variable.\n\nIn the Statistics section of the dialog, select the checkbox for Mean and Median.\n\nClick the preview icon  to see a preview of the scatterplot.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nBasic Summary Statistic dialog in Rguroo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.1: Mean and Median Weight of Cats\n\n\n\n\n\nThe mean weight is 2.72 kg.\n\nThe median weight is 2.7 kg also. It appears the average weight is 2.7 kg of all cats.\n\n\n\n4.1.4 Example: Finding Mean and Median with Factor\nLooking at the data frame for cats weights Table 4.1 you see that there are several variables You may want to know what the other variables are. A Code Book describes the data set, explains what the variables are including the units, and the source of the data frame. The code book for the cats is below.\nImage 3.1.1: Code book for cats data frame\n\n\n\nCode book for cats data frame\n\n\nSuppose you want to know if male cats weigh more than female cats. Looking at the variables, you notice that there is a variable for the sex of the cat. You can look at the weights of males and females separately. This looks like:\n\n4.1.4.1 Solution\nClick to expand the box below see how to calculate the mean and median, separated by sex, in Rguroo.\n\n\n\n\n\n\n Calculating the Mean and Median Weight of Cats by Sex\n\n\n\n\n\nBefore you begin: Make sure you have already imported the cats dataset into your Data Toolbox, as was shown here.\n\nOpen the Data toolbox.\n\nClick on the Functions dropdown, and select Summary Statistic. This opens the Basic Summary Statistic dialog.\n\nFrom the Dataset dropdown, select the cats dataset.\n\nFrom the Numerical dropdown, select the Bwt variable.\nFrom the Factor 1 dropdown, select the Sex factor.\nIn the Statistics section of the dialog, select the checkbox for Mean and Median.\n\nClick the preview icon  to see a preview of the scatterplot.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nBasic Summary Statistic dialog in Rguroo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: Mean and Median Weight of Cats by Sex\n\n\n\n\n\nNotice that the female cats’ mean weight is 2.4 kg and the male cats’ mean weight is 2.9 kg. The median weight of female cats is 2.3 kg and for males it is 2.9 kg. So it does appear that male cats weigh a bit more than the female cats.\nThere are many different summary statistics that can be found. An example is the minimum and maximum value. In this example, you will see how to find the min and max values and then filter them out of a data set to see what effect they have on the mean and median.\n\n\n\n4.1.5 Example: Affect of Extreme Values on Mean and Median\nFind the minimum and maximum values of cats weights.\n\n4.1.5.1 Solution\nThe steps to find the minimum and the maximum in Rguroo. are similar to finding the mean and median. In the Statistics section of the dialog, select the checkbox for Minimum and Maximum.\nHere is the Basic Summary Statistic Dialog and Output from Rguroo for the minimum and maximum of cat’s body weight.\n\n\n\n\n\n\n\n\n\n\n\n(a) Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 4.3: Maximum and Minimum Weight of Cats\n\n\n\n\nThe minimum weight of a cat in this data frame is 2 kg and the maximum weight of a cat is 3.9 kg.\nNow create two new data sets. One data set will exclude the maximum value. You can call it anything you want, but it would make sense to call it something like nomax. The command to create the new data set is:\n\nnomax &lt;- filter(cats, Bwt&lt;3.9)\n\nThen create a data set that excludes the minimum value; call it nomin:\n\nnomin&lt;-filter(cats, Bwt&gt;2)\n\nThe &lt;- is the way to indicate to r what the data set nomin is equivalent to what follows the symbol. Notice that it doesn’t look like anything happened, but new data sets were created in the background. Now you can find the mean and median of each new data set:\n\ndf_stats(~Bwt, data=nomax, mean, median)\n\n  response     mean median\n1      Bwt 2.707042    2.7\n\n\nThe mean without the maximum value is 2.70 kg, and the median is 2.7 kg.\n\ndf_stats(~Bwt, data=nomin, mean, median)\n\n  response    mean median\n1      Bwt 2.74964    2.7\n\n\nThe mean without the minimum value is 2.75 kg, and the median is 2.7 kg.\nFrom Example: Affect of Extreme Values on Mean and Median, the mean of the data set with all the values is 2.72 kg where the median is 2.7 kg. Notice that when the maximum value was excluded from the data set, the mean decreased a little but the median didn’t change, and when the minimum value was excluded from the data set, the mean increased a little but the median didn’t change. The mean is much higher than the median. Why is this? This is because the mean is affected by extreme values, while the median is not. We say the median is a much more resistant measure of center because it isn’t affected by extreme values as much.\nAn outlier is a data value that is very different from the rest of the data. It can be really high or really low. Extreme values may be an outlier if the extreme value is far enough from the center. If there are extreme values in the data, the median is a better measure of the center than the mean. If there are no extreme values, the mean and the median will be similar so most people use the mean. The mean is not a resistant measure because it is affected by extreme values. The median is a resistant measure because it not affected by extreme values.\nAs a consumer you need to be aware that people choose the measure of center that best supports their claim. When you read an article in the newspaper and it talks about the “average” it usually means the mean but sometimes it refers to the median. Some articles will use the word “median” instead of “average” to be more specific. If you need to make an important decision and the information says “average”, it would be wise to ask if the “average” is the mean or the median before you decide.\nAs an example, suppose that a company wants to use the mean salary as the average salary for the company. This is because the high salaries of the administrators will pull the mean higher. The company can say that the employees are paid well because the average is high. However, the employees want to use the median since it discounts the extreme values of the administration and will give a lower value of the average. This will make the salaries seem lower and that a raise is in order.\nWhy use the mean instead of the median? The reason is because when multiple samples are taken from the same population, the sample means tend to be more consistent than other measures of the center.\nTo understand how the different measures of center related to skewed or symmetric distributions, see Graph \\#3.1.1. As you can see sometimes the mean is smaller than the median, sometimes the mean is larger than the median, and sometimes they are the same values.\nGraph \\#3.1.1: Mean, Median, Mode as Related to a Distribution\n\n\n\nMean, median, mode as related to distribution\n\n\nOne last type of average is a weighted average. Weighted averages are used quite often in different situations. Some teachers use them in calculating a student’s grade in the course, or a grade on a project. Some employers use them in employee evaluations. The idea is that some activities are more important than others. As an example, a full time teacher at a community college may be evaluated on their service to the college, their service to the community, whether their paperwork is turned in on time, and their teaching. However, teaching is much more important than whether their paperwork is turned in on time. When the evaluation is completed, more weight needs to be given to the teaching and less to the paperwork. This is a weighted average.\n\n\n\n4.1.6 Weighted Average\n\\(\\text{weighted average}=\\frac{\\sum{x*w}}{\\sum{w}}\\)\nwhere w is the weight of the data value, x.\n\n\n4.1.7 Example: Weighted Average\nIn your biology class, your final grade is based on several things: a lab score, scores on two major tests, and your score on the final exam. There are 100 points available for each score. The lab score is worth 15% of the course, the two exams are worth 25% of the course each, and the final exam is worth 35% of the course. Suppose you earned scores of 95 on the labs, 83 and 76 on the two exams, and 84 on the final exam. Compute your weighted average for the course.\n\n4.1.7.1 Solution\nVariable: x = score\nA weighted average can be found using technology. The commands for finding the weighted mean using RStudio is as follows:\nx&lt;-c(type in the scores with commas in between)\nw&lt;-c(type in the weights as decimals with commas in between\nweighted.mean(x,w)\nThe x and w represent the variables, &lt;- means make the variables equivalent to what follows, the c( means combine all the values in the () as one combined variable.\nFor this example, the commands would be\n\nx&lt;-c(95, 83, 76, 84) \nw&lt;-c(.15, .25, .25, .35) \nweighted.mean(x,w) \n\n[1] 83.4\n\n\nYour weighted mean in the biology class is 83.4%. Using the traditional grading scale, you have a B in the class.\n\n\n\n4.1.8 Example: Weighted Average\nThe faculty evaluation process at John Jingle University rates a faculty member on the following activities: teaching, publishing, committee service, community service, and submitting paperwork in a timely manner. The process involves reviewing student evaluations, peer evaluations, and supervisor evaluation for each teacher and awarding him/her a score on a scale from 1 to 10 (with 10 being the best). The weights for each activity are 20 for teaching, 18 for publishing, 6 for committee service, 4 for community service, and 2 for paperwork.\n\nOne faculty member had the following ratings: 8 for teaching, 9 for publishing, 2 for committee work, 1 for community service, and 8 for paperwork. Compute the weighted average of the evaluation.\nAnother faculty member had ratings of 6 for teaching, 8 for publishing, 9 for committee work, 10 for community service, and 10 for paperwork. Compute the weighted average of the evaluation.\nWhich faculty member had the higher average evaluation?\n\n\n4.1.8.1 Solution\n\nOne faculty member had the following ratings: 8 for teaching, 9 for publishing, 2 for committee work, 1 for community service, and 8 for paperwork. Compute the weighted average of the evaluation.\n\nVariable: x = rating, w = weight\n\nx&lt;-c(8, 9, 2, 1, 8) \nw&lt;-c(20, 18, 6, 4, 2) \nweighted.mean(x,w)\n\n[1] 7.08\n\n\nThe weighted average is 7.08.\n\nAnother faculty member had ratings of 6 for teaching, 8 for publishing, 9 for committee work, 10 for community service, and 10 for paperwork. Compute the weighted average of the evaluation.\n\n\nx&lt;-c(6, 8, 9, 10, 10) \nw&lt;-c(20, 18, 6, 4, 2) \nweighted.mean(x,w)\n\n[1] 7.56\n\n\nThe weighted average for this employee is 7.56.\n\nWhich faculty member had the higher average evaluation?\nThe second faculty member has a higher average evaluation.\n\nThe last thing to mention is which average is used on which type of data.\nMode can be found on nominal, ordinal, interval, and ratio data, since the mode is just the data value that occurs most often. You are just counting the data values.\nMedian can be found on ordinal, interval, and ratio data, since you need to put the data in order. As long as there is order to the data you can find the median.\nMean can be found on interval and ratio data, since you must have numbers to add together.\n\n\n\n4.1.9 Homework for Measures of Center Section\nUse Technology on all problems. State the variable on all problems.\n\nCholesterol levels were collected from patients certain days after they had a heart attack and are in Table 4.2. Find the mean and median for cholesterol levels 2 days after the heart attack.\n\n\nCholesterol&lt;-read.csv( \"https://krkozak.github.io/MAT160/cholesterol.csv\") \nknitr::kable(head(Cholesterol))\n\n\n\nTable 4.2: Head of Cholesterol Levels of Patients After Heart Attack\n\n\n\n\n\n\npatient\nday2\nday4\nday14\n\n\n\n\n1\n270\n218\n156\n\n\n2\n236\n234\nNA\n\n\n3\n210\n214\n242\n\n\n4\n142\n116\nNA\n\n\n5\n280\n200\nNA\n\n\n6\n272\n276\n256\n\n\n\n\n\n\n\n\nCode book for Data Frame Cholesterol\nDescription Cholesterol levels were collected from patients certain days after they had a heart attack\nThis data frame contains the following columns:\nPatient: Patient number\nday2: Cholesterol level of patient 2 days after heart attack. (mg/dL)\nday4: Cholesterol level of patient 4 days after heart attack. (mg/dL)\nday14: Cholesterol level of patient 14 days after heart attack. (mg/dL)\nSource Ryan, B. F., Joiner, B. L., & Ryan, Jr, T. A. (1985). Cholesterol levels after heart attack.Retrieved from http://www.statsci.org/data/general/cholest.html\nReferences Ryan, Joiner & Ryan, Jr, 1985\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 4.3 (Lee, 1994). Find the mean and median length of rivers that flow into the Pacific Ocean and the mean and median length of rivers that flow into the Tasman Sea.\n\n\nLength&lt;-read.csv( \"https://krkozak.github.io/MAT160/length.csv\") \nknitr::kable(head(Length))\n\n\n\nTable 4.3: Head of Length of New zealand rivers (km)\n\n\n\n\n\n\nriver\nlength\nflowsto\n\n\n\n\nClarence\n209\nPacific\n\n\nConway\n48\nPacific\n\n\nWaiau\n169\nPacific\n\n\nHurunui\n138\nPacific\n\n\nWaipara\n64\nPacific\n\n\nAshley\n97\nPacific\n\n\n\n\n\n\n\n\nCode book for data frame Length\nDescription Rivers in New Zealand, the lengths of river and what body of water the river flows into\nThis data frame contains the following columns:\nRiver: Name of the river\nlength: how long the river is in kilometers\nflowsto: what body of water the river flows into Pacific Ocean is Pacific and the Tasman Sea is Tasman\nSource Lee, A. (1994). Data analysis: An introduction based on r. Auckland. Retrieved from http://www.statsci.org/data/oz/nzrivers.html\nReferences Lee, A. (1994). Data analysis: An introduction based on r. Auckland.\n\nPrint-O-Matic printing company’s employees have salaries that are contained in Table 4.4.\n\n\nPay&lt;-read.csv( \"https://krkozak.github.io/MAT160/pay.csv\") \nknitr::kable(head(Pay))\n\n\n\nTable 4.4: Head of Salaries of Print-O-Matic Printing Company Employees\n\n\n\n\n\n\nemployee\nsalary\n\n\n\n\nCEO\n272500\n\n\nDriver\n58456\n\n\nCD74\n100702\n\n\nCD65\n57380\n\n\nEmbellisher\n73877\n\n\nFolder\n65270\n\n\n\n\n\n\n\n\nCode book for data frame Pay\nDescription Salaries of Print-O-Matic printing company’s employees\nThis data frame contains the following columns:\nemployee:employees position in the company\nsalary: salary of that employee (Australian dollars (AUD))\nSource John Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nReferences John Matic (2013)\n\nFind the mean and median.\nFind the mean and median with the CEO’s salary removed.\nWhat happened to the mean and median when the CEO’s salary was removed? Why?\nIf you were the CEO, who is answering concerns from the union that employees are underpaid, which average (mean or median) using the complete data set of the complete data set would you prefer? Why?\nIf you were a platen worker, who believes that the employees need a raise, which average (mean or median) using the complete data set would you prefer? Why?\n\n\n\nPrint-O-Matic printing company spends specific amounts on fixed costs every month. The costs of those fixed costs are in a Table 4.5.\n\n\nCost&lt;-read.csv( \"https://krkozak.github.io/MAT160/cost.csv\") \nknitr::kable(head(Cost))\n\n\n\nTable 4.5: Fixed Costs for Print-O-Matic Printing Company\n\n\n\n\n\n\ncharges\ncost\n\n\n\n\nBank charges\n482\n\n\nCleaning\n2208\n\n\nComputer expensive\n2471\n\n\nLease payments\n2656\n\n\nPostage\n2117\n\n\nUniforms\n2600\n\n\n\n\n\n\n\n\nCode book for data frame Cost\nDescription fixed monthly charges for Print-0-Matic printing company\nThis data frame contains the following columns:\ncharges: Categories of monthly fixed charges\ncost: fixed month costs (AUD)\nSource John Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nReferences John Matic (2013)\n\nFind the mean and median.\nFind the mean and median with the bank charges removed.\nWhat happened to the mean and median when the bank charges was removed? Why?\nIf it is your job to oversee the fixed costs, which average (mean or median) using the complete data set would you prefer to use when submitting a report to administration to show that costs are low? Why?\nIf it is your job to find places in the budget to reduce costs, which average (mean or median) using the complete data set would you prefer to use when submitting a report to administration to show that fixed costs need to be reduced? Why?\n\n\n\nLooking at graph 3.1.2, state if the graph is skewed left, skewed right, or symmetric and then state which is larger, the mean or the median?\n\nGraph 3.1.2: Skewed or Symmetric Graph\n\n\n\nGraph #3.1.2\n\n\n\nLooking at graph 3.1.3, state if the graph is skewed left, skewed right, or symmetric and then state which is larger, the mean or the median?\n\nGraph 3.1.3: Skewed or Symmetric Graph\n\n\n\nGraph 3.1.3\n\n\n\nAn employee at Coconino Community College (CCC) is evaluated based on goal setting and accomplishments toward the goals, job effectiveness, competencies, and CCC core values. Suppose for a specific employee, goal 1 has a weight of 30%, goal 2 has a weight of 20%, job effectiveness has a weight of 25%, competency 1 has a weight of 4%, competency 2 has a weight of 3%, competency 3 has a weight of 3%, competency 4 has a weight of 3%, competency 5 has a weight of 2%, and core values has a weight of 10%. Suppose the employee has scores of 3.0 for goal 1, 3.0 for goal 2, 2.0 for job effectiveness, 3.0 for competency 1, 2.0 for competency 2, 2.0 for competency 3, 3.0 for competency 4, 4.0 for competency 5, and 3.0 for core values. Find the weighted average score for this employee. If an employee has a score less than 2.5, they must have a Performance Enhancement Plan written. Does this employee need a plan?\nAn employee at Coconino Community College (CCC) is evaluated based on goal setting and accomplishments toward goals, job effectiveness, competencies, CCC core values. Suppose for a specific employee, goal 1 has a weight of 20%, goal 2 has a weight of 20%, goal 3 has a weight of 10%, job effectiveness has a weight of 25%, competency 1 has a weight of 4%, competency 2 has a weight of 3%, competency 3 has a weight of 3%, competency 4 has a weight of 5%, and core values has a weight of 10%. Suppose the employee has scores of 2.0 for goal 1, 2.0 for goal 2, 3.0 for goal 3, 2.0 for job effectiveness, 2.0 for competency 1, 3.0 for competency 2, 2.0 for competency 3, 3.0 for competency 4, and 4.0 for core values. Find the weighted average score for this employee. If an employee that has a score less than 2.5, they must have a Performance Enhancement Plan written. Does this employee need a plan?\nA statistics class has the following activities and weights for determining a grade in the course: test 1 worth 15% of the grade, test 2 worth 15% of the grade, test 3 worth 15% of the grade, homework worth 10% of the grade, semester project worth 20% of the grade, and the final exam worth 25% of the grade. If a student receives an 85 on test 1, a 76 on test 2, an 83 on test 3, a 74 on the homework, a 65 on the project, and a 79 on the final, what grade did the student earn in the course?\nA statistics class has the following activities and weights for determining a grade in the course: test 1 worth 15% of the grade, test 2 worth 15% of the grade, test 3 worth 15% of the grade, homework worth 10% of the grade, semester project worth 20% of the grade, and the final exam worth 25% of the grade. If a student receives a 92 on test 1, an 85 on test 2, a 95 on test 3, a 92 on the homework, a 55 on the project, and an 83 on the final, what grade did the student earn in the course?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical description of Data.html#measures-of-spread",
    "href": "Numerical description of Data.html#measures-of-spread",
    "title": "4  Numerical Description of Data",
    "section": "4.2 Measures of Spread",
    "text": "4.2 Measures of Spread\nVariability is an important idea in statistics. If you were to measure the height of everyone in your classroom, every observation gives you a different value. That means not every student has the same height. Thus there is variability in people’s heights. If you were to take a sample of the income level of people in a town, every sample gives you different information. There is variability between samples too. Variability describes how the data are spread out. If the data are very close to each other, then there is low variability. If the data are very spread out, then there is high variability. How do you measure variability? It would be good to have a number that measures it. This section will describe some of the different measures of variability, also known as variation.\nIn [Example: Finding the Mean and Median using r], the average weight of a cat was calculated to be 2.72 kg. How much does this tell you about the weight of all cats? Can you tell if most of the weights were close to 2.72 kg or were the weights really spread out? The highest weight and the lowest weight are known, but is there more that you can tell? All you know is that the center of the weights is 2.72 kg.\nYou need more information.\nThe range of a set of data is the difference between the highest and the lowest data values (or maximum and minimum values). The interval is the lowest and highest values. The range is one value while the interval is two.\n\n4.2.1 Example: Range\nFrom Example: Affect of Extreme Values on Mean and Median, the maximum is 3.9 kg and the minimum is 2 kg. So the range is \\(3.9-2=1.9 kg\\). But what does that tell you? You don’t know if the weights are really spread out, or if they are close together.\nUnfortunately, range doesn’t really provide a very accurate picture of the variability. A better way to describe how the data is spread out is needed. Instead of looking at the distance the highest value is from the lowest how about looking at the distance each value is from the mean. This distance is called the deviation. You might want to find the average of the deviation. Though the calculation for finding the average deviation is not very straight forward, you end up with a value called the variance. The symbol for the population variance is \\(\\sigma^2\\), and it is the average squared distance from the mean. Statisticians like the variance, but many other people who work with statistics use a descriptive statistics which is the square root of the variance. This gives you the average distance from the mean. This is called the standard deviation, and is denoted with the letter \\(\\sigma\\).\nThe standard deviation is the average (mean) distance from a data point to the mean. It can be thought of as how much a typical data point differs from the mean.\nThe sample variance formula: \\(s^2=\\frac{\\sum\\left(x-\\bar{x}\\right)^2}{n-1}\\), where \\(\\bar{x}\\) is the sample mean, \\(n\\) is the sample size, and \\(\\sum{}\\) means to find the sum of the values.The \\(n-1\\) on the bottom has to do with a concept called degrees of freedom. Basically, it makes the sample variance a better approximation of the population variance.\nThe sample standard deviation formula: \\(s=\\sqrt{ \\frac{\\sum\\left(x-\\bar{x}\\right)^2}{n-1}}\\).\nThe population variance formula: \\(\\sigma^2 = \\frac{\\sum\\left(x-\\mu \\right)^2}{N}\\), where \\(\\sigma\\) is the Greek letter sigma and \\(\\sigma^2\\) represents the population variance, \\(\\mu\\) is the population mean, and \\(N\\) is the size of the population.\nThe population standard deviation formula: \\(\\sigma =\\sqrt{ \\frac{\\sum\\left(x-\\mu \\right)^2}{N}}\\)\nBoth the sample variance and sample standard deviation can be found using technology. If using rStudio, you would use\ndf_stats(~variable, data=data_frame, var, sd)\nThe next example will demonstrate this command.\n\n\n4.2.2 Example: Finding the Standard Deviation\nFor the data frame Cats Table 4.1 find the variance and standard derivation for weight of cats. Then find the variance and standard deviation separated by sex of the cat.\n\n4.2.2.1 Solution\nThe variance and standard deviation for all cats is found by performing the command:\n\ndf_stats(~Bwt, data=cats, var, sd) \n\n  response       var        sd\n1      Bwt 0.2355225 0.4853066\n\n\nThe variance for all cats is 0.24 \\(kg^2\\) and the standard deviation is 0.49 kg.\nTo find out the mean, variance, and standard deviation for each sex of the cats, use the command:\n\ndf_stats(Bwt~Sex, data=cats, mean, var, sd) \n\n  response Sex     mean        var        sd\n1      Bwt   F 2.359574 0.07506938 0.2739879\n2      Bwt   M 2.900000 0.21854167 0.4674844\n\n\nYou can see that the mean weight of females cats is 2.36 kg, the variance is 0.075 \\(kg^2\\), and the standard deviation is 0.27 kg. For males cats, the mean is 2.9 kg, the variance is 0.22 \\(kg^2\\), and the standard deviation is 0.47 kg. This means that female cats weigh less than males and since the variance and standard deviations are much less for female cats than males cats, female cats’ weights are more consistent than male cats.\nIn general a “small” variance and standard deviation means the data is close together (more consistent) and a “large” variance and standard deviation means the data is spread out (less consistent). Sometimes you want consistent data and sometimes you don’t. As an example if you are making bolts, you want the lengths to be very consistent so you want a small standard deviation. If you are administering a test to see who can be a pilot, you want a large standard deviation so you can tell who are the good pilots and who are the not so good pilots.\nWhat do “small” and “large” standard deviation mean? To a bicyclist whose average speed is 20 mph, \\(s = 20 mph\\) is huge. To an airplane whose average speed is 500 mph, \\(s = 20 mph\\) is nothing. The “size” of the variation depends on the size of the numbers in the problem and the mean. Another situation where you can determine whether a standard deviation is small or large is when you are comparing two different samples such as in Example: Finding the Standard Deviation. A sample with a smaller standard deviation is more consistent than a sample with a larger standard deviation.\nMany other books and authors stress that there is a computational formula for calculating the standard deviation. However, this formula doesn’t give you an idea of what standard deviation is and what you are doing. It is only good for doing the calculations quickly. It goes back to the days when standard deviations were calculated by hand, and the person needed a quick way to calculate the standard deviation. It is an archaic formula that this author is trying to eradicate. It is not necessary anymore, computers will do the calculations for you with as much meaning as this formula gives. It is suggested that you never use it. If you want to understand what the standard deviation is doing, then you should use the definition formula. If you want an answer quickly, use a computer.\n\n\n\n4.2.3 Use of Standard Deviation\nOne of the uses of the standard deviation is to describe how a population is distributed. This describes where much of the data is for most distributions. A general rule is that about 95% of the data is within 2 standard deviations of the mean. This is not perfect, but is works for many distributions. There are rules like the empirical rule and Chebyshev’s theorem that give you more detailed percentages, but 95% in 2 standard deviations is a very good approximation.\n\n\n4.2.4 Example: the general rule\nThe U.S. Weather Service has provided the information in Table 4.6 about the total monthly/annual number of reported tornadoes in Oklahoma for the years 1950 to 2018. (US Department of Commerce & Noaa, 2016)\n\nTornado&lt;-read.csv(\"https://krkozak.github.io/MAT160/Tornado_OK.csv\") \nknitr::kable(head(Tornado))\n\n\n\nTable 4.6: Monthly/Annual Number of tornadoes in Oklahoma\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\n\n\n1950\n0\n1\n1\n5\n12\n1\n0\n0\n2\n1\n0\n0\n23\n\n\n1951\n0\n2\n0\n11\n11\n11\n4\n2\n1\n1\n0\n0\n43\n\n\n1952\n0\n0\n0\n7\n5\n5\n4\n1\n0\n0\n0\n0\n22\n\n\n1953\n0\n4\n7\n9\n8\n13\n4\n2\n0\n0\n5\n2\n54\n\n\n1954\n0\n0\n7\n13\n19\n4\n4\n2\n3\n1\n0\n0\n53\n\n\n1955\n1\n1\n0\n15\n32\n22\n4\n2\n0\n0\n0\n0\n77\n\n\n\n\n\n\n\n\nCode book for data frame Tornado\nDescription The U.S. Weather Service has collected data on the monthly and annual number of tornadoes in Oklahoma.\nThis data frame contains the following columns:\nYear: Year from 1950-2018\nJan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec: Tornado numbers in each moth of the year\nAnnual: Total number of tornadoes for each year\nSource US Department of Commerce, & Noaa. (2016, November 15). 1950 Oklahoma Tornadoes. Retrieved from https://www.weather.gov/oun/tornadodata-ok-1950\nReferences The data was supplied by The U.S. Weather Service\nFind the general interval that contains about 95% of the data.\n\n4.2.4.1 Solution\nVariable: \\(x\\) = number of annual tornadoes in Oklahoma\nFind the mean and standard deviation:\n\ndf_stats(~Annual, data=Tornado, mean, sd)\n\n  response     mean       sd\n1   Annual 56.02899 27.56061\n\n\nThe mean is \\(\\mu=56\\) tornadoes and the standard deviation is \\(\\sigma=27.6\\) tornadoes. The interval will be \\(\\mu\\pm2*\\sigma=56\\pm2*27.6=(0.8,111.2)\\)\nAbout 95% of the years have between 0.8 or 1 and 111 tornadoes in Oklahoma.\nThe general rule says that about 95% of the data is within two standard deviations of the mean. That percentage is fairly high. There isn’t much data outside two standard deviations. A rule that can be followed is that if a data value is within two standard deviations, then that value is a common data value. If the data value is outside two standard deviations of the mean, either above or below, then the number is uncommon. It could even be called unusual. An easy calculation that you can do to figure it out is to find the difference between the data point and the mean, and then divide that answer by the standard deviation. As a formula this would be\n\\(z=\\frac{x-\\mu}{\\sigma}\\)\nIf you don’t know the population mean, \\(\\mu\\), and the population standard deviation, \\(\\sigma\\), then use the sample mean, \\(\\bar{x}\\), and the sample standard deviation, \\(s\\), to estimate the population parameter values. Realize that using the sample standard deviation may not actually be very accurate.\n\n\n\n4.2.5 Example: Determining If a Value Is Unusual\n\nIn 1974, there were 45 tornadoes in Oklahoma. Is this value unusual? Why or why not?\nIn 1999, there were 145 tornadoes in the Oklahoma. Is this value unusual? Why or why not?\n\n\n4.2.5.1 Solution\n\nIn 1974, there were 45 tornadoes in Oklahoma. Is this value unusual? Why or why not?\nVariable: \\(x\\) = number of tornadoes in Oklahoma\n\nTo answer this question, first find how many standard deviations 45 is from the mean. From 3.2.4 example, we know \\(\\mu=56\\) and \\(\\sigma=27.6\\). For \\(x\\)=45, \\(z=\\frac{45-56}{27.6}=-0.399\\)\nSince this value is between -2 and 2, then it is not unusual to have 45 tornadoes in a year in Oklahoma. The z value is negative, so that means that 45 is less than the mean number of tornadoes.\n\nIn 1999, there were 145 tornadoes in the Oklahoma. Is this value unusual? Why or why not?\nVariable: \\(x\\) = number of tornadoes in Oklahoma\n\nFor this question the \\(x\\) = 145, \\(z=\\frac{145-56}{27.6}=3.22\\)\nSince this value is more than 2, then it is unusual to have only 145 tornadoes in a year in Oklahoma.\n\n\n\n4.2.6 Homework for Measures of Spread Section\nUse Technology on all problems. State the variable on all problems.\n\nCholesterol levels were collected from patients certain days after they had a heart attack and are in Table 4.2. Find the mean, median, range, variance, and standard deviation for cholesterol levels 2 days after the heart attack.\n\nCode book for Data Frame Cholesterol is below Table 4.2.\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 4.3 (Lee, 1994). Find the mean, median, range, variance, and standard deviation of the length of rivers that flow into the Pacific Ocean and the mean, median, range, variance, and standard deviation of the length of rivers that flow into the Tasman Sea. Compare and contrast the length of rivers that flow to the Pacific Ocean versus the ones that flow into the Tasman Sea using both measures of spread and measures of variability.\n\nCode book for data frame Length is below Table 4.3.\n\nPrint-O-Matic printing company’s employees have salaries that are contained in Table 4.4. Find the mean, median, range, variance, and standard deviation for the salaries of all employees.\n\nCode book for data frame Pay below Table 4.4.\n\nPrint-O-Matic printing company spends specific amounts on fixed costs every month. The costs of those fixed costs are in Table 4.5. Find the mean, median, range, variance, and standard deviation for the fixed costs.\n\nCode book for Data frame Cost is below Table 4.5.\n\nThe data frame Pulse Table 4.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute.\n\n\nPulse&lt;-read.csv(\"https://krkozak.github.io/MAT160/pulse.csv\")\nknitr::kable(head(Pulse))\n\n\n\nTable 4.7: Head of Pulse Rates of people Before and After Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\n\n\n\n\n170\n68\n22\nmale\nyes\nyes\nmoderate\nsat\n70\n71\n93\n\n\n182\n75\n26\nmale\nyes\nyes\nmoderate\nsat\n80\n76\n93\n\n\n180\n85\n19\nmale\nyes\nyes\nmoderate\nran\n68\n125\n95\n\n\n182\n85\n20\nmale\nyes\nyes\nlow\nsat\n70\n68\n95\n\n\n167\n70\n22\nmale\nyes\nyes\nlow\nsat\n92\n84\n96\n\n\n178\n86\n21\nmale\nyes\nyes\nlow\nsat\n76\n80\n98\n\n\n\n\n\n\n\n\nCode book for data frame Pulse\nDescription Students in an introductory statistics class (MS212 taught by Professor John Eccleston and Dr Richard Wilson at The University of Queensland) participated in a simple experiment. The students took their own pulse rate. They were then asked to flip a coin. If the coin came up heads, they were to run in place for one minute. Otherwise they sat for one minute. Then everyone took their pulse again. The pulse rates and other physiological and lifestyle data are given in the data.\nFive class groups between 1993 and 1998 participated in the experiment. The lecturer, Richard Wilson, was concerned that some students would choose the less strenuous option of sitting rather than running even if their coin came up heads, In the years 1995-1998 a different method of random assignment was used. In these years, data forms were handed out to the class before the experiment. The forms were pre-assigned to either running or non-running and there were an equal number of each. In 1995 and 1998 not all of the forms were returned so the numbers running and sitting was still not entirely controlled.\nThis data frame contains the following columns:\nheight: height of subject in cm\nweight: weight of subject in kg\nage: age of subject in years\ngender: sex of subject, male, female\nSmokes: whether a subject regularly smokes, yes means does smoke, no means does not smoke\nalcohol: whether a subject regularly drinks alcohol, yes means the person does, no means the person does not\nexercise: whether a subject exercises, low, moderate, high\nran: whether a subject ran one minute between pulse measurements (ran) or sat between pulse measurement (sat)\npulse_before: the pulse rate before a subject either ran or sat (bpm)\npulse_after: the pulse rate after a subject either ran or sat (bpm)\nyear: what year the data was collected (93-98)\nSource Pulse rates before and after exercise. (2013, September 25). Retrieved from http://www.statsci.org/data/oz/ms212.html\nReferences The data was supplied by Dr Richard J. Wilson, Department of Mathematics, University of Queensland.\nCreate a data frame that contains only males, who drink alcohol, but do not smoke. Then compare the pulse before and the pulse after using the mean and standard deviation. Discuss whether pulse before or pulse after has a higher mean and larger spread. The following command creates a new data frame with just males, who drink alcohol, but do not smoke, use the following command, where the new name is Males in Table 4.8.\n\nMales&lt;- Pulse |&gt; \n  filter(gender==\"male\", smokes == \"no\", alcohol == \"yes\")\nknitr::kable(head(Males))\n\n\n\nTable 4.8: Head of Pulse Rates of Nonsmoking Males Before and After Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\n\n\n\n\n195\n84\n18\nmale\nno\nyes\nhigh\nsat\n71\n73\n93\n\n\n184\n74\n22\nmale\nno\nyes\nlow\nran\n78\n141\n93\n\n\n168\n60\n23\nmale\nno\nyes\nmoderate\nran\n88\n150\n93\n\n\n170\n75\n20\nmale\nno\nyes\nhigh\nran\n76\n88\n93\n\n\n187\n59\n18\nmale\nno\nyes\nhigh\nsat\n78\n82\n93\n\n\n180\n72\n18\nmale\nno\nyes\nmoderate\nsat\n69\n67\n93\n\n\n\n\n\n\n\n\n\nThe data frame Pulse Table 4.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute. Create a data frame that contains females, who do not smoke but do drink alcohol. Compare the pulse rate before and after exercise using the mean and standard deviation. Discuss whether pulse before or pulse after has a higher mean and larger spread.\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997) and the data is in Table 4.9.\n\n\nReiki&lt;- read.csv( \"https://krkozak.github.io/MAT160/reki.csv\") \nknitr::kable(head(Reiki))\n\n\n\nTable 4.9: Head of Pain Measurements Before and After Reiki Treatment\n\n\n\n\n\n\nvas.before\nvas.after\nlikert_before\nlikert_after\n\n\n\n\n6\n3\n2\n1\n\n\n2\n1\n2\n1\n\n\n2\n0\n3\n0\n\n\n9\n1\n3\n1\n\n\n3\n0\n2\n0\n\n\n3\n2\n2\n2\n\n\n\n\n\n\n\n\nCode book for data frame Reiki\nDescription The purpose of this study was to explore the usefulness of Reiki as an adjuvant to opioid therapy in the management of pain. Since no studies in this area could be found, a pilot study was carried out involving 20 volunteers experiencing pain at 55 sites for a variety of reasons, including cancer. All Reiki treatments were provided by a certified second-degree Reiki therapist. Pain was measured using both a visual analogue scale (VAS) and a Likert scale immediately before and after the Reiki treatment. Both instruments showed a highly significant (p &lt; 0.0001) reduction in pain following the Reiki treatment.\nThis data frame contains the following columns:\nvas.before: pain measured using a visual analogue scale (VAS) before Reiki treatment\nvas.after: pain measured using a visual analogue scale (VAS) after Reiki treatment\nlikert_before: pain measured using a likert before Reiki treatment\nlikert_after: pain measured using a likert after Reiki treatment\nSource Olson, K., & Hanson, J. (1997). Using reiki to manage pain: a preliminary report. Cancer Prev Control, 1(2), 108-13. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/9765732\nReferences** Using Reiki to manage pain: a preliminary report. Olson K1, Hanson J., Cancer Prev Control 1997, Jun; 1(2): 108-13.\nSince the data was collected both before and after the treatment for all of the units of observations, you want to look at the effect size of the treatment. You want to find the difference between before and after for the pain scale. First you must create a new data frame that adds a column for the difference in before and after. This data is known as paired data. To create the new column in a new data frame called Newreiki use the following commands, Table 4.10.\n\nNewreiki&lt;-Reiki |&gt;\n  mutate(vas.diff=vas.before-vas.after) \nknitr::kable(head(Newreiki))\n\n\n\nTable 4.10: Head of Pain Measurements Before and After Reiki Treatment with Difference column\n\n\n\n\n\n\nvas.before\nvas.after\nlikert_before\nlikert_after\nvas.diff\n\n\n\n\n6\n3\n2\n1\n3\n\n\n2\n1\n2\n1\n1\n\n\n2\n0\n3\n0\n2\n\n\n9\n1\n3\n1\n8\n\n\n3\n0\n2\n0\n3\n\n\n3\n2\n2\n2\n1\n\n\n\n\n\n\n\n\nNow find the mean and standard deviation of the vas.diff variable in Newreiki. Perform similar commands to create the likert.diff variable. Then find the mean and standard deviation for likert.diff, and compare and contrast the vas and likert methods for describing pain.\n8.Yearly rainfall amounts (in millimeters) in Sydney, Australia, are in Table 4.11 (Annual maximums of, 2013). a. Calculate the mean and standard deviation. b. Suppose Sydney, Australia received 300 mm of rainfall in a year. Would this be unusual?\n\nRainfall&lt;-read.csv(\"https://krkozak.github.io/MAT160/rainfall.csv\") \nknitr::kable(head(Rainfall))\n\n\n\nTable 4.11: Head of Yearly rainfall amounts in Sydney, Australia\n\n\n\n\n\n\namount\n\n\n\n\n146.8\n\n\n383.0\n\n\n90.9\n\n\n178.1\n\n\n267.5\n\n\n95.5\n\n\n\n\n\n\n\n\nCode book for data frame Rainfall\nDescription Daily rainfall (in millimeters) was recorded over a 47-year period in Turramurra, Sydney, Australia. For each year, the wettest day was identified (that having the greatest rainfall). The data show the rainfall recorded for the 47 annual maxima.\nThis data frame contains the following columns:\namount: daily rainfall (mm)\nSource Annual maximums of daily rainfall in Sydney. (2013, September 25). Retrieved from http://www.statsci.org/data/oz/sydrain.html\nReferences Rayner J.C.W. and Best D.J. (1989) Smooth tests of goodness of fit. Oxford: Oxford University Press. Hand D.J., Daly F., Lunn A.D., McConway K.J., Ostrowski E. (1994). A Handbook of Small Data Sets. London: Chapman & Hall. Data set 157. Thanks to Jim Irish of the University of Technology, Sydney, for assistance in identifying the correct units for this data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical description of Data.html#ranking",
    "href": "Numerical description of Data.html#ranking",
    "title": "4  Numerical Description of Data",
    "section": "4.3 Ranking",
    "text": "4.3 Ranking\nAlong with the center and the variability, another useful numerical measure is the ranking of a number. A percentile is a measure of ranking. It represents a location measurement of a data value to the rest of the values. Many standardized tests give the results as a percentile. Doctors also use percentiles to track a child’s growth.\nThe \\(k^{th}\\) percentile is the data value that has k% of the data at or below that value.\n\n4.3.1 Example: Interpreting Percentile\n\nWhat does a score of the \\(90^{th}\\) percentile mean?\nWhat does a score of the \\(70^{th}\\) percentile mean?\n\n\n4.3.1.1 Solution\n\nWhat does a score of the \\(90^{th}\\) percentile mean?\nThis means that 90% of the scores were at or below this score. (A person did the same as or better than 90% of the test takers.)\nWhat does a score of the \\(70^{th}\\) percentile mean?\nThis means that 70% of the scores were at or below this score.\n\n\n\n\n4.3.2 Example: Percentile Versus Score\nIf the test was out of 100 points and you scored at the \\(80^{th}\\) percentile, what was your score on the test?\n\n4.3.2.1 Solution\nYou don’t know! All you know is that you scored the same as or better than 80% of the people who took the test. If all the scores were really low, you could have still failed the test. On the other hand, if many of the scores were high you could have gotten a 95% or more.\nThere are special percentiles called quartiles. Quartiles are numbers that divide the data into fourths. One fourth (or a quarter) of the data falls between consecutive quartiles.\n\n\n\n4.3.3 To find the quartiles:\nThe command in rStudio is\ndf_stats(~variable, data=data_frame, summary)\nIf you record the quartiles together with the maximum and minimum you have five numbers. This is known as the five-number summary. The five-number summary consists of the minimum, the first quartile (\\(Q1\\)), the median, the third quartile (\\(Q3\\)), and the maximum (in that order).\nThe interquartile range, \\(IQR\\), is the difference between the first and third quartiles, \\(Q1\\) and $Q3$. Half of the data (50%) falls in the interquartile range. If the \\(IQR\\) is “large” the data is spread out and if the \\(IQR\\) is “small” the data is closer together.\nInterquartile Range (\\(IQR\\))\nDetermining probable outliers from \\(IQR\\): fences\nA value that is less than \\(Q1-1.5*IQR\\) (this value is often referred to as a low fence) is considered an outlier.\nSimilarly, a value that is more than \\(Q3+1.5*IQR\\) (the high fence) is considered an outlier.\nA boxplot (or box-and-whisker plot) is a graphical display of the five-number summary. It can be drawn vertically or horizontally. The basic format is a box from \\(Q1\\) to \\(Q3\\), a vertical line across the box for the median and horizontal lines as whiskers extending out each end to the minimum and maximum. The minimum and maximum can be represented with dots. Don’t forget to label the tick marks on the number line and give the graph a title.\nAn alternate form of a Boxplot, known as a modified box plot, only extends the left line to the smallest value greater than the low fence, and extends the left line to the largest value less than the high fence, and displays markers (dots, circles or asterisks) for each outlier.\nIf the data are symmetrical, then the box plot will be visibly symmetrical. If the data distribution has a left skew or a right skew, the line on that side of the box plot will be visibly long. If the plot is symmetrical, and the four quartiles are all about the same length, then the data are likely a near uniform distribution. If a box plot is symmetrical, and both outside lines are noticeably longer than the \\(Q1\\) to median and median to \\(Q3\\) distance, the distribution is then probably bell-shaped.\n\n\n4.3.4 Example: Five-number Summary and Boxplot\nFind the five-number summary, the interquartile range (*IQR*), and draw a box-and-whiskers plot for the weight of cats Table 4.1.\n\n4.3.4.1 Solution\nVariable: \\(x\\) = weight of cats To compute the five-number summary on RStudio, use the command:\n\ndf_stats(~Bwt, data=cats, summary)\n\n  response Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1      Bwt    2     2.3    2.7 2.723611   3.025  3.9\n\n\nNote rStudio also calculates the mean as part of the summary command, but the five-number summary is just the five numbers:\nMinimum: 2 kg \\(Q1\\): 2.3 kg Median: 2.7 kg \\(Q3\\): 3.025 kg Maximum: 3.9 kg\nTo find the interquartile range, \\(IQR\\) find $Q3-Q1$, so \\(IQR=3.025-2.3=0.725 kg\\)\nTo create a boxplot use the command\ngf_boxplot(~variable, data=data_frame)\nThis is a modified boxplot which shows the outliers in the data.\n\ngf_boxplot(~Bwt, data=cats, title=\"Weight of Cats\", xlab=\"Body Weight (kg)\")\n\n\n\n\n\n\n\nFigure 4.4: Weight of Cats\n\n\n\n\n\nThere are no outliers since there are no dots outside of the fences.\n\n\n\n4.3.5 Example: Separating based on a factor\nFind the five-number summary of the weights of cats separated by the sex of the cat. Then create a box plot of the weights of cats for each sex of the cat.\n\n4.3.5.1 Solution\nVariable: \\(x_1\\) = weight of female cat\nVariable: \\(x_2\\) = weight of male cat\nTo find the five-number summary separated based on gender use the following command:\n\ndf_stats(~Bwt|Sex, data=cats, summary)\n\n  response Sex Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1      Bwt   F    2    2.15    2.3 2.359574     2.5  3.0\n2      Bwt   M    2    2.50    2.9 2.900000     3.2  3.9\n\n\nThe five-number summary for female cats is (in kg)\nMinimum: 2 \\(Q1\\): 2.15 Median: 2.3 \\(Q3\\): 2.5 Maximum: 3.0\nThe five-number summary for male cats is (in kg)\nMinimum: 2 \\(Q1\\): 2.50 Median: 2.9 \\(Q3\\): 3.2 Maximum: 3.9\n\ngf_boxplot(~Bwt|Sex, data=cats, title=\"Weights of Cats\", xlab=\"Body Weight in (kg)\") \n\n\n\n\n\n\n\nFigure 4.5: Weight of Cats Faceted by Sex\n\n\n\n\n\nNotice that the weights of female cats has a median less than male cats, and in fact it can be seen that the \\(Q1\\) to \\(Q3\\) of the female cats is less than the \\(Q1\\) to \\(Q3\\) of the male cats.\n\n\n\n4.3.6 Example: Putting it all together\nThe time (in 1/50 seconds) between successive pulses along a nerve fiber (“Time between nerve,” 2013) are given in Table 4.12.\n\nNerve&lt;-read.csv( \"https://krkozak.github.io/MAT160/Nerve_pulse.csv\") \nknitr::kable(head(Nerve))\n\n\n\nTable 4.12: Head of Successive pulses along a nerve fiber\n\n\n\n\n\n\ntime\n\n\n\n\n10.5\n\n\n1.5\n\n\n2.5\n\n\n5.5\n\n\n29.5\n\n\n3.0\n\n\n\n\n\n\n\n\nCode book for data frame Nerve\nDescription The data gives the time between 800 successive pulses along a nerve fiber. There are 799 observations rounded to the nearest half in units of 1/50 second.\nThis data frame contains the following columns:\ntime: time between successive Pulses along a nerve fiber, 1/50 second.\nSource Time between nerve pulses. (2019, July 3). Retrieved from &lt;http://www.statsci.org/data/general/nerve.html\nReferences Fatt, P., and Katz, B. (1952). Spontaneous subthreshold activity at motor nerve endings. Journal of Physiology 117, 109-128.\nCox, D. R., and Lewis, P. A. W. (1966). The Statistical Analysis of Series of Events. Methuen, London.\nJorgensen, B. (1982). The Generalized Inverse-Gaussian Distribution. Springer-Verlag.\n\n4.3.6.1 Solution\nFirst, it might be useful to look at a visualization of the data, so create a density plot\n\ngf_density(~time, data=Nerve, title=\"Time between Successive Nerve Pulses\", xlab=\"Time (1/50 second)\") \n\n\n\n\n\n\n\nFigure 4.6: Weight of Cats Faceted by Sex\n\n\n\n\n\nFrom the graph Figure 4.6 the data appears to be skewed right. Most of the time between successive nerve pulses appear to be around 5 or 10 1/50 second, but there are some times that are 60 1/50 second.\n\ndf_stats(~time, data=Nerve, mean, median, sd, summary)\n\n  response     mean median       sd Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1     time 10.95119    7.5 10.45956  0.5     3.5    7.5 10.95119      15   69\n\n\nNumerical descriptions might also be useful. Using technology, the mean is 11 1/50 second,the median is 7.5 1/50 second, the standard deviation is 10.5 1/50 second, and the five-number summary is minimum = 3.5, Q1 = 3.5, median = 7.5, Q3 = 15, and maximum = 69 1/50 second.\nTo visualize the five-number summary, create a box plot.\n\ngf_boxplot(~time, data=Nerve, title=\"Nerve Pulses\", xlab=\"Time (1/50 second)\")\n\n\n\n\n\n\n\nFigure 4.7: Boxplot of Nerve Pulses\n\n\n\n\n\nSince there are many dots outside the upper fence the data has many outliers. From all of this information, one could say that nerve pulses between successive pulses is around 11 1/50 second, with a spread of 19.5 1/50 second. Most of the values are round 11 1/50 second, but they are not very consistent. The density plot and boxplot show that there is a great deal of spread of the data and it is skewed to the right. This means mostly the speed is around 11 1/50 second, but there is a great deal of variability in the values.\n\n\n\n4.3.7 Homework for Ranking Section\nUse Technology on all problems. State the variable on all problems.\n\nSuppose you take a standardized test and you are in the \\(10^{th}\\) percentile. What does this percentile mean? Can you say that you failed the test? Explain.\nSuppose your child takes a standardized test in mathematics and scores in the \\(96^{th}\\) percentile. What does this percentile mean? Can you say your child passed the test? Explain.\nSuppose your child is in the \\(83^{rd}\\) percentile in height and \\(24^{th}\\) percentile in weight. Describe what this tells you about your child’s stature.\nSuppose your work evaluates the employees and places them on a percentile ranking. If your evaluation is in the \\(65^{th}\\) percentile, do you think you are working hard enough? Explain.\nCholesterol levels were collected from patients certain days after they had a heart attack and are in table Table 4.2.\n\nCode book for Data Frame Cholesterol below Table 4.2.\nFind the five-number summary and interquartile range (IQR) for the cholesterol level on day 2, and draw a boxplot\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in table Table 4.3 (Lee, 1994).\n\nCode book for data frame Length below Table 4.3.\nFind the five-number summary and interquartile range (IQR) for the lengths of rivers that go to the Pacific Ocean and ones that go to the Tasman Sea, and draw a boxplot of both.\n\nPrint-O-Matic printing company’s employees have salaries that are contained in Table 4.4 Find the five number summary and draw a boxplot for the salaries of all employees.\n\nCode book for data frame Pay below Table 4.4.\n\nThe data frame Pulse Table 4.7 contains various variables about a person including their pulse rates before the subject exercised and after after the subject ran in place for one minute.\n\nCode book for data frame Pulse below Table 4.7.\nCreate a data frame that contains only people who drink alcohol, but do not smoke. Then find the five number summary and draw a boxplot for both males and females separately.\n\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997) and the data is in Table 4.9.\n\nCode book for data frame Reiki below Table 4.9.\nFind the five number summary for both the before and after VAS scores and draw boxplots of before and after VAS scores. To draw two boxplots at the same time, after the command to create the first box plot type the piping symbol |&gt; (base r) or %&gt;% (magrittr package) before pressing enter. (Note: |&gt; and %&gt;% are piping symbols that can be thought of as “and then.”) Then type the command for the second boxplot after the + symbol or on the next line in the r chunk if using an rmd or qmd file. Then press enter. You may want to graph each boxplot as a different color. To do this, the command would be\ngf_boxplot(~variable, data=data_frame, color=“red”, xlab=“type a label”)\nYou can pick any color you want. Just replace the word red with the color you want to use. Now compare and contrast the before and after VAS scores.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Discrete Probability Distribution.html",
    "href": "Discrete Probability Distribution.html",
    "title": "6  Discrete Probability Distribution",
    "section": "",
    "text": "6.0.1 Examples of each:\nWhen computing probabilities, the sample space, which contains all the outcomes of the experiment, is listed. If the probabilities for all of the outcomes are also listed then these two together are called a probability distribution. With a probability distribution, the shape can be determined, the mean and standard deviation can be calculated, and the probability of events can be found. How to find all of these concepts depends on what type of quantitative variables are being considered. Remember there are different types of quantitative variables, called discrete or continuous. What is the difference between discrete and continuous data? Discrete data can only take on particular values in a range. Continuous data can take on any value in a range. Discrete data usually arises from counting while continuous data usually arises from measuring.\nIf you have a variable, and can find a probability associated with that variable, it is called a random variable. In many cases the random variable is what you are measuring, but when it comes to discrete random variables, it is usually what you are counting. So for the example of how tall is a plant given a new fertilizer, the random variable is the height of the plant given a new fertilizer. For the example of how many fleas are on prairie dogs in a colony, the random variable is the number of fleas on a prairie dog in a colony.\nHow tall is a plant given a new fertilizer? Continuous. This is something you measure.\nHow many fleas are on prairie dogs in a colony? Discrete. This is something you count.\nNow suppose you put all the values of the random variable together with the probability that the random variable would occur. You could then have a distribution like before, but now it is called a probability distribution since it involves probabilities. A probability distribution is an assignment of probabilities to the values of the random variable.\nWith the idea of a probability distribution, the next thing is to look at the basics of a probability distribution.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Discrete Probability Distribution.html#basics-of-probability-distributions",
    "href": "Discrete Probability Distribution.html#basics-of-probability-distributions",
    "title": "6  Discrete Probability Distribution",
    "section": "6.1 Basics of Probability Distributions",
    "text": "6.1 Basics of Probability Distributions\nAs a reminder, a variable or what will be called the random variable from now on, is represented by the letter \\(x\\) and it represents a quantitative (numerical) variable that is measured or observed in an experiment.\nAs with probabilities, probability distributions, have the properties, \\(0 \\le P(outcome)\\le1\\) and \\(\\sum{P(outcomes)}=1\\)\n\n6.1.1 Example: Probability Distribution\nThe 2010 U.S. Census found the chance of a household being a certain size. The data is in Table 6.1 (\\“Households by age,\\” 2013). Note, the category 7 is really 7 or more people in the household. Draw the probability distribution and find the mean, variance, and standard deviation.\n\n\n\n\nTable 6.1: Household Size from U.S. Census of 2010\n\n\n\n\n\n\nsize\nprob\n\n\n\n\n1\n0.267\n\n\n2\n0.336\n\n\n3\n0.158\n\n\n4\n0.137\n\n\n5\n0.063\n\n\n6\n0.024\n\n\n7\n0.015\n\n\n\n\n\n\n\n\n\n6.1.1.1 Solution\nIn this case, the random variable is \\(x\\) = size of household. This is a discrete random variable, since you are counting the number of people in a household.\nIt is a probability distribution since you have the \\(x\\) value and the probabilities that go with it, all of the probabilities are between zero and one, and the sum of all of the probabilities is one.\nYou can give a probability distribution in table form (as in Table 6.1) or as a graph. The graph looks like a histogram. To graph the histogram, use the following commands and process in rStudio.\nFirst you need to load a few packages using the following commands. These packages are “arm” and “Weighted.Desc.Stat”. If these packages have not been installed, they need to be installed before you can load them using library. Once you have installed them, they will always be available in /r Studio to be loaded. To load a package, use the command\nlibrary(“name of package”)\nIn this case the packages you need are arm and Weighted.Desc.Stat.\n\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\nWarning: package 'lme4' was built under R version 4.5.2\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is /Users/jessicakramer/stats_using_rguroo_positron/Statistics-Using-Technology-book\n\n\nTo draw the probability distribution, use the following command. First you need to create variables for \\(x\\), size, and the probability, \\(prob\\), in r Studio. Then you can draw the distribution.\n(ref:discrete-histogram-cap) Histogram of Size of Family\n\ndiscrete.histogram(Household$size,Household$prob, bar.width = 1, main=\"Size of family\", xlab=\"Size\") \n\n\n\n\n\n\n\nFigure 6.1: Histogram of Household Size from U.S. Census of 2010\n\n\n\n\n\nThis command is different than the commands used in the past, but is needed for discrete probability distributions. So putting a title on the graph uses the command main=“title you want” instead of title= as before.\nNotice this graph Figure 6.1 is skewed right, which means that most families have around 2 people in them and larger families become more and more rare.\nTo find the mean, variance, and standard deviation using r Studio, make sure that the package Weighted.Desc.Stat is loaded, then use the following commands.\n\nw.mean(Household$size, Household$prob) \n\n[1] 2.525\n\nw.var(Household$size, Household$prob) \n\n[1] 2.023375\n\nw.sd(Household$size, Household$prob)\n\n[1] 1.422454\n\n\nThe mean is 2.525 people, the variance is 2.02 \\(people^2\\), and the standard deviation is 1.42 people.\nWhen calculating the mean and standard deviation of a probability distribution, you can consider the population distribution the population even though it was most likely created from a large sample. Since a probability distribution is basically a population, the mean and standard deviation that are calculated are actually the population parameters and not the sample statistics. The notation used is the same as the notation for population mean, \\(\\mu\\), and population standard deviation, $\\sigma$, that was used in chapter 3. Note: the mean can also be thought of as the expected value. It is the value you expect to get if the trials were repeated infinite number of times. The mean or expected value does not need to be a whole number, even if the possible values of \\(x\\) are whole numbers. This means one can find what value they can expect to get in the long run for gambling or insurance including extended warranties using the mean of a probability distribution. First one needs to figure out the probability distribution, and then follow the process in example 5.1.1.\n\n\n\n6.1.2 Example: Calculating the Expected Value\nIn the Arizona lottery game called Pick 3, a player pays \\$1 and then picks a three-digit number. If those three numbers are picked in that specific order the person wins \\$500. What is the expected value in this game?\n\n6.1.2.1 Solution\nTo find the expected value, you need to first create the probability distribution. In this case, the random variable \\(x\\) = winnings. If you pick the right numbers in the right order, then you win \\$500, but you paid \\$1 to play, so you actually win \\$499. If you didn’t pick the right numbers, you lose the \\$1, the \\(x\\) value is $-1$. You also need the probability of winning and losing. Since you are picking a three-digit number, and for each digit there are 10 numbers you can pick with each independent of the others, you can use the multiplication rule. To win, you have to pick the right numbers in the right order. The first digit, you pick 1 number out of 10, the second digit you pick 1 number out of 10, and the third digit you pick 1 number out of 10. The probability of picking the right number in the right order is $\\frac{1}{1000}$. The probability of losing (not winning) would be\n\\(1-\\frac{1}{1000}=\\frac{999}{1000}\\).\nPutting this information into a table will help to organize the information and find the expected value.\n\nProbability Distribution of Lottery\n\n\noutcome\namount\nprobability\n\n\n\n\nwin\n499\n0.001\n\n\nlose\n-1\n0.999\n\n\n\nNow type the values into r using the following command:\nNow to find the expected value, it is the same as finding the mean, though the command is a little different since you don’t have a data frame for this data.\n\nweighted.mean(amount, probability)\n\n[1] -0.5\n\n\nThe expected value (or mean) is -0.5. That is -\\$0.50. Since it is negative, that means you lose \\$0.50 every time you play the Pick 3. It seems you would be better off putting the \\$1 every week into a savings account then playing the Pick 3 lottery.\nThe reason probability is studied in statistics is to help in making decisions in inferential statistics. To understand how that is done the concept of a rare event is needed.\n\n\n\n6.1.3 Rare Event Rule for Inferential Statistics\nIf, under a given assumption, the probability of a particular observed event is extremely small, then you can conclude that the assumption is probably not correct.\nAn example of this is suppose you roll an assumed fair die 1000 times and get a six 600 times, when you should have only rolled a six around 160 times, then you should believe that your assumption about it being a fair die is untrue.\n\n\n6.1.4 Determining if an event is unusual\nIf you are looking at a value of \\(x\\) for a discrete variable, and the P(the variable has a value of \\(x\\) or more) is less than 0.05, then you can consider the \\(x\\) an unusually high value. Another way to think of this is if the probability of getting such a high value is less than 0.05, then the event of getting the value x is unusual.\nSimilarly, if the P(the variable has a value of \\(x\\) or less) is less than 0.05, then you can consider this an unusually low value. Another way to think of this is if the probability of getting a value as small as \\(x\\) is less than 0.05, then the event \\(x\\) is considered unusual.\nWhy is it “\\(x\\) or more” or “\\(x\\) or less” instead of just “\\(x\\)” when you are determining if an event is unusual? Consider this example: you and your friend go out to lunch every day. Instead of Going Dutch (each paying for their own lunch), you decide to flip a coin, and the loser pays for both. Your friend seems to be winning more often than you’d expect, so you want to determine if this is unusual before you decide to change how you pay for lunch (or accuse your friend of cheating). The process for how to calculate these probabilities will be presented in the next section on the binomial distribution. If your friend won 6 out of 10 lunches, the probability of that happening turns out to be about 20.5%, not unusual. The probability of winning 6 or more is about 37.7%. But what happens if your friend won 501 out of 1,000 lunches? That doesn’t seem so unlikely! The probability of winning 501 or more lunches is about 47.8%, and that is consistent with your hunch that this isn’t so unusual. But the probability of winning exactly 501 lunches is much less, only about 2.5%. That is why the probability of getting exactly that value is not the right question to ask: you should ask the probability of getting that value or more (or that value or less on the other side).\nThe value 0.05 will be explained later, and it is not the only value you can use for unusual events.\n\n\n6.1.5 Example: Is the Event Unusual\nThe 2010 U.S. Census found the chance of a household being a certain size. The data is in the table (\\“Households by age,\\” 2013).\nThe 2010 U.S. Census found the chance of a household being a certain size. The data is in Table 6.1 (\\“Households by age,\\” 2013). Note, the category 7 is really 7 or more people in the household.\nState random variable:\nSolution\nState random variable\n\nIs it unusual for a household to have six people in the family?\n\nsize = number of people in a household\n\nIs it unusual for a household to have six people in the family?\nIf you did come upon many families that had six people in the family, what would you think?\nIs it unusual for a household to have four people in the family?\nIf you did come upon a family that has four people in it, what would you think?\n\n\n6.1.5.1 Solution\nTo determine this, you need to look at probabilities. However, you cannot just look at the probability of six people. You need to look at the probability of \\(x\\) being six or less people or the probability of \\(x\\) being six or more people. The\n\\(P(x \\le 6)=P(1)+P(2)+P(3)+P(4)+P(5)+P(6)\\) \\(=0.267+0.336+0.158+0.137+0.063+0.024=0.985\\)\nSince this probability is more than 5%, then six is not an unusually low value.\nThe \\(P(x \\ge 6)=P(6)+P(7)=0.024+0.015=0.039\\)\nSince this probability is less than 5%, then six is an unusually high value. It is unusual for a household to have six people in the family.\n\nIf you did come upon many families that had six people in the family, what would you think?\n\nSince it is unusual for a family to have six people in it, then you may think that either the size of families is increasing from what it was or that you are in a location where families are larger than in other locations.\n\nIs it unusual for a household to have four people in the family?\n\nTo determine this, you need to look at probabilities. Again, look at the probability of \\(x\\) being four or less or the probability of \\(x\\) being four or more. The\n\\(P(x \\le 4)=P(0)+P(1)+P(2)+P(3)+P(4)\\) \\(=0.267+0.336+0.158+0.137=0.898\\)\nSince this probability is more than 5%, four is not an unusually low value.\nThe\n\\(P(\\ge4)=P(4)+P(5)+P(5)+P(7)\\) \\(=0.137+0.063+0.024+0.015=0.239\\)\nSince this probability is more than 5%, four is not an unusually low value. Thus, four is not an unusual size of a family.\n\nIf you did come upon a family that has four people in it, what would you think?\n\nSince it is not unusual for a family to have four members, then you would not think anything is amiss.\n\n\n\n6.1.6 Homework for Basics of Probability Distributions Section\n\nEyeglassomatic manufactures eyeglasses for different retailers. The number of days it takes to fix defects in an eyeglass and the probability that it will take that number of days are in Table 6.2.\n\n\n\nDays&lt;- read.csv(\n  \"https://krkozak.github.io/MAT160/table_5_1_3.csv\") \nknitr::kable(Days)\n\n\n\nTable 6.2: Nuumber of Days to fix Eyeglasses\n\n\n\n\n\n\ndays\nprob\n\n\n\n\n1\n0.249\n\n\n2\n0.108\n\n\n3\n0.091\n\n\n4\n0.123\n\n\n5\n0.133\n\n\n6\n0.114\n\n\n7\n0.070\n\n\n8\n0.046\n\n\n9\n0.019\n\n\n10\n0.013\n\n\n11\n0.010\n\n\n12\n0.008\n\n\n13\n0.006\n\n\n14\n0.004\n\n\n15\n0.002\n\n\n16\n0.002\n\n\n17\n0.001\n\n\n18\n0.001\n\n\n\n\n\n\n\n\n\nState the random variable.\nDraw a histogram of the number of days to fix defects\nFind the mean number of days to fix defects.\nFind the variance for the number of days to fix defects.\nFind the standard deviation for the number of days to fix defects.\nFind probability that a lens will take at least 16 days to make a fix the defect.\nIs it unusual for a lens to take 16 days to fix a defect?\nIf it does take 16 days for eyeglasses to be repaired, what would you think?\n\n\n\nSuppose you have an experiment where you flip a coin three times. You then count the number of heads.\n\n\n\nState the random variable.\nWrite the probability distribution for the number of heads.\nDraw a histogram for the number of heads.\nFind the mean number of heads.\nFind the variance for the number of heads.\nFind the standard deviation for the number of heads.\nFind the probability of having two or more number of heads.\nIs it unusual for to flip two heads?\n\n\n\nThe Ohio lottery has a game called Pick 4 where a player pays \\$1 and picks a four-digit number. If the four numbers come up in the order you picked, then you win \\$2,500. What is your expected value?\nAn LG Dishwasher, which costs \\$800, has a 20% chance of needing to be replaced in the first 2 years of purchase. A two-year extended warranty costs \\$112.10 on a dishwasher. What is the expected value of the extended warranty assuming it is replaced in the first 2 years?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Discrete Probability Distribution.html#binomial-probability-distribution",
    "href": "Discrete Probability Distribution.html#binomial-probability-distribution",
    "title": "6  Discrete Probability Distribution",
    "section": "6.2 Binomial Probability Distribution",
    "text": "6.2 Binomial Probability Distribution\nSection 5.1 introduced the concept of a probability distribution. The focus of the section was on discrete probability distributions. To find the probability distribution for a situation, you usually needed to actually conduct the experiment and collect data. Then you can calculate the experimental probabilities. Normally you cannot calculate the theoretical probabilities. However, there are certain types of experiment that allow you to calculate the theoretical probability. One of those types is called a Binomial Experiment.\nProperties of a binomial experiment (or Bernoulli trial):\n\nFixed number of trials, \\(n\\), which means that the experiment is repeated a specific number of times.\nThe \\(n\\) trials are independent, which means that what happens on one trial does not influence the outcomes of other trials.\nThere are only two outcomes, which are called a success and a failure.\nThe probability of a success doesn’t change from trial to trial, where \\(p\\) = probability of success and \\(q = 1-p\\) = probability of failure.\n\nIf you know you have a binomial experiment, then you can calculate binomial probabilities. This is important because binomial probabilities come up often in real life. Examples of binomial experiments are:\nToss a fair coin ten times, and find the probability of getting two heads.\nQuestion twenty people in class, and look for the probability of more than half being women?\nShoot five arrows at a target, and find the probability of hitting it five times?\n\n6.2.1 Formula for the probabilities for a Binomial experiment\nFirst, the random variable in a binomial experiment is \\(x\\) = number of successes.\nBe careful, a success is not always a good thing. Sometimes a success is something that is bad, like finding a defect. A success just means you observed the outcome you wanted to see happen.\nBinomial Formula for the probability of \\(r\\) successes in \\(n\\) trials is \\(P(X=r)=_nC_r*p^r*q^{n-r}\\)\nwhere \\(_nC_r\\) is the number of combinations of \\(n\\) things taking \\(r\\) at a time. It is read “\\(n\\) choose \\(r\\)”.\nWhen solving problems, make sure you define your random variable and state what \\(n, p\\), and \\(r\\) are. Without doing this, the problems are a great deal harder.\nThe command to find a binomial probability in r Studio is\nP\\((X=r)=\\)\ndbinom(r, n, p)\n\\(P(x \\le r)=\\)\npbinom(r, n, p, lower.tail=TRUE)\n\\(P(x \\ge r)=\\)\npbinom(r-1, n, p, lower.tail = FALSE)\n\n\n6.2.2 Example: Calculating Binomial Probabilities\nWhen looking at a person’s eye color, it turns out that 1% of people in the world has green eyes (“What percentage of,” 2013). Consider a group of 20 people.\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that none of the 20 people have green eyes.\nFind the probability that nine have green eyes.\nFind the probability that at most three have green eyes.\nFind the probability that at most two have green eyes.\nFind the probability that at least four have green eyes.\nIn Europe, four people out of twenty have green eyes. Is this unusual? What does that tell you?\n\n\n6.2.2.1 Solution\n\nState the random variable.\n\n\\(x\\) = number of people with green eyes\n\nArgue that this is a binomial experiment.\n\n\n\nThere are 20 people, and each person is a trial, so there are a fixed number of trials. In this case, \\(n\\) = 20.\nIf you assume that each person in the group is chosen at random the eye color of one person doesn’t affect the eye color of the next person, thus the trials are independent.\nEither a person has green eyes or they do not have green eyes, so there are only two outcomes. In this case, the success is a person has green eyes.\nThe probability of a person having green eyes is 0.01. This is the same for every trial since each person has the same chance of having green eyes.\n\n\n\nFind the probability that none of the 20 people have green eyes.\n\nIf none have green eyes, then \\(r=0\\).\nProbability that none have green eyes is \\(P(X=0)=0.818\\), using the command:\n\ndbinom(0,20,0.01) \n\n[1] 0.8179069\n\n\n\nFind the probability that nine have green eyes.\n\nIf nine have green eyes, then \\(r=9\\).\nProbability that 9 have green eyes is\n\\(P(X=9)=1.50X10^{-13}\\). Notice that r gives the answer as 1.50391e-13. This is the way many computer programs write a number in scientific notation. It isn’t possible for a computer to write it as \\(1.50381X10^{-13}\\), but it is possible for humans to write it correctly. So make sure the answer is written in the correct scientific notation.\n\ndbinom(9,20,0.01)\n\n[1] 1.50381e-13\n\n\n\nFind the probability that at most three have green eyes.\n\nAt most three means that three is the highest value you will have. Find the probability of \\(x\\) is less than or equal to three.\nSince this is less than, then the lower tail of the probability distribution is being used, so \\(P(X \\le 3)=0.99996\\) using the command in r Studio of\n\npbinom(3,20,0.01, lower.tail=TRUE)\n\n[1] 0.9999574\n\n\nThe reason the answer is written to more decimal places is because when it is rounded to three decimal places the rounding makes the answer 1. But 1 means that the event will happen, when in reality there is a slight chance that it won’t happen. It is best to write the answer to more decimal places or it can be written as \\(&gt;0.999\\) to represent that the number is very close to 1, but isn’t 1.\n\nFind the probability that at most two have green eyes.\n\nAt most 2 means 2 or less. So find the probability that there are less than or equal to 2. \\(P(X \\le 2)=0.999\\), and again, this is the lower tail of the probability distribution, so use lower.tail=TRUE in the r command:\n\npbinom(2,20,0.01, lower.tail=TRUE)\n\n[1] 0.9989964\n\n\n\nFind the probability that at least four have green eyes.\n\nAt least four means four or more. Find the probability of \\(x\\) being greater than or equal to four. Since it is greater than or equal to, this is the right tail of the probability distribution. However, if you just use lower.tail=FALSE, then the 4 is not included in r calculations. You want all numbers from 4 on up, so you need to use\n\\(r=4-1=3\\) in the r command. This will include 4 in the calculation. \\(P(X \\ge 4)=4.26X10^{-5}\\)\n\npbinom(4-1,20,0.01, lower.tail=FALSE) \n\n[1] 4.262093e-05\n\n\n\nIn Europe, four people out of twenty have green eyes. Is this unusual? What does that tell you?\n\nSince the probability of finding four or more people with green eyes is much less than 0.05, it is unusual to find four people out of twenty with green eyes. That should make you wonder if the proportion of people in Europe with green eyes is more than the 1% for the general population. If this is true, then you may want to ask why Europeans have a higher proportion of green-eyed people. That of course could lead to more questions.\n\n\n\n6.2.3 Example: Calculating Binomial Probabilities\nAccording to the Center for Disease Control (CDC), about 1 in 88 children in the U.S. have been diagnosed with autism (“CDC-data and statistics,” 2013). Suppose you consider a group of 10 children.\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that none have autism.\nFind the probability that seven have autism.\nFind the probability that at least five have autism.\nFind the probability that at most two have autism.\nSuppose five children out of ten have autism. Is this unusual? What does that tell you?\n\n\n6.2.3.1 Solution\n\nState the random variable.\n\n\\(x\\) = number of children with autism.\n\nArgue that this is a binomial experiment\n\n\n\nThere are 10 children, and each child is a trial, so there are a fixed number of trials. In this case, \\(n\\) = 10.\nIf you assume that each child in the group is chosen at random, then whether a child has autism does not affect the chance that the next child has autism. Thus the trials are independent.\nEither a child has autism or they do not have autism, so there are two outcomes. In this case, the success is a child has autism.\nThe probability of a child having autism is \\(\\frac{1}{88}\\). This is the same for every trial since each child has the same chance of having autism.\n\n\n\nFind the probability that none have autism.\n\n\\(P(X=0)=0.892\\)\n\ndbinom(0,10, 1/88)\n\n[1] 0.892002\n\n\n\nFind the probability that seven have autism.\n\n\\(P(X=7)=2.84X10^{-12}\\)\n\ndbinom(7,10, 1/88)\n\n[1] 2.837346e-12\n\n\n\nFind the probability that at least five have autism.\n\n\\(P(X \\ge 5)=4.553X10^{-8}\\). Again, this is the upper tail of the probability distribution, so use lower=tail=FALSE and\n\\(r=5-1=4\\) to make sure that r calculates for 5 and on up.\n\npbinom(5-1, 10, 1/88, lower.tail=FALSE)\n\n[1] 4.553416e-08\n\n\n\nFind the probability that at most two have autism.\n\n\\(P(X \\le 2)=0.9998\\). This is using the lower tail of the probability distribution.\n\npbinom(2, 10, 1/88, lower.tail=TRUE)\n\n[1] 0.9998341\n\n\n\nSuppose five children out of ten have autism. Is this unusual? What does that tell you?\n\nSince the probability of five or more children in a group of ten having autism is much less than 5%, it is unusual to happen. If this does happen, then one may think that the proportion of children diagnosed with autism is actually more than \\(\\frac{1}{88}\\).\n\n\n\n6.2.4 Homework for Binomial Probability Distribution Section\n\nApproximately 10% of all people are left-handed (\\“11 little-known facts,\\” 2013). Consider a grouping of fifteen people.\n\n\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that none are left-handed.\nFind the probability that seven are left-handed.\nFind the probability that at least two are left-handed.\nFind the probability that at most three are left-handed.\nFind the probability that at least seven are left-handed.\nSeven of the last 15 U.S. Presidents were left-handed. Is this unusual? What does that tell you?\n\n\n\nAccording to an article in the American Heart Association’s publication *Circulation*, 24% of patients who had been hospitalized for an acute myocardial infarction did not fill their cardiac medication by the seventh day of being discharged (Ho, Bryson & Rumsfeld, 2009). Suppose there are twelve people who have been hospitalized for an acute myocardial infarction.\n\n\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that all filled their cardiac medication.\nFind the probability that seven did not fill their cardiac medication.\nFind the probability that none filled their cardiac medication.\nFind the probability that at most two did not fill their cardiac medication.\nFind the probability that at least three did not fill their cardiac medication.\nFind the probability that at least ten did not fill their cardiac medication.\nSuppose of the next twelve patients discharged, ten did not fill their cardiac medication, would this be unusual? What does this tell you?\n\n\n\nEyeglassomatic manufactures eyeglasses for different retailers. In March 2010, they tested to see how many defective lenses they made, and there were 16.9% defective lenses due to scratches. Suppose Eyeglassomatic examined twenty eyeglasses.\n\n\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that none are scratched.\nFind the probability that all are scratched.\nFind the probability that at least three are scratched.\nFind the probability that at most five are scratched.\nFind the probability that at least ten are scratched.\nIs it unusual for ten lenses to be scratched? If it turns out that ten lenses out of twenty are scratched, what might that tell you about the manufacturing process?\n\n\n\nThe proportion of brown M&M’s in a milk chocolate packet is approximately 14% (Madison, 2013). Suppose a package of M&M’s typically contains 52 M&M’s.\n\n\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that six M&M’s are brown.\nFind the probability that twenty-five M&M’s are brown.\nFind the probability that all of the M&M’s are brown.\nWould it be unusual for a package to have only brown M&M’s? If this were to happen, what would you think is the reason?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Discrete Probability Distribution.html#mean-and-standard-deviation-of-binomial-distribution",
    "href": "Discrete Probability Distribution.html#mean-and-standard-deviation-of-binomial-distribution",
    "title": "6  Discrete Probability Distribution",
    "section": "6.3 Mean and Standard Deviation of Binomial Distribution",
    "text": "6.3 Mean and Standard Deviation of Binomial Distribution\nIf you list all possible values of \\(x\\) in a Binomial distribution, you get the Binomial Probability Distribution. You can draw a histogram of the probability distribution and find the mean (expected value), variance, and standard deviation of it. To have r Studio calculate the binomial values and save them to a variable, use the command\nx&lt;-c(0:n) p&lt;-dbinom(0:n, n, p)\n\n6.3.1 Example: Finding the Probability Distribution, Mean, Variance and Standard Deviation of a Binomial Distribution\nWhen looking at a person’s eye color, it turns out that 1% of people in the world has green eyes (“What percentage of,” 2013). Consider a group of 20 people.\n\nState the random variable.\nWrite the probability distribution.\nDraw a histogram.\nFind the mean, variance, and standard deviation.\n\n\n6.3.1.1 Solution\n\nState the random variable.\n\n\\(x\\) = number of people who have green eyes\n\nWrite the probability distribution.\n\nIn this case you need to write each value of \\(x\\) and its corresponding probability. It is easiest to do this by using the r Command:\n\ngreen&lt;-c(0:20) \nprobability_green&lt;-dbinom(0:20,20, 0.01) \n\nIt looks like nothing happened, but r save the values as variables. To see what is in each of those values, type\n\ngreen\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nprobability_green\n\n [1] 8.179069e-01 1.652337e-01 1.585576e-02 9.609552e-04 4.125313e-05\n [6] 1.333434e-06 3.367259e-08 6.802543e-10 1.116579e-11 1.503810e-13\n[11] 1.670900e-15 1.534344e-17 1.162381e-19 7.225371e-22 3.649177e-24\n[16] 1.474415e-26 4.654088e-29 1.106141e-31 1.862190e-34 1.980000e-37\n[21] 1.000000e-40\n\n\nThese can now be typed into a table if desired.\n\nDraw a histogram.\n\nOn r, this is like what was done in Section 5.1. Makes sure that the packages “arm” and “Weighted.Desc.Stat” are loaded. Then perform the command to get:\n\ndiscrete.histogram(green, probability_green, bar.width = 1, main=\"Number of People with Green Eyes\", xlab=\"Numbr of People with Green Eyes\")\n\n\n\n\n\n\n\nFigure 6.2: Histogram of Number of People with Green Eyes\n\n\n\n\n\nNotice this graph Figure 6.2 is skewed right.\n\nFind the mean, variance, and standard deviation\n\nUsing r Studio command such as those in Section 5.1:\n\nw.mean(green, probability_green) \n\n[1] 0.2\n\nw.var(green, probability_green) \n\n[1] 0.198\n\nw.sd(green, probability_green)\n\n[1] 0.4449719\n\n\nYou expect on average that out of 20 people, less than 1 person would have green eyes, with are variance of 0.198 \\(people^2\\) and a standard deviation of 0.44 people.\n\n\n\n6.3.2 Homework for Mean and Standard Deviation of Binomial Distribution Section\n\nSuppose a random variable, \\(x\\), arises from a binomial experiment. Suppose \\(n = 6\\), and \\(p = 0.13\\).\n\n\n\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nSuppose a random variable, \\(x\\), arises from a binomial experiment. Suppose \\(n = 10\\), and \\(p = 0.81\\).\n\n\n\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nSuppose a random variable, \\(x\\), arises from a binomial experiment. Suppose \\(n = 7\\), and \\(p = 0.50\\).\n\n\n\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nApproximately 10% of all people are left-handed. Consider a grouping of fifteen people.\n\n\n\nState the random variable.\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nAccording to an article in the American Heart Association’s publication *Circulation*, 24% of patients who had been hospitalized for an acute myocardial infarction did not fill their cardiac medication by the seventh day of being discharged (Ho, Bryson & Rumsfeld, 2009). Suppose there are twelve people who have been hospitalized for an acute myocardial infarction.\n\n\n\nState the random variable.\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nEyeglassomatic manufactures eyeglasses for different retailers. In March 2010, they tested to see how many defective lenses they made, and there were 16.9% defective lenses due to scratches. Suppose Eyeglassomatic examined twenty eyeglasses.\n\n\n\nState the random variable.\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nThe proportion of brown M&M’s in a milk chocolate packet is approximately 14% (Madison, 2013). Suppose a package of M&M’s typically contains 52 M&M’s.\n\n\n\nState the random variable.\nFind the mean.\nFind the variance.\nFind the standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html",
    "href": "Continuous Probability Distribution.html",
    "title": "7  Continuous Probability Distribution",
    "section": "",
    "text": "7.1 Normal Distribution\nChapter 5 dealt with probability distributions arising from discrete random variables. Mostly that chapter focused on the binomial experiment. There are many other experiments from discrete random variables that exist but are not covered in this book.\nChapter 6 deals with probability distributions that arise from continuous random variables. The focus of this chapter is a distribution known as the normal distribution, though realize that there are many other distributions that exist. A few others are examined in future chapters.\nLooking at the density plot of a quantitative variable, one can guess what the distribution of that variable is. As an example, consider the NHANES data frame. One variable to consider is Weight. The density plot of Weight is\nFigure 7.1 looks somewhat symmetric, and maybe bell shaped.\nConsider, the variable head circumference (HeadCirc) in the NHANES data frame. The density plot for this variable is Figure 7.2\nFigure 7.2 looks somewhat skewed left.\nNow consider the variable BMI from the NHANES data frame. The density plot is\nThis density plot Figure 9.1 appears to be skewed right\nNow consider the variable SmokeAge. Its density plot is Figure 7.4\nThis distribution appears to be bimodal.\nlastly, consider the variable Pulse. The density plot is Figure 7.5\nThis density plot appears to be symmetric and could almost be considered bell shaped.\nThe reason that one considers the density plots to understand the distribution of the population, is that in some cases the distribution can be approximated with a known distribution that has certain properties. There are many known distribution. Some examples are the Uniform distribution, the Chi-Squared distribution, the Student’s T distribution, and the normal distribution. The normal distribution is one of the more common distributions to use as a model, and it will be explored in this chapter. But do realize that there are many other distributions that one can use.\nMany populations have a distribution that is a symmetric, unimodal, and bell-shaped. For example: height, blood pressure, and cholesterol level. However, not every bell shaped curve is a normal curve. In a normal curve, there is a specific relationship between its “height” and its “width.” Normal curves can be tall and skinny or they can be short and fat. They are all symmetric, unimodal, and centered at \\(\\mu\\), the population mean.\nFigure 7.6 and Figure 7.7 show two different normal curves drawn on the same scale. Both have \\(\\mu=2\\) but the one in Figure 7.6 has a standard deviation of 1 and the one in Figure 7.7 has a standard deviation of 4. Notice that the larger standard deviation makes the graph wider (more spread out) and shorter.\nFigure 7.6: Normal curve with mean 2 and standard deviation 1\nFigure 7.7: Normal curve with mean 2 and standard deviation 4\nEvery normal curve has common features.\nJust as in a discrete probability distribution, the object is to find the probability of an event occurring. However, unlike in a discrete probability distribution where the event can be a single value, in a continuous probability distribution the event must be a range. You are interested in finding the probability of \\(x\\) occurring in the range between \\(a\\) and \\(b\\), or \\(P(a \\le x \\le b) = P(a&lt;x&lt;b)\\). Calculus tells us this probability is the area under the curve in the interval from \\(a\\) to \\(b\\).\nBefore looking at the process for finding the probabilities under the normal curve, it is somewhat useful to look at the Empirical Rule that gives approximate values for these areas. The Empirical Rule is just an approximation and it will only be used in this section to give you an idea of what the size of the probabilities is for different shadings. A more precise method for finding probabilities for the normal curve will be demonstrated in the next section. Please do not use the empirical rule except for real rough estimates.\nThe Empirical Rule for any normal distribution: Approximately 68% of the data is within one standard deviation of the mean. Approximately 95% of the data is within two standard deviations of the mean. Approximately 99.7% of the data is within three standard deviations of the mean.\nBe careful, there is still some area left over in each end. Remember, the maximum a probability can be is 100%, so if you calculate you will see that for both ends together there is 0.3% of the curve. Because of symmetry, you can divide this equally between both ends and find that there is 0.15% in each tail beyond the 3rd standard deviations.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html#normal-distribution",
    "href": "Continuous Probability Distribution.html#normal-distribution",
    "title": "7  Continuous Probability Distribution",
    "section": "",
    "text": "The center, or the highest point, is at the population mean, \\(\\mu\\).\nThe transition points are the places where the curve changes from a “hill” to a “valley”. The distance from the mean to the transition point is one standard deviation.\nThe area under the whole curve is exactly 1. Therefore, the area under the half below or above the mean is 0.5.\n\n\n\n\n\n\n\nEmpirical Rule Graph",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html#finding-probabilities-for-the-normal-distribution",
    "href": "Continuous Probability Distribution.html#finding-probabilities-for-the-normal-distribution",
    "title": "7  Continuous Probability Distribution",
    "section": "7.2 Finding Probabilities for the Normal Distribution",
    "text": "7.2 Finding Probabilities for the Normal Distribution\nThe Empirical Rule is just an approximation and only works for certain values. What if you want to find the probability for \\(x\\) values that are not integer multiples of the standard deviation? The probability is the area under the curve. To find areas under the curve, you need calculus. Before technology, you needed to convert every \\(x\\) value to a standardized number, called the \\(z\\)-score or \\(z\\)-value or simply just \\(z\\). The \\(z\\)-score is a measure of how many standard deviations an \\(x\\) value is from the mean. To convert from a normally distributed \\(x\\) value to a \\(z\\)-score, you use the following formula.\n\\(z-score=\\frac{x-\\mu}{\\sigma}\\)\nwhere \\(\\mu\\) = mean of the population of the \\(x\\) value and \\(\\sigma\\) = standard deviation for the population of the \\(x\\) value\nThe \\(z\\)-score is normally distributed, with a mean of 0 and a standard deviation of 1. It is known as the standard normal curve. The \\(z\\)-score is a measure of how many standard deviations a data value is from its mean. If the \\(z\\) - score is positive, the data value is above the mean. If the \\(z\\)-score is negative, the data value is below the mean. The farther the \\(z\\)-value is from 0, the farther the data value is from the mean.\nThese days technology can find probabilities without converting to the \\(z\\)-score and looking the probabilities up in a table. There are many programs available that will calculate the probability for a normal curve. The command on r to find the area to the left \\(P(x&lt;value)\\) is\npnorm(value, mean, standard_deviation, lower.tail=TRUE)\nThe command on r to find the area to the right, \\(P(x&gt;value)\\) is\npnorm(value, mean, standard_deviation, lower.tail=FALSE)\n\n7.2.1 Example: General Normal Distribution\nThe length of a human pregnancy is normally distributed with a mean of 272 days with a standard deviation of 9 days (Bhat & Kushtagi, 2006).\n\nState the random variable\nFind the probability of a pregnancy lasting more than 280 days.\nFind the probability of a pregnancy lasting less than 250 days.\nFind the probability that a pregnancy lasts between 265 and 280 days.\nFind the length of pregnancy that 10% of all pregnancies last less than.\nSuppose you meet a woman who says that she was pregnant for less than 250 days. Would this be unusual and what might you think?\n\n\n7.2.1.1 Solution\n\nState the random variable.\n\n\\(x\\) = length of a human pregnancy\n\nFind the probability of a pregnancy lasting more than 280 days.\n\nFirst translate the statement into a mathematical statement. \\(P(x&gt;280)\\)\nNow, draw a picture Figure 7.8.\n\n\n\n\n\n\n\n\nFigure 7.8: Normally distributed with mean 272 and standard deviation 9, and P(x&gt;280)\n\n\n\n\n\nThe probability of a pregnancy lasting longer than 280 days is \\(P(x&gt;280)=0.187\\). The command in rStudio is\n\npnorm(280, 272, 9, lower.tail=FALSE)\n\n[1] 0.1870314\n\n\nThus 18.7% of all pregnancies last more than 280 days. This is not unusual since the probability is greater than 5%.\n\nFind the probability of a pregnancy lasting less than 250 days.\n\nFirst translate the statement into a mathematical statement. \\(P(x&lt;250)\\)\nNow, draw a picture Figure 7.9.\n\n\n\n\n\n\n\n\nFigure 7.9: Density plot of pregnancy length. Normally distributed with mean 272 and standard deviation 9, and P(x&lt;250)\n\n\n\n\n\nThe probability of a pregnancy lasting longer than 250 days is \\(P(x&lt;250)=0.0073\\). The command in r Studio is\n\npnorm(250, 272, 9, lower.tail=TRUE)\n\n[1] 0.007253771\n\n\nThus 0.73% of all pregnancies last less than 250 days. This is unusual since the probability is less than 5%.\n\nFind the probability that a pregnancy lasts between 265 and 280 days.\n\nFirst translate the statement into a mathematical statement. \\(P(265&lt;x&lt;280)\\)\nNow draw a picture Figure 7.10.\n\n\n\n\n\n\n\n\nFigure 7.10: Density plot of pregnancy length. Normally distributed with mean 272 and standard deviation 9, and P(265&lt;x&lt;280)\n\n\n\n\n\nThe probability of a pregnancy lasting between 265 days and 280 days is \\(P(265&lt;x&lt;280)=0.187\\). To find the area between two values on the normal distribution, first, find the area to the left of the lower value, Graphically, this looks like Figure 7.11\n\n\n\n\n\n\n\n\nFigure 7.11: Density plot of pregnancy length. Normally distributed with mean 272 and standard deviation 9, and P(x&lt;265)\n\n\n\n\n\nNow find the area less than 280. Graphically this looks like Figure 7.12\n\n\n\n\n\n\n\n\nFigure 7.12: Density plot of pregnancy length. Normally distributed with mean 272 and standard deviation 9, and P(x&lt;280)\n\n\n\n\n\nLooking at the three figures, if you take the area in Figure 7.12 and subtract the area in Figure 7.11 you get the area in Figure 7.10. In rStudio, the way to find the probability the probability of a pregnancy lasting between 265 days and 280 days, \\(P(265&lt;x&lt;280)=0.595\\) use the following command\n\npnorm(280, 272, 9, lower.tail=TRUE)-pnorm(265, 272, 9, lower.tail=TRUE) \n\n[1] 0.5946186\n\n\nThus 59.5% of all pregnancies last between 265 and 280 days.\n\nFind the length of pregnancy that 10% of all pregnancies last less than.\n\nThis problem is asking you to find an \\(x\\) value from a probability. You want to find the \\(x\\) value that has 10% of the length of pregnancies to the left of it. In this case, you are given the probability. In r, the command is\nqnorm(area, mean, standard_deviation, lower.tail=TRUE or FALSE)\nFor this example since you know the area in the lower tail, then use lower.tail=TRUE. So the command is\n\nqnorm(0.1, 272, 9, lower.tail = TRUE)\n\n[1] 260.466\n\n\nThus 10% of all pregnancies last less than approximately 260 days.\n\nSuppose you meet a woman who says that she was pregnant for less than 250 days. Would this be unusual and what might you think?\n\nFrom part (c) you found the probability that a pregnancy lasts less than 250 days is 0.73%. Since this is less than 5%, it is very unusual. You would think that either the woman had a premature baby, or that she may be wrong about when she actually became pregnant.\n\n\n\n7.2.2 Example: General Normal Distribution\nThe mean mathematics SAT score in 2012 was 514 with a standard deviation of 117 (“Total group profile,” 2012). Assume the mathematics SAT score is normally distributed.\n\nState the random variable.\nFind the probability that a person has a mathematics SAT score over 700.\nFind the probability that a person has a mathematics SAT score of less than 400.\nFind the probability that a person has a mathematics SAT score between a 500 and a 650.\nFind the mathematics SAT score that represents the top 1% of all scores.\n\n\n7.2.2.1 Solution\n\nState the random variable.\n\n\\(x\\) = mathematics SAT score\n\nFind the probability that a person has a mathematics SAT score over 700.\n\nFirst translate the statement into a mathematical statement. \\(P(x&gt;700)\\)\nNow, draw a picture Figure 7.13\n\n\n\n\n\n\n\n\nFigure 7.13: Density plot of SAT mathematics score. Normally distributed with mean 514 and standard deviation 117 and P(x&gt;700)\n\n\n\n\n\nTo find \\(P(x&gt;700)=0.0559\\), the command in r would be\n\npnorm(700, 514,117, lower.tail = FALSE)\n\n[1] 0.05594631\n\n\nThere is a 5.6% chance that a person scored above a 700 on the mathematics SAT test. This is not unusual.\n\nFind the probability that a person has a mathematics SAT score of less than 400.\n\nFirst translate the statement into a mathematical statement. \\(P(x&lt;400)\\)\nNow, draw a picture Figure 7.14\n\n\n\n\n\n\n\n\nFigure 7.14: Density plot of SAT mathematics score. Normally distributed with mean 514 and standard deviation 117 and P(x&lt;400)\n\n\n\n\n\nTo find \\(P(x&lt;400)=0.165\\), the command in r would be\n\npnorm(400, 514, 117, lower.tail = TRUE) \n\n[1] 0.1649392\n\n\nSo, there is a 16.5% chance that a person scores less than a 400 on the mathematics part of the SAT.\n\nFind the probability that a person has a mathematics SAT score between a 500 and a 650.\n\nFirst translate the statement into a mathematical statement \\(P(500&lt;x&lt;650)\\)\nNow, draw a picture Figure 7.15\n\n\n\n\n\n\n\n\nFigure 7.15: Density plot of SAT mathematics score. Normally distributed with mean 514 and standard deviation 117 and P(514&lt;x&lt;650)\n\n\n\n\n\nTo find \\(P(500&lt;x&lt;650)=0.425\\), the command in r would be\n\npnorm(650, 514, 117, lower.tail = TRUE)-pnorm(500, 514, 117, lower.tail=TRUE)\n\n[1] 0.4250851\n\n\nSo, there is a 42.5% chance that a person has a mathematical SAT score between 500 and 650.\n\nFind the mathematics SAT score that represents the top 1% of all scores.\n\nThis problem is asking you to find an \\(x\\) value from a probability. You want to find the \\(x\\) value that has 1% of the mathematics SAT scores to the right of it. In this case you are using the upper tail of the curve. To find this \\(x\\) value on rStudio, use the command\n\nqnorm(0.01, 514, 117, lower.tail=FALSE) \n\n[1] 786.1827\n\n\nSo, 1% of all people who took the SAT scored over about 786 points on the mathematics SAT.\n\n\n\n7.2.3 Homework for Normal Distribution Section\n\nFind each of the probabilities, where \\(z\\) is a \\(z\\)-score from the standard normal distribution with mean of \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). It helps to draw a picture for each problem.\n\n\n\n\\(P(z&lt;2.36)\\)\n\\(P(z&gt;0.67)\\)\n\\(P(0&lt;x&lt;2.11)\\)\n\\(P(-2.78&lt;z&lt;1.97)\\)\n\n\n\nFind the z-score corresponding to the given area. Remember, z is distributed as the standard normal distribution with mean of \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\).\n\n\n\nThe area to the left of \\(z\\) is 15%.\nThe area to the right of \\(z\\) is 65%.\nThe area to the left of \\(z\\) is 10%.\nThe area to the right of \\(z\\) is 5%.\nThe area between \\(-z\\) and \\(z\\) is 95%. (Hint draw a picture and figure out the area to the left of \\(-z\\).)\nThe area between \\(-z\\) and \\(z\\) is 99%.\n\n\n\nIf a random variable that is normally distributed has a mean of 25 and a standard deviation of 3, convert the given value to a \\(z\\)-score.\n\n\n\n\\(x\\) = 23\n\\(x\\) = 33\n\\(x\\) = 19\n\\(x\\) = 45\n\n\n\nAccording to the WHO MONICA Project the mean blood pressure for people in China is 128 mmHg with a standard deviation of 23 mmHg (Kuulasmaa, Hense & Tolonen, 1998). Assume that blood pressure is normally distributed.\n\n\n\nState the random variable.\nFind the probability that a person in China has blood pressure of 135 mmHg or more.\nFind the probability that a person in China has blood pressure of 141 mmHg or less.\nFind the probability that a person in China has blood pressure between 120 and 125 mmHg.\nIs it unusual for a person in China to have a blood pressure of 135 mmHg? Why or why not?\nWhat blood pressure do 90% of all people in China have less than?\n\n\n\nThe size of fish is very important to commercial fishing. A study conducted in 2012 found the length of Atlantic cod caught in nets in Karlskrona to have a mean of 49.9 cm and a standard deviation of 3.74 cm (Ovegard, Berndt & Lunneryd, 2012). Assume the length of fish is normally distributed.\n\n\n\nState the random variable.\nFind the probability that an Atlantic cod has a length less than 52 cm.\nFind the probability that an Atlantic cod has a length of more than 74 cm.\nFind the probability that an Atlantic cod has a length between 40.5 and 57.5 cm.\nIf you found an Atlantic cod to have a length of more than 74 cm, what could you conclude?\nWhat length are 15% of all Atlantic cod longer than?\n\n\n\nThe mean cholesterol levels of women age 45-59 in Ghana, Nigeria, and Seychelles is 5.1 mmol/l and the standard deviation is 1.0 mmol/l (Lawes, Hoorn, Law & Rodgers, 2004). Assume that cholesterol levels are normally distributed.\n\n\n\nState the random variable.\nFind the probability that a woman age 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level above 6.2 mmol/l (considered a high level).\nFind the probability that a woman age 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level below 5.2 mmol/l (considered a normal level).\nFind the probability that a woman age 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level between 5.2 and 6.2 mmol/l (considered borderline high).\nIf you found a woman age 45-59 in Ghana, Nigeria, or Seychelles having a cholesterol level above 6.2 mmol/l, what could you conclude?\nWhat value do 5% of all woman ages 45-59 in Ghana, Nigeria, or Seychelles have a cholesterol level less than?\n\n\n\nIn the United States, males between the ages of 40 and 49 eat on average 103.1 g of fat every day with a standard deviation of 4.32 g (“What we eat,” 2012). Assume that the amount of fat a person eats is normally distributed.\n\n\n\nState the random variable.\nFind the probability that a man age 40-49 in the U.S. eats more than 110 g of fat every day.\nFind the probability that a man age 40-49 in the U.S. eats less than 93 g of fat every day.\nFind the probability that a man age 40-49 in the U.S. eats less than 65 g of fat every day.\nIf you found a man age 40-49 in the U.S. who says he eats less than 65 g of fat every day, would you believe him? Why or why not?\nWhat daily fat level do 5% of all men age 40-49 in the U.S. eat more than?\n\n\n\nA dishwasher has a mean life of 12 years with an estimated standard deviation of 1.25 years (“Appliance life expectancy,” 2013). Assume the life of a dishwasher is normally distributed.\n\n\n\nState the random variable.\nFind the probability that a dishwasher will last more than 15 years.\nFind the probability that a dishwasher will last less than 6 years.\nFind the probability that a dishwasher will last between 8 and 10 years.\nIf you found a dishwasher that lasted less than 6 years, would you think that you have a problem with the manufacturing process? Why or why not?\nA manufacturer of dishwashers only wants to replace free of charge 5% of all dishwashers. How long should the manufacturer make the warranty period?\n\n\n\nThe mean starting salary for nurses is \\$67,694 nationally (“Staff nurse -,” 2013). The standard deviation is approximately $10,333. Assume that the starting salary is normally distributed.\n\n\n\nState the random variable.\nFind the probability that a starting nurse will make more than \\$80,000.\nFind the probability that a starting nurse will make less than \\$60,000.\nFind the probability that a starting nurse will make between \\$55,000 and \\$72,000.\nIf a nurse made less than \\$50,000, would you think the nurse was under paid? Why or why not?\nWhat salary do 30% of all nurses make more than?\n\n\n\nThe mean yearly rainfall in Sydney, Australia, is about 137 mm and the standard deviation is about 69 mm (“Annual maximums of,”2013). Assume rainfall is normally distributed.\n\n\n\nState the random variable.\nFind the probability that the yearly rainfall is less than 100 mm.\nFind the probability that the yearly rainfall is more than 240 mm.\nFind the probability that the yearly rainfall is between 140 and 250 mm.\nIf a year has a rainfall less than 100mm, does that mean it is an unusually dry year? Why or why not?\nWhat rainfall amount are 90% of all yearly rainfalls more than?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html#assessing-normality",
    "href": "Continuous Probability Distribution.html#assessing-normality",
    "title": "7  Continuous Probability Distribution",
    "section": "7.3 Assessing Normality",
    "text": "7.3 Assessing Normality\nThe distributions you have seen up to this point have been assumed to be normally distributed, but how do you determine if it is normally distributed. One way is to take a sample and look at the sample to determine if it appears normal. If the sample looks normal, then most likely the population is also. Here are some guidelines that are use to help make that determination.\n\nDensity Plot: Make a density plot. For a normal distribution, the density plot should be roughly bell-shaped. For small samples, this is not very accurate, and another method is needed. A distribution may not look normally distributed from the density plot, but it still may be normally distributed.\nNormal quantile plot (or normal probability plot): This plot is provided through statistical software on a computer. If the points lie close to a line, the data comes from a distribution that is approximately normally distributed. If the points do not lie close to a line or they show a pattern that is not a line, the data are likely to come from a distribution that is not normally distributed.\n\n\n7.3.1 To create a density plot on rStudio:\nRead the Data Frame into r Studio. The command for density is\ngf_density(~variable, data=Data_Frame)\nSee chapter 2 for more examples of this.\n\n\n7.3.2 To create a normal quantile plot on rStudio\nRead the Data Frame into rStudio. The command for normal quantile plot is\ngf_qqnorm(~variable, data=Data_Frame)\nRealize that your random variable may be normally distributed, even if the sample fails the two tests. However, if the density plot definitely doesn’t look symmetric and bell shaped, and the normal probability plot doesn’t look linear, then you can be fairly confident that the data set does not come from a population that is normally distributed.\n\n\n7.3.3 Example: Is It Normal?\nIn Kiama, NSW, Australia, there is a blowhole. The data in Table 7.1 are times in seconds between eruptions (“Kiama blowhole eruptions,” 2013). Do the data come from a population that is normally distributed?\n\nEruption&lt;-read.csv( \"https://krkozak.github.io/MAT160/Blowhole_eruptions.csv\") \nknitr::kable(head(Eruption))\n\n\n\nTable 7.1: Time (in Seconds) Between Kiama Blowhole Eruptions\n\n\n\n\n\n\nInterval\n\n\n\n\n83\n\n\n51\n\n\n87\n\n\n60\n\n\n28\n\n\n95\n\n\n\n\n\n\n\n\nCode book for Data Frame Eruption\nDescription The ocean swell produces spectacular eruptions of water through a hole in the cliff at Kiama, about 120km south of Sydney, known as the Blowhole. The times at which 65 successive eruptions occurred from 1340 hours on 12 July 1998 were observed using a digital watch.\nFormat This data frame contains the following columns:\nInterval: Waiting time between eruptions (seconds)\nSource Kiama Blowhole Eruptions. (n.d.). Retrieved from http://www.statsci.org/data/oz/kiama.html\nReferences The data was collected and contributed by Jim Irish, Faculty of Engineering, University of Technology, Sydney.\n\nState the random variable\nDraw a Density plot\nDraw the normal quantile plot.\nDo the data come from a population that is normally distributed?\n\n\n7.3.3.1 Solution\n\nState the random variable\n\n\\(x\\) = time in seconds between eruptions of Kiama Blowhole\n\nDraw a Density plot\n\nThe density plot produced is in Figure 7.16\n\ngf_density(~Interval, data=Eruption, title=\"Eruption times for Kiama Blowhole\", xlab=\"Time (seconds)\") \n\n\n\n\n\n\n\nFigure 7.16: Density Plot of Eruption Times for Kiama Blowhole\n\n\n\n\n\nThis looks skewed right and not symmetric.\n\nDraw the normal quantile plot.\n\nThe normal quantile plot is in Figure 7.17\n\ngf_qq(~Interval, data=Eruption, title=\"Eruption times for Kiama Blowhole\")\n\n\n\n\n\n\n\nFigure 7.17: Normal Quantile Plot of Eruption Times for Kiama blowhole.\n\n\n\n\n\nFigure 7.17 looks more like an exponential growth than linear.\n\nDo the data come from a population that is normally distributed?\n\nConsidering the density plot is skewed right, and the normal probability plot does not look linear, then the conclusion is that this sample is not from a population that is normally distributed.\n\n\n\n7.3.4 Example: Is It Normal?\nThe US National Center for Health Statistics (NCHS) conducted a series of health and nutrition surveys called NHANES. One of the many variables in NHANES is pulse. Determine if pulse is a normally distributed variable. The NHANES data frame is Table 3.5.\n\nState the random variable\nDraw a density plot\nDraw the normal quantile plot.\nDo the data come from a population that is normally distributed?\n\n\n7.3.4.1 Solution\n\nState the random variable\n\n\\(x\\) = pulse\n\nDraw a density plot\n\nThe density plot is in Figure 7.18\n\ngf_density(~Pulse, data=NHANES, title=\"Pulse Rate\", xlab=\"Pulse Rate (bpm)\")\n\n\n\n\n\n\n\nFigure 7.18: Density plot of Pulse Rate (bpm)\n\n\n\n\n\nThis looks somewhat symmetric and bell shaped.\n\nDraw the normal quantile plot.\n\nThe normal quantile plot is in Figure 7.19\n\ngf_qq(~Pulse, data=NHANES, title=\"Pulse Rate\") \n\n\n\n\n\n\n\nFigure 7.19: Normal Quantile Plot of Pulse Rate (bmp)\n\n\n\n\n\nFigure 7.19 looks fairly linear.\n\nDo the data come from a population that is normally distributed?\n\nConsidering the density plot is bell shaped and the normal probability plot looks linear. The conclusion is that this sample is from a population that is normally distributed.\n\n\n\n7.3.5 Homework for Assessing Normality Section\n\nCholesterol data was collected on patients four days after having a heart attack. The data is in Table 4.2. Assess if the data is from a population that is normally distributed.\n\nCode Book for Cholesterol See is below Table 4.2.\n\nThe size of fish is very important to commercial fishing. A study conducted in 2012 collected the lengths of Atlantic cod caught in nets in Karlskrona (Ovegard, Berndt & Lunneryd, 2012). Data based on information from the study is in Table 7.2. Determine if the data is from a population that is normally distributed.\n\n\nCod&lt;-read.csv( \"https://krkozak.github.io/MAT160/cod.csv\") \nknitr::kable(head(Cod))\n\n\n\nTable 7.2: Atlantic Cod Lengths\n\n\n\n\n\n\nlength\n\n\n\n\n48\n\n\n50\n\n\n50\n\n\n55\n\n\n53\n\n\n50\n\n\n\n\n\n\n\n\n\nThe WHO MONICA Project collected blood pressure data for people in China (Kuulasmaa, Hense & Tolonen, 1998). Data based on information from the study is in Table 7.3. Determine if the data is from a population that is normally distributed.\n\n\nBP&lt;-read.csv( \"https://krkozak.github.io/MAT160/bp.csv\") \nknitr::kable(head(BP))\n\n\n\nTable 7.3: Blood Pressure Values for People in China\n\n\n\n\n\n\npressure\n\n\n\n\n114\n\n\n141\n\n\n154\n\n\n137\n\n\n131\n\n\n132\n\n\n\n\n\n\n\n\n\nAnnual rainfalls for Sydney, Australia are given in Table 7.4 (“Annual maximums of,” 2013). Can you assume rainfall is normally distributed?\n\n\nAnnual&lt;-read.csv( \"https://krkozak.github.io/MAT160/annual.csv\") \nknitr::kable(head(Annual))\n\n\n\nTable 7.4: Annual Rainfall in Sydney, Australia\n\n\n\n\n\n\namount\n\n\n\n\n146.8\n\n\n383.0\n\n\n90.9\n\n\n178.1\n\n\n267.5\n\n\n95.5",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html#sampling-distribution-and-the-central-limit-theorem",
    "href": "Continuous Probability Distribution.html#sampling-distribution-and-the-central-limit-theorem",
    "title": "7  Continuous Probability Distribution",
    "section": "7.4 Sampling Distribution and the Central Limit Theorem",
    "text": "7.4 Sampling Distribution and the Central Limit Theorem\nYou now have most of the skills to start statistical inference, but you need one more concept.\nFirst, it would be helpful to state what statistical inference is in more accurate terms.\nStatistical Inference: to make accurate decisions about parameters from statistics\nWhen it says “accurate decision,” you want to be able to measure how accurate. You measure how accurate using probability. In both binomial and normal distributions, you needed to know that the random variable followed either distribution. You need to know how the statistic is distributed and then you can find probabilities. In other words, you need to know the shape of the sample mean or whatever statistic you want to make a decision about.\nHow is the statistic distributed? This is answered with a sampling distribution.\nSampling Distribution: how a sample statistic is distributed when repeated trials of size \\(n\\) are taken.\n\n7.4.1 Example: Sampling Distribution\nThe NHANES data frame has the pulse rates for approximately 50,000 individuals. The random variable is \\(x\\) = pulse rate. The probability distribution of this random variable is presented in Figure 7.20. Although pulse rates from 50,000 individuals isn’t the entire population, the sample is most likely a good representation of the population. Thus, it is safe to assume the population is normally distributed. An estimate for the population mean is 73.6 pbm, and the population standard deviation estimate is 12.2 bpm.\n\ngf_density(~Pulse, data=NHANES, title = \"Pulse Rate\", xlab=\"Pulse (bpm)\") \ndf_stats(~Pulse, data=NHANES, mean, sd)\n\n  response     mean       sd\n1    Pulse 73.55973 12.15542\n\n\n\n\n\n\n\n\nFigure 7.20: Distribution of Pulse Rate\n\n\n\n\n\nSuppose you take a random sample of 10 pulse rates from those 50,000 individuals. A random sample of data from 10 individuals is:\n\nNHANES|&gt; \n  sample_n(size=10) \n\n# A tibble: 10 × 76\n      ID SurveyYr Gender   Age AgeDecade AgeMonths Race1    Race3    Education  \n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;    &lt;fct&gt;      \n 1 53377 2009_10  female    68 \" 60-69\"        816 White    &lt;NA&gt;     Some Colle…\n 2 52347 2009_10  female    18 \" 10-19\"        224 Mexican  &lt;NA&gt;     &lt;NA&gt;       \n 3 54061 2009_10  male      25 \" 20-29\"        309 Black    &lt;NA&gt;     9 - 11th G…\n 4 61420 2009_10  male      27 \" 20-29\"        328 White    &lt;NA&gt;     Some Colle…\n 5 69843 2011_12  male      40 \" 40-49\"         NA Other    Asian    College Gr…\n 6 59826 2009_10  female    67 \" 60-69\"        815 White    &lt;NA&gt;     Some Colle…\n 7 66121 2011_12  male      47 \" 40-49\"         NA White    White    College Gr…\n 8 65735 2011_12  female    69 \" 60-69\"         NA Hispanic Hispanic Some Colle…\n 9 67365 2011_12  female    58 \" 50-59\"         NA Black    Black    Some Colle…\n10 63449 2011_12  male      38 \" 30-39\"         NA Hispanic Hispanic 9 - 11th G…\n# ℹ 67 more variables: MaritalStatus &lt;fct&gt;, HHIncome &lt;fct&gt;, HHIncomeMid &lt;int&gt;,\n#   Poverty &lt;dbl&gt;, HomeRooms &lt;int&gt;, HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;,\n#   Length &lt;dbl&gt;, HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;,\n#   BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, BPSysAve &lt;int&gt;,\n#   BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;,\n#   BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, Testosterone &lt;dbl&gt;, DirectChol &lt;dbl&gt;,\n#   TotChol &lt;dbl&gt;, UrineVol1 &lt;int&gt;, UrineFlow1 &lt;dbl&gt;, UrineVol2 &lt;int&gt;, …\n\n\nIt might be useful to find the mean pulse rate from a random sample of size 10.\n\n\n  response     mean\n1    Pulse 80.88889\n\n\nNow suppose you took another random sample of size 10 and found the mean pulse rate for that sample. Repeat this process 100 times. At this point you would basically have a new sample of 100 mean pulse rates. You could assess how this sample is distributed by creating a density plot Figure 7.21\n\nTrials &lt;- do(100) * { NHANES |&gt; \n    sample_n(size = 10) |&gt;\n    df_stats( ~Pulse, means = mean)}\ngf_density( ~means, data = Trials, title = \"Density plot of sample mean when n=10\")\n\ndf_stats(~means, data=Trials, mean, sd)\n\n  response     mean       sd\n1    means 73.34002 4.429717\n\n\n\n\n\n\n\n\nFigure 7.21: Density Plot of Sample Means When n = 10\n\n\n\n\n\nThis distribution is a sampling distribution. That is all a sampling distribution is. It is a distribution created from statistics.\nNotice the distribution does look a great deal like the distribution of the original random variable. Notice the mean of the sample means \\(\\mu_{\\bar{x}} = 73.8\\) bpm which is almost the same of as the mean of the population. The standard deviation of the sample means, \\(\\sigma_{\\bar{x}}=4.35\\) pbm is about \\(\\frac{1}{3}\\) of the population standard deviation.\nWhat does this distribution look like if instead of repeating the experiment 10 times you repeat it 50 times instead?\nThis density plot of the sampling distribution is displayed in Figure 7.22\n\nTrials &lt;- do(100) * { NHANES |&gt;\n    sample_n(size = 50) |&gt;\n    df_stats( ~Pulse, means = mean) }\ngf_density( ~means, data = Trials, title=\"Sample means when n=50\")|&gt;\n  gf_lims(x=c(68,79))\ndf_stats(~means, data=Trials, mean, sd) \n\n  response     mean      sd\n1    means 73.79208 2.05234\n\n\n\n\n\n\n\n\nFigure 7.22: Density Plot of Sample Means When n = 50\n\n\n\n\n\nNotice this density plot of the sample mean looks approximately symmetrical and could almost be called normal. Notice, the mean of the sample means is 73.6 bpm which is approximately what the population mean is. The standard deviation of the sample means is 1.77 bpm which is around \\(\\frac{1}{7}\\) of the population standard deviation. What if you keep increasing \\(n\\)? What will the sampling distribution of the sample mean look like? In other words, what does the sampling distribution of \\(\\bar{x}\\) look like as \\(n\\) gets even larger?\nThis depends on how the original distribution is distributed. In Example: Sampling Distribution, the random variable was approximately normally distributed. When \\(n\\) was 10, the distribution of the mean looked approximately normal. What if the original distribution wasn’t normal? How big would \\(n\\) have to be? Consider a different variable in the NHANES data frame that isn’t normally distributed such as age when a participant started to smoke cigarettes (SmokeAge). The density plot for the large sample is in Figure 7.23. The mean for the large sample is 17.8 years and the standard deviation is 5.3 years, so \\(\\mu=17.8\\) dollars and \\(\\sigma=5.3\\) dollars approximately.\n\ngf_density(~SmokeAge, data=NHANES, title = \"Density Plot of Age when Person Started Smoking\", xlab=\"Age\") \ndf_stats(~SmokeAge, data=NHANES, mean, sd) \n\n  response     mean      sd\n1 SmokeAge 17.82662 5.32666\n\n\n\n\n\n\n\n\nFigure 7.23: Density Plot of Age When Person Started Smoking.\n\n\n\n\n\nNow take 100 samples of size 50 individuals from the NHANES Data Frame. Then graph a density plot of SmokeAge, the age when someone started to smoke. Notice the the sampling distribution of the sample means looks fairly normally distributed even though the original random variable was not normally distributed. The mean of the sample mean. \\(\\mu_{ \\bar{x}}=\\) 17.8 years and the standard deviation of the sample mean, \\(\\sigma_{\\bar{x}}=\\) 1.56 years. The mean of the sample mean is the same as the mean of the population, but the standard deviation of the sample mean is much less than the standard deviation of the original data.\n\n\n  response     mean       sd\n1    means 17.81021 1.337446\n\n\n\n\n\n\n\n\nFigure 7.24: Density Plot of Age When Person Started Smoking when sample size is 50.\n\n\n\n\n\nOne question is, why is the mean of the sample means the same as the mean of the population? Suppose you have a random variable that has a population mean, \\(\\mu\\), and a population standard deviation, \\(\\sigma\\). If a sample of size \\(n\\) is taken, then the sample mean, has a mean \\(\\mu_{ \\bar{x}}=\\mu\\) and standard deviation of \\(\\sigma_{ \\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}\\) . The standard deviation of the sample mean is lower because by taking the mean you are averaging out the extreme values, which makes the distribution of the sample mean less spread out.\nYou now know the center and the variability of \\(\\bar{x}\\). You also want to know the shape of the distribution of \\(\\bar{x}\\). You hope it is normal, since you know how to find probabilities using the normal curve. The following theorem tells you the requirement to have \\(\\bar{x}\\) be normally distributed.\n\n\n7.4.2 Central Limit Theorem\nSuppose a random variable is from any distribution. If a sample of size \\(n\\) is taken, the the sample mean, \\(\\bar{x}\\), becomes normally distributed as \\(n\\) increases.\nWhat this says is that no matter what \\(x\\) looks like, \\(\\bar{x}\\) would look normal if \\(n\\) is large enough. Now, what size of \\(n\\) is large enough? That depends on how \\(x\\) is distributed in the first place. If the original random variable is normally distributed, then \\(n\\) just needs to be 2 or more data points. If the original random variable is somewhat mound shaped and symmetrical, then \\(n\\) needs to be greater than or equal to 30. Sometimes the sample size can be smaller, but this is a good general rule to use. The sample size may have to be much larger if the original random variable is really skewed one way or another.\nNow that you know when the sample mean will look like a normal distribution, then you can find the probability related to the sample mean. Remember that the mean of the sample mean is just the mean of the original data (\\(\\mu_{\\bar{x}}=\\mu\\)), but the standard deviation of the sample mean, \\(\\sigma_{\\bar{x}}\\), also known as the standard error of the mean, is actually \\(\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}\\). Make sure you use this in all calculations. If you are using the \\(z\\)-score, the formula when working with \\(\\bar{x}\\) is \\(z=\\frac{x-\\mu_{\\bar{x}}}{\\sigma_{\\bar{x}}}=\\frac{x-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\). To use rStudio to calculate probabilities use \\(P(\\bar{x}&lt;a)= pnorm(a, \\mu_{\\bar{x}}, \\sigma_{\\bar{x}}, lower.tail=TRUE)\\) \\(P(\\bar{x}&gt;a)= pnorm(a, \\mu_{\\bar{x}}, \\sigma_{\\bar{x}}, lower.tail=FALSE)\\).\n\n\n7.4.3 Example: Finding Probabilities for Sample Means\nThe birth weight of boy babies of European descent who were delivered at 40 weeks is normally distributed with a mean of 3687.6 g with a standard deviation of 410.5 g (Janssen, Thiessen, Klein, Whitfield, MacNab & Cullis-Kuhl, 2007). Suppose there were nine European descent boy babies born on a given day and the mean birth weight is calculated.\n\nState the random variable.\nWhat is the mean of the sample mean?\nWhat is the standard deviation of the sample mean?\nWhat distribution is the sample mean distributed as?\nFind the probability that the mean weight of the nine boy babies born was less than 3500.4 g.\nFind the probability that the mean weight of the nine babies born was less than 3452.5 g.\n\n\n7.4.3.1 Solution\n\nState the random variable.\n\n\\(x\\) = birth weight of boy babies (Note: the random variable is something you measure, and it is not the mean birth weight. Mean weight is calculated.)\n\nWhat is the mean of the sample mean?\n\n\\(\\mu_{\\bar{x}}=\\mu=3687.4g\\)\n\nWhat is the standard deviation of the sample mean?\n\n\\(\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}=\\frac{410.5g}{\\sqrt{9}}=136.8g\\)\n\nWhat distribution is the sample mean distributed as?\n\nSince the original random variable is distributed normally, then the sample mean is distributed normally.\n\nFind the probability that the mean weight of the nine boy babies born was less than 3500.4 g.\n\nTo find \\(P(\\bar{x}&lt;3500.4)=0.086\\). use the rStudio command\n\npnorm(3500.4,3687.6, 410.5/sqrt(9), lower.tail = TRUE)\n\n[1] 0.08564231\n\n\nThere is an 8.6% chance that the mean birth weight of the nine boy babies born would be less than 3500.4 g. Since this is more than 5%, this is not unusual.\n\nFind the probability that the mean weight of the nine babies born was less than 3452.5 g.\n\nYou are looking for the \\(P(\\bar{x}&lt;3452.5)\\).\nTo find in rStudio, \\(P(\\bar{x}&lt;3452.5)=0.043\\) use the command\n\npnorm(3452.5, 3687.4, 410.5/sqrt(9), lower.tail = TRUE)\n\n[1] 0.04301819\n\n\nThere is a 4.3% chance that the mean birth weight of the nine boy babies born would be less than 3452.5 g. Since this is less than 5% this would be an unusual event. If it actually happened, then you may think there is something unusual about this sample. Maybe some of the nine babies were born as multiples, which brings the mean weight down, or some or all of the babies were not of European descent (in fact the mean weight of South Asian boy babies is 3452.5 g), or some were born before 40 weeks, or the babies were born at high altitudes.\n\n\n\n7.4.4 Example: Finding Probabilities for Sample Means\nFor Americans that smoke, the average age that they started smoking is 17.8 years, with a standard deviation of approximately 1.56 years from the NHANES data. This random variable is not normally distributed, though it is somewhat mound shaped.\n\nState the random variable.\nSuppose a sample of 35 smoking American’s is taken. Find the probability that the mean age that these 35 smoking Americans started to smoke is more than 21 years.\n\n\n7.4.4.1 Solution\n\nState the random variable.\n\n\\(x\\) = age that smoking Americans started to smoke\n\nSuppose a sample of 35 smoking American’s is taken. Find the probability that the mean age that these 35 smoking Americans started to smoke is more than 21 years.\n\nEven though the original random variable is not normally distributed, the sample size is over 30, by the central limit theorem the sample mean will be normally distributed. The mean of the sample mean is \\(\\mu_{\\bar{x}}=17.8\\). The standard deviation of the sample mean is \\(\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}=\\frac{1.56}{\\sqrt{35}}\\). You have all the information you need to use the normal command using rStudio. Without the central limit theorem, you couldn’t use the normal command, and you would not be able to answer this question.\nThe probability that the mean age that 35 smoking Americans start to smoke is more than 21 years, is the mathematical statement \\(P(\\bar{x}&gt;21)\\)\nTo find \\(P(\\bar{x}&gt;21)= 3.42X10^{-34}\\) using r studio, use the command:\n\npnorm(21, 17.8, 1.56/sqrt(35), lower.tail=FALSE) \n\n[1] 3.422499e-34\n\n\nThe probability of a sample mean of 35 smoking Americans being more than 21 years when they smoked for the first time is very small. This is extremely unlikely to happen. If it does, it may make you wonder about the sample. Could the population mean have increased from the 17.8 years as was stated? Could the sample not have been random, and instead have been a group of smoking Americans who had started to smoke much later? These questions, and more, are ones that you would want to ask as a researcher\n\n\n\n7.4.5 Homework for Sampling Distribution and the Central Limit Theorem Section\n\nA random variable is not normally distributed, but it is mound shaped. It has a mean of 14 and a standard deviation of 3.\n\n\n\nIf you take a sample of size 10, can you say what the shape of the sampling distribution for the sample mean is? Why?\nFor a sample of size 10, state the mean of the sample mean and the standard deviation of the sample mean.\nIf you take a sample of size 35, can you say what the shape of the distribution of the sample mean is? Why?\nFor a sample of size 35, state the mean of the sample mean and the standard deviation of the sample mean.\n\n\n\nA random variable is normally distributed. It has a mean of 245 and a standard deviation of 21.\n\n\n\nIf you take a sample of size 10, can you say what the shape of the distribution for the sample mean is? Why?\nFor a sample of size 10, state the mean of the sample mean and the standard deviation of the sample mean.\nFor a sample of size 10, find the probability that the sample mean is more than 241.\nIf you take a sample of size 35, can you say what the shape of the distribution of the sample mean is? Why?\nFor a sample of size 35, state the mean of the sample mean and the standard deviation of the sample mean.\nFor a sample of size 35, find the probability that the sample mean is more than 241.\nCompare your answers in part c and f. Why is one smaller than the other?\n\n\n\nThe mean starting salary for nurses is \\$67,694 nationally (“Staff nurse -,” 2013). The standard deviation is approximately $10,333. The starting salary is not normally distributed but it is mound shaped. A sample of 42 starting salaries for nurses is taken.\n\n\n\nState the random variable.\nWhat is the mean of the sample mean?\nWhat is the standard deviation of the sample mean?\nWhat is the shape of the sampling distribution of the sample mean? Why?\nFind the probability that the sample mean is more than \\$75,000.\nFind the probability that the sample mean is less than \\$60,000.\nIf you did find a sample mean of more than \\$75,000 would you find that unusual? What could you conclude?\n\n\n\nAccording to the WHO MONICA Project the mean blood pressure for people in China is 128 mmHg with a standard deviation of 23 mmHg (Kuulasmaa, Hense & Tolonen, 1998). Blood pressure is normally distributed.\n\n\n\nState the random variable.\nSuppose a sample of size 15 is taken. State the shape of the distribution of the sample mean.\nSuppose a sample of size 15 is taken. State the mean of the sample mean.\nSuppose a sample of size 15 is taken. State the standard deviation of the sample mean.\nSuppose a sample of size 15 is taken. Find the probability that the sample mean blood pressure is more than 135 mmHg.\nWould it be unusual to find a sample mean of 15 people in China of more than 135 mmHg? Why or why not?\nIf you did find a sample mean for 15 people in China to be more than 135 mmHg, what might you conclude?\n\n\n\nThe size of fish is very important to commercial fishing. A study conducted in 2012 found the length of Atlantic cod caught in nets in Karlskrona to have a mean of 49.9 cm and a standard deviation of 3.74 cm (Ovegard, Berndt & Lunneryd, 2012). The length of fish is normally distributed. A sample of 15 fish is taken.\n\n\n\nState the random variable.\nFind the mean of the sample mean.\nFind the standard deviation of the sample mean\nWhat is the shape of the distribution of the sample mean? Why?\nFind the probability that the sample mean length of the Atlantic cod is less than 52 cm.\nFind the probability that the sample mean length of the Atlantic cod is more than 74 cm.\nIf you found sample mean length for Atlantic cod to be more than 74 cm, what could you conclude?\n\n\n\nThe mean cholesterol levels of women age 45-59 in Ghana, Nigeria, and Seychelles is 5.1 mmol/l and the standard deviation is 1.0 mmol/l (Lawes, Hoorn, Law & Rodgers, 2004). Assume that cholesterol levels are normally distributed.\n\n\n\nState the random variable.\nFind the probability that a woman age 45-59 in Ghana has a cholesterol level above 6.2 mmol/l (considered a high level).\nSuppose doctors decide to test the woman’s cholesterol level again and average the two values. Find the probability that this woman’s mean cholesterol level for the two tests is above 6.2 mmol/l.\nSuppose doctors being very conservative decide to test the woman’s cholesterol level a third time and average the three values. Find the probability that this woman’s mean cholesterol level for the three tests is above 6.2 mmol/l.\nIf the sample mean cholesterol level for this woman after three tests is above 6.2 mmol/l, what could you conclude?\n\n\n\nIn the United States, males between the ages of 40 and 49 eat on average 103.1 g of fat every day with a standard deviation of 4.32 g (“What we eat,” 2012). The amount of fat a person eats is not normally distributed but it is relatively mound shaped.\n\n\n\nState the random variable.\nFind the probability that a sample mean amount of daily fat intake for 35 men age 40-59 in the U.S. is more than 100 g.\nFind the probability that a sample mean amount of daily fat intake for 35 men age 40-59 in the U.S. is less than 93 g.\nIf you found a sample mean amount of daily fat intake for 35 men age 40-59 in the U.S. less than 93 g, what would you conclude?\n\n\n\nA dishwasher has a mean life of 12 years with an estimated standard deviation of 1.25 years (“Appliance life expectancy,” 2013). The life of a dishwasher is normally distributed. Suppose you are a manufacturer and you take a sample of 10 dishwashers that you made.\n\n\n\nState the random variable.\nFind the mean of the sample mean.\nFind the standard deviation of the sample mean.\nWhat is the shape of the sampling distribution of the sample mean? Why?\nFind the probability that the sample mean of the dishwashers is less than 6 years.\nIf you found the sample mean life of the 10 dishwashers to be less than 6 years, would you think that you have a problem with the manufacturing process? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "One Sample Inference.html",
    "href": "One Sample Inference.html",
    "title": "8  One Sample Inference",
    "section": "",
    "text": "8.1 Basics of Hypothesis Testing\nNow that you have all this information about descriptive statistics and probabilities, it is time to start inferential statistics. There are two branches of inferential statistics: hypothesis testing and confidence intervals.\nHypothesis Testing: making a decision about a parameter(s) based on a statistic(s).\nConfidence Interval: estimating a parameter(s) based on a statistic(s).\nThis chapter will describe hypothesis testing, but as was stated in Chapter 1, the American Statistical Association (ASA) is suggesting not discussing statistical significance and p-values. So this chapter is mostly for background to understand previously published studies.\nTo understand the process of a hypothesis tests, you need to first have an understanding of what a hypothesis is, which is an educated guess about a parameter. Once you have the hypothesis, you collect data and use the data to make a determination to see if there is enough evidence to show that the hypothesis is true. However, in hypothesis testing you actually assume something else is true, and then you look at your data to see how likely it is to get an event that your data demonstrates with that assumption. If the event is very unusual, then you might think that your assumption is actually false. If you are able to say this assumption is false, then your hypothesis must be true. This is known as a proof by contradiction. You assume the opposite of your hypothesis is true and show that it can’t be true. If this happens, then your hypothesis must be true. All hypothesis tests go through the same process. Once you have the process down, then the concept is much easier. It is easier to see the process by looking at an example. Concepts that are needed will be detailed in this example.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>One Sample Inference</span>"
    ]
  },
  {
    "objectID": "One Sample Inference.html#basics-of-hypothesis-testing",
    "href": "One Sample Inference.html#basics-of-hypothesis-testing",
    "title": "8  One Sample Inference",
    "section": "",
    "text": "8.1.1 Example: Basics of Hypothesis Testing\nSuppose a manufacturer of the XJ35 battery claims the mean life of the battery is 500 days with a standard deviation of 25 days. You are the buyer of this battery and you think this claim is incorrect. You would like to test your belief because without a good reason you can’t get out of your contract.\n\n8.1.1.1 Solution\nWhat do you do?\nWell first, you should know what you are trying to measure. Define the random variable.\nLet \\(x\\) = life of a XJ35 battery\nNow you are not just trying to find different $x$ values. You are trying to find what the true mean is. Since you are trying to find it, it must be unknown. You don’t think it is 500 days. If you did, you wouldn’t be doing any testing. The true mean, \\(\\mu\\), is unknown. That means you should define that too.\nLet \\(\\mu\\) = mean life of a XJ35 battery\nNow what?\nYou may want to collect a sample. What kind of sample?\nYou could ask the manufacturers to give you batteries, but there is a chance that there could be some bias in the batteries they pick. To reduce the chance of bias, it is best to take a random sample.\nHow big should the sample be?\nA sample of size 30 or more means that you can use the central limit theorem. Pick a sample of size 50.\nTable 8.1 contains the data for the sample you collected:\n\nBattery&lt;- read.csv( \"https://krkozak.github.io/MAT160/battery.csv\") \nknitr::kable(head(Battery))\n\n\n\nTable 8.1: Data on Battery Life\n\n\n\n\n\n\nlife\n\n\n\n\n491\n\n\n485\n\n\n503\n\n\n492\n\n\n482\n\n\n490\n\n\n\n\n\n\n\n\nNow what should you do? Looking at the data set, you see some of the times are above 500 and some are below. But looking at all of the numbers is too difficult. It might be helpful to calculate the mean for this sample.\n\ndf_stats(~life, data=Battery, mean)\n\n  response mean\n1     life  490\n\n\nThe sample mean is 491.42 days. Looking at the sample mean, one might think that you are right. However, the standard deviation and the sample size also plays a role, so maybe you are wrong.\nBefore going any farther, it is time to formalize a few definitions.\nYou have a guess that the mean life of a battery is not 500 days. This is opposed to what the manufacturer claims. There really are two hypotheses, which are just guesses here — the one that the manufacturer claims and the one that you believe. It is helpful to have names for them.\nNull Hypothesis: historical value, claim, or product specification. The symbol used is \\(H_o\\).\nAlternate Hypothesis: what you want to prove. This is what you want to accept as true when you reject the null hypothesis. There are two symbols that are commonly used for the alternative hypothesis: \\(H_a\\) or \\(H_1\\). The symbol \\(H_a\\) will be used in this book.\nIn general, the hypotheses look something like this:\n\\(H_0:\\mu=\\mu_o\\)\n\\(H_a:\\mu\\ne \\mu_o\\)\nwhere \\(\\mu_o\\) just represents the value that the claim says the population mean is actually equal to.\nAlso, \\(H_a\\) can be less than, greater than, or not equal to, though not equal to is more common these days.\nFor this problem:\n\\(H_o:\\mu=500\\text{ days}\\), since the manufacturer says the mean life of a battery is 500 days.\n\\(H_a:\\mu\\ne 500\\text{ days}\\), since you believe that the mean life of the battery is not 500 days.\nNow back to the mean. You have a sample mean of 491.42 days. Is this different enough to believe that you are right and the manufacturer is wrong? How different does it have to be?\nIf you calculated a sample mean of 235 or 690, you would definitely believe the population mean is not 500. But even if you had a sample mean of 435 or 575 you would probably believe that the true mean was not 500. What about 475? or 535? Or 483? or 514? There is some point where you would stop being so sure that the population mean is not 500. That point separates the values of where you are sure or pretty sure that the mean is not 500 from the area where you are not so sure. How do you find that point?\nWell it depends on how much error you want to make. Of course you don’t want to make any errors, but unfortunately that is unavoidable in statistics. You need to figure out how much error you made with your sample. Take the sample mean, and find the probability of getting another sample mean less than it, assuming for the moment that the manufacturer is right. The idea behind this is that you want to know what is the chance that you could have come up with your sample mean even if the population mean really is 500 days.\nChances are probabilities. So you want to find the probability that the sample mean of 491.42 is unusual given that the population mean is really 500 days. To compute this probability, you need to know how the sample mean is distributed. Since the sample size is at least 30, then you know the sample mean is approximately normally distributed. Now, you want to find the \\(z\\)-value. The \\(z\\)-value is \\(z=\\frac{491.42-500}{\\frac{25}{\\sqrt{50}}}=-2.43\\).\nThis is more than 2 standard deviations below the mean, so that seems that the sample mean is usual. It might be helpful to find the probability though. Since you are saying that the sample mean is different from 500 days, then you are asking if it is greater than or less than. This means that you are in the tails of the normal curve. So the probability you want to find is the probability being more than 2.43 or less than \\(-2.43\\). This is \\(P(-2.43&lt;z)+P(z&gt;2.43)=0.015\\)\n\npnorm(-2.43, 0, 1, lower.tail=TRUE)+pnorm(2.43, 0, 1, lower.tail=FALSE) \n\n[1] 0.01509882\n\n\nSo the probability of being in the tails is 0.015. This probability is known as a p-value for probability-value. This is unusual, so it is unlikely to get a sample mean of 491.42 if the population mean is 500 days.\nSo it appears the assumption that the population mean is 500 days is wrong, and you can reject the manufacturer’s claim.\nBut how do you quantify really small? Is 5% or 10% or 15% really small? How do you decide?\nBefore you answer that question, a couple more definitions are needed.\nTest statistic: \\(z=\\frac{\\bar{x}-\\mu_o}{\\frac{\\sigma}{\\sqrt{n}}}\\) since it is calculated as part of the testing of the hypothesis\np - value: probability that the test statistic will take on more extreme values than the observed test statistic, given that the null hypothesis is true. It is the probability that was calculated above.\nNow, how small is small enough? To answer that, you really want to know the types of errors you can make.\nThere are actually only two errors that can be made. The first error is if you say that is false, when in fact it is true. This means you reject when was true. The second error is if you say that is true, when in fact it is false. This means you fail to reject when is false. The following table organizes this for you:\n\n\n\n8.1.2 Type of errors:\n\nType of errors\n\n\n\nHo true\nHo false\n\n\n\n\nReject Ho\nType I error\nno error\n\n\nFail to reject Ho\nno error\nType II error\n\n\n\nThus\nType I Error is rejecting \\(H_o\\) when \\(H_o\\) is true, and\nType II Error is failing to reject \\(H_o\\) when is \\(H_o\\) false.\nSince these are the errors, then one can define the probabilities attached to each error.\n\\(\\alpha\\)= P(type I error) = P(rejecting$H_o$ given it is true)\n\\(\\beta\\)= P(type II error) = P(failing to reject$H_o$ given it is false)\n\\(\\alpha\\) is also called the level of significance.\nAnother common concept that is used is Power = \\(1-\\beta\\)\nNow there is a relationship between \\(\\alpha\\) and \\(\\beta\\). They are not complements of each other. How are they related?\nIf \\(\\alpha\\) increases that means the chances of making a type I error will increase. It is more likely that a type I error will occur. It makes sense that you are less likely to make type II errors, only because you will be rejecting more often. You will be failing to reject less, and therefore, the chance of making a type II error will decrease. Thus, as \\(\\alpha\\) increases, \\(\\beta\\) will decrease, and vice versa. That makes them seem like complements, but they aren’t complements. What gives? Consider one more factor -- sample size.\nConsider if you have a larger sample that is representative of the population, then it makes sense that you have more accuracy then with a smaller sample. Think of it this way, which would you trust more, a sample mean of 490 if you had a sample size of 35 or sample size of 350 (assuming a representative sample)? Of course the 350 because there are more data points and so more accuracy. If you are more accurate, then there is less chance that you will make any error. By increasing the sample size of a representative sample, you decrease both \\(\\alpha\\) and \\(\\beta\\).\nSummary of all of this:\n\nFor a certain sample size, \\(\\alpha\\) increases, \\(\\beta\\) decreases.\nFor a certain level of significance, \\(\\alpha\\), if \\(n\\) increases, \\(\\beta\\) decreases.\n\nNow how do you find \\(\\alpha\\) and \\(\\beta\\)? Well \\(\\alpha\\) is actually chosen. There are only two values that are usually picked for \\(\\alpha\\): 0.01 and 0.05. is very difficult to find \\(\\beta\\), so usually it isn’t found. If you want to make sure it is small you take as large of a sample as you can afford provided it is a representative sample. This is one use of the Power. You want to be small and the Power of the test is large. The Power word sounds good.\nWhich pick of \\(\\alpha\\) do you pick? Well that depends on what you are working on. Remember in this example you are the buyer who is trying to get out of a contract to buy these batteries. If you create a type I error, you said that the batteries are bad when they aren’t, most likely the manufacturer will sue you. You want to avoid this. You might pick \\(\\alpha\\) to be 0.01. This way you have a small chance of making a type I error. Of course this means you have more of a chance of making a type II error. No big deal right? What if the batteries are used in pacemakers and you tell the person that their pacemaker’s batteries are good for 500 days when they actually last less, that might be bad. If you make a type II error, you say that the batteries do last 500 days when they last less, then you have the possibility of killing someone. You certainly do not want to do this. In this case you might want to pick \\(\\alpha\\) as 0.05. If both errors are equally bad, then pick \\(\\alpha\\) as 0.05.\nThe above discussion is why the choice of depends on what you are researching. As the researcher, you are the one that needs to decide what level to use based on your analysis of the consequences of making each error is.\nIf a type I error is really bad, then pick \\(\\alpha\\)= 0.01.\nIf a type II error is really bad, then pick \\(\\alpha\\)= 0.05\nIf neither error is bad, or both are equally bad, then pick \\(\\alpha\\) = 0.05\nUsually \\(\\alpha\\) is picked to be 0.05 in most cases.\nThe main thing is to always pick the \\(\\alpha\\) before you collect the data and start the test.\nThe above discussion was long, but it is really important information. If you don’t know what the errors of the test are about, then there really is no point in making conclusions with the tests. Make sure you understand what the two errors are and what the probabilities are for them.\nNow it is time to go back to the example and put this all together. This is the basic structure of testing a hypothesis, usually called a hypothesis test. Since this one has a test statistic involving \\(z\\), it is also called a \\(z\\)-test. And since there is only one sample, it is usually called a one-sample \\(z\\)-test.\n\n\n8.1.3 Example: Battery Example Revisited.\nSteps of a hypothesis test:\n\nState the random variable and the parameter in words\nState the null and alternative hypothesis and the level of significance\nState and check the conditions for a hypothesis test\nFind the sample statistic, test statistic, and p-value\nConclusion:\nInterpretation:\n\n\n8.1.3.1 Solution\n\nState the random variable and the parameter in words\n\n\\(x\\) = life of battery\n\\(\\mu\\) = mean life of a XJ35 battery\n\nState the null and alternative hypothesis and the level of significance\n\n\\(H_o:\\mu=500\\)\n\\(H_a:\\mu\\ne500\\)\n\\(\\alpha\\) = 0.05 (from above discussion about consequences)\n\nState and check the conditions for a hypothesis test\n\nEvery hypothesis has some conditions that be met to make sure that the results of the hypothesis are valid. The conditions are different for each test. This test has the following conditions.\n\nA random sample of size \\(n\\) is taken.\n\nThis occurred in this example, since it was stated that a random sample of 50 battery lives were taken.\n\nThe population standard deviation is known.\n\nThis is true, since it was given in the problem.\n\nThe sample size is at least 30 or the population of the random variable is normally distributed.\n\nThe sample size was 30, so this condition is met.\n\nFind the sample statistic, test statistic, and p-value\n\nThe test statistic depends on how many samples there are, what parameter you are testing, and conditions that need to be checked. In this case, there is one sample and you are testing the mean. The conditions were checked above.\nSample statistic:\n\ndf_stats(~life, data=Battery, mean)\n\n  response mean\n1     life  490\n\n\nTest statistic: The z-value is \\(z=\\frac{491.42-400}{\\frac{25}{\\sqrt{n}}}=-2.43\\).\np-value: \\(P(-2.43&lt;z)+P(z&gt;2.43)=0.015\\)\n\nConclusion:\n\nNow what? Well, this p-value is 0.015. This is a lot smaller than the amount of error you would accept in the problem \\(\\alpha\\) = 0.05. That means that finding a sample mean less than 490 days is unusual to happen if is true. This should make you think that is not true. You should reject \\(H_o\\).\nIn fact, in general:\nReject \\(H_o\\) if the p-value \\(&lt;\\alpha\\)\nFail to reject \\(H_o\\) if the p-value \\(\\ge\\alpha\\).\n\nInterpretation:\n\nSince you rejected \\(H_o\\), what does this mean in the real world? That it what goes in the interpretation. Since you rejected the claim by the manufacturer that the mean life of the batteries is 500 days, then you now can believe that your hypothesis was correct. In other words, there is enough evidence to support that the mean life of the battery is less than 500 days.\nNow that you know that the batteries last less than 500 days, should you cancel the contract? Statistically, there is evidence that the batteries do not last as long as the manufacturer says they should. However, based on this sample there are only ten days less on average that the batteries last. There may not be practical significance in this case. Ten days do not seem like a large difference. In reality, if the batteries are used in pacemakers, then you would probably tell the patient to have the batteries replaced every year. You have a large buffer whether the batteries last 490 days or 500 days. It seems that it might not be worth it to break the contract over ten days. What if the 10 days was practically significant? Are there any other things you should consider? You might look at the business relationship with the manufacturer. You might also look at how much it would cost to find a new manufacturer. These are also questions to consider before making any changes. What this discussion should show you is that just because a hypothesis has statistical significance does not mean it has practical significance. The hypothesis test is just one part of a research process. There are other pieces that you need to consider.\nThat’s it. That is what a hypothesis test looks like. All hypothesis tests are done with the same six steps. Those general six steps are outlined below.\n\n\n\n8.1.4 Steps for hypothesis test\n\nState the random variable and the parameter in words. This is where you are defining what the unknowns are in this problem.\n\n\\(x\\) = random variable\n\\(\\mu\\) = mean of random variable, if the parameter of interest is the mean. There are other parameters you can test, and you would use the appropriate symbol for that parameter.\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu=\\mu_o\\), where \\(\\mu_o\\) is the known mean\n\\(H_a:\\mu\\ne\\mu_o\\), You can replace \\(\\ne\\) with \\(&lt;\\) or \\(&gt;\\) but usually you use \\(\\ne\\)\nAlso, state your level here.\n\nState and check the conditions for a hypothesis test\n\nEach hypothesis test has its own conditions. They will be stated when the different hypothesis tests are discussed.\n\nFind the sample statistic, test statistic, and p-value\n\nThis depends on what parameter you are working with, how many samples, and the conditions of the test. Technology will be used to find the sample statistic, test statistic, and p-value.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge\\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\nSorry, one more concept about the conclusion and interpretation. First, the conclusion is that you reject or you fail to reject $H_o$. Why was it said like this? It is because you never accept the null hypothesis. If you wanted to accept the null hypothesis, then why do the test in the first place? In the interpretation, you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\). You wouldn’t want to go to all this work and then find out you wanted to accept the claim. Why go through the trouble? You always want to have enough evidence to support the alternative hypothesis. Sometimes you can do that and sometimes you can’t. If you don’t have enough evidence to support \\(H_a\\), it doesn’t mean you support the null hypothesis; it just means you can’t support the alternative hypothesis. Here is an example to demonstrate this.\n\n\n8.1.5 Example: Conclusions in Hypothesis Tests\nIn the U.S. court system a jury trial could be set up as a hypothesis test. To really help you see how this works, let’s use OJ Simpson as an example. In the court system, a person is presumed innocent until he/she is proven guilty, and this is your null hypothesis. OJ Simpson was a football player in the 1970s. In 1994 his ex-wife and her friend were killed. OJ Simpson was accused of the crime, and in 1995 the case was tried. The prosecutors wanted to prove OJ was guilty of killing his wife and her friend, and that is the alternative hypothesis. In this case, a verdict of not guilty was given. That does not mean that he is innocent of this crime. It means there was not enough evidence to prove he was guilty. Many people believe that OJ was guilty of this crime, but the jury did not feel that the evidence presented was enough to show there was guilt. The verdict in a jury trial is always guilty or not guilty!\nThe same is true in a hypothesis test. There is either enough or not enough evidence to support the alternative hypothesis. It is not that you proved the null hypothesis true.\nWhen identifying hypothesis, it is important to state your random variable and the appropriate parameter you want to make a decision about. If you count something, then the random variable is the number of whatever you counted. The parameter is the proportion of what you counted. If the random variable is something you measured, then the parameter is the mean of what you measured. (Note: there are other parameters you can calculate, and some analysis of those will be presented in later chapters.)\n\n\n8.1.6 Example: Stating Hypotheses\nIdentify the hypotheses necessary to test the following statements:\n\nThe average salary of a teacher is different from \\$30,000.\nThe proportion of students who like math is not 10%.\nThe average age of students in this class differs from 21.\n\n\n8.1.6.1 Solution\n\nThe average salary of a teacher is different from \\$30,000.\n\n\\(x\\) = salary of teacher\n\\(\\mu=\\) mean salary of teacher\nThe guess is that \\(\\mu\\ne30000\\) and that is the alternative hypothesis.\nThe null hypothesis has the same parameter and number with an equal sign.\n\\(H_o:\\mu=30000\\) \\(H_a:\\mu\\ne30000\\)\n\nThe proportion of students who like math is not 10%.\n\n\\(x\\) = number of students who like math\n\\(p\\) = proportion of students who like math\nThe guess is that \\(p\\) is not 0.10 and that is the alternative hypothesis. \\(H_a:p\\ne0.10\\) and the null hypothesis would be \\(H_o:p=0.10\\)\n\nThe average age of students in this class differs from 21.\n\n\\(x\\) = age of students in this class\n\\(\\mu\\)=mean age of students in this class\nThe guess is that \\(\\mu\\ne21\\) and that is the alternative hypothesis. \\(H_a:\\mu\\ne21\\) and the null hypothesis would be \\(H_o: \\mu=21\\)\n\n\n\n8.1.7 Example: Stating Type I and II Errors and Picking Level of Significance\n\nThe plant-breeding department at a major university developed a new hybrid raspberry plant called YumYum Berry. Based on research data, the claim is made that from the time shoots are planted 90 days on average are required to obtain the first berry with a standard deviation of 9.2 days. A corporation that is interested in marketing the product tests 60 shoots by planting them and recording the number of days before each plant produces its first berry. The sample mean is 92.3 days. The corporation wants to know if the mean number of days is more than the 90 days claimed. State the type I and type II errors in terms of this problem, consequences of each error, and state which level of significance to use.\nA concern was raised in Australia that the percentage of deaths of Aboriginal prisoners was higher than the percent of deaths of non-indigenous prisoners, which is 0.27%. State the type I and type II errors in terms of this problem, consequences of each error, and state which level of significance to use.\n\n\n8.1.7.1 Solution\n\nThe plant-breeding department at a major university developed a new hybrid raspberry plant called YumYum Berry. Based on research data, the claim is made that from the time shoots are planted 90 days on average are required to obtain the first berry with a standard deviation of 9.2 days. A corporation that is interested in marketing the product tests 60 shoots by planting them and recording the number of days before each plant produces its first berry. The sample mean is 92.3 days. The corporation wants to know if the mean number of days is more than the 90 days claimed. State the type I and type II errors in terms of this problem, consequences of each error, and state which level of significance to use.\n\n\\(x\\) = time to first berry for YumYum Berry plant\n\\(\\mu\\)= mean time to first berry for YumYum Berry plant\nType I Error: If the corporation does a type I error, then they will say that the plants take longer to produce than 90 days when they don’t. They probably will not want to market the plants if they think they will take longer. They will not market them even though in reality the plants do produce in 90 days. They may have loss of future earnings, but that is all.\nType II error: The corporation do not say that the plants take longer then 90 days to produce when they do take longer. Most likely they will market the plants. The plants will take longer, and so customers might get upset and then the company would get a bad reputation. This would be really bad for the company.\nLevel of significance: It appears that the corporation would not want to make a type II error. Pick a 5% level of significance, \\(\\alpha=0.05\\).\n\nA concern was raised in Australia that the percentage of deaths of Aboriginal prisoners was higher than the percent of deaths of non-indigenous prisoners, which is 0.27%. State the type I and type II errors in terms of this problem, consequences of each error, and state which level of significance to use.\n\n\\(x\\) = number of Aboriginal prisoners who have died\n\\(p\\) = proportion of Aboriginal prisoners who have died\nType I error: Rejecting that the proportion of Aboriginal prisoners who died was 0.27%, when in fact it was 0.27%. This would mean you would say there is a problem when there isn’t one. You could anger the Aboriginal community, and spend time and energy researching something that isn’t a problem.\nType II error: Failing to reject that the proportion of Aboriginal prisoners who died was 0.27%, when in fact it is higher than 0.27%. This would mean that you wouldn’t think there was a problem with Aboriginal prisoners dying when there really is a problem. You risk causing deaths when there could be a way to avoid them.\nLevel of significance: It appears that both errors may be issues in this case. You wouldn’t want to anger the Aboriginal community when there isn’t an issue, and you wouldn’t want people to die when there may be a way to stop it. It may be best to pick a 5% level of significance, \\(\\alpha=0.05\\).\nHint -- hypothesis testing is really easy if you follow the same recipe every time. The only differences in the various problems are the conditions of the test and the test statistic you calculate so you can find the p-value. Do the same steps, in the same order, with the same words, every time and these problems become very easy.\n\n\n\n8.1.8 Homework for Basics of Hypothesis Testing Section\nFor the problems in this section, a question is being asked. This is to help you understand what the hypotheses are. You are not to run any hypothesis tests nor come up with any conclusions in this section.\n\nThe Arizona Republic/Morrison/Cronkite News poll published on Monday, October 20, 2016, found 390 of the registered voters surveyed favor Proposition 205, which would legalize marijuana for adults. The statewide telephone poll surveyed 779 registered voters between Oct. 10 and Oct. 15. (Sanchez, 2016) Fifty-five percent of Colorado residents supported the legalization of marijuana. Does the data provide evidence that the percentage of Arizona residents who support legalization of marijuana is different from the proportion of Colorado residents who support it? State the random variable, population parameter, and hypotheses.\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, 23% of all complaints in 2007 were for identity theft. In that year, Alaska had 321 complaints of identity theft out of 1,432 consumer complaints (\\“Consumer fraud and,\\” 2008). Does this data provide enough evidence to show that Alaska had a different proportion of identity theft than 23%? State the random variable, population parameter, and hypotheses.\nThe Kyoto Protocol was signed in 1997, and required countries to start reducing their carbon emissions. The protocol became enforceable in February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per capita. Is there enough evidence to show that the mean CO2 emission is different in 2010 than in 2004? State the random variable, population parameter, and hypotheses.\nThe FDA regulates that fish that is consumed is allowed to contain 1.0 mg/kg of mercury. In Florida, bass fish were collected in 53 different lakes to measure the amount of mercury in the fish. Do the data provide enough evidence to show that the fish in Florida lakes has a different amount of mercury than the allowable amount? State the random variable, population parameter, and hypotheses.\nThe Arizona Republic/Morrison/Cronkite News poll published on Monday, October 20, 2016, found 390 of the registered voters surveyed favor Proposition 205, which would legalize marijuana for adults. The statewide telephone poll surveyed 779 registered voters between Oct. 10 and Oct. 15. (Sanchez, 2016) Fifty-five percent of Colorado residents supported the legalization of marijuana. Does the data provide evidence that the percentage of Arizona residents who support legalization of marijuana is different from the proportion of Colorado residents who support it. State the type I and type II errors in this case, consequences of each error type for this situation from the perspective of the manufacturer, and the appropriate alpha level to use. State why you picked this alpha level.\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, 23% of all complaints in 2007 were for identity theft. In that year, Alaska had 321 complaints of identity theft out of 1,432 consumer complaints (\\“Consumer fraud and,\\” 2008). Does this data provide enough evidence to show that Alaska had a different proportion of identity theft than 23%? State the type I and type II errors in this case, consequences of each error type for this situation from the perspective of the state of Alaska, and the appropriate alpha level to use. State why you picked this alpha level.\nThe Kyoto Protocol was signed in 1997, and required countries to start reducing their carbon emissions. The protocol became enforceable in February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per capita. Is there enough evidence to show that the mean CO2 emission is lower in 2010 than in 2004? State the type I and type II errors in this case, consequences of each error type for this situation from the perspective of the agency overseeing the protocol, and the appropriate alpha level to use. State why you picked this alpha level.\nThe FDA regulates that fish that is consumed is allowed to contain 1.0 mg/kg of mercury. In Florida, bass fish were collected in 53 different lakes to measure the amount of mercury in the fish. Do the data provide enough evidence to show that the fish in Florida lakes has different amount of mercury than the allowable amount? State the type I and type II errors in this case, consequences of each error type for this situation from the perspective of the FDA, and the appropriate alpha level to use. State why you picked this alpha level.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>One Sample Inference</span>"
    ]
  },
  {
    "objectID": "One Sample Inference.html#one-sample-proportion-test",
    "href": "One Sample Inference.html#one-sample-proportion-test",
    "title": "8  One Sample Inference",
    "section": "8.2 One-Sample Proportion Test",
    "text": "8.2 One-Sample Proportion Test\nThere are many different parameters that you can test. There is a test for the mean, such as was introduced with the $z$-test. There is also a test for the population proportion, \\(p\\). This is where you might be curious if the proportion of students who smoke at your school is lower than the proportion in your area. Or you could question if the proportion of accidents caused by teenage drivers who do not have a drivers’ education class is more than the national proportion.\nTo test a population proportion, there are a few things that need to be defined first. Usually, Greek letters are used for parameters and Latin letters for statistics. When talking about proportions, it makes sense to use \\(p\\) for proportion. The Greek letter for \\(p\\) is \\(\\pi\\), but that is too confusing to use. Instead, it is best to use \\(p\\) for the population proportion. That means that a different symbol is needed for the sample proportion. The convention is to use, \\(\\hat{p}\\), known as p-hat. This way you know that \\(p\\) is the population proportion, and that \\(\\hat{p}\\) is the sample proportion related to it.\nNow proportion tests are about looking for the percentage of individuals who have a particular attribute. You are really looking for the number of successes that happen. Thus, a proportion test involves a binomial distribution.\n\n8.2.1 Hypothesis Test for One Population Proportion (1-Prop Test)\n\nState the random variable and the parameter in words.\n\n\\(x\\) = number of successes\n\\(p\\) = proportion of successes\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:p=p_o\\), where \\(p_o\\) is the known proportion\n\\(H_a:p\\ne p_o\\), you can also use &lt; or &gt;, but \\(\\ne\\) is the more common one to use.\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A simple random sample of size \\(n\\) is taken. Check: describe how the sample was collected\nState: The conditions for the binomial experiment are satisfied. Check: Show all four properties are true.\nState: The sampling distribution of \\(\\hat{p}\\) is normally distributed. Check: you need to show that \\(p*n\\ge5\\) and \\(q*n\\ge5\\), where \\(q=1-p\\). If this requirement is true, then the sampling distribution of \\(\\hat{p}\\) is well approximated by a normal curve.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nThis will be computed on r Studio using the command\nprop.test(r, n, p=what_Ho_says)\nwhere \\(r\\)=observed number of successes and \\(n\\) = number of trials.\n\nConclusion\n\nThis is where you write reject or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_0\\). If the p-value \\(\\ge\\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n8.2.2 Example: Hypothesis Test for One Proportion\nA concern was raised in Australia that the percentage of deaths of Aboriginal prisoners was different than the percent of deaths of non-Aboriginal prisoners, which is 0.27%. A sample of six years (1990-1995) of data was collected, and it was found that out of 14,495 Aboriginal prisoners, 51 died (\\“Indigenous deaths in,\\” 1996). Do the data provide enough evidence to show that the proportion of deaths of Aboriginal prisoners is different from 0.27%?\n\n8.2.2.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = number of Aboriginal prisoners who die\n\\(p\\) = proportion of Aboriginal prisoners who die\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:p=0.0027\\)\n\\(H_a:p\\ne0.0027\\)\nFrom Example: Stating Type I and II Errors and Picking Level of Significance part b, the argument was made to pick 5% for the level of significance. So \\(\\alpha=0.05\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nA simple random sample of 14,495 Aboriginal prisoners was taken. Check: The sample was not a random sample, since it was data from six years. It is the numbers for all prisoners in these six years, but the six years were not picked at random. Unless there was something special about the six years that were chosen, the sample is probably a representative sample. This condition is probably met.\nThe properties of a binomial experiment are met. There are 14,495 prisoners in this case. Check: The prisoners are all Aboriginals, so you are not mixing Aboriginal with non-Aboriginal prisoners. There are only two outcomes, either the prisoner dies or doesn’t. The chance that one prisoner dies over another may not be constant, but if you consider all prisoners the same, then it may be close to the same probability. Thus the conditions for the binomial distribution are satisfied\nThe sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distributed. Check: In this case \\(p = 0.0027\\) and \\(n = 14,495\\). \\(n*p=39.1365\\ge5\\) and \\(n*q=14455.86\\ge5\\). So, the sampling distribution for \\(\\hat{p}\\) is normally distributed.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nUse the following command in rStudio:\n\nprop.test(51, 14495, p=0.0027)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  51 out of 14495\nX-squared = 3.3084, df = 1, p-value = 0.06893\nalternative hypothesis: true p is not equal to 0.0027\n95 percent confidence interval:\n 0.002647440 0.004661881\nsample estimates:\n          p \n0.003518455 \n\n\nSample Proportion: \\(\\hat{p}=0.0035\\)\nTest Statistic: \\(\\chi^2=3.3085\\)\np-value: \\(p-value=0.06893\\)\n\nConclusion\n\nSince the \\(p-value\\ge0.05\\), then fail to reject \\(H_o\\).\n\nInterpretation\n\nThere is not enough evidence to support that the proportion of deaths of Aboriginal prisoners is different from non-Aboriginal prisoners.\n\n\n\n8.2.3 Example: Hypothesis Test for One Proportion\nA researcher who is studying the effects of income levels on breastfeeding of infants hypothesizes that countries with a low income level have a different rate of infant breastfeeding than higher income countries. It is known that in Germany, considered a high-income country by the World Bank, 22% of all babies are breastfeed. In Tajikistan, considered a low-income country by the World Bank, researchers found that in a random sample of 500 new mothers that 125 were breastfeeding their infant. At the 5% level of significance, does this show that low-income countries have a different incident of breastfeeding?\n\n8.2.3.1 Solution\n\nState you random variable and the parameter in words.\n\n\\(x\\) = number of woman who breastfeed in a low-income country\n\\(p\\) = proportion of woman who breastfeed in a low-income country\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:p=0.22\\)\n\\(H_a:p\\ne0.22\\)\n\\(\\alpha=0.05\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nA simple random sample of 500 breastfeeding habits of woman in a low-income country was taken. Check: This was stated in the problem.\nThe properties of a Binomial Experiment have been met. Check: There were 500 women in the study. The women are considered identical, though they probably have some differences. There are only two outcomes, either the woman breastfeeds or she doesn’t. The probability of a woman breastfeeding is probably not the same for each woman, but it is probably not very different for each woman. The conditions for the binomial distribution are satisfied\nThe sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distributed. Check: In this case, \\(n = 500\\) and \\(p = 0.22\\). \\(n*p= 110\\ge5\\) and \\(n*q=390\\ge5\\), so the sampling distribution of \\(\\hat{p}\\) is well approximated by a normal curve.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nOn r studio, use the following command\n\nprop_test(125, 500, p=0.22) \n\n\n    1-sample proportions test with continuity correction\n\ndata:  125 out of 500\nX-squared = 2.4505, df = 1, p-value = 0.1175\nalternative hypothesis: true p is not equal to 0.22\n95 percent confidence interval:\n 0.2131062 0.2908059\nsample estimates:\n   p \n0.25 \n\n\nSample Statistic: \\(\\hat{p}=0.25\\)\ntest Statistic: \\(\\chi^2=2.4505\\)\np-value: \\(p-value=0.1175\\)\n\nConclusion\n\nSince the p-value is more than 0.05, you fail to reject \\(H_o\\).\n\nInterpretation\n\nThere is not enough evidence to support that the proportion of women who breastfeed in low-income countries is different from the proportion of women in high-income countries who breastfeed.\nNotice, the conclusion is that there wasn’t enough evidence to support \\(H_a\\). The conclusion was not that you support \\(H_o\\). There are many reasons why you can’t say that \\(H_o\\) is true. It could be that the countries you chose were not very representative of what truly happens. If you instead looked at all high-income countries and compared them to low-income countries, you might have different results. It could also be that the sample you collected in the low-income country was not representative. It could also be that income level is not an indication of breastfeeding habits. It could be that the sample that was taken didn’t show evidence but another sample would show evidence. There could be other factors involved. This is why you can’t say that you support \\(H_o\\). There are too many other factors that could be the reason that you failed to reject \\(H_0\\).\n\n\n\n8.2.4 Homework for One-Sample Proportion Test Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nThe Arizona Republic/Morrison/Cronkite News poll published on Monday, October 20, 2016, found 390 of the registered voters surveyed favor Proposition 205, which would legalize marijuana for adults. The statewide telephone poll surveyed 779 registered voters between Oct. 10 and Oct. 15. (Sanchez, 2016) Fifty-five percent of Colorado residents supported the legalization of marijuana. Does the data provide evidence that the percentage of Arizona residents who support legalization of marijuana is different from the proportion of Colorado residents who support it. Test at the 1% level.\nIn July of 1997, Australians were asked if they thought unemployment would increase, and 47% thought that it would increase. In November of 1997, they were asked again. At that time 284 out of 631 said that they thought unemployment would increase (\\“Morgan Gallup poll,\\” 2013). At the 5% level, is there enough evidence to show that the proportion of Australians in November 1997 who believe unemployment would increase is different from the proportion who felt it would increase in July 1997?\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, 23% of all complaints in 2007 were for identity theft. In that year, Arkansas had 1,601 complaints of identity theft out of 3,482 consumer complaints (\\“Consumer fraud and,\\” 2008). Does this data provide enough evidence to show that Arkansas had a different percentage of identity theft than 23%? Test at the 5% level.\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, 23% of all complaints in 2007 were for identity theft. In that year, Alaska had 321 complaints of identity theft out of 1,432 consumer complaints (\\“Consumer fraud and,\\” 2008). Does this data provide enough evidence to show that Alaska had a different proportion of identity theft than 23%? Test at the 5% level.\nIn 2001, the Gallup poll found that 81% of American adults believed that there was a conspiracy in the death of President Kennedy. In 2013, the Gallup poll asked 1,039 American adults if they believe there was a conspiracy in the assassination, and found that 634 believe there was a conspiracy (\\“Gallup news service,\\” 2013). Do the data show that the proportion of Americans who believe in this conspiracy has changed? Test at the 1% level.\nIn 2008, there were 507 children in Arizona out of 32,601 who were diagnosed with Autism Spectrum Disorder (ASD) (\\“Autism and developmental,\\” 2008). Nationally 1 in 88 children are diagnosed with ASD (\\“CDC features -,\\” 2013). Is there sufficient data to show that the incident of ASD is different in Arizona than nationally? Test at the 1% level.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>One Sample Inference</span>"
    ]
  },
  {
    "objectID": "One Sample Inference.html#one-sample-test-for-the-mean",
    "href": "One Sample Inference.html#one-sample-test-for-the-mean",
    "title": "8  One Sample Inference",
    "section": "8.3 One-Sample Test for the Mean",
    "text": "8.3 One-Sample Test for the Mean\nIt is time to go back to look at the test for the mean that was introduced in section 7.1 called the \\(z\\)-test. In the example, you knew what the population standard deviation, \\(\\sigma\\), was. What if you don’t know \\(\\sigma\\)?\nIf you don’t know \\(\\sigma\\), then you don’t know the sampling distribution of the mean. Can it be found another way? The answer is of course, yes. One way is to use a method called resampling. The following example explains how resampling is performed.\n\n8.3.1 Example: Resampling\nA random sample of 10 body mass index (BMI) were taken from the NHANES Data frame The mean BMI of Australians is 27.2 \\(kg/m^2\\). Is there evidence that Americans have a different BMI from people in Australia. Test at the 5% level.\n\n8.3.1.1 Solution\nThe standard deviation of BMI is not known for Australians. To answer this questions, first look at the sample from NHANES Table 8.2.\n\nsample_NHANES_10&lt;- \n  NHANES |&gt;\n  slice_sample(n=10)\nknitr::kable(head(sample_NHANES_10))\n\n\n\nTable 8.2: Sample of size 10 from NHANES\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n55125\n2009_10\nfemale\n11\n10-19\n132\nMexican\nNA\nNA\nNA\n10000-14999\n12500\n0.47\n4\nOwn\nNA\n70.0\nNA\nNA\n152.6\n30.06\nNA\n30.0_plus\n70\n102\n65\nNA\nNA\n104\n64\n100\n66\nNA\n1.03\n4.65\n135\n0.625\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n66359\n2011_12\nfemale\n10\n10-19\nNA\nWhite\nWhite\nNA\nNA\nmore 99999\n100000\n5.00\n8\nOwn\nNA\n66.7\nNA\nNA\n162.8\n25.20\nObese\n25.0_to_29.9\n88\n125\n81\n116\n82\n128\n78\n122\n84\n21.68\n1.45\n3.00\n8\n0.222\n182\n1.957\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2_hr\n4_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n69228\n2011_12\nmale\n17\n10-19\nNA\nWhite\nWhite\nNA\nNA\nmore 99999\n100000\n4.50\n7\nOwn\nNotWorking\n61.5\nNA\nNA\n169.8\n21.30\nNormWeight\n18.5_to_24.9\n54\n103\n54\n104\n58\n104\n62\n102\n46\n394.53\n1.32\n3.44\n28\n0.368\nNA\nNA\nNo\nNA\nVgood\n0\n2\nNA\nNA\nNA\nNA\nNA\n7\nYes\nNo\nNA\n4_hr\n4_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n60112\n2009_10\nfemale\n48\n40-49\n584\nWhite\nNA\nCollege Grad\nSeparated\n55000-64999\n60000\n5.00\n5\nOwn\nWorking\n123.9\nNA\nNA\n167.3\n44.27\nNA\n30.0_plus\n66\n114\n70\n116\n68\n118\n72\n110\n68\nNA\n1.73\n5.25\n95\n0.338\nNA\nNA\nNo\nNA\nGood\n30\n7\nMost\nNone\nNA\nNA\nNA\n5\nYes\nYes\n1\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nNo\nNA\n0\n0\nNo\nHeterosexual\nNA\n\n\n54073\n2009_10\nfemale\n80\nNA\nNA\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n2.16\n6\nRent\nNotWorking\n75.9\nNA\nNA\n153.5\n32.21\nNA\n30.0_plus\n58\n128\n60\n128\n64\n126\n62\n130\n58\nNA\n1.16\n3.72\n43\n0.270\nNA\nNA\nYes\n74\nVgood\n2\n0\nNone\nNone\n6\n4\n21\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n71833\n2011_12\nfemale\n30\n30-39\nNA\nOther\nAsian\nCollege Grad\nMarried\n65000-74999\n70000\n4.76\n4\nRent\nNotWorking\n48.8\nNA\nNA\n158.2\n19.50\nNA\n18.5_to_24.9\n84\n83\n54\n88\n56\n86\n60\n80\n48\n17.49\n1.27\n5.12\n23\n0.793\n68\n0.791\nNo\nNA\nGood\n7\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nNo\nNA\n1_hr\n2_hr\nNA\nNA\nYes\n1\n12\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n20\n8\n1\nNo\nHeterosexual\nNo\n\n\n\n\n\n\n\n\nThe mean BMI from this sample is\n\ndf_stats(~BMI, data=sample_NHANES_10, mean)\n\n  response     mean\n1      BMI 30.75889\n\n\nThe sample mean for Americans is different from the mean BMI for Australians, but could it just be by chance. Suppose you take another sample of size 10, but you only have these 10 BMIs to work with. So how could you do this. One way is to assume that the sample you took is representative of the entire population, and so you create a population by copying this sample over and over again. So you could have over 1000 copies of this sample of 10 BMIs. Then take a sample of size 10 from this created population. When doing this, you could conceivably choose the same number several times that was in the original sample and not choose some of the numbers that were in the original sample. Instead of physically creating this new population, you could just take samples from your original sample but with replacement. This means that you randomly pick the first number, record it, and then put it back that value back before collecting the next number. This kind a sampling is called randomization sampling. A sample using randomization could be Table 8.3.\n\nknitr::kable(resample(sample_NHANES_10))\n\n\n\nTable 8.3: Resample from NHANES Sample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\norig.id\n\n\n\n\n69228\n2011_12\nmale\n17\n10-19\nNA\nWhite\nWhite\nNA\nNA\nmore 99999\n100000\n4.50\n7\nOwn\nNotWorking\n61.5\nNA\nNA\n169.8\n21.30\nNormWeight\n18.5_to_24.9\n54\n103\n54\n104\n58\n104\n62\n102\n46\n394.53\n1.32\n3.44\n28\n0.368\nNA\nNA\nNo\nNA\nVgood\n0\n2\nNA\nNA\nNA\nNA\nNA\n7\nYes\nNo\nNA\n4_hr\n4_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n3\n\n\n59460\n2009_10\nmale\n46\n40-49\n560\nOther\nNA\n9 - 11th Grade\nLivePartner\n10000-14999\n12500\n0.66\n6\nOwn\nWorking\n55.0\nNA\nNA\n173.9\n18.19\nNA\n12.0_18.5\n80\n89\n58\n96\n56\n90\n60\n88\n56\nNA\n1.53\n4.76\n53\n0.366\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n6\nNo\nYes\n3\nNA\nNA\nNA\nNA\nYes\n5\n312\nYes\nYes\nSmoker\n16\nYes\n14\nYes\n14\nNo\nYes\n13\n30\n1\nNo\nHeterosexual\nNA\n8\n\n\n55209\n2009_10\nfemale\n0\n0-9\n10\nWhite\nNA\nNA\nNA\nmore 99999\n100000\n4.99\n4\nOwn\nNA\n9.6\n75.6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n9\n\n\n66359\n2011_12\nfemale\n10\n10-19\nNA\nWhite\nWhite\nNA\nNA\nmore 99999\n100000\n5.00\n8\nOwn\nNA\n66.7\nNA\nNA\n162.8\n25.20\nObese\n25.0_to_29.9\n88\n125\n81\n116\n82\n128\n78\n122\n84\n21.68\n1.45\n3.00\n8\n0.222\n182\n1.957\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2_hr\n4_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n\n\n66359\n2011_12\nfemale\n10\n10-19\nNA\nWhite\nWhite\nNA\nNA\nmore 99999\n100000\n5.00\n8\nOwn\nNA\n66.7\nNA\nNA\n162.8\n25.20\nObese\n25.0_to_29.9\n88\n125\n81\n116\n82\n128\n78\n122\n84\n21.68\n1.45\n3.00\n8\n0.222\n182\n1.957\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2_hr\n4_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n\n\n55209\n2009_10\nfemale\n0\n0-9\n10\nWhite\nNA\nNA\nNA\nmore 99999\n100000\n4.99\n4\nOwn\nNA\n9.6\n75.6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n9\n\n\n60112\n2009_10\nfemale\n48\n40-49\n584\nWhite\nNA\nCollege Grad\nSeparated\n55000-64999\n60000\n5.00\n5\nOwn\nWorking\n123.9\nNA\nNA\n167.3\n44.27\nNA\n30.0_plus\n66\n114\n70\n116\n68\n118\n72\n110\n68\nNA\n1.73\n5.25\n95\n0.338\nNA\nNA\nNo\nNA\nGood\n30\n7\nMost\nNone\nNA\nNA\nNA\n5\nYes\nYes\n1\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nNo\nNA\n0\n0\nNo\nHeterosexual\nNA\n4\n\n\n55125\n2009_10\nfemale\n11\n10-19\n132\nMexican\nNA\nNA\nNA\n10000-14999\n12500\n0.47\n4\nOwn\nNA\n70.0\nNA\nNA\n152.6\n30.06\nNA\n30.0_plus\n70\n102\n65\nNA\nNA\n104\n64\n100\n66\nNA\n1.03\n4.65\n135\n0.625\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n\n\n71833\n2011_12\nfemale\n30\n30-39\nNA\nOther\nAsian\nCollege Grad\nMarried\n65000-74999\n70000\n4.76\n4\nRent\nNotWorking\n48.8\nNA\nNA\n158.2\n19.50\nNA\n18.5_to_24.9\n84\n83\n54\n88\n56\n86\n60\n80\n48\n17.49\n1.27\n5.12\n23\n0.793\n68\n0.791\nNo\nNA\nGood\n7\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nNo\nNA\n1_hr\n2_hr\nNA\nNA\nYes\n1\n12\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n20\n8\n1\nNo\nHeterosexual\nNo\n6\n\n\n55209\n2009_10\nfemale\n0\n0-9\n10\nWhite\nNA\nNA\nNA\nmore 99999\n100000\n4.99\n4\nOwn\nNA\n9.6\n75.6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n9\n\n\n\n\n\n\n\n\nNotice that some of the unit of observations are repeated. That is what happens when you resample. Now one resampling isn’t enough. So you want to resample many times so you can create a resampling distribution Figure 8.1\n\n# mutate NHANES to subtract 27.2 (Australia's BMI) from US BMI measurements \nmutate_NHANES &lt;- NHANES |&gt;\n  mutate(NewBMI=BMI-27.2)\n# Generate the single sample\nSingle_sample&lt;- mutate_NHANES |&gt;\n  sample_n(size = 10)\n#Calculate the mean age of the single sample \nSingle_sample_mean &lt;- \n  Single_sample |&gt;\n  df_stats( ~ NewBMI, means = mean)\n#Take 200 resamples from the single sample\nTrials_resample &lt;- \n  do(200) * { Single_sample |&gt;\n      resample() |&gt;\n      df_stats( ~ NewBMI, means = mean) }\n# Plot the resample distribution of means\ngf_density( ~ means, data = Trials_resample, bins = 10) |&gt;\n  gf_lims(x = c(-5, 10)) |&gt;\n  gf_labs(title = \"Resampling Distribution\") |&gt;\n  gf_vline(data = Single_sample_mean, xintercept = ~ means, color=\"red\")\ndf_stats( ~ means, data = Trials_resample, mean, sd)\n\n  response     mean       sd\n1    means 0.143775 2.853875\n\n\n\n\n\n\n\n\nFigure 8.1: Resampling distribution of mean BMI with sample size 10\n\n\n\n\n\nNotice the sample mean from the resampling is very close to 0, so that means that the US BMI are not that different from the Australian BMI. There doesn’t seem to be enough evidence to show that the US BMI is different from the Australian BMI. One note, the sample size used here was 10 so you could see the sample, but really the sample size should be more than 100 for this method to be valid.\nSo this is one way to answer the question about if there is evidence to show a population mean is different from a value. This is actually the method that Ronald Fisher developed when he create all the foundation work that he did in statistics in the early 1900s. However, at the time, computers didn’t exist, so taking 100 resampling samples was not possible at that time. So other methods had to be developed that could be computed during that time. One method was developed by William (W.S) Gossett, a Chemist who worked for Guinness as their head brewer. Gossett developed a distribution called the Student’s T-distribution. His process was to use the sample standard deviation, \\(s\\), as an approximation of \\(\\sigma\\). This means the test statistic is now \\(t=\\frac{x-\\mu}{\\frac{s}{\\sqrt{n}}}\\). This new test statistic is actually distributed as a Student’s t-distribution, developed by W.S. Gossett. There are some conditions that must be made for this formula to be a Student’s t-distribution. These are outlined in the following theorem. Note: the t-distribution is called the Student’s t-distribution because that is the name he published under because he couldn’t publish under his own name due to his employer not wanting him to publish under his own name. His employer by the way was Guinness and they didn’t want competitors knowing they had a chemist/statistician working for them. It is not called the Student’s t-distribution because it is only used by students.\nTheorem: If the following conditions are met\n\nA random sample of size \\(n\\) is taken.\nThe distribution of the random variable is normal.\n\nThen the distribution of is a Student’s t-distribution with \\(n-1\\) degrees of freedom.\nExplanation of degrees of freedom: Recall the formula for sample standard deviation is \\(\\sqrt{{\\frac{\\sum{x-\\bar{x}}}{n-1}}}\\). Notice the denominator is \\(n-1\\). This is the same as the degrees of freedom. This is no accident. The reason the denominator and the degrees of freedom are both comes from how the standard deviation is calculated. First you take each data value and subtract \\(\\bar{x}\\). If you add up all of these new values, you will get 0. This must happen. Since it must happen, the first \\(n-1\\) data values you have “freedom of choice”, but the nth data value, you have no freedom to choose. Hence, you have \\(n-1\\) degrees of freedom. Another way to think about it is that if you five people and five chairs, the first four people have a choice of where they are sitting, but the last person does not. They have no freedom of where to sit. Only \\(n-1\\) people have freedom of choice.\nThe Student’s t-distribution is bell-shape that is more spread out than the normal distribution. There are many \\(t\\)-distributions, one for each different degree of freedom.\nFigure 8.2 is of the normal distribution and the Student’s t-distribution for df = 1, df = 3, df=8, df=30.\n\n\n\n\n\n\n\n\nFigure 8.2: Typical Student t-Distributions\n\n\n\n\n\nAs the degrees of freedom increases, the student’s t-distribution looks more like the normal distribution.\nTo find probabilities for the t-distribution, again technology can do this for you. There are many technologies out there that you can use.\n\n\n\n8.3.2 Hypothesis Test for One Population Mean (t-Test)\n\nState the random variable and the parameter in words.\n\n\\(x\\) = random variable\n\\(\\mu\\) = mean of random variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu=\\mu_o\\) , where \\(\\mu_o\\) is the known mean\n\\(H_a:\\mu\\ne\\mu_o\\), you can also use &lt; or &gt;, but \\(\\ne\\) is the more modern one to use.\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A random sample of size \\(n\\) is taken. Check: Describe the process taken to collect the sample.\nState: The population of the random variable is normally distributed. Check: examine density graph and normal quantile plot. Note: The t-test is fairly robust to the condition if the sample size is large. This means that if this condition isn’t met, but your sample size is quite large, then the results of the t-test are valid.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nOn rStudio, the command is\nt.test(~variable, data=data_frame, mu=what_Ho_says)\n\nConclusion\n\nThis is where you write reject or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\nNote: if the conditions behind this test are not valid, then the conclusions you make from the test are not valid. If you do not have a random sample, that is your fault. Make sure the sample you take is as random as you can make it following sampling techniques from chapter 1. If the population of the random variable is not normal, then take a larger sample. If you cannot afford to do that, or if it is not logistically possible, then you do different tests called non-parametric tests or you can try resampling. The advantage fo resampling is that you don’t need to know the under laying distribution of the random variable.\n\n\n8.3.3 Example: Test of the Mean Using One Sample T-test\nA random sample of 50 body mass index (BMI) were taken from the NHANES Data frame Table 8.4. The mean BMI of Australians is 27.2 \\(kg/m^2\\). Is there evidence that Americans have a different BMI from people in Australia. Test at the 5% level.\n\nsample_NHANES_50&lt;- sample_n(NHANES, size=50) \nknitr::kable(head(sample_NHANES_50))\n\n\n\nTable 8.4: BMI of Americans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n53958\n2009_10\nfemale\n35\n30-39\n424\nWhite\nNA\nSome College\nMarried\n75000-99999\n87500\n4.37\n8\nOwn\nWorking\n66.3\nNA\nNA\n164.7\n24.44\nNA\n18.5_to_24.9\n64\n108\n62\n112\n64\n108\n56\n108\n68\nNA\n1.24\n4.19\n68\n0.667\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\n2\n1\nNA\n7\nNo\nYes\n4\nNA\nNA\nNA\nNA\nYes\n1\n24\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n16\n4\n1\nNo\nHeterosexual\nNo\n\n\n59338\n2009_10\nfemale\n16\n10-19\n193\nBlack\nNA\nNA\nNA\n25000-34999\n30000\n1.53\n6\nOwn\nNotWorking\n93.1\nNA\nNA\n155.9\n38.31\nNA\n30.0_plus\n74\n118\n76\n120\n76\n116\n78\n120\n74\nNA\n0.85\n3.31\n64\nNA\nNA\nNA\nNo\nNA\nGood\n5\n5\nNA\nNA\nNA\nNA\nNA\n8\nNo\nYes\n3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n58811\n2009_10\nfemale\n18\n10-19\n225\nWhite\nNA\nNA\nNA\n65000-74999\n70000\n3.82\n7\nOwn\nWorking\n112.6\nNA\nNA\n172.7\n37.75\nNA\n30.0_plus\n80\n102\n45\n104\n46\n106\n46\n98\n44\nNA\n1.24\n4.63\n395\n0.477\nNA\nNA\nNo\nNA\nGood\n7\n2\nNone\nNone\nNA\nNA\nNA\n7\nNo\nYes\n3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n16\nNo\nNA\nNo\nYes\n16\n4\n3\nNo\nHeterosexual\nNA\n\n\n69462\n2011_12\nfemale\n36\n30-39\nNA\nBlack\nBlack\nSome College\nSeparated\n25000-34999\n30000\n0.96\n7\nOwn\nWorking\n89.5\nNA\nNA\n167.5\n31.90\nNA\n30.0_plus\n72\n129\n86\n136\n94\n130\n88\n128\n84\nNA\nNA\nNA\n114\n0.864\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\n4\n4\n21\n5\nNo\nNo\n4\n3_hr\n1_hr\nNA\nNA\nYes\n1\n104\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n19\n11\n1\nNo\nHeterosexual\nNo\n\n\n69456\n2011_12\nmale\n52\n50-59\nNA\nBlack\nBlack\nCollege Grad\nMarried\nNA\nNA\nNA\n10\nOwn\nWorking\n126.6\nNA\nNA\n185.6\n36.80\nNA\n30.0_plus\n68\n129\n80\n134\n80\n130\n80\n128\n80\n315.27\n1.32\n5.17\n297\n2.152\nNA\nNA\nNo\nNA\nFair\n30\n30\nSeveral\nNone\nNA\nNA\nNA\n5\nYes\nYes\n2\n4_hr\n1_hr\nNA\nNA\nYes\n2\n104\nNA\nNo\nNon-Smoker\nNA\nYes\n11\nYes\n13\nNo\nYes\n16\n5\n1\nNo\nHeterosexual\nNA\n\n\n56747\n2009_10\nmale\n33\n30-39\n406\nWhite\nNA\nCollege Grad\nMarried\nmore 99999\n100000\n5.00\n5\nOwn\nWorking\n105.1\nNA\nNA\n194.2\n27.87\nNA\n25.0_to_29.9\n68\n115\n75\n110\n74\n110\n78\n120\n72\nNA\n1.29\n4.68\n322\n2.776\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\nNA\nNA\nNA\n7\nNo\nYes\n2\nNA\nNA\nNA\nNA\nYes\n6\n104\nNo\nYes\nSmoker\nNA\nYes\n16\nYes\n19\nYes\nYes\n15\n19\n1\nNo\nHeterosexual\nNA\n\n\n\n\n\n\n\n\n\n8.3.3.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = BMI of an American\n\\(\\mu\\) = mean BMI of Americans\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu=27.2\\)\n\\(H_a:\\mu\\ne 27.2\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nA random sample of 50 BMI levels was taken. Check: A random sample was taken from the NHANES data frame using r Studio\nThe population of BMI levels is normally distributed. Check:\n\n(ref:sample-NHANES-50-density-cap) Density Plot of BMI from NHANES sample\n\ngf_density(~BMI, data=sample_NHANES_50, title=\"Body Mass Index\", xlab=\"Body Mass Index\")\ngf_qq(~BMI, data=sample_NHANES_50, title=\"Body Mass Index\", xlab=\"Body Mass Index\")\n\n\n\n\n\n\n\nFigure 8.3: Density Plot of BMI from NHANES sample\n\n\n\n\n\n\n\n\n\n\n\nFigure 8.4: Density Plot of BMI from NHANES sample\n\n\n\n\n\nThe density plot looks somewhat skewed right and the normal quantile plot looks somewhat linear. However, there doesn’t seem to be strong evidence that the sample comes from a population that is normally distributed. However, since the sample is moderate to large, the \\(t\\)-test is robust to this condition not being met. So the results of the test are probably valid.\n\nFind the sample statistic, test statistic, and \\(p\\)-value\n\nOn rStudio, the command would be\n\nt.test(~BMI, data= sample_NHANES_50, mu=27.2) \n\n\n    One Sample t-test\n\ndata:  BMI\nt = 0.92979, df = 47, p-value = 0.3572\nalternative hypothesis: true mean is not equal to 27.2\n95 percent confidence interval:\n 26.07683 30.25359\nsample estimates:\nmean of x \n 28.16521 \n\n\nThe test statistic is the \\(t\\) in the output, the sample statistic is the mean of \\(x\\) in the output, and the \\(p\\)-value is the \\(p\\)-value is the output.\n\nConclusion\n\nSince the \\(p\\)-value is not less than 5%, then fail to reject \\(H_o\\).\n\nInterpretation\n\nThere is not enough evidence to support that Americans have a different BMI from Australians.\nNote: this is the same conclusion that was found when using resampling. So the two method could give similar conclusions.\n\n\n\n8.3.4 Example: Test of the Mean Using One Sample T-test\nIn 2011, the average life expectancy for a woman in Europe was 79.8 years. The data in Table 8.5 are the life expectancies for all people in European countries (\\“WHO life expectancy,” 2013). The Table 8.6 filtered the data frame for just males and just year 2000. The year 2000 was randomly chosen as the year to use. Do the data indicate that men’s life expectancy is different from women’s? Test at the 1% level.\n\nExpectancy&lt;-read.csv( \"https://krkozak.github.io/MAT160/Life_expectancy_Europe.csv\") \nknitr::kable(head(Expectancy))\n\n\n\nTable 8.5: Life Expectancies for European Countries\n\n\n\n\n\n\nyear\nWHO_region\ncountry\nsex\nexpect\n\n\n\n\n1990\nEurope\nAlbania\nMale\n67\n\n\n1990\nEurope\nAlbania\nFemale\n71\n\n\n1990\nEurope\nAlbania\nBoth sexes\n69\n\n\n2000\nEurope\nAlbania\nMale\n68\n\n\n2000\nEurope\nAlbania\nFemale\n73\n\n\n2000\nEurope\nAlbania\nBoth sexes\n71\n\n\n\n\n\n\n\n\n\nExpectancy_male&lt;- \n  Expectancy |&gt;\n  filter(sex==\"Male\", year==\"2000\") \nknitr::kable(head(Expectancy_male))\n\n\n\nTable 8.6: Life Expectancies of males in European Countries in 2000\n\n\n\n\n\n\nyear\nWHO_region\ncountry\nsex\nexpect\n\n\n\n\n2000\nEurope\nAlbania\nMale\n68\n\n\n2000\nEurope\nAndorra\nMale\n76\n\n\n2000\nEurope\nArmenia\nMale\n68\n\n\n2000\nEurope\nAustria\nMale\n75\n\n\n2000\nEurope\nAzerbaijan\nMale\n64\n\n\n2000\nEurope\nBelarus\nMale\n63\n\n\n\n\n\n\n\n\nCode book for data frame Expectancy\nDescription This data extract has been generated by the Global Health Observatory of the World Health Organization. The data was extracted on 2013-09-19 13:10:20.0.\nThis data frame contains the following columns:\nyear: year for life expectancies\nWHO_region: World Health Organizations designation for the location of the country\ncountry: country where the epectancies are from\nsex: sex of the group that expectancies are calculated for\nexpect: average life expectancies of the different groups of the different countries.\nSource http://apps.who.int/gho/athena/data/download.xsl?format=xml&target=GHO/WHOSIS_000001&profile=excel&filter=COUNTRY:*;SEX:*;REGION:EUR\nReferences World Health Organization (WHO).\n\n8.3.4.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = life expectancy for a European man\n\\(\\mu\\) = mean life expectancy for European men\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu=79.8\\)\n\\(H_a:\\mu\\ne79.8\\)\n\\(\\alpha=0.01\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A random sample of 53 life expectancies of European men in 2000 was taken.\nCheck: The data is actually all of the life expectancies for every country that is considered part of Europe by the World Health Organization in the year 2000. Since the year 2000 was picked at random, then the sample is a random sample.\nState: The distribution of life expectancies of European men in 2000 is normally distributed.\nCheck:\n\n\ngf_density(~expect, data=Expectancy_male, title=\"Life Expectancies of Males in Europe in 2000\", xlab=\"Life expectancy\")\n\n\n\n\n\n\n\nFigure 8.5: Density Plot of Life Expectancy of Males in Europe in 2000\n\n\n\n\n\n\ngf_qq(~expect, data=Expectancy_male, title=\"Life Expectancies of Males in Europe in 2000\")\n\n\n\n\n\n\n\nFigure 8.6: Quantile Plot of Life Expectancy of Males in Europe in 2000\n\n\n\n\n\nThis sample does not appear to come from a population that is normally distributed. This sample is moderate to large, so it is good that the t-test is robust.\n\nFind the sample statistic, test statistic, and \\(p\\)-value\n\nOn rStudio, the command is\n\nt.test(~expect, data=Expectancy_male, mu=79.8)\n\n\n    One Sample t-test\n\ndata:  expect\nt = -11.733, df = 52, p-value = 3.145e-16\nalternative hypothesis: true mean is not equal to 79.8\n95 percent confidence interval:\n 69.11930 72.23919\nsample estimates:\nmean of x \n 70.67925 \n\n\nSample statistic is 70.68 years, test statistic is \\(t = -11.733\\), and \\(p-value =3.14X10^{-16}\\).\n\nConclusion\n\nSince the p-value is less than 1%, then reject \\(H_o\\).\n\nInterpretation\n\nThere is enough evidence to support that the mean life expectancy for European men is different than the mean life expectancy for European women of 79.8 years.\nNote: if you want to conduct a hypothesis test with \\(H_a:\\mu&gt;\\mu_o\\), then the rStudio command would be\nt.test(~variable, data=Data_Frame, mu=number \\(H_0\\) equals, alternative=“greater”)\nIf you want to conduct a hypothesis test with \\(H_a:\\mu&lt;\\mu_o\\), then the r Studio command would be\nt.test(~variable, data=Data_Frame, mu=number \\(H_0\\) equals, alternative=“less”)\n\n\n\n8.3.5 Homework for One-Sample Test for the Mean Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nThe Kyoto Protocol was signed in 1997, and required countries to start reducing their carbon emissions. The protocol became enforceable in February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per capita. The Table 8.7 contains a random sample of CO2 emissions in 2010 (CO2 emissions (metric tons per capita), 2018). Is there enough evidence to show that the mean CO2 emission is different in 2010 than in 2004? Test at the 1% level.\n\n\nEmission &lt;- read.csv( \"https://krkozak.github.io/MAT160/CO2_emission.csv\") \nknitr::kable(head(Emission))\n\n\n\nTable 8.7: CO2 Emissions (in metric tons per capita) in 2010\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\ny1960\ny1961\ny1962\ny1963\ny1964\ny1965\ny1966\ny1967\ny1968\ny1969\ny1970\ny1971\ny1972\ny1973\ny1974\ny1975\ny1976\ny1977\ny1978\ny1979\ny1980\ny1981\ny1982\ny1983\ny1984\ny1985\ny1986\ny1987\ny1988\ny1989\ny1990\ny1991\ny1992\ny1993\ny1994\ny1995\ny1996\ny1997\ny1998\ny1999\ny2000\ny2001\ny2002\ny2003\ny2004\ny2005\ny2006\ny2007\ny2008\ny2009\ny2010\ny2011\ny2012\ny2013\ny2014\ny2015\ny2016\ny2017\ny2018\n\n\n\n\nAruba\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2.8683194\n7.2351980\n10.0261792\n10.6347326\n26.3745032\n26.0461298\n21.4425588\n22.0007862\n21.0362451\n20.7719362\n20.3183534\n20.4268177\n20.5876692\n20.3115668\n26.1948752\n25.9340244\n25.6711618\n26.4204521\n26.517293\n27.2007078\n26.9477260\n27.8950228\n26.2295527\n25.9153221\n24.6705289\n24.5075162\n13.1577223\n8.353561\n8.4100642\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n0.0460567\n0.0535888\n0.0737208\n0.0741607\n0.0861736\n0.1012849\n0.1073989\n0.1234095\n0.1151425\n0.0865099\n0.1496515\n0.1652083\n0.1299956\n0.1353666\n0.1545032\n0.1676124\n0.1535579\n0.1815222\n0.1618942\n0.1670664\n0.1317829\n0.1506147\n0.1631039\n0.2012243\n0.2319613\n0.2939569\n0.2677719\n0.2692296\n0.2468233\n0.2338822\n0.2106434\n0.1833636\n0.0961966\n0.0850871\n0.0758065\n0.0686399\n0.0624346\n0.0566423\n0.0527632\n0.0407225\n0.0372348\n0.0378461\n0.0473773\n0.0504813\n0.038410\n0.0517440\n0.0624275\n0.0838928\n0.1517209\n0.2383985\n0.2899876\n0.4064242\n0.3451488\n0.310341\n0.2939464\nNA\nNA\nNA\nNA\n\n\nAngola\n0.1008353\n0.0822038\n0.2105315\n0.2027373\n0.2135603\n0.2058909\n0.2689414\n0.1721017\n0.2897181\n0.4802340\n0.6082236\n0.5645482\n0.7212460\n0.7512399\n0.7207764\n0.6285689\n0.4513535\n0.4692212\n0.6947369\n0.6830629\n0.6409664\n0.6111351\n0.5193546\n0.5513486\n0.5209829\n0.4719028\n0.4516189\n0.5440851\n0.4635083\n0.4372955\n0.4317436\n0.4155308\n0.4105229\n0.4417211\n0.2881191\n0.7870325\n0.7262335\n0.4963612\n0.4758152\n0.5770829\n0.5819615\n0.5743161\n0.7229589\n0.5002254\n1.001878\n0.9857364\n1.1050190\n1.2031340\n1.1850005\n1.2344251\n1.2440915\n1.2526808\n1.3302186\n1.253776\n1.2903068\nNA\nNA\nNA\nNA\n\n\nAlbania\n1.2581949\n1.3741860\n1.4399560\n1.1816811\n1.1117420\n1.1660990\n1.3330555\n1.3637463\n1.5195513\n1.5589676\n1.7532399\n1.9894979\n2.5159144\n2.3038974\n1.8490067\n1.9106336\n2.0135846\n2.2758764\n2.5306250\n2.8982085\n1.9350583\n2.6930239\n2.6248568\n2.6832399\n2.6942914\n2.6580154\n2.6653562\n2.4140608\n2.3315985\n2.7832431\n1.6781067\n1.3122126\n0.7747249\n0.7237903\n0.6002037\n0.6545371\n0.6366253\n0.4903651\n0.5602714\n0.9601644\n0.9781747\n1.0533042\n1.2295407\n1.4126972\n1.376213\n1.4124982\n1.3025764\n1.3223349\n1.4843111\n1.4956002\n1.5785736\n1.8037147\n1.6929083\n1.749211\n1.9787633\nNA\nNA\nNA\nNA\n\n\nAndorra\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7.4673357\n7.1824566\n6.9120534\n6.7360548\n6.4942004\n6.6620517\n7.0650715\n7.2397127\n7.6607839\n7.9754544\n8.0192843\n7.7869500\n7.5906151\n7.3157607\n7.358625\n7.2998719\n6.7460521\n6.5193871\n6.4278100\n6.1215799\n6.1225947\n5.8674102\n5.9168840\n5.901775\n5.8329062\nNA\nNA\nNA\nNA\n\n\nArab World\n0.6457359\n0.6874654\n0.7635736\n0.8782377\n1.0030533\n1.1705403\n1.2781736\n1.3374436\n1.5522420\n1.7986689\n1.8103078\n2.0037220\n2.1208746\n2.4095329\n2.2858907\n2.1967827\n2.5843424\n2.6487624\n2.7623331\n2.8636143\n3.0928915\n2.9302350\n2.7231544\n2.8165670\n2.9813539\n3.0618504\n3.2844996\n3.1978064\n3.2950428\n3.2566742\n3.0169588\n3.2366449\n3.4154849\n3.6694456\n3.6743582\n3.4240095\n3.3283037\n3.1455322\n3.3499672\n3.3283411\n3.7038571\n3.6079561\n3.6046128\n3.7964674\n4.068562\n4.1856773\n4.2857192\n4.1171475\n4.4089483\n4.5620151\n4.6368134\n4.5594617\n4.8377796\n4.674925\n4.8869875\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nCode book for data frame Emission\nDescription Carbon dioxide emissions are those stemming from the burning of fossil fuels and the manufacture of cement. They include carbon dioxide produced during consumption of solid, liquid, and gas fuels and gas flaring.\nThis data frame contains the following columns:\ncountry: country around the world\ny1960-y2018: weighted averages of CO2 emission for the years 1960 through 2018 in metric tons per capita\nSource CO2 emissions (metric tons per capita). (n.d.). Retrieved July 18, 2019, from https://data.worldbank.org/indicator/EN.ATM.CO2E.PC\nReferences Carbon Dioxide Information Analysis Center, Environmental Sciences Division, Oak Ridge National Laboratory, Tennessee, United States.\n\nThe amount of sugar in a Krispy Kream glazed donut is 10 g. Many people feel that cereal is a healthier alternative for children over glazed donuts. The Table 8.8 contains the amount of sugar in a sample of cereal (breakfast cereal, 2019). Is there enough evidence to show that the mean amount of sugar in children’s cereal is different than in a glazed donut? Test at the 5% level.\n\n\nSugar &lt;- read.csv( \"https://krkozak.github.io/MAT160/cereal.csv\") \nknitr::kable(head(Sugar))\n\n\n\nTable 8.8: Nutrition Amounts in Cereal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nmanf\nage\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarb\nsugar\nshelf\npotassium\nvit\nweight\nserving\n\n\n\n\n100%_Bran\nNabisco\nadult\ncold\n70\n4\n1\n130\n10.0\n5.0\n6\n3\n280\n25\n1\n0.33\n\n\n100%_Natural_Bran\nQuaker_Oats\nadult\ncold\n120\n3\n5\n15\n2.0\n8.0\n8\n3\n135\n0\n1\n-1.00\n\n\nAll-Bran\nKelloggs\nadult\ncold\n70\n4\n1\n260\n9.0\n7.0\n5\n3\n320\n25\n1\n0.33\n\n\nAll-Bran_with_Extra_Fiber\nKelloggs\nadult\ncold\n50\n4\n0\n140\n14.0\n8.0\n0\n3\n330\n25\n1\n0.50\n\n\nAlmond_Delight\nRalston_Purina\nadult\ncold\n110\n2\n2\n200\n1.0\n14.0\n8\n3\n-1\n25\n1\n0.75\n\n\nApple_Cinnamon_Cheerios\nGeneral_Mills\nchild\ncold\n110\n2\n2\n180\n1.5\n10.5\n10\n1\n70\n25\n1\n0.75\n\n\n\n\n\n\n\n\nCode book for data frame Sugar\nDescription Nutritional information about cereals.\nThis data frame contains the following columns:\nname: the cereal brand\nmanf: manufacturer\nage: whether the cereal is geared towards children or adults\ntype: whether the cereal is considered a hot or cold cereal\ncalories: the number of calories in the cereal (number)\nprotein: the amount of protein in a serving of the cereal (g)\nfat: the amount of fat a serving of the cereal (g)\nsodium: the amount of sodium in a serving of the cereal (mg)\nfiber: the amount of fiber in a serving of the cereal (g)\ncarb: the amount of complex carbohydrates in a serving of the cereal (g)\nsugars: the amount of sugar in a serving of the cereal (g)\ndisplay shelf: what shelf the cereal is on counting from the floor\npotassium: the amount of potassium in a serving of the cereal (mg)\nvit: the amount of vitamins and minerals in a serving of the cereal (0, 25, or 100)\nweight: weight in ounces of one serving\nserving: cups per serving\nSource (n.d.). Retrieved July 18, 2019, from https://www.idvbook.com/teaching-aid/data-sets/the-breakfast-cereal-data-set/ The Best Kids’ Cereal. (n.d.). Retrieved July 18, 2019, from https://www.ranker.com/list/best-kids-cereal/ranker-food\nReferences Interactive Data Visualization Foundations, Techniques, Applications (Matthew Ward | Georges Grinstein | Daniel Keim)\nA new data frame Table 8.9 will need to be created of just cereal for children. To create that use the following command in rStudio\n\nSugar_children&lt;- \n  Sugar%&gt;% \n  filter(age==\"child\") \nknitr::kable(head(Sugar_children))\n\n\n\nTable 8.9: Nutrition Amounts in Children’s Cereal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nmanf\nage\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarb\nsugar\nshelf\npotassium\nvit\nweight\nserving\n\n\n\n\nApple_Cinnamon_Cheerios\nGeneral_Mills\nchild\ncold\n110\n2\n2\n180\n1.5\n10.5\n10\n1\n70\n25\n1\n0.75\n\n\nApple_Jacks\nKelloggs\nchild\ncold\n110\n2\n0\n125\n1.0\n11.0\n14\n2\n30\n25\n1\n1.00\n\n\nBran_Chex\nRalston_Purina\nchild\ncold\n90\n2\n1\n200\n4.0\n15.0\n6\n1\n125\n25\n1\n0.67\n\n\nCap’n’Crunch\nQuaker_Oats\nchild\ncold\n120\n1\n2\n220\n0.0\n12.0\n12\n2\n35\n25\n1\n0.75\n\n\nCheerios\nGeneral_Mills\nchild\ncold\n110\n6\n2\n290\n2.0\n17.0\n1\n1\n105\n25\n1\n1.25\n\n\nCinnamon_Toast_Crunch\nGeneral_Mills\nchild\ncold\n120\n1\n3\n210\n0.0\n13.0\n9\n2\n45\n25\n1\n0.75\n\n\n\n\n\n\n\n\n\nThe FDA regulates that fish that is consumed is allowed to contain 1.0 mg/kg of mercury. In Florida, bass fish were collected in 53 different lakes to measure the health of the lakes. The data frame of measurements from Florida lakes is in Table 8.10 (NISER 081107 ID Data, 2019). Do the data provide enough evidence to show that the fish in Florida lakes has different amounts of mercury than the allowable amount? Test at the 10% level.\n\n\nMercury&lt;- read.csv( \"https://krkozak.github.io/MAT160/mercury.csv\") \nknitr::kable(head(Mercury))\n\n\n\nTable 8.10: Health of Florida lake Fish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nlake\nalkalinity\nph\ncalcium\nchlorophyll\nmercury\nno.samples\nmin\nmax\nX3_yr_standmercury\nage_data\n\n\n\n\n1\nAlligator\n5.9\n6.1\n3.0\n0.7\n1.23\n5\n0.85\n1.43\n1.53\n1\n\n\n2\nAnnie\n3.5\n5.1\n1.9\n3.2\n1.33\n7\n0.92\n1.90\n1.33\n0\n\n\n3\nApopka\n116.0\n9.1\n44.1\n128.3\n0.04\n6\n0.04\n0.06\n0.04\n0\n\n\n4\nBlue_Cypress\n39.4\n6.9\n16.4\n3.5\n0.44\n12\n0.13\n0.84\n0.44\n0\n\n\n5\nBrick\n2.5\n4.6\n2.9\n1.8\n1.20\n12\n0.69\n1.50\n1.33\n1\n\n\n6\nBryant\n19.6\n7.3\n4.5\n44.1\n0.27\n14\n0.04\n0.48\n0.25\n1\n\n\n\n\n\n\n\n\nCode book for data frame Mercury\nDescription Largemouth bass were studied in 53 different Florida lakes to examine the factors that influence the level of mercury contamination. Water samples were collected from the surface of the middle of each lake in August 1990 and then again in March 1991. The pH level, the amount of chlorophyll, calcium, and alkalinity were measured in each sample. The average of the August and March values were used in the analysis. Next, a sample of fish was taken from each lake with sample sizes ranging from 4 to 44 fish. The age of each fish and mercury concentration in the muscle tissue was measured. (Note: Since fish absorb mercury over time, older fish will tend to have higher concentrations). Thus, to make a fair comparison of the fish in different lakes, the investigators used a regression estimate of the expected mercury concentration in a three year old fish as the standardized value for each lake. Finally, in 10 of the 53 lakes, the age of the individual fish could not be determined and the average mercury concentration of the sampled fish was used instead of the standardized value. ( Reference: Lange, Royals, & Connor. (1993))\nThis data frame contains the following columns:\nID: ID number\nLake: Name of lake\nalkalinity: Alkalinity (mg/L as Calcium Carbonate)\npH: pH\ncalcium: calcium (mg/l)\nchlorophyll: chlorophyll (mg/l)\nmercury: Average mercury concentration (parts per million) in the muscle tissue of the fish sampled from that lake\nno.samples: How many fish were sampled from the lake\nmin: Minimum mercury concentration among the sampled fish\nmax: Maximum mercury concentration among the sampled fish\nX3_yr_Standard_mercury: Regression estimate of the mercury concentration in a 3 year old fish from the lake (or = Avg Mercury when age data was not available)\nage_data: Indicator of the availability of age data on fish sampled\nSource Lange TL, Royals HE, Connor LL (1993) Influence of water chemistry on mercury concentration in largemouth bass from Florida lakes. Trans Am Fish Soc 122:74-84. Michael K. Saiki, Darell G. Slotton, Thomas W. May, Shaun M. Ayers, and Charles N. Alpers (2000) Summary of Total Mercury Concentrations in Fillets of Selected Sport Fishes Collected during 2000–2003 from Lake Natoma, Sacramento County, California (Raw data is included in appendix), U.S. Geological Survey Data Series 103, 1-21. NISER 081107 ID Data. (n.d.). Retrieved July 18, 2019, from http://wiki.stat.ucla.edu/socr/index.php/NISER_081107_ID_Data\nReferences NISER 081107 ID Data\n\nThe data frame Table 4.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute. The mean pulse rate after running for 1 minute of females who do not drink is 97 beats per minute. Do the data show that the mean pulse rate of females who do drink alcohol is higher than the mean pulse rate of females who do not drink? Test at the 5% level.\n\nCode book for data frame Pulse is below Table 4.7.\nCreate a data frame Table 10.2 that contains only females who drink alcohol. Then test the pulse after for woman who do drink alcohol to the known value for females who do not drink alcohol. To create a new data frame with just females who drink alcohol use the following command, where the new name is Females:\n\nFemales&lt;- Pulse%&gt;% filter(gender==\"female\", alcohol==\"yes\") \nknitr::kable(head(Females))\n\n\n\nTable 8.11: Pulse Rates Before and After Exercise of Females who do drink Alcohol\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\n\n\n\n\n165\n60\n19\nfemale\nyes\nyes\nlow\nran\n88\n120\n98\n\n\n163\n47\n23\nfemale\nyes\nyes\nlow\nran\n71\n125\n98\n\n\n173\n57\n18\nfemale\nno\nyes\nmoderate\nsat\n86\n88\n93\n\n\n179\n58\n19\nfemale\nno\nyes\nmoderate\nran\n82\n150\n93\n\n\n167\n62\n18\nfemale\nno\nyes\nhigh\nran\n96\n176\n93\n\n\n173\n64\n18\nfemale\nno\nyes\nlow\nsat\n90\n88\n93\n\n\n\n\n\n\n\n\n\nThe economic dynamism is an index of productive growth in dollars. Economic data for many countries are in Table 8.12 (SOCR Data 2008 World CountriesRankings, 2019). Countries that are considered high-income have a mean economic dynamism of 60.29.\n\n\nEconomics &lt;- read.csv( \"https://krkozak.github.io/MAT160/Economics_country.csv\") \nknitr::kable(head(Economics))\n\n\n\nTable 8.12: Economic Data for Countries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nId\nincGroup\nkey\nname\npopGroup\nregion\nkey2\nED\nEdu\nHI\nQOL\nPE\nOA\nRelig\n\n\n\n\n0\nLow\nal\nAlbania\nSmall\nSouthern_Europe\npopS\n34.0862\n81.0164\n71.0244\n67.9240\n58.6742\n57\n39\n\n\n1\nMiddle\ndz\nAlgeria\nMedium\nNorth_Africa\npopM\n25.8057\n74.8027\n66.1951\n60.9347\n32.6054\n85\n95\n\n\n2\nMiddle\nar\nArgentina\nMedium\nSouth_America\npopM\n37.4511\n69.8825\n78.2683\n68.1559\n68.6647\n46\n66\n\n\n3\nHigh\nau\nAustralia\nMedium\nAustralia\npopM\n71.4888\n91.4802\n95.1707\n90.5729\n90.9629\n4\n65\n\n\n4\nHigh\nat\nAustria\nSmall\nCentral_Europe\npopS\n53.9431\n90.4578\n90.3415\n87.5630\n91.2073\n18\n20\n\n\n5\nLow\naz\nAzerbaijan\nSmall\ncentral_Asia\npopS\n53.6457\n68.9880\n58.9512\n68.9572\n40.0390\n69\n50\n\n\n\n\n\n\n\n\nCode book for data frame Economics\nDescription These data represent commonly accepted measures for raking Countries on variety of factors which affect the country’s internal and external international perception of the country’s rank relative the to rest of the World.\nThis data frame contains the following columns:\nid: Unique country identifier\nincGroup: Income group: Low: GNI per capita &lt; \\$3,946, Middle: \\$3,946 &lt; GNI per capita &lt; \\$12,195, High: GNI per capita &gt; \\$12,196\nkey: unique 2-letter country code\nname: Country Name\npopGroup: Population Group: Small: Population &lt; 20 million, Medium: 20 million &lt; Population &lt; 50 million, Large: Population &gt; 50 million\nregion: Relative geographic position of the Country\nkey2: Country Group Classification Label: world: All countries, g7: G7, g20: G20, latin: Latin America & Caribbean, eu: European Union, centasia: Europe & Central Asia, pacasia: East Asia & Pacific, asean: Asean, sasia: South Asia, mideast: Middle East & North Africa, africa: Sub-Saharan Africa, bric: Brazil, Russia, India and China (BRIC)\nED: Economic Dynamism: Index of Productive growth in dollars (GDP/capita at PPP, Avg of GDP/capita growth rate over last ten years, GDP/capita growth rate over next ten years, Economic Dynamism: Manufacturing percent of GDP, Services percent of GDP percent (100=best, 0=worst).\nEdu: Education/Literacy Rate (percent of population able to read and write at a specified age)\nHI: Health Index: The average number of years a person lives in full health, taking into account years lived in less than full health\nQOL: Quality of Life: Population percent living on &lt; \\$2/day\nPE: Political Environment: Freedom house rating of political participation (qualitative assessment of voter participation/turn-out for national elections, citizens engagement with politics)\nOA: Overall country ranking taking all measures into account.\nRelig: Religiosity of the Country as a percent (%) of the population.\nSource SOCR Data 2008 World CountriesRankings. (n.d.). Retrieved July 19, 2019, from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_2008_World_CountriesRankings#SOCR_Data_-_Ranking_of_the_top_100_Countries_based_on_Political.2C_Economic.2C_Health.2C_and_Quality-of-Life_Factors\nReferences SOCR Data 2008 World CountriesRankings, Amazon Web-Services World’s Best Countries.\nCreate a data frame that contains only middle income countries. Do the data show that the mean economic dynamism of middle-income countries is less than the mean for high-income countries? Test at the 5% level. To create a new data frame Table 8.13 with just middle income countries use the following command, where the new name is Middle_economics:\n\nMiddle_economics&lt;- \n  Economics |&gt; \n  filter(incGroup==\"Middle\") \nknitr::kable(head(Middle_economics))\n\n\n\nTable 8.13: Economic Data for Middle income Countries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nId\nincGroup\nkey\nname\npopGroup\nregion\nkey2\nED\nEdu\nHI\nQOL\nPE\nOA\nRelig\n\n\n\n\n1\nMiddle\ndz\nAlgeria\nMedium\nNorth_Africa\npopM\n25.8057\n74.8027\n66.1951\n60.9347\n32.6054\n85\n95\n\n\n2\nMiddle\nar\nArgentina\nMedium\nSouth_America\npopM\n37.4511\n69.8825\n78.2683\n68.1559\n68.6647\n46\n66\n\n\n7\nMiddle\nby\nBelarus\nSmall\ncentral_Asia\npopS\n51.9150\n86.6155\n66.1951\n74.1467\n34.0501\n56\n34\n\n\n10\nMiddle\nbw\nBotswana\nSmall\nAfrica\npopS\n43.6952\n73.4608\n34.8049\n50.0875\n72.6833\n80\n80\n\n\n11\nMiddle\nbr\nBrazil\nLarge\nSouth_America\npopL\n47.8506\n71.3735\n71.0244\n62.4238\n67.4131\n48\n87\n\n\n12\nMiddle\nbg\nBulgaria\nSmall\nSouthern_Europe\npopS\n43.7178\n82.2277\n75.8537\n73.1197\n73.1686\n38\n50\n\n\n\n\n\n\n\n\n\nIn 1999, the average percentage of women who received prenatal care per country is 80.1%. Table 3.18 contains the percentage of woman receiving prenatal care in a sample of countries over several years. (births per woman), 2019). Do the data show that the average percentage of women receiving prenatal care in 2009 (p2009) is different than in 1999? Test at the 5% level.\n\nCode book for Data frame Fert_prenatal is below Table 3.18.\n\nMaintaining your balance may get harder as you grow older. A study was conducted to see how steady the elderly is on their feet. They had the subjects stand on a force platform and have them react to a noise. The force platform then measured how much they swayed forward and backward, and the data is in Table 8.14 (Maintaining Balance while Concentrating, 2019). Do the data show that the elderly sway more than the mean forward sway of younger people, which is 18.125 mm? Test at the 5% level. Follow the filtering methods in other homework problems to create a data frame for only Elderly.\n\n\nSway &lt;- read.csv( \"https://krkozak.github.io/MAT160/sway.csv\") \nknitr::kable(head(Sway))\n\n\n\nTable 8.14: Sway (in mm) of Elderly Subjects\n\n\n\n\n\n\nage\nfbsway\nsidesway\n\n\n\n\nElderly\n19\n14\n\n\nElderly\n30\n41\n\n\nElderly\n20\n18\n\n\nElderly\n19\n11\n\n\nElderly\n29\n16\n\n\nElderly\n25\n24\n\n\n\n\n\n\n\n\nCode book for data frame Sway\nDescription How difficult is it to maintain your balance while concentrating? It is more difficult when you are older? Nine elderly (6 men and 3 women) and eight young men were subjects in an experiment. Each subject stood barefoot on a “force platform” and was asked to maintain a stable upright position and to react as quickly as possible to an unpredictable noise by pressing a hand held button. The noise came randomly and the subject concentrated on reacting as quickly as possible. The platform automatically measured how much each subject swayed in millimeters in both the forward/backward and the side-to-side directions.\nThis data frame contains the following columns:\nAge: Elderly or Young\nFBSway: Sway in forward/backward direction\nSideSwayy: Sway in side to side direction\nSource Maintaining Balance while Concentrating. (n.d.). Retrieved July 19, 2019, from http://www.statsci.org/data/general/balaconc.html\nReferences Teasdale, N., Bard, C., La Rue, J., and Fleury, M. (1993). On the cognitive penetrability of posture control. Experimental Aging Research 19, 1-13. The data was obtained from the DASL Data and Story Line online database.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>One Sample Inference</span>"
    ]
  },
  {
    "objectID": "Estimation.html",
    "href": "Estimation.html",
    "title": "9  Estimation",
    "section": "",
    "text": "9.1 Basics of Confidence Intervals\nIn hypothesis tests, the purpose was to make a decision about a parameter, in terms of it being greater than, less than, or not equal to a value. But what if you want to actually know what the parameter is. You need to do estimation. There are two types of estimation -- point estimator and confidence interval. The American Statistical Association (ASA) is recommending that confidence intervals are the process that should be followed when analyzing data.\nA point estimator is just the statistic that you have calculated previously. As an example, when you wanted to estimate the population mean, \\(\\mu\\), the point estimator is the sample mean, \\(\\bar{x}\\). To estimate the population proportion, \\(p\\), you use the sample proportion, \\(\\hat{p}\\). In general, if you want to estimate any population parameter, we will call it \\(\\theta\\), you use the sample statistic, \\(\\hat{\\theta}\\).\nPoint estimators are really easy to find, but they have some drawbacks. First, if you have a large sample size, then the estimate is better. But with a point estimator, you don’t know what the sample size is. Also, you don’t know how accurate the estimate is. Both of these problems are solved with a confidence interval.\nConfidence interval: This is where you have an interval surrounding your parameter, and the interval has a chance of being a true statement. In general, a confidence interval looks like: \\(\\hat{\\theta}\\pm E\\), where \\(\\hat{\\theta}\\) is the point estimator and \\(E\\) is the margin of error term that is added and subtracted from the point estimator. Thus making an interval.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "Estimation.html#basics-of-confidence-intervals",
    "href": "Estimation.html#basics-of-confidence-intervals",
    "title": "9  Estimation",
    "section": "",
    "text": "9.1.1 Interpreting a confidence interval:\nThe statistical interpretation is that the confidence interval has a probability \\(C=(1-\\alpha)\\) (where \\(\\alpha\\) is the complement of the confidence level) of containing the population parameter. As an example, if you have a 95% confidence interval of $0.65 &lt; p &lt; 0.73$, then you would say, “you are 95% confident that the interval 0.65 to 0.73 contains the true population proportion.” This means that if you have 100 intervals, 95 of them will contain the true proportion, and 5 will not. The wrong interpretation is that there is a 95% confidence that the true value of \\(p\\) will fall between 0.65 and 0.73. The reason that this interpretation is wrong is that the true value is fixed out there somewhere. You are trying to capture it with this interval. So this is the chance that your interval captures it, and not that the true value falls in the interval.\nThere is also a real world interpretation that depends on the situation. It is where you are telling people what numbers you found the parameter to lie between. So your real world is where you tell what values your parameter is between. There is no probability attached to this statement. That probability is in the statistical interpretation.\nThe common probabilities used for confidence intervals are 90%, 95%, and 99%. These are known as the confidence level. The confidence level and the alpha level are related. If you are conducting a hypothesis test with \\(H_a:\\mu\\ne \\mu_o\\), then the confidence level is \\(C=1-\\alpha\\). This is because the \\(\\alpha\\) is both tails and the confidence level is area between the two tails. As an example, for a hypothesis test \\(H_a:\\mu\\ne \\mu_o\\) with \\(\\alpha\\) equal to 0.05, the confidence level would be 0.95 or 95%. If you have a hypothesis test with \\(H_a:\\mu&lt;\\mu_o\\), then your \\(\\alpha\\) is only one tail of the curve. Because of symmetry the other tail is also \\(\\alpha\\). You have \\(2\\alpha\\) with both tails. So the confidence level, which is the area between the two tails, is \\(C-2\\alpha\\).\n\n\n9.1.2 Example: Stating the Statistical and Real World Interpretations for a Confidence Interval\n\nSuppose you have a 95% confidence interval for the mean age a woman gets married in 2013 is $26&lt;\\mu&lt;28$. State the statistical and real world interpretations of this statement.\nSuppose a 99% confidence interval for the proportion of Americans who have tried marijuana as of 2013 is $0.35&lt;p&lt;0.41$. State the statistical and real world interpretations of this statement.\n\n\n9.1.2.1 Solution\n\nSuppose you have a 95% confidence interval for the mean age a woman gets married in 2013 is $26&lt;\\mu&lt;28$. State the statistical and real world interpretations of this statement.\n\nStatistical Interpretation: You are 95% confident that the interval contains the mean age in 2013 that a woman gets married.\nReal World Interpretation: The mean age that a woman married in 2013 is between 26 and 28 years of age.\n\nSuppose a 99% confidence interval for the proportion of Americans who have tried marijuana as of 2013 is $0.35&lt;p&lt;0.41$. State the statistical and real world interpretations of this statement.\n\nStatistical Interpretation: You are 99% confident that the interval contains the proportion of Americans who have tried marijuana as of 2013.\nReal World Interpretation: The proportion of Americans who have tried marijuana as of 2013 is between 0.35 and 0.41.\nOne last thing to know about confidence is how the sample size and confidence level affect how wide the interval is. The following discussion demonstrates what happens to the width of the interval as you get more confident.\nThink about shooting an arrow into the target. Suppose you are really good at that and that you have a 90% chance of hitting the bull’s eye. Now the bull’s eye is very small. Since you hit the bull’s eye approximately 90% of the time, then you probably hit inside the next ring out 95% of the time. You have a better chance of doing this, but the circle is bigger. You probably have a 99% chance of hitting the target, but that is a much bigger circle to hit. You can see, as your confidence in hitting the target increases, the circle you hit gets bigger. The same is true for confidence intervals. This is demonstrated in Image \\#8.1.1.\n\n\n\nImage #8.1.1 Confidence Level Effect\n\n\nThe higher level of confidence makes a wider interval. There’s a trade off between width and confidence level. You can be really confident about your answer but your answer will not be very precise. Or you can have a precise answer (small margin of error) but not be very confident about your answer.\nNow look at how the sample size affects the size of the interval. Suppose Image \\#8.1.2 represents confidence intervals calculated on a 95% interval. A larger sample size from a representative sample makes the width of the interval narrower. This makes sense. Large samples are closer to the true population so the point estimate is pretty close to the true value.\n\n\n\nImage #8.1.2 Effect of Sample size\n\n\nNow you know everything you need to know about confidence intervals except for the actual formula. The formula depends on which parameter you are trying to estimate. With different situations you will be given the confidence interval for that parameter.\n\n\n\n9.1.3 Homework for Basics of Confidence Intervals Section\n\nSuppose you compute a confidence interval with a sample size of 25. What will happen to the confidence interval if the sample size increases to 50?\nSuppose you compute a 95% confidence interval. What will happen to the confidence interval if you increase the confidence level to 99%?\nSuppose you compute a 95% confidence interval. What will happen to the confidence interval if you decrease the confidence level to 90%?\nSuppose you compute a confidence interval with a sample size of 100. What will happen to the confidence interval if the sample size decreases to 80?\nA 95% confidence interval is \\(6353km&lt; \\mu&lt;6384km\\), where \\(\\mu\\) is the mean diameter of the Earth. State the statistical interpretation.\nA 95% confidence interval is \\(6353 km &lt; \\mu &lt; 6384 km\\), where \\(\\mu\\) is the mean diameter of the Earth. State the real world interpretation.\nIn 2013, Gallup conducted a poll and found a 95% confidence interval of $0.52 &lt; p &lt; 0.60$, where p is the proportion of Americans who believe it is the government’s responsibility for health care. Give the real world interpretation.\nIn 2013, Gallup conducted a poll and found a 95% confidence interval of $0.52 &lt; p &lt; 0.60$, where p is the proportion of Americans who believe it is the government’s responsibility for health care. Give the statistical interpretation.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "Estimation.html#one-sample-interval-for-the-proportion",
    "href": "Estimation.html#one-sample-interval-for-the-proportion",
    "title": "9  Estimation",
    "section": "9.2 One-Sample Interval for the Proportion",
    "text": "9.2 One-Sample Interval for the Proportion\nSuppose you want to estimate the population proportion, p. As an example you may be curious what proportion of students at your school smoke. Or you could wonder what is the proportion of accidents caused by teenage drivers who do not have a drivers’ education class.\n\n9.2.1 Confidence Interval for One Population Proportion (1-Prop Interval)\n\nState the random variable and the parameter in words.\n\n\\(x\\) = number of successes\n\\(p\\) = proportion of successes\n\nState and check the conditions for the confidence interval\n\n\n\nState: A simple random sample of size \\(n\\) is taken. Check: describe how sample was taken.\nState: The condition for the binomial distribution are satisfied. Check: argue that each condition has been met.\nState: The sampling distribution of \\(\\hat{p}\\) can be approximated by a normal distributed. check: To determine the sampling distribution of \\(\\hat{p}\\) is normally distributed, you need to show that \\(n*\\hat{p}\\ge5\\) and , \\(n*\\hat{q}\\ge5\\) where \\(\\hat{q}=1-\\hat{p}\\). If this requirement is true, then the sampling distribution of \\(\\hat{p}\\) is well approximated by a normal curve. (In reality this is not really true, since the correct condition deals with \\(p\\). However, in a confidence interval you do not know \\(p\\), so you must use \\(\\hat{p}\\).)\n\n\n\nFind the sample statistic and the confidence interval\n\nThis will be conducted using rStudio. The command is\nprop.test(r, n, conf.level=C) #type C as a decimal\n\nStatistical Interpretation: In general this looks like, “you are C% confident that \\(\\hat{p}\\pm E\\) contains the true proportion.”\nReal World Interpretation: This is where you state what interval contains the true proportion.\n\n\n\n9.2.2 Example: Confidence Interval for the Population Proportion\nA concern was raised in Australia that the percentage of deaths of Aboriginal prisoners was higher than the percent of deaths of non-Aboriginal prisoners, which is 0.27%. A sample of six years (1990-1995) of data was collected, and it was found that out of 14,495 Aboriginal prisoners, 51 died (\\“Indigenous deaths in,\\” 1996). Find a 95% confidence interval for the proportion of Aboriginal prisoners who died.\n\n9.2.2.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = number of Aboriginal prisoners who die\n\\(p\\) = proportion of Aboriginal prisoners who die\n\nState and check the conditions for the confidence interval\n\n\n\nState: A simple random sample of 14,495 Aboriginal prisoners was taken. Check: The sample was not a random sample, since it was data from six years. It is the numbers for all prisoners in these six years, but the six years were not picked at random. Unless there was something special about the six years that were chosen, the sample is probably a representative sample. This condition is probably met.\nState: The properties of the binomial experiment have been met. Check: There are 14,495 prisoners in this case. The prisoners are all Aboriginals, so you are not mixing Aboriginal with non-Aboriginal prisoners. There are only two outcomes, either the prisoner dies or doesn’t. The chance that one prisoner dies over another may not be constant, but if you consider all prisoners the same, then it may be close to the same probability. Thus the properties of the binomial experiment are satisfied\nState: The sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distribution. Check: \\(\\hat{p}*n=\\frac{51}{14495}*14495=51\\ge5\\) and \\(\\hat{q}*n=\\frac{14495-51}{14495}*14495=14444\\ge5\\). The sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distribution.\n\n\n\nFind the sample statistic and the confidence interval\n\nThe command in r Studio for a confidence interval for a proportion is\n\nprop.test(51,14495, conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  51 out of 14495\nX-squared = 14290, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.002647440 0.004661881\nsample estimates:\n          p \n0.003518455 \n\n\nthe 95% confidence level is \\(0.002647440&lt;p&lt;0.004661881\\).\n\nStatistical Interpretation: You are 95% confident that the interval \\(0.0026&lt;p&lt;0.0047\\) contains the proportion of Aboriginal prisoners who have died in prison.\nReal World Interpretation: The proportion of Aboriginal prisoners who died in prison is between 0.26% and 0.47%.\n\n\n\n\n9.2.3 Example: Confidence Interval for the Population Proportion\nA researcher who is studying the effects of income levels on breastfeeding of infants hypothesizes that countries with a low income level have a different rate of infant breastfeeding than higher income countries. It is known that in Germany, considered a high-income country by the World Bank, 22% of all babies are breastfeed. In Tajikistan, considered a low-income country by the World Bank, researchers found that in a random sample of 500 new mothers that 125 were breastfeeding their infant. Find a 90% confidence interval of the proportion of mothers in low-income countries who breastfeed their infants?\n\n9.2.3.1 Solution\n\nState you random variable and the parameter in words.\n\n\\(x\\) = number of woman who breastfeed in a low-income country\n\\(p\\) = proportion of woman who breastfeed in a low-income country\n\nState and check the conditions for the confidence interval\n\n\n\nState: A simple random sample of 500 breastfeeding habits of woman in a low-income country was taken. Check: This was stated in the problem.\nState: The properties of a Binomial Experiment have been met. Check: There were 500 women in the study. The women are considered identical, though they probably have some differences. There are only two outcomes, either the woman breastfeeds or she doesn’t. The probability of a woman breastfeeding is probably not the same for each woman, but it is probably not very different for each woman. The conditions for the binomial distribution are satisfied\nState: The sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distributed. Check:\\(n*\\hat{p}= 500*\\frac{125}{500}=125\\ge5\\) and \\(n*\\hat{q}=500*\\frac{500-125}{500}=375\\ge5\\), so the sampling distribution of \\(\\hat{p}\\) is well approximated by a normal distribution.\n\n\n\nFind the sample statistic and confidence interval\n\nOn rstudio, use the following command\n\nprop.test(125, 500, conf.level = .90)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  125 out of 500\nX-squared = 124, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n90 percent confidence interval:\n 0.2185980 0.2841772\nsample estimates:\n   p \n0.25 \n\n\n90% confidence interval for \\(p\\) is \\(0.2185980&lt;p&lt;0.2841772\\).\n\nStatistical Interpretation: You are 90% confident that \\(0.2185980&lt;p&lt;0.2841772\\) contains the proportion of women in low-income countries who breastfeed their infants.\nReal World Interpretation: The proportion of women in low-income countries who breastfeed their infants is between 0.219 and 0.284.\n\n\n\n\n9.2.4 Homework for One-Sample Interval for the Proportion Section\nIn each problem show all steps of the confidence interval. If some of the conditions are not met, note that the results of the interval may not be correct and then continue the process of the confidence interval.\n\nThe Arizona Republic/Morrison/Cronkite News poll published on Monday, October 20, 2016, found 390 of the registered voters surveyed favor Proposition 205, which would legalize marijuana for adults. The statewide telephone poll surveyed 779 registered voters between Oct. 10 and Oct. 15. (Sanchez, 2016) Find a 99% confidence interval for the proportion of Arizona’s who supported legalizing marijuana for adults.\nIn November of 1997, Australians were asked if they thought unemployment would increase. At that time 284 out of 631 said that they thought unemployment would increase (\\“Morgan gallup poll,\\” 2013). Estimate the proportion of Australians in November 1997 who believed unemployment would increase using a 95% confidence interval?\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, Arkansas had 1,601 complaints of identity theft out of 3,482 consumer complaints (\\“Consumer fraud and,\\” 2008). Calculate a 90% confidence interval for the proportion of identity theft in Arkansas.\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, Alaska had 321 complaints of identity theft out of 1,432 consumer complaints (\\“Consumer fraud and,\\” 2008). Calculate a 90% confidence interval for the proportion of identity theft in Alaska.\nIn 2013, the Gallup poll asked 1,039 American adults if they believe there was a conspiracy in the assassination of President Kennedy, and found that 634 believe there was a conspiracy (\\“Gallup news service,\\” 2013). Estimate the proportion of American’s who believe in this conspiracy using a 98% confidence interval.\nIn 2008, there were 507 children in Arizona out of 32,601 who were diagnosed with Autism Spectrum Disorder (ASD) (\\“Autism and developmental,\\” 2008). Find the proportion of ASD in Arizona with a confidence level of 99%.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "Estimation.html#one-sample-interval-for-the-mean",
    "href": "Estimation.html#one-sample-interval-for-the-mean",
    "title": "9  Estimation",
    "section": "9.3 One-Sample Interval for the Mean",
    "text": "9.3 One-Sample Interval for the Mean\nSuppose you want to estimate the mean height of Americans, or you want to estimate the mean salary of college graduates. A confidence interval for the mean would be the way to estimate these means.\n\n9.3.1 Confidence Interval for One Population Mean (t-Interval)\n\nState the random variable and the parameter in words.\n\n\\(x\\) = random variable\n\\(\\mu\\) = mean of random variable\n\nState and check the conditions for the confidence interval\n\n\n\nState: A random sample of size \\(n\\) is taken. Check: describe how the sample was collected.\nState: The population of the random variable is normally distributed. Check: look at density plot and normal quantile plot. Note: though the t-test is fairly robust to the condition if the sample size is large. This means that if this condition isn’t met, but your sample size is quite large, then the results of the t-test are valid.\n\n\n\nFind the sample statistic and confidence interval\n\nUse rStudio to find the confidence interval. The command is\nt.test(~variable, data= Data_Frame, conf.level=C) #type C as a decimal\n\nStatistical Interpretation: In general this looks like, “You are C% confident that the interval contains the true mean.”\nReal World Interpretation: This is where you state what interval contains the true mean.\n\n\n\n9.3.2 Example: Confidence Interval for the Population Mean\nA random sample of 50 body mass index (BMI) were taken from the NHANES Data frame Table 9.1. Estimate the mean BMI of Americans at the 95% level.\n\nsample_NHANES_50&lt;- sample_n(NHANES, size=50) \nknitr::kable(head(sample_NHANES_50))\n\n\n\nTable 9.1: BMI of Americans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n71326\n2011_12\nmale\n6\n0-9\nNA\nBlack\nBlack\nNA\nNA\n25000-34999\n30000\n0.86\n8\nRent\nNA\n23.4\nNA\nNA\n122.5\n15.60\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1.70\n1.29\n4.24\n38\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n3_hr\nMore_4_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n69060\n2011_12\nmale\n20\n20-29\nNA\nWhite\nWhite\nSome College\nNeverMarried\n10000-14999\n12500\n0.92\n1\nRent\nWorking\n57.3\nNA\nNA\n169.1\n20.00\nNA\n18.5_to_24.9\n68\n112\n59\n116\n52\n114\n60\n110\n58\n378.22\n1.55\n5.82\n156\n1.076\nNA\nNA\nNo\nNA\nVgood\n0\n7\nSeveral\nNone\nNA\nNA\nNA\n7\nNo\nYes\nNA\n1_hr\n1_hr\nNA\nNA\nYes\n4\n24\nNA\nNo\nNon-Smoker\nNA\nYes\n16\nNo\nNA\nNo\nYes\n15\n2\n1\nNo\nHeterosexual\nNA\n\n\n58885\n2009_10\nmale\n13\n10-19\n156\nWhite\nNA\nNA\nNA\nmore 99999\n100000\n3.60\n12\nOwn\nNA\n50.6\nNA\nNA\n161.7\n19.35\nNA\n18.5_to_24.9\n62\n107\n67\n104\n72\n108\n66\n106\n68\nNA\nNA\nNA\n233\nNA\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n54300\n2009_10\nmale\n16\n10-19\n192\nWhite\nNA\nNA\nNA\n65000-74999\n70000\n2.71\n6\nOwn\nNotWorking\n100.4\nNA\nNA\n179.5\n31.16\nNA\n30.0_plus\n80\n126\n52\n126\n54\n124\n54\n128\n50\nNA\n1.29\n3.93\n201\n2.365\nNA\nNA\nNo\nNA\nGood\n30\n0\nNA\nNA\nNA\nNA\nNA\n8\nNo\nYes\n5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n52400\n2009_10\nmale\n78\n70+\n946\nWhite\nNA\nSome College\nMarried\n25000-34999\n30000\n1.99\n8\nOwn\nNotWorking\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1.40\n5.79\n64\n0.529\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n9\nNo\nYes\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nYes\nSmoker\n18\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n67221\n2011_12\nmale\n22\n20-29\nNA\nBlack\nBlack\nSome College\nNeverMarried\n55000-64999\n60000\n1.81\n7\nOwn\nWorking\n116.4\nNA\nNA\n184.2\n34.30\nNA\n30.0_plus\n78\n133\n59\n142\n66\n132\n60\n134\n58\n294.82\n1.03\n4.01\n354\n2.744\nNA\nNA\nNo\nNA\nExcellent\n0\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nYes\n7\n4_hr\n3_hr\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nNo\nNA\n0\n0\nNo\nHeterosexual\nNA\n\n\n\n\n\n\n\n\n\n9.3.2.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = BMI of an American\n\\(\\mu\\) = mean BMI of Americans\n\nState and check the conditions for the confidence interval\n\n\n\nA random sample of 50 BMI levels was taken. Check: A random sample was taken from the NHANES data frame using r Studio\nThe population of BMI levels is normally distributed. Check:\n\n\ngf_density(~BMI, data=sample_NHANES_50, title=\"BMI of an American\", xlab=\"Body Mass Index\")\n\n\n\n\n\n\n\nFigure 9.1: Density Plot of BMI from NHANES sample\n\n\n\n\n\n\ngf_qq(~BMI, data=sample_NHANES_50, title=\"BMI of an American\") \n\n\n\n\n\n\n\nFigure 9.2: Normal quantile Plot of BMI from NHANES sample\n\n\n\n\n\nThe density plot looks somewhat skewed right and the normal quantile plot looks somewhat linear. There doesn’t seem to be strong evidence that the sample comes from a population that is normally distributed. However, since the sample is moderate to large, the t-test is robust to this condition not being met. So the results of the test are probably valid.\n\nFind the sample statistic and confidence interval\n\nOn r Studio, the command would be\n\nt.test(~BMI, data= sample_NHANES_50, conf.level=0.95)\n\n\n    One Sample t-test\n\ndata:  BMI\nt = 24.822, df = 44, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 25.31228 29.78594\nsample estimates:\nmean of x \n 27.54911 \n\n\nThe sample statistic is the mean of \\(x\\) in the output, and confidence interval is under the words 95 percent confidence interval.\n\nStatistical Interpretation: You are 95% confident that \\(24.87190&lt;\\mu&lt;28.71422\\) contains the mean BMI of Americans.\nReal World Interpretation: The mean BMI of Americans is between 24.87 and 28.71 \\(kg/m^2\\).\n\nNotice that in example the chapter 7, you were asked if the mean BMI of Americans was different from Australians’ mean BMI of 27.2 \\(kg/m^2\\). The interval that Example: Confidence Interval for the Population Mean calculated does contain the value of 27.2. So you can’t say that Americans’ mean BMI and Australians’ mean BMI are different.This means that you can just use confidence intervals and not conduct hypothesis tests at all if you prefer.\nNote: When creating this book, the random samples may change. So the answers may be different from what is said in the interpretations. This shows sampling variability, so it was not adjusted to show that this could happen.\n\n\n\n9.3.3 Example: Confidence Interval for the Population Mean\nThe data in Table 8.5 are the life expectancies for all people in European countries (\\“WHO life expectancy,\\” 2013). The data in Table 8.6 filtered the data frame for just males and just year 2000. The year 2000 was randomly chosen as the year to use. Estimate the mean life expectancy for a man in Europe at the 99% level.\nCode book for data frame Expectancy is below Table 8.5.\n\n9.3.3.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = life expectancy for a European man\n\\(\\mu\\) = mean life expectancy for European men\n\nState and check the conditions for the confidence interval\n\n\n\nState: A random sample of 53 life expectancies of European men in 2000 was taken.\nCheck: The data is actually all of the life expectancies for every country that is considered part of Europe by the World Health Organization in the year 2000. Since the year 2000 was picked at random, then the sample is a random sample.\nState: The distribution of life expectancies of European men in 2000 is normally distributed.\nCheck:\n\n\ngf_density(~expect, data=Expectancy_male, title=\"Life Expectancy of a male\", xlab=\"Life Expectancy of a Male\")\n\n\n\n\n\n\n\nFigure 9.3: Density Plot of Life Expectancy of Males in Europe in 2000\n\n\n\n\n\n\ngf_qq(~expect, data=Expectancy_male, title=\"Male Life Expectancy\")\n\n\n\n\n\n\n\nFigure 9.4: Quantile Plot of Life Expectancy of Males in Europe in 2000\n\n\n\n\n\nThis sample does not appear to come from a population that is normally distributed. This sample is moderate to large, so it is good that the t-test is robust.\n\nFind the sample statistic and confidence interval\n\nOn rStudio, the command would be\n\nt.test(~expect, data=Expectancy_male, conf.level=0.99) \n\n\n    One Sample t-test\n\ndata:  expect\nt = 90.919, df = 52, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n99 percent confidence interval:\n 68.60071 72.75778\nsample estimates:\nmean of x \n 70.67925 \n\n\nSample statistic is 70.68 years, and the confidence interval is \\(68.60071&lt;\\mu&lt;72.75778\\).\n\nStatistical Interpretation: You are 99% confident that \\(68.60071&lt;\\mu&lt;72.75778\\) contains the mean life expectancy of European men.\nReal World Interpretation: The mean life expectancy of European men is between 68.60 and 72.76 years.\n\n\n\n\n9.3.4 Homework for One-Sample Interval for the Mean Section\nIn each problem show all steps of the confidence interval. If some of the conditions are not met, note that the results of the interval may not be correct and then continue the process of the confidence interval.\n\nThe Kyoto Protocol was signed in 1997, and required countries to start reducing their carbon emissions. The protocol became enforceable in February 2005. Table 8.7 contains a random sample of CO2 emissions in 2010 (CO2 emissions (metric tons per capita), 2018). Find a 99% confidence interval for the mean CO-2 emissions in 2010.\n\nCode book for data frame Emission is below Table 8.7.\n\nThe amount of sugar in a Krispy Kream glazed donut is 10 g. Many people feel that cereal is a healthier alternative for children over glazed donuts. Table 8.8 contains the amount of sugar in a sample of cereal that is geared towards children (breakfast cereal, 2019). Estimate the mean amount of sugar in children’s cereal at the 95% confidence level.\n\nCode book for data frame Sugar is below Table 8.8.\nA new data frame will need to be created of just cereal for children. It is Table 8.9.\n\nThe FDA regulates that fish that is consumed is allowed to contain 1.0 mg/kg of mercury. In Florida, bass fish were collected in 53 different lakes to measure the health of the lakes. The data frame of measurements from Florida lakes is in Table 8.10 (NISER 081107 ID Data, 2019). Calculate with 90% confidence the mean amount of mercury in fish in Florida lakes. Is there too much mercury in the fish in Florida?\n\nCode book for data frame Mercury is below Table 8.10.\n\nThe data frame Table 4.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute. Estimate the mean pulse rate before exercise of females who do drink alcohol with a 95% level of confidence?\n\nCode book for data frame Pulse below Table 4.7.\nA new data frame with just females who drink alcohol is Table 10.2 from chapter 7.\n\nThe economic dynamism is an index of productive growth in dollars. Economic data for many countries are in Table 8.12 (SOCR Data 2008 World CountriesRankings, 2019).\n\nCode book for data frame Economics is below Table 8.12.\nA data frame that contains only middle income countries was created in chapter 7 and is Table 8.13. Find a 95% confidence interval for the mean economic dynamism for middle income countries.\n\nTable 3.18 contains the percentage of woman receiving prenatal care in a sample of countries over several years. (births per woman), 2019). Estimate the average percentage of women receiving prenatal care in 2009 (p2009) with a 95% confidence interval?\n\nCode book for Data frame Fert_prenatal is below Table 3.18.\n\nMaintaining your balance may get harder as you grow older. A study was conducted to see how steady the elderly is on their feet. They had the subjects stand on a force platform and have them react to a noise. The force platform then measured how much they swayed forward and backward, and the data is in Table 8.14 (Maintaining Balance while Concentrating, 2019). Find the mean forward/backward sway of elderly person? Use a 95% confidence level. Follow the filtering methods in other homework problems to create a data frame for only Elderly.\n\nCode book for data frame Sway is below Table 8.14.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html",
    "href": "Two Sample Inference.html",
    "title": "10  Two Sample Inference",
    "section": "",
    "text": "10.1 Two Proportions\nChapter 7 discussed methods of hypothesis testing about one-population parameters. Chapter 8 discussed methods of estimating population parameters from one sample using confidence intervals. This chapter will look at methods of confidence intervals and hypothesis testing for two populations. Since there are two populations, there are two random variables, two means or proportions, and two samples (though with paired samples you usually consider there to be one sample with pairs collected). Examples of where you would do this are:\nTesting and estimating the difference in testosterone levels of men before and after they had children (Gettler, McDade, Feranil & Kuzawa, 2011).\nTesting the claim that a diet works by looking at the weight before and after subjects are on the diet.\nEstimating the difference in proportion of those who approve of President Obama in the age group 18 to 26 year old and the 55 and over age group.\nAll of these are examples of hypothesis tests or confidence intervals for two populations. The methods to conduct these hypothesis tests and confidence intervals will be explored in this chapter. As a reminder, all hypothesis tests are the same process. The only thing that changes is the formula that you use and the conditions. Confidence intervals are also the same process, except that the formula is different.\nThere are times you want to test a claim about two population proportions or construct a confidence interval estimate of the difference between two population proportions. As with all other hypothesis tests and confidence intervals, the process is the same though the formulas and conditions are different.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html#two-proportions",
    "href": "Two Sample Inference.html#two-proportions",
    "title": "10  Two Sample Inference",
    "section": "",
    "text": "10.1.1 Hypothesis Test for Two Population Proportion (2-Prop Test)\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = number of successes from group 1\n\\(x_2\\) = number of successes from group 2\n\\(p_1\\) = proportion of successes in group 1\n\\(p_2\\) = proportion of successes in group 2\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:p_1=p_2\\)\n\\(H_a: p_1\\ne p_2\\). the \\(\\ne\\) can be replaced with \\(&lt;\\) or \\(&gt;\\) depending on the question.\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A simple random sample of size \\(n_1\\) is taken from population 1, and a simple random sample of size \\(n_2\\) is taken from population 2. Check: describe how each sample was collected.\nState: The samples are independent. Check: describe why the two samples are independent.\nState: The properties for the binomial distribution are satisfied for both populations. Check: describe how each population meets all the properties.\nState: The sampling distribution of \\(\\hat{p_1}\\) can be approximated as a normal distribution. Check: To determine the sampling distribution of \\(\\hat{p_1}\\), you need to show that \\(p_1*n_1\\ge5\\) and \\(q_1*n_1\\ge5\\) where \\(q_1=1-p_1\\). If this requirement is true, then the sampling distribution of \\(\\hat{p_1}\\) is well approximated by a normal curve. State: The sampling distribution of \\(\\hat{p_2}\\) can be approximated as a normal distribution. Check: To determine the sampling distribution of \\(\\hat{p_2}\\), you need to show that \\(p_2*n_2\\ge 5\\) and \\(q_2*n_2\\ge 5\\) where \\(q_2=1-p_2\\). If this requirement is true, then the sampling distribution of \\(\\hat{p_2}\\) is well approximated by a normal curve. However, if you do not know \\(p_1\\) and \\(p_2\\), you will need to use \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) instead. This is not perfect, but it is the best you can do.\n\n\n\nFind the sample statistics, test statistic, and p-value\n\nOn rStudio, use the command\nprop.test(c(x1,x2), c(n1, n2)\n\nConclusion\n\nThis is where you write reject or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\).\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n10.1.2 Confidence Interval for the Difference Between Two Population Proportion (2-Prop Interval)\nThe confidence interval for the difference in proportions has the same random variables and proportions and the same conditions as the hypothesis test for two proportions. If you have already completed the hypothesis test, then you do not need to state them again. If you haven’t completed the hypothesis test, then state the random variables and proportions and state and check the conditions before completing the confidence interval step.\n\nFind the sample statistics and the confidence interval\n\nThe confidence interval estimate of the difference is found using the following command in r Studio:\nprop.test(c(x1,x2), c(n1, n2), conf.level=C) Type C as a decimal\n\nStatistical Interpretation: In general this looks like, “You are C% confident that the confidence interval contains the true difference in proportions.”\nReal World Interpretation: This is where you state how much more (or less) the first proportion is from the second proportion.\n\n\n\n10.1.3 Example: Hypothesis Test for Two Population Proportions\nDo husbands cheat on their wives in a different proportion from the proportion of wives cheat on their husbands (“Statistics brain,” 2013)? Suppose you take a group of 1000 randomly selected husbands and find that 231 had cheated on their wives. Suppose in a group of 1200 randomly selected wives, 176 cheated on their husbands. Do the data show that the proportion of husbands who cheat on their wives is different from the proportion of wives who cheat on their husbands. Test at the 5% level.\n\n10.1.3.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = number of husbands who cheat on his wife\n\\(x_2\\) = number of wives who cheat on her husband\n\\(p_1\\) = proportion of husbands who cheat on his wife\n\\(p_2\\) = proportion of wives who cheat on her husband\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: p_1=p_2\\)\n\\(H_a: p_1\\ne p_2\\)\nlevel of significance is \\(\\alpha=0.05\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A simple random sample of 1000 responses about cheating from husbands is taken. Check: This was stated in the problem. State: A simple random sample of 1200 responses about cheating from wives is taken. Check: This was stated in the problem.\nState: The samples are independent. Check: The samples are independent. This is true since the samples involved different genders.\nState: The properties of the binomial distribution are satisfied in both populations. Check: This is true since there are only two responses, there are a fixed number of trials, the probability of a success is the same, and the trials are independent.\nState: The sampling distributions of \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) can be approximated with a normal distribution. Check: \\(n_1*p_1\\), \\(n_2*p_2\\), \\(n_1*q_1\\), and \\(n_2*q_2\\) are all greater than or equal to 5. So both sampling distributions of \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) can be approximated with a normal distribution.\n\n\n\nFind the sample statistics, test statistic, and p-value\n\nOn r use the command:\n\nprop.test(c(231,176), c(1000, 1200))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c out of c231 out of 1000176 out of 1200\nX-squared = 25.173, df = 1, p-value = 5.241e-07\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.05050705 0.11815962\nsample estimates:\n   prop 1    prop 2 \n0.2310000 0.1466667 \n\n\n\nConclusion\n\nReject \\(H_o\\), since the p-value is less than 5%.\n\nInterpretation\n\nThis is enough evidence to support that the proportion of husbands having affairs is different from the proportion of wives having affairs.\n\n\n\n10.1.4 Example: Confidence Interval for Two Population Proportions\nWhat is the difference in proportion that husbands cheat on their wives than wives cheat on the husbands (“Statistics brain,” 2013)? Suppose you take a group of 1000 randomly selected husbands and find that 231 had cheated on their wives. Suppose in a group of 1200 randomly selected wives, 176 cheated on their husbands. Estimate the difference in the proportion of husbands and wives who cheat on their spouses using a 95% confidence level.\n\n10.1.4.1 Solution\n\nState the random variables and the parameters in words.\n\nThese were stated in Example: Hypothesis Test for Two Population Proportions.\n\nState and check the conditions for the confidence interval\n\nThe conditions were stated and checked in Example: Hypothesis Test for Two Population Proportions.\n\nFind the sample statistics and the confidence interval\n\nOn r use the command:\n\nprop.test(c(231,176), c(1000, 1200), conf.level = .95)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c out of c231 out of 1000176 out of 1200\nX-squared = 25.173, df = 1, p-value = 5.241e-07\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.05050705 0.11815962\nsample estimates:\n   prop 1    prop 2 \n0.2310000 0.1466667 \n\n\n\nStatistical Interpretation: You are 95% confident that \\(0.05050705&lt;p_1-p_2&lt;0.11815962\\) contains the true difference in proportions.\nReal World Interpretation: The proportion of husbands who cheat on their wives is anywhere from 5.05% to 11.82% higher than the proportion of wives who cheat on their husband.\n\n\n\n\n10.1.5 Homework for Two Proportions Section\nIn each problem show all steps of the hypothesis test or confidence interval. If some of the conditions are not met, note that the results of the test or interval may not be correct and then continue the process of the hypothesis test or confidence interval.\n\nMany high school students take the AP tests in different subject areas. In 2007, of the 144,796 students who took the biology exam 84,199 of them were female. In that same year, of the 211,693 students who took the calculus AB exam 102,598 of them were female (“AP exam scores,” 2013). Is there enough evidence to show that the proportion of female students taking the biology exam is different than the proportion of female students taking the calculus AB exam? Test at the 5% level.\nMany high school students take the AP tests in different subject areas. In 2007, of the 144,796 students who took the biology exam 84,199 of them were female. In that same year, of the 211,693 students who took the calculus AB exam 102,598 of them were female (“AP exam scores,” 2013). Estimate the difference in the proportion of female students taking the biology exam and female students taking the calculus AB exam using a 90% confidence level.\nMany high school students take the AP tests in different subject areas. In 2007, of the 211,693 students who took the calculus AB exam 102,598 of them were female and 109,095 of them were male (“AP exam scores,” 2013). Is there enough evidence to show that the proportion of female students taking the calculus AB exam is different from the proportion of male students taking the calculus AB exam? Test at the 5% level.\nMany high school students take the AP tests in different subject areas. In 2007, of the 211,693 students who took the calculus AB exam 102,598 of them were female and 109,095 of them were male (“AP exam scores,” 2013). Estimate using a 90% level the difference in proportion of female students taking the calculus AB exam versus male students taking the calculus AB exam.\nAre there more children diagnosed with Autism Spectrum Disorder (ASD) in states that have larger urban areas over states that are mostly rural? In the state of Pennsylvania, a fairly urban state, there are 245 eight year old diagnosed with ASD out of 18,440 eight year old evaluated. In the state of Utah, a fairly rural state, there are 45 eight year old diagnosed with ASD out of 2,123 eight year old evaluated (“Autism and developmental,” 2008). Is there enough evidence to show that the proportion of children diagnosed with ASD in Pennsylvania is different than the proportion in Utah? Test at the 1% level.\nAre there more children diagnosed with Autism Spectrum Disorder (ASD) in states that have larger urban areas over states that are mostly rural? In the state of Pennsylvania, a fairly urban state, there are 245 eight year old diagnosed with ASD out of 18,440 eight year old evaluated. In the state of Utah, a fairly rural state, there are 45 eight year old diagnosed with ASD out of 2,123 eight year old evaluated (“Autism and developmental,” 2008). Estimate the difference in proportion of children diagnosed with ASD between Pennsylvania and Utah. Use a 98% confidence level.\nA child dying from an accidental poisoning is a terrible incident. Is it more likely that a male child will get into poison than a female child? To find this out, data was collected that showed that out of 1830 children between the ages one and four who pass away from poisoning, 1031 were males and 799 were females (Flanagan, Rooney & Griffiths, 2005). Do the data show that there is different proportion of male children dying of poisoning than female children? Test at the 1% level.\nA child dying from an accidental poisoning is a terrible incident. Is it more likely that a male child will get into poison than a female child? To find this out, data was collected that showed that out of 1830 children between the ages one and four who pass away from poisoning, 1031 were males and 799 were females (Flanagan, Rooney & Griffiths, 2005). Compute a 99% confidence interval for the difference in proportions of poisoning deaths of male and female children ages one to four.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html#paired-samples-for-two-means",
    "href": "Two Sample Inference.html#paired-samples-for-two-means",
    "title": "10  Two Sample Inference",
    "section": "10.2 Paired Samples for Two Means",
    "text": "10.2 Paired Samples for Two Means\nAre two populations the same? Is the average height of men taller than the average height of women? Is the mean weight less after a diet than before?\nYou can compare populations by comparing their means. You take a sample from each population and compare the statistics.\nAnytime you compare two populations you need to know if the samples are independent or dependent. The formulas you use are different for different types of samples.\nIf how you choose one sample has no effect on the way you choose the other sample, the two samples are independent. The way to think about it is that in independent samples, the observations from one sample are overall different from the observations from the other sample. This will mean that sample one has no affect on sample two. The sample values from one sample are not related or paired with values from the other sample.\nIf you choose the samples so that a measurement in one sample is paired with a measurement from the other sample, the samples are dependent or matched or paired. (Often a before and after situation.) You want to make sure the there is a meaning for pairing data values from one sample with a specific data value from the other sample. One way to think about it is that in dependent samples, the observations from one sample are the same observations from the other sample, though there can be other reasons to pair values. This makes the sample values from each sample paired.\nIn tidy data, remember each row is a unit of observation, and each column is a variable. In paired samples, you would have two variables that you are working with. In independent samples, you would have a variable that distinguishes an observation from another observation. As an example, in the Pulse data frame, consider the variables pulse_before and pulse_after. Since they are measured off the same observation, then comparing the two variables would be a paired samples analysis. However, consider the pulse_after and whether a person smokes would be comparing the variable pulse_after against the variable smokes to see if smoking effects a person’s pulse rate after exercise. In this case, the observations would be different based on smoking yes or smoking no. Consider the variable smoking to be the factor that one is interested in seeing how it effects pulse rate in the data frame Table 4.7.\n\n10.2.1 Example: Independent or Dependent Samples\nDetermine if the following are dependent or independent samples.\n\nRandomly choose 5 men and 6 women and compare their heights\nChoose 10 men and weigh them. Give them a new diet drug and later weigh them again.\nTake 10 people and measure the strength of their dominant arm and their non-dominant arm.\n\n\n10.2.1.1 Solution\n\nRandomly choose 5 men and 6 women and compare their heights\nIndependent, since there is no reason that one value belongs to another. The units of observations are not the same for both samples. The units of observations are definitely different. A way to think about this is that the knowledge that a man is chosen in one sample does not give any information about any of the woman chosen in the other sample.\n\n\n\nChoose 10 men and weigh them. Give them a new diet drug and later weigh them again.\nDependent, since each person’s before weight can be matched with their after weight. The units of observations are the same for both samples. A way to think about this is that the knowledge that a person weighs 400 pounds at the beginning will tell you something about their weight after the diet drug.\nTake 10 people and measure the strength of their dominant arm and their non-dominant arm.\nDependent, since you can match the two arm strengths. The units of observations are the same for both samples. So the knowledge of one person’s dominant arm strength will tell you something about the strength of their non-dominant arm.\n\nTo analyze data when there are matched or paired samples, called dependent samples, you conduct a paired t-test. Since the samples are matched, you can find the difference between the values of the two random variables.\n\n\n\n10.2.2 Hypothesis Test for Two Sample Paired t-Test\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = random variable 1\n\\(x_2\\) = random variable 2\n\\(\\mu_1\\) = mean of random variable 1\n\\(\\mu_2\\) = mean of random variable 2\n\nState the null and alternative hypotheses and the level of significance\n\nThe hypotheses would be\n\\(H_o:\\mu_1=\\mu_2\\) or\\(H_o:\\mu_1-\\mu_2=0\\)\n\\(H_a:\\mu_1\\ne \\mu_2\\) or \\(H_a:\\mu_1-\\mu_2\\ne0\\)\nHowever, since you are finding the differences, then you can actually think of \\(\\mu_1-\\mu_2=\\mu_d\\).\nSo the hypotheses could become\n\\(H_o:\\mu_d=0\\)\n\\(H_a:\\mu_d\\ne 0\\) Remember, you can replace \\(\\ne\\) with \\(&lt;\\) or \\(&gt;\\).\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of \\(n\\) pairs is taken. Check: state how the sample was collected.\nCheck: The population of the difference between random variables is normally distributed. Check: In this case the population you are interested in has to do with the differences that you find. It does not matter if each random variable is normally distributed. It is only important if the differences you find are normally distributed. Just as before, the t-test is fairly robust to the condition if the sample size is large. This means that if this condition isn’t met, but your sample size is quite large, then the results of the t-test are valid.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nRealize that a paired test is a one sample t-test on the difference between two variables. So you are running a one-sample t-test on a new variable known as the difference variable. You need to create this difference variable by creating a new data frame. This is done on rStudio by doing the following command (The following shows how to create the variable difference for pulse_after-pulse_before on the data frame Pulse. Change the variables used and data frame used to your data frame and variables):\n\nPulse&lt;-\n  Pulse |&gt;\n  mutate(difference=pulse_after-pulse_before) \nknitr::kable(head(Pulse))\n\n\n\nTable 10.1: Pulse Data frame with Difference Column Added\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\ndifference\n\n\n\n\n170\n68\n22\nmale\nyes\nyes\nmoderate\nsat\n70\n71\n93\n1\n\n\n182\n75\n26\nmale\nyes\nyes\nmoderate\nsat\n80\n76\n93\n-4\n\n\n180\n85\n19\nmale\nyes\nyes\nmoderate\nran\n68\n125\n95\n57\n\n\n182\n85\n20\nmale\nyes\nyes\nlow\nsat\n70\n68\n95\n-2\n\n\n167\n70\n22\nmale\nyes\nyes\nlow\nsat\n92\n84\n96\n-8\n\n\n178\n86\n21\nmale\nyes\nyes\nlow\nsat\n76\n80\n98\n4\n\n\n\n\n\n\n\n\nNotice rStudio added a new variable called difference to the data frame Table 10.1. Now to conduct a paired t-test use the rStudio command\nt.test(~difference_variable, data=Data_Frame)\nNote: if the \\(H_a\\) is &lt;, then the command becomes\nt.test(~difference_variable, data=Data_Frame, alternative=“less”)\nSimilarly for &gt; put alternative=“greater”\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge\\alpha\\), then fail to reject \\(H_o\\).\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n10.2.3 Confidence Interval for Difference in Means from Paired Samples (t-Interval)\nThe confidence interval for the difference in means has the same random variables and means and the same conditions as the hypothesis test for two paired samples. If you have already completed the hypothesis test, then you do not need to state them again. If you haven’t completed the hypothesis test, then state the random variables and means, and state and check the conditions before completing the confidence interval step.\n\nFind the sample statistic and confidence interval. Again, you will need to create a new data frame with a difference variable. Then on rStudio the command is\nt.test(~difference_variable, data=Data_Frame, conf.level=C) Type C as a decimal\n\n\n\nStatistical Interpretation: In general this looks like, “You are C% confident that the statement contains the true mean difference.”\nReal World Interpretation: This is where you state what interval contains the true mean difference.\n\n\n\n10.2.4 Example: Hypothesis Test for Paired Samples\nIs the pulse rate after exercise different from the pulse rate before exercise for a woman who drinks alcohol? Use the data frame Table 4.7. Test at the 5% level.\nCode book for data frame Pulse below Table 4.7.\n\n10.2.4.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = pulse of a smoking woman who drinks alcohol after exercise\n\\(x_2\\) = pulse of a smoking woman who drinks alcohol before exercise\n\\(\\mu_1\\) = mean pulse of a smoking woman who drinks alcohol after exercise\n\\(\\mu_2\\) = mean pulse of a smoking woman who drinks alcohol after exercise\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\mu_1=\\mu_2\\)\n\\(H_a: \\mu_1\\ne \\mu_2\\)\nlevel of significance, \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of 110 pairs of pulse rates after and before exercise was taken. Check: The data frame says that the data was collected from students in classes for several years. Though this was not a random sample, it is probably a representative sample.\nState: The population of the difference in after and before pulse rates is normally distributed. Check: To see if this is true, look at the density plot and the normal quantile plot for the difference between after and before. This variable must be created before the density plot and normal quantile plot can be created. The data frame Table 10.2 is females who drink alcohol.\n\n\nPulse_female&lt;- \n  Pulse |&gt; \n  filter(gender==\"female\", alcohol==\"yes\") \nknitr::kable(head(Pulse_female))\n\n\n\nTable 10.2: Pulse Rates Before and After Exercise of Females who do drink Alcohol with Difference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\ndifference\n\n\n\n\n165\n60\n19\nfemale\nyes\nyes\nlow\nran\n88\n120\n98\n32\n\n\n163\n47\n23\nfemale\nyes\nyes\nlow\nran\n71\n125\n98\n54\n\n\n173\n57\n18\nfemale\nno\nyes\nmoderate\nsat\n86\n88\n93\n2\n\n\n179\n58\n19\nfemale\nno\nyes\nmoderate\nran\n82\n150\n93\n68\n\n\n167\n62\n18\nfemale\nno\nyes\nhigh\nran\n96\n176\n93\n80\n\n\n173\n64\n18\nfemale\nno\nyes\nlow\nsat\n90\n88\n93\n-2\n\n\n\n\n\n\n\n\nNow mutate Table 10.2 data frame to include a difference variable.\n\nPulse_female&lt;-\n  Pulse_female |&gt; \n  mutate(difference=pulse_after-pulse_before) \nknitr::kable(head(Pulse_female))\n\n\n\nTable 10.3: Pulse Rates Before and After Exercise of Females who do drink Alcohol with Difference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\ndifference\n\n\n\n\n165\n60\n19\nfemale\nyes\nyes\nlow\nran\n88\n120\n98\n32\n\n\n163\n47\n23\nfemale\nyes\nyes\nlow\nran\n71\n125\n98\n54\n\n\n173\n57\n18\nfemale\nno\nyes\nmoderate\nsat\n86\n88\n93\n2\n\n\n179\n58\n19\nfemale\nno\nyes\nmoderate\nran\n82\n150\n93\n68\n\n\n167\n62\n18\nfemale\nno\nyes\nhigh\nran\n96\n176\n93\n80\n\n\n173\n64\n18\nfemale\nno\nyes\nlow\nsat\n90\n88\n93\n-2\n\n\n\n\n\n\n\n\nUsing Table 10.3 create a density plot and normal quantile plot on the difference variable.\n\ngf_density(~difference, data=Pulse_female, title = \"Difference in Pulse Rates for Females who drink Alcohol\", xlab=\"Difference Between Before and After\")\n\n\n\n\n\n\n\nFigure 10.1: Density plot of differences in pulse rates\n\n\n\n\n\n\ngf_qq(~difference, data=Pulse_female, title = \"Difference in Pulse Rates for Females who drink Alcohol\") \n\n\n\n\n\n\n\nFigure 10.2: Normal Quantile Plot of Differences in Pulse Rates\n\n\n\n\n\nThe density plot is not symmetrical and the normal quantile plot on the differences is not linear. So you cannot assume that the distribution of the difference in pulse rates is normal. It is good that the t-test is robust if there is a large sample. The sample is of size 110, so that should be adequate to assume the conclusion is valid.\n\nFind the sample statistic, test statistic, and p-value On r Studio, use the command:\n\n\nt.test(~difference, data=Pulse_female)\n\n\n    One Sample t-test\n\ndata:  difference\nt = 4.1353, df = 26, p-value = 0.0003283\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.51152 34.26625\nsample estimates:\nmean of x \n 22.88889 \n\n\n\nConclusion\n\nSince the p-value &lt; 0.05, reject \\(H_o\\).\n\nInterpretation\n\nThere is enough evidence to support that there is a difference in pulse rate before and after exercise of females who smoke.\n\n\n\n10.2.5 Example: Hypothesis Test for Paired Samples\nThe New Zealand Air Force purchased a batch of flight helmets. They then found out that the helmets didn’t fit. In order to make sure that they order the correct size helmets, they measured the head size of recruits. To save money, they wanted to use cardboard calipers, but were not sure if they will be accurate enough. So they took 18 recruits and measured their heads with the cardboard calipers and also with metal calipers. The data frame is in Table 10.4 (Helmet Sizes for New Zealand Airforce, 2019). Do the data provide enough evidence to show that there is a difference in measurements between the cardboard and metal calipers? Use a 5% level of significance.\n\nHelmet&lt;-read.csv( \"https://krkozak.github.io/MAT160/helmet.csv\") \nknitr::kable(head(Helmet))\n\n\n\nTable 10.4: Helmet Head Measurments\n\n\n\n\n\n\nCardboard\nMetal\n\n\n\n\n146\n145\n\n\n151\n153\n\n\n163\n161\n\n\n152\n151\n\n\n151\n145\n\n\n151\n150\n\n\n\n\n\n\n\n\nCode book for data frame Helmet\nDescription After purchasing a batch of flight helmets that did not fit the heads of many pilots, the NZ Airforce decided to measure the head sizes of all recruits. Before this was carried out, information was collected to determine the feasibility of using cheap cardboard calipers to make the measurements, instead of metal ones which were expensive and uncomfortable. The data lists the head diameters of 18 recruits measured once using cardboard calipers and again using metal calipers. One question is whether there is any systematic difference between the two sets of calipers. One might also ask whether there is more variability in the cardboard calipers measurement than that of the metal calipers.\nThis data frame contains the following columns:\nCardboard: measurement using cardboard calipers (cm)\nMetal: measurement using metal calipers (cm)\nSource Helmet Sizes for New Zealand Airforce. (n.d.). Retrieved July 20, 2019, from http://www.statsci.org/data/oz/nzhelmet.html\nReferences Data courtesy of Dr Stephen Legg. Seber and Lee (1998). Page 545.\n\n10.2.5.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = head measurement of recruit using cardboard caliper\n\\(x_2\\) = head measurement of recruit using metal caliper\n\\(\\mu_1\\)= mean head measurement of recruit using cardboard caliper\n\\(\\mu_2\\) = mean head measurement of recruit using metal caliper\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu_1=\\mu_2\\)\n\\(H_a:\\mu_1\\ne \\mu_2\\)\nlevel of significance, \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of 18 pairs of head measures of recruits with cardboard and metal caliper was taken. Check: This was not stated, but probably could be safely assumed.\nState: The population of the difference in head measurements between cardboard and metal calipers is normally distributed. Check: First create the difference variable, then the density plot and normal quantile plot.\n\n\nHelmet&lt;-\n  Helmet |&gt; \n  mutate(difference=Cardboard-Metal) \nknitr::kable(head(Helmet))\n\n\n\nTable 10.5: Helmet Head Measurments\n\n\n\n\n\n\nCardboard\nMetal\ndifference\n\n\n\n\n146\n145\n1\n\n\n151\n153\n-2\n\n\n163\n161\n2\n\n\n152\n151\n1\n\n\n151\n145\n6\n\n\n151\n150\n1\n\n\n\n\n\n\n\n\n\ngf_density(~difference, data=Helmet, title=\"Differences in Head Measurements\", xlab=\"Difference Between Cardboard and Metal\") \n\n\n\n\n\n\n\nFigure 10.3: Density plot of differences in head measurements\n\n\n\n\n\n\ngf_qq(~difference, data=Helmet, title=\"Differences in Head Measurements\") \n\n\n\n\n\n\n\nFigure 10.4: Normal Quantile Plot of Differences in Head Measurements\n\n\n\n\n\nThis density plot Figure 10.3 looks somewhat bell shaped. The normal quantile plot Figure 10.4 on the differences looks somewhat linear. So you can assume that the distribution of the difference in weights is normal.\n\nFind the sample statistic, test statistic, and p-value\n\nUsing rStudio the command is\n\nt.test(~difference, data=Helmet)\n\n\n    One Sample t-test\n\ndata:  difference\nt = 3.1854, df = 17, p-value = 0.005415\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.5440163 2.6782060\nsample estimates:\nmean of x \n 1.611111 \n\n\nThe sample statistic is 1.6111, the test statistic is 3.1854, and the p-value is 0.005415.\n\nConclusion\n\nSince the p-value \\(&lt;\\) 0.05, reject \\(H_o\\).\n\nInterpretation\n\nThere is enough evidence to support that the mean head measurements using the cardboard calipers are not the same as when using the metal calipers. So it looks like the New Zealand Air Force shouldn’t use the cardboard calipers.\n\n\n\n10.2.6 Example: Confidence Interval for Paired Samples\nThe New Zealand Air Force purchased a batch of flight helmets. They then found out that the helmets didn’t fit. In order to make sure that they order the correct size helmets, they measured the head size of recruits. To save money, they wanted to use cardboard calipers, but were not sure if they will be accurate enough. So they took 18 recruits and measured their heads with the cardboard calipers and also with metal calipers. The data frame is in Table 10.4 (Helmet Sizes for New Zealand Airforce, 2019). Estimate the difference in measurements between the cardboard and metal calipers using a 95% confidence interval.\n\n10.2.6.1 Solution\n\nState the random variables and the parameters in words.\n\nThese were stated in Example: Hypothesis Test for Paired Samples.\n\nState and check the conditions for the confidence interval\n\nThe conditions were stated and checked in Example: Hypothesis Test for Paired Samples.\n\nFind the sample statistic and confidence interval\n\nUsing the data frame Table 10.5 the rStudio the command is\n\nt.test(~difference, data=Helmet, conf.leve=0.95)\n\n\n    One Sample t-test\n\ndata:  difference\nt = 3.1854, df = 17, p-value = 0.005415\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.5440163 2.6782060\nsample estimates:\nmean of x \n 1.611111 \n\n\n\nStatistical Interpretation: You are 95% confidence that \\(0.5440163&lt;\\mu_1-\\mu_2&lt;2.6782060\\) contains the true mean difference in head measurement between using the cardboard and metal calibers.\nReal World Interpretation: The mean head measurement using the cardboard calibers is anywhere from 0.54 cm to 2.68 cm more than the head measurement using the metal calibers.\n\nExamples 9.2.6 and 9.2.7 use the same data set, but one is conducting a hypothesis test and the other is conducting a confidence interval. Notice that the hypothesis test’s conclusion was to reject and say that there was a difference in the means, and the confidence interval does not contain the number 0. If the confidence interval did contain the number 0, then that would mean that the two means could be the same. Since the interval did not contain 0, then you could say that the means are different just as in the hypothesis test. This means that the hypothesis test and the confidence interval can produce the same interpretation. Do be careful though, you can run a hypothesis test with a particular significance level and a confidence interval with a confidence level that is not compatible with your significance level. This will mean that the conclusion from the confidence interval would not be the same as with a hypothesis test. So if you want to estimate the mean difference, then conduct a confidence interval. If you want to show that the means are different, then conduct a hypothesis test. As a reminder, the American Statistical Association (ASA) suggests not conducting hypothesis tests and just create confidence intervals.\n\n\n\n10.2.7 Homework for Paired Samples for Two Means Section\nIn each problem show all steps of the hypothesis test or confidence interval. If some of the conditions are not met, note that the results of the test or interval may not be correct and then continue the process of the hypothesis test or confidence interval.\n\nThe cholesterol level of patients who had heart attacks was measured multiple times after the heart attack. The researchers want to see if the cholesterol level of patients who have heart attacks changes as the time since their heart attack increases. The data is in Table 4.2. Do the data show that the mean cholesterol level of patients that have had a heart attack changes as the time increases since their heart attack? Use day2 and day4 variables to answer the question. Test at the 1% level.\n\nCode book for Data Frame Cholesterol is below Table 10.5.\n\nThe cholesterol level of patients who had heart attacks was measured multiple times after the heart attack. The researchers want to see if the cholesterol level of patients who have heart attacks changes as the time since their heart attack increases. The data is in Table 10.5. Calculate a 98% confidence interval for the mean difference in cholesterol levels from day two to day four.\nAll Fresh Seafood is a wholesale fish company based on the east coast of the U.S. Catalina Offshore Products is a wholesale fish company based on the west coast of the U.S. Table 10.6 contains prices from both companies for specific fish types (\\“Seafood online,\\” 2013) (\\“Buy sushi grade,\\” 2013). Do the data provide enough evidence to show that fish cost different from west coast fish wholesaler and east coast wholesaler? Test at the 5% level.\n\n\nPrice &lt;- read.csv( \"https://krkozak.github.io/MAT160/price.csv\") \nknitr::kable(head(Price))\n\n\n\nTable 10.6: Wholesale Prices of Fish in Dollars\n\n\n\n\n\n\nfish\neast\nwest\n\n\n\n\nCod\n19.99\n17.99\n\n\nTilapi\n6.00\n13.99\n\n\nFarmed Salmon\n19.99\n22.99\n\n\nOrganic Salmon\n24.99\n24.99\n\n\nGrouper Fillet\n29.99\n19.99\n\n\nTuna\n28.99\n31.99\n\n\n\n\n\n\n\n\nCode book for data frame Price\nDescription Price of fish was collected from two websites. One for Catalina Offshore Products (west coast) and the other for All Fresh Seafood (east coast) in 2013.\nThis data frame contains the following columns:\nfish: type of fish for sale\neast: price of fish from east coast supplier ($)\nwest: price of fish from west coast supplier ($)\nSource Seafood online. (2013, November 20). Retrieved from http://www.allfreshseafood.com/\nBuy sushi grade fish online. (2013, November 20). Retrieved from http://www.catalinaop.com/\nReferences Websites of Catalina Offshore Products and All Fresh Seafood\n\nAll Fresh Seafood is a wholesale fish company based on the east coast of the U.S. Catalina Offshore Products is a wholesale fish company based on the west coast of the U.S. Table 10.6 contains prices from both companies for specific fish types (\\“Seafood online,\\” 2013) (\\“Buy sushi grade,\\” 2013). Find a 95% confidence interval for the mean difference in wholesale price between the east coast and west coast suppliers.\nThe British Department of Transportation studied to see if people avoid driving or shopping, or have more accidents on Friday the 13th. They collected data from different locations (Friday the 13th, 2019). The data for each location on the two different dates is in Table 10.7. Do the data show that on average different number of people are engaged in activities on Friday the 13th? Test at the 5% level.\n\n\nTraffic &lt;- read.csv( \"https://krkozak.github.io/MAT160/traffic.csv\") \nknitr::kable(head(Traffic))\n\n\n\nTable 10.7: Traffic Count\n\n\n\n\n\n\nsource\nyear\nmonth\nX6th\nX13th\nlocation\n\n\n\n\ntraffic\n1990,\nJuly\n139246\n138548\n7 to 8\n\n\ntraffic\n1990,\nJuly\n134012\n132908\n9 to 10\n\n\ntraffic\n1991,\nSeptember\n137055\n136018\n7 to 8\n\n\ntraffic\n1991,\nSeptember\n133732\n131843\n9 to 10\n\n\ntraffic\n1991,\nDecember\n123552\n121641\n7 to 8\n\n\ntraffic\n1991,\nDecember\n121139\n118723\n9 to 10\n\n\n\n\n\n\n\n\nCode book for data frame Traffic\nDescription This file consists of three separate data sets, all of which address the issues of how superstitions regarding Friday the 13th affect human behavior, and whether Friday the 13th is an unlucky day. Scanlon, et al. collected data on traffic and shopping patterns and accident frequency for Fridays the 6th and 13th between October of 1989 and November of 1992.\nFor the first data set, the researchers obtained information from the British Department of Transport regarding the traffic flows between junctions 7 to 8 and junctions 9 to 10 of the M25 motorway. They collected the numbers of shoppers in nine different supermarkets in southeast England for the second data set. The third data set contains numbers of emergency admissions to hospitals due to transport accidents.\nWe present the three data sets in a combined format, with the variable “Data set” as an identifier that may be used to separate them.\nThis data frame contains the following columns:\nsource: which data set the data were obtained from\nyear: which year the data was collected from\nMonth: the month that the Friday was in\nx6th: Number of cars passing through junction (traffic data set), shoppers for each supermarket (shopping data set), or admissions due to transport accidents (accident data set) on Friday the 6th\nx13th: Number of cars passing through junction (traffic data set), shoppers for each supermarket (shopping data set), or admissions due to transport accidents (accident data set) on Friday the 13th\nlocation: Motorway junction (traffic data set), supermarket location (shopping data set) or hospital (accident data set) to which the data correspond\nSource (n.d.). Retrieved from https://www3.nd.edu/~busiforc/handouts/Data and Stories/t test/Friday The Thirteenth/Friday The Thirteenth Data.html\nReferences Scanlon, T.J., Luben, R.N., Scanlon, F.L., Singleton, N. (1993), “Is Friday the 13th Bad For Your Health?,” BMJ, 307, 1584-1586.\n\nThe British Department of Transportation studied to see if people avoid driving or shopping, or have more accidents on Friday the 13th. They collected data from different locations (Friday the 13th, 2019). The data for each location on the two different dates is in Table 10.7. Do the data show that on average different number of people are engaged in activities on Friday the 13th? Estimate the mean difference in activity count between the 6th and the 13th using a 95% level.\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997). The data is in Table 4.9. Do the data show that Reiki treatment reduces pain? Test at the 5% level.\n\nCode book for data frame Reiki is below Table 4.9.\n\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997). The data is in Table 4.9. Compute a 90% confidence level for the mean difference in VAS score from before and after Reiki treatment.\nThe female labor force participation rates (FLFPR) of women in countries from 1990 to 2018 are in table 9.2.8.5 (Labor force participation rate, female (% of female population ages 15+) (modeled ILO estimate), 2019). Do the data show that the mean female labor force participation rate in 1990 is different from that in the 2018 using a 5% level of significance?\n\n\nLabor &lt;- read.csv( \"https://krkozak.github.io/MAT160/labor.csv\")\nknitr::kable(head(Labor))\n\n\n\nTable 10.8: Female Labor Force Participation Rates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry.Name\nCountry.Code\nRegion\nIncomeGroup\ny1990\ny1991\ny1992\ny1993\ny1994\ny1995\ny1996\ny1997\ny1998\ny1999\ny2000\ny2001\ny2002\ny2003\ny2004\ny2005\ny2006\ny2007\ny2008\ny2009\ny2010\ny2011\ny2012\ny2013\ny2014\ny2015\ny2016\ny2017\ny2018\n\n\n\n\nAruba\nABW\nLatin America & Caribbean\nHigh income\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\nAFG\nSouth Asia\nLow income\n43.11500\n43.12400\n43.12900\n43.07200\n43.00300\n43.01700\n42.77000\n42.55400\n42.41300\n42.3340\n42.27400\n42.53900\n42.89900\n43.28600\n43.66100\n44.02500\n43.59700\n43.19200\n42.8730\n42.70900\n42.73500\n43.32800\n44.11700\n45.03900\n46.01700\n47.00100\n47.76600\n48.47400\n48.66000\n\n\nAngola\nAGO\nSub-Saharan Africa\nLower middle income\n74.94500\n74.87900\n74.82600\n74.78200\n74.77000\n74.78400\n74.78300\n74.80600\n74.84600\n74.8940\n74.94100\n74.96200\n74.98400\n75.01100\n75.04800\n75.09400\n75.12600\n75.16500\n75.2090\n75.25600\n75.30700\n75.34400\n75.38900\n75.43300\n75.46500\n75.47900\n75.47000\n75.45100\n75.41200\n\n\nAlbania\nALB\nEurope & Central Asia\nUpper middle income\n53.77100\n56.29600\n56.68700\n55.74700\n54.90400\n53.74600\n53.07500\n53.81200\n53.15400\n52.2540\n51.76900\n51.11000\n50.67900\n49.75900\n48.87800\n48.05100\n47.38900\n46.80300\n46.2690\n44.94500\n45.69300\n47.10400\n48.80600\n44.65000\n44.78900\n47.67600\n47.45900\n47.31200\n47.19100\n\n\nAndorra\nAND\nEurope & Central Asia\nHigh income\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nArab World\nARB\n\n\n19.18997\n19.24094\n19.13159\n19.29515\n19.64479\n19.66156\n19.51602\n19.27293\n19.07511\n19.5351\n19.59284\n19.52237\n19.08892\n19.32403\n19.44488\n19.53444\n19.68183\n20.17107\n19.8473\n20.05784\n20.17166\n20.27703\n20.46453\n20.76731\n20.70378\n20.51515\n20.61605\n20.56842\n20.58152\n\n\n\n\n\n\n\n\nCode book for data frame Labor\nDescription Labor force participation rate, female (% of female population ages 15+)\nThis data frame contains the following columns:\nCountry Name: The name of a country around the world\nCountry Code: The 3 letter country code\nRegion: The location of the country in the world\nIncomeGroup: The World Bank’s income classification\ny1990-y2018: Labor force participation rate, female (% of female population ages 15+) for the years 100–2018\nSource Labor force participation rate, female (% of female population ages 15 ) (modeled ILO estimate). (n.d.). Retrieved July 20, 2019, from https://data.worldbank.org/indicator/SL.TLF.CACT.FE.ZS\nReferences International Labour Organization, ILOSTAT database. Data retrieved in April 2019.\n\nThe female labor force participation rates (FLFPR) of women in countries from 1990 to 2018 are in Table 10.8 (Labor force participation rate, female (% of female population ages 15+) (modeled ILO estimate), 2019). Estimate the mean difference in the female labor force participation rate in 1990 to 2018 using a 95% confidence level?\nIs the pulse rate after exercise different from the pulse rate before exercise for a man who drinks alcohol but doesn’t smoke? Use the data frame Pulse Table 4.7. Test at the 5% level.\n\nCode book for data frame Pulse is below Table 4.7.\n\nTable 4.7 contains pulse rates Compute a 95% confidence interval for the mean difference in pulse rates from before and after exercise for males who drink but do not smoke.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html#independent-samples-for-two-means",
    "href": "Two Sample Inference.html#independent-samples-for-two-means",
    "title": "10  Two Sample Inference",
    "section": "10.3 Independent Samples for Two Means",
    "text": "10.3 Independent Samples for Two Means\nThis section will look at how to analyze when two samples are collected that are independent. As with all other hypothesis tests and confidence intervals, the process is the same though the formulas and conditions are different.\n\n10.3.1 Hypothesis Test for the Difference in Means from Two Independent Samples\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = random variable 1\n\\(x_2\\)= random variable 2\n\\(\\mu_1\\)= mean of random variable 1\n\\(\\mu_2\\)= mean of random variable 2\n\nState the null and alternative hypotheses and the level of significance\n\nThe hypotheses would be\n\\(H_o:\\mu_1=\\mu_2\\)\n\\(H_a:\\mu_1\\ne \\mu_2\\), the \\(\\ne\\) can be replaced with \\(&lt;\\) or \\(&gt;\\)\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of size \\(n_1\\) is taken from population 1. A random sample of size \\(n_2\\) is taken from population 2. Check: describe how both samples are collected. Note: the samples do not need to be the same size, but the test is more robust if they are.\nState: The two samples are independent. Check: describe whey the samples are independent of each other.\nState: Population 1 is normally distributed. Population 2 is normally distributed. Check: draw the density graph and normal quantile plot for both samples and discuss if they meet the criteria. Just as before, the t-test is fairly robust to the condition if the sample size is large. This means that if this condition isn’t met, but your sample sizes are quite large, then the results of the t-test are valid.\nState: The population variances are unknown and not assumed to be equal. The old condition is that the variances are equal. However, this condition is no longer a condition that most statisticians use. This is because it isn’t really realistic to assume that the variances are equal. So just assume the condition of the variances being unknown and not assumed to be equal is true, and it will not be checked.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nThe command using r is\nt.test(variable~factor, data=Data_Frame)\nNote: if the \\(H_a\\) is &lt;, then the command becomes\nt.test(variable~factor, data=Data_Frame, alternative=“less”)\nSimilarly for &gt; put alternative=“greater”\n\nConclusion\n\nThis is where you write reject or fail to reject \\(H_0\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\).\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n10.3.2 Confidence Interval for the Difference in Means from Two Independent Samples\nThe confidence interval for the difference in means has the same random variables and means and the same conditions as the hypothesis test for independent samples. If you have already completed the hypothesis test, then you do not need to state them again. If you haven’t completed the hypothesis test, then state the random variables and means and state and check the conditions before completing the confidence interval step.\nFind the sample statistic and confidence interval\nOn r Studio, the command is\nt.test(variable~factor, data=Data_Frame, conf.level=C) type C as a decimal\n\nStatistical Interpretation: In general this looks like, “You are C% confident that the interval contains the true mean difference.”\nReal World Interpretation: This is where you state what interval contains the true difference in means, though often you state how much more (or less) the first mean is from the second mean.\n\n\n\n10.3.3 Example: Hypothesis Test for Two Means\nThe cholesterol level of people vary for many reasons. The question is do people with diabetes have different cholesterol levels from people who do not have diabetes? Use the NHANES data frame. Test at the 5% level.\n\nnames(NHANES) #displays the names of the variables in a data frame\n\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n[13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n[17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n[21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n[25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n[29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n[33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n[37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n[41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n[45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n[49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n[53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n[57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n[61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n[65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n[69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n[73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"     \n\n\nCode book for data frame NHANES type help(“NHANES”) in the r Console.\n\n10.3.3.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = Cholesterol level of people with diabetes\n\\(x_2\\) = Cholesterol level of people without diabetes\n\\(\\mu_1\\) = mean cholesterol level of people with diabetes\n\\(\\mu_2\\) = mean cholesterol level of people without diabetes\n\nState the null and alternative hypotheses and the level of significance\n\nThe hypotheses would be\n\\(H_o: \\mu_1=\\mu_2\\)\n\\(H_a: \\mu_1 \\ne \\mu_2\\)\nlevel of significance, \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of cholesterol levels of people with diabetes is taken. A random sample of cholesterol levels of people without diabetes is taken.\nCheck: The NHANES data frame uses cluster sampling which incorporates random sampling, so the sample is probably representative. This condition has been met.\nState: The two samples are independent.\nCheck: This is because either they were dealing with people who have diabetes or not.\nState: Population of all cholesterol levels of people who have diabetes is normally distributed. Population of all cholesterol levels of people without diabetes is normally distributed.\nCheck:\n\n\nNHANES_no_NA&lt;- \n  NHANES |&gt; \n  drop_na(Diabetes) \ngf_density(~TotChol|Diabetes, data=NHANES_no_NA, title = \"Cholesterol of a person with and without Diabetes\", xlab=\"Total Cholesterol\") \n\n\n\n\n\n\n\nFigure 10.5: Density Plot of Cholesterol of a person with and without Diabetes\n\n\n\n\n\nBoth the yes group and the no group look somewhat bell shaped.\n\ngf_qq(~TotChol|Diabetes, data=NHANES_no_NA, title = \"Cholesterol of a person with and without Diabetes\")\n\n\n\n\n\n\n\nFigure 10.6: quantile Plot of Cholesterol of a person with and without Diabetes\n\n\n\n\n\nBoth the yes group and the no group look somewhat linear.\nThe population of all cholesterol levels of people who have diabetes is probably normally distributed. The population of all cholesterol levels of people who do not have diabetes is probably normally distributed.\n\nFind the sample statistic, test statistic, and p-value\n\nThe variable is cholesterol (TotChol) and separating based on if a person has diabetes or not. So the factor is Diabetes. Using r Studio the command would be\n\nt.test(TotChol~Diabetes, data=NHANES) \n\n\n    Welch Two Sample t-test\n\ndata:  TotChol by Diabetes\nt = 2.4286, df = 809.7, p-value = 0.01537\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 0.02105115 0.19851114\nsample estimates:\n mean in group No mean in group Yes \n         4.887936          4.778155 \n\n\n\nConclusion\n\nReject \\(H_o\\) since the p-value \\(&lt;\\alpha\\).\n\nInterpretation\n\nThere is enough evidence to support that people who have diabetes have different cholesterol levels on average from people who do not have diabetes.\n\n\n\n10.3.4 Example: Confidence Interval in Two Samples\nThe cholesterol level of people vary for many reasons. The question is how different is the cholesterol levels of people with diabetes from people who do not have diabetes? Use the NHANES data frame. Compute a 95% confidence interval.\n\n10.3.4.1 Solution\n\nState the random variables and the parameters in words.\n\nThese were stated in Example: Hypothesis Test for Two Means.\n\nState and check the conditions for the hypothesis test\n\nThe conditions were stated and checked in Example: Hypothesis Test for Two Means.\n\nFind the sample statistic and confidence interval\n\nThe variable is cholesterol (TotChol) and separating based on if a person has diabetes or not. So the factor is Diabetes. Using rStudio the command would be\n\nt.test(TotChol~Diabetes, data=NHANES, conf.level=0.95) \n\n\n    Welch Two Sample t-test\n\ndata:  TotChol by Diabetes\nt = 2.4286, df = 809.7, p-value = 0.01537\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 0.02105115 0.19851114\nsample estimates:\n mean in group No mean in group Yes \n         4.887936          4.778155 \n\n\n\nStatistical Interpretation: You are 95% confident that the interval \\(0.02105115&lt;\\mu_1-\\mu_2&lt;0.19851114\\) contains the true difference in means.\nReal World Interpretation: The mean cholesterol level for people with diabetes is anywhere from 0.021 mmol/L to 0.199 mmol/L more than the mean cholesterol level for people without diabetes.\n\n\n\n\n10.3.5 Example: Hypothesis Test for Two Means\nThe amount of sodium in beef and poultry hot dogs was measured. (\\“SOCR 012708 id,\\” 2013). The data is in Table 10.9. Is there enough evidence to show that beef has different amounts of sodium on average than poultry hot dogs? Use a 5% level of significance.\n\nHotdog&lt;-read.csv( \"https://krkozak.github.io/MAT160/hotdog_beef_poultry.csv\") \nknitr::kable(head(Hotdog))\n\n\n\nTable 10.9: Hot dog Data\n\n\n\n\n\n\ntype\ncalories\nsodium\n\n\n\n\nBeef\n186\n495\n\n\nBeef\n181\n477\n\n\nBeef\n176\n425\n\n\nBeef\n149\n322\n\n\nBeef\n184\n482\n\n\nBeef\n190\n587\n\n\n\n\n\n\n\n\nCode book for data frame Hot dog\nDescription Results of a laboratory analysis of calories and sodium content of major hot dog brands. Researchers for Consumer Reports analyzed three types of hot dog: beef, poultry, and meat (mostly pork and beef, but up to 15% poultry meat). The meat was left off this data frame so a two-sample t-test could be performed.\nThis data frame contains the following columns:\ntype: Type of hot dog (beef or poultry)\ncalories: Calories per hot dog\nsodium: Milligrams of sodium per hot dog\nSource SOCR 012708 id data hotdogs. (2013, November 13). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_012708_ID_Data_HotDogs\nReferences SOCR Home page: http://www.socr.ucla.edu\n\n10.3.5.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = sodium level in beef hot dogs\n\\(x_2\\) = sodium level in poultry hot dogs\n\\(\\mu_1\\) = mean sodium level in beef hot dogs\n\\(\\mu_2\\) = mean sodium level in poultry hot dogs\n\nState the null and alternative hypotheses and the level of significance\n\nThe hypotheses would be\n\\(H_o:\\mu_1=\\mu_2\\)\n\\(H_o:\\mu_1\\ne \\mu_2\\)\nlevel of significance: \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of 20 sodium levels in beef hot dogs is taken. A random sample of 20 sodium levels in poultry hot dogs.\nCheck: The code does not state if either sample was randomly selected, but since Consumer Reports performed the test, it is safe to assume the samples were both random.\nState: The two samples are independent.\nCheck: These are different types of hot dogs so this is true.\nState; Population of all sodium levels in beef hot dogs is normally distributed. Population of all sodium levels in poultry hot dogs is normally distributed.\nCheck:\n\n\ngf_density(~sodium|type, data=Hotdog, title=\"Sodium amount in Hot Dogs facetted by Type of Meat\", xlab=\"Total Sodium Level\") \n\n\n\n\n\n\n\nFigure 10.7: Density Plot of Sodium Amount in Hot Dogs facetted by Type of Meat\n\n\n\n\n\nThe density plot for beef hot dogs looks somewhat bell shaped, but the density plot for poultry hot dogs does not look bell shaped.\n\ngf_qq(~sodium|type, data=Hotdog, title=\"Sodium amoount in Hot Dogs facetted by Type of Meat\") \n\n\n\n\n\n\n\nFigure 10.8: Quantile Plot of Sodium Amount in Hot Dogs facetted by Type of Meat\n\n\n\n\n\nThe normal quantile plot Figure 10.7 for the sodium level in beef hot dogs looks somewhat linear. The normal quantile plot Figure 10.8 for the sodium level in poultry hot dogs does not look linear. The population of all sodium levels in beef hot dogs may be normally distributed, but the population of all sodium levels in poultry hot dogs is probably not normally distributed. The sample size is not very large so the results of the test may not be valid. A larger sample would be a good idea.\n\nFind the sample statistic, test statistic, and p-value\n\nUsing rStudio the variable is sodium levels (sodium) in different types of hot dogs. So the factor is type. The command is\n\nt.test(sodium~type, data=Hotdog)\n\n\n    Welch Two Sample t-test\n\ndata:  sodium by type\nt = -1.8798, df = 34.983, p-value = 0.06848\nalternative hypothesis: true difference in means between group Beef and group Poultry is not equal to 0\n95 percent confidence interval:\n -120.325706    4.625706\nsample estimates:\n   mean in group Beef mean in group Poultry \n               401.15                459.00 \n\n\n\nConclusion: Fail to reject \\(H_o\\) since the p-value \\(\\ge \\alpha\\).\nInterpretation\n\nThis is not enough evidence to support that beef hot dogs’ sodium level is different from poultry hot dogs. (Though do realize that the population conditions is not valid, so this interpretation may be invalid.)\n\n\n\n10.3.6 Example: Confidence Interval for Two Independent Samples\nThe amount of sodium in beef and poultry hot dogs was measured. (“SOCR 012708 id,” 2013). The data is in Table 10.9. Find a 95% confidence interval for the mean difference in sodium levels between beef and poultry hot dogs.\n\n10.3.6.1 Solution\n\nState the random variables and the parameters in words.\nThese were stated in Example: Hypothesis Test for Two Means.\nState and check the conditions for the hypothesis test\n\nThe conditions were stated and checked in Example: Hypothesis Test for Two Means.\n\nFind the sample statistic and confidence interval Using r Studio the variable is sodium levels (sodium) in different types of hot dogs. So the factor is type. The command is\n\n\nt.test(sodium~type, data=Hotdog, conf.level=0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  sodium by type\nt = -1.8798, df = 34.983, p-value = 0.06848\nalternative hypothesis: true difference in means between group Beef and group Poultry is not equal to 0\n95 percent confidence interval:\n -120.325706    4.625706\nsample estimates:\n   mean in group Beef mean in group Poultry \n               401.15                459.00 \n\n\n\nStatistical Interpretation: You are 95% confident that the interval \\(-120.325706&lt;\\mu_1-\\mu_2&lt;4.625706\\) contains the true difference in mean sodium level between beef and poultry hot dogs.\nReal World Interpretation: The mean sodium level of beef hot dogs is anywhere from 120.33 mg less than the mean sodium level of poultry hot dogs to 4.63 mg more. (The negative sign on the lower limit implies that the first mean is less than the second mean. The positive sign on the upper limit implies that the first mean is greater than the second mean.)\n\nDo realize that the population conditions is not valid, so this interpretation may be invalid.\n\n\n\n10.3.7 Homework for Independent Samples for Two Means Section\nIn each problem show all steps of the hypothesis test or confidence interval. If some of the conditions are not met, note that the results of the test or interval may not be correct and then continue the process of the hypothesis test or confidence interval.\n\nThe NHANES data contains many variables. One variable is the income of households derived from the middle income of different income categories. The variable is called HHIncomeMid. Is there enough evidence to show that the mean income of males is different from the mean income of females? Test at the 1% level.\n\n\nnames(NHANES)\n\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n[13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n[17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n[21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n[25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n[29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n[33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n[37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n[41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n[45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n[49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n[53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n[57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n[61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n[65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n[69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n[73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"     \n\n\n\nThe NHANES data contains many variables. One variable is the income of households derived from the middle income of different income categories. The variable is called HHIncomeMid. Estimate with 95% confidence the mean difference in incomes between males and females in the U.S.\nA study was conducted that measured the total brain volume (TBV) of patients that had schizophrenia and patients that do not have schizophrenia. Table 10.10 contains the TBV of the all patients (“SOCR data oct2009,\\” 2013). Is there enough evidence to show that the patients with schizophrenia have a different TBV on average than a patient without schizophrenia? Test at the 10% level.\n\n\nBrain &lt;- read.csv( \"https://krkozak.github.io/MAT160/brain.csv\") \nknitr::kable(head(Brain))\n\n\n\nTable 10.10: Total Brain Volume of Patients\n\n\n\n\n\n\ntype\nvolume\n\n\n\n\nn\n1663407\n\n\nn\n1583940\n\n\nn\n1299470\n\n\nn\n1535137\n\n\nn\n1431890\n\n\nn\n1578698\n\n\n\n\n\n\n\n\nCode book for data frame Brain\nDescription A study to measure the total brain volume (TBV) (in ) of patients that had schizophrenia and patients that do not have schizophrenia.\nThis data frame contains the following columns:\ntype: whether the patient had schizophrenia (s) or did not have schizophrenia (n)\nvolume: the total brain volume of a patient.(\\(mm^3\\))\nSource SOCR data Oct2009 id ni. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Oct2009_ID_NI\nReferences “SOCR data nips,” 2013\n\nA study was conducted that measured the total brain volume (TBV) of patients that had schizophrenia and patients that do not have schizophrenia. Table 10.10 contains the TBV of the all patients (“SOCR data oct2009,” 2013). Is there enough evidence to show that the patients with schizophrenia have a different TBV on average than a patient without schizophrenia? Test at the 10% level. Compute a 90% confidence interval for the difference in TBV of patients with Schizophrenia and patients without Schizophrenia.\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 4.3 (Lee, 1994). Do the data provide enough evidence to show on average that the rivers that travel to the Pacific Ocean are different length than the rivers that travel to the Tasman Sea? Use a 5% level of significance.\n\nCode book for data frame Length below Table 4.3.\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 4.3 (Lee, 1994). Estimate the difference in mean lengths of rivers between rivers in New Zealand that travel to the Pacific Ocean and ones that travel to the Tasman Sea. Use a 95% confidence level.\nA vitamin K shot is given to infants soon after birth. Nurses at Northbay Healthcare were involved in a study to see if how they handle the infants could reduce the pain the infants feel (\\“SOCR data nips,\\” 2013). The data frame is in Table 10.11. Is there enough evidence to show that infants cried a different amount on average when they are held by their mothers than if held using conventional methods? Test at the 5% level.\n\n\n10.3.7.1 Table: Crying Time of Infants Given Shots Using New Methods\n\nCrying&lt;- read.csv( \"https://krkozak.github.io/MAT160/crying.csv\") \nknitr::kable(head(Crying))\n\n\n\nTable 10.11: Crying Time of Infants Given Shots Using New Methods\n\n\n\n\n\n\nmethod\ncrying\n\n\n\n\nconvent\n63\n\n\nconvent\n0\n\n\nconvent\n2\n\n\nconvent\n46\n\n\nconvent\n33\n\n\nconvent\n33\n\n\n\n\n\n\n\n\nCode book for data frame Crying\nDescription Nurses at Northbay Healthcare were involved in a study to see if how they handle the infants could reduce the pain the infants feel. One of the measurements taken was how long, in seconds, the infant cried after being given the shot. A random sample was taken from the group that was given the shot using conventional methods, and a random sample was taken from the group that was given the shot where the mother held the infant prior to and during the shot.\nThis data frame contains the following columns:\nmethod: whether the infant was given the conventional method (convent) or the new method (new) prior to being given the vitamin K shot.\ncrying: how long the infant cried after given a vitamin K shot. (seconds)\nSource SOCR data nips infantvitK shotdata. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_NIPS_InfantVitK_ShotData\nReferences \\“SOCR data nips,\\” 2013\n\nA vitamin K shot is given to infants soon after birth. Nurses at Northbay Healthcare were involved in a study to see if how they handle the infants could reduce the pain the infants feel (\\“SOCR data nips,\\” 2013). The data frame is in Table 10.11. Calculate a 95% confidence interval for the mean difference in mean crying time after being given a vitamin K shot between infants held using conventional methods and infants held by their mothers.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html#which-analysis-should-you-conduct",
    "href": "Two Sample Inference.html#which-analysis-should-you-conduct",
    "title": "10  Two Sample Inference",
    "section": "10.4 Which Analysis Should You Conduct?",
    "text": "10.4 Which Analysis Should You Conduct?\nOne of the most important concept that you need to understand is deciding which analysis you should conduct for a particular situation. To help you to figure out the analysis to conduct, there are a series of questions you should ask yourself.\n\nDoes the problem deal with mean or proportion?\n\nSometimes the problem states explicitly the words mean or proportion, but other times you have to figure it out based on the information you are given. If you counted number of individuals that responded in the affirmative to a question, then you are dealing with proportion. If you measured something, then you are dealing with mean.\n\nDoes the problem have one or two samples?\n\nSo look to see if one group was measured or if two groups were measured. You need to decide if the problem describes collecting data from one group or from two groups, or if you are comparing two different groups.\n\nIf you have two samples, then you need to determine if the samples are independent or dependent.\n\nIf the individuals are different for both samples, then most likely the samples are independent. If you can’t tell, then determine if a data value from the first sample influences the data value in the second sample. In other words, can you pair data values together so you can find the difference, and that difference has meaning. If the answer is yes, then the samples are paired. Otherwise, the samples are independent.\n\nDoes the situation involve a hypothesis test or a confidence interval?\n\nIf the problem talks about “do the data show”, “is there evidence of”, “test to see”, then you are doing a hypothesis test. If the problem talks about “find the value”, “estimate the” or “find the interval”, then you are doing a confidence interval.\nSo if you have a situation that has two samples, independent samples, involving the mean, and is a hypothesis test, then you have a two-sample independent t-test. Now you look up the conditions and the technology process for doing this test. Every hypothesis test involves the same six steps, and you just have to use the correct conditions and calculations. Every confidence interval has the same five steps, and again you just need to use the correct conditions and calculations. So this is why it is so important to figure out what analysis you should conduct.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Regression.html",
    "href": "Regression.html",
    "title": "11  Regression",
    "section": "",
    "text": "11.1 Regression\nThe previous chapter looked at comparing populations to see if there is a difference between the two. That involved two random variables that are similar measures. This chapter will look at two random variables that do not need to be similar measures, and see if there is a relationship between the two or more variables. To do this, you look at regression, which finds the linear relationship, and correlation, which measures the strength of a linear relationship.\nPlease note: there are many other types of relationships besides linear that can be found for the data. This book will only explore linear, but realize that there are other relationships that can be used to describe data.\nWhen comparing different variables, two questions come to mind: “Is there a relationship between two variables?” and “How strong is that relationship?” These questions can be answered using regression and correlation. Regression answers whether there is a relationship, (again this book will explore linear relationships only) and correlation answers how strong the linear relationship is. The variable that is used to explain the change (or variability) in the other variable is called the explanatory variable or predictor variable, while the variable whose variability is being explained is called the response variable. In general, variables that help to explain the changes in a response variable are known as covariates. To introduce the concepts of regression and correlation, it is easier to look at a set of data.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#regression",
    "href": "Regression.html#regression",
    "title": "11  Regression",
    "section": "11.2 Correlation",
    "text": "11.1.1 Example: Determining if there is a Relationship\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is one, we explore a dataset of 227 beers, including their alcohol content and their calorie counts (Find Out How Many Calories in Beer?, 2019). Table 11.1 shows the first five rows of the dataset.\n\n\n\n\nTable 11.1: Alcohol and Calorie Content in Beer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbeer\nbrewery\nlocation\nalcohol\ncalories\ncarbs\n\n\n\n\nAmerican Amber Lager\nStraub Brewery\ndomestic\n0.04\n136\n10.5\n\n\nAmerican Lager\nStraub Brewery\ndomestic\n0.04\n132\n10.5\n\n\nAmerican Light\nStraub Brewery\ndomestic\n0.03\n96\n7.6\n\n\nAnchor Porter\nAnchor\ndomestic\n0.06\n209\nNA\n\n\nAnchor Steam\nAnchor\ndomestic\n0.05\n153\n16.0\n\n\nAnheuser Busch Natural Light\nAnheuser Busch\ndomestic\n0.04\n95\n3.2\n\n\n\n\n\n\n\n\n\nClick to expand the box below to see instructions to import and view the beer_data dataset.\n\n\n\n\n\n\n Importing beer_data to Rguroo\n\n\n\n\n\n\nOpen the Data toolbox in Rguroo.\n\nFrom the Data Import dropdown, select Dataset Repository.\n\nIn the top search box, type kozak, then select the Statistics Using Technology – Kozak repository.\n\nIn the middle search box, type beer, and select the beer_data dataset that appears in the lower panel.\n\nClick the Import. The dataset will be imported into your Rguroo account.\n\nClick the Close to close the dialog.\n\nTo view the dataset, double-click beer_data under the Data toolbox list.\n\n\n\n\nCode book for data frame Beer\nDescription Collection of the most popular beers from large breweries. The data is of the calories, carbs and alcohol of a specific beer. The data is shown for a 12 ounce serving. The collection includes both domestic and import beer. For the imported beers the information is per 12 oz. serving even though many imports come in pints.\nThis dataset contains the following columns:\nbeer: The name of the beer.\nbrewery: the brewery that brews the beer.\nlocation: whether the beer is brewed in the U.S. (domestic) or brewed in another country (import).\nalcohol: the alcohol content of the beer.\ncalories: the number of calories in the beer.\ncarbs: the amount of carbohydrates in the beer (g).\nSource: Find Out How Many Calories in Beer? (n.d.). Retrieved July 21, 2019, from \nReferences: (Find Out How Many Calories in Beer?, 2019)\n\n11.1.1.1 Solution\nTo aid in figuring out if there is a relationship, it helps to draw a scatterplot of the data. First, it is helpful to state the variables, and since variables in an algebra class are typically represented as \\(x\\) and \\(y\\), those labels will be used here. It helps to state which variable is \\(x\\) (explanatory or predictor) and which is \\(y\\) (response).\nState variables\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nClick to expand the box below see how to create the scatterplot of calories vs. alcohol content.\n\n\n\n\n\n\n Creating Scatterplot of Calories vs. Alcohol Content in Rguroo\n\n\n\n\n\nBefore you begin: Make sure you have already imported the beer_data dataset into your Rguroo account, as was shown here.\n\nOpen the Plots toolbox in Rguroo.\n\nOpen the Create Plot dropdown, and select Scatterplot. This opens the Scatterplot dialog.\n\nIn the Scatterplot dialog, choose the beer_data dataset from the Dataset dropdown.\n\nChoose the alcohol variable from the Predictor (x) dropdown.\n\nChoose the calories variable from the Response (y) dropdown.\n\n(Optional) In the Label section of the dialog:\n\nEnter Calories versus Alcohol Content in Beer in the Title textbox.\n\nEnter Alcohol Content in the X-Axis textbox,.\n\nEnter Number of Calories in the Y-Axis textbox.\n\n\nClick the preview icon  to see a preview of the scatterplot.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nScatterplot dialog showing settings for plotting alcohol content versus calories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.1: Calories versus Alcohol Content in Beer\n\n\n\n\n\nThe scatter plot shown in Figure 11.1 looks fairly linear.\nTo find the equation for the linear relationship, the process of regression is used to determine the line that best fits the data (sometimes called the best-fitting line). The process is to draw a line through the data and measure the vertical distance from each point to the line. These distances are called the residuals. The regression line is the line that makes the makes square of the residuals as small as possible, which is why it is sometimes called the least squares line. The regression line on the scatter plot is displayed in Figure 11.2.\n\nClick to expand the box below to see how to add a regression line to the scatterplot.\n\n\n\n\n\n\n Adding Regression Line to a Scatterplot\n\n\n\n\n\nBefore you begin: Ensure you’ve created the scatterplot of calories vs. alcohol content in your Rguroo account, as shown here.\n\nOpen the Rguroo tab with your scatterplot of calories vs. alcohol content.\n\nOpen the Basics dialog by clicking the  button.\n\nIn the Superimpose section of the dialog, Select the checkbox LS Line.\n\nClick the preview icon  to see a preview of the scatterplot with the regression line.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nScatterplot dialog for adding a least-squares line to a plot of alcohol content versus calorie counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11.2: Scatter Plot of Beer Data with Regression Line\n\n\n\n\n\n\n11.1.2 Find the regression equation (also known as best fitting line or least squares line)\nGiven a collection of paired sample data, the regression equation is \\(\\hat{y}=mx+b\\), where the slope = \\(m\\) and \\(y\\)-intercept = \\((0,b)\\).\nAs we will show, to find the linear model using Rguroo, we use the Linear Regression function in the Analytics toolbox.\nThe residuals are the difference between the actual values and the estimated values, namely \\(\\text{residual} = y - \\hat{y}\\).\nThe independent variable (the \\(x\\)-value) corresponds to the explanatory or predictor variable, the variable used to predict or explain changes in another variable. The dependent variable (the \\(y\\)-value) corresponds to the response variable—the variable whose values change in relation to the predictor.In our example, alcohol content is the predictor (independent, \\(x\\)) variable, and the number of calories is the response (dependent, \\(y\\)) variable, since it makes more sense to predict calories from alcohol content rather than vice versa.\nAssumptions for the Validity of the Regression Equation and Line:\n\nRandomness: The set of ordered pairs is a random sample from the population of all such possible pairs.\nLinearity: For a given \\(x\\)-value, the distribution of \\(y\\)-values has a mean that lies on the least squares line.\n\nThe assumption of linearity is essential. If the relationship between the two variables is not linear, then a regression line will not fit the data well. To check linearity, we look at the scatterplot: a roughly straight-line pattern suggests the assumption is reasonable, while a curved pattern suggests it is not. We can also examine the residual plot. If the residuals are randomly scattered around the horizontal line \\(y = 0\\), the linearity assumption is likely valid. If the residuals show a curved pattern, then the assumption of linearity is likely violated.\nAssumptions for the validity of inference on the parameters of the regression line:\n\nConstant Variance: The variance of the \\(y\\)-values should be the same across all \\(x\\)-values.\nNormality: The \\(y\\)-values for each fixed value of \\(x\\) should follow a normal distribution.\n\nThe assumptions of constant variance and normality are mainly needed when we make inferences about the regression model. Inference in regression includes creating confidence intervals for the regression parameters, testing hypotheses about those parameters, and creating confidence intervals or prediction intervals for the value of the response variable we predict at a given \\(x\\)-value.. These ideas are similar to the confidence intervals and hypothesis tests for means that you have already seen, but here they apply to the slope and intercept of the regression line and to predicted values.\nAlthough this book does not cover inference for regression, it is helpful to understand the basic ideas behind these assumptions. To check constant variance, we examine the residual plot: if the residuals are scattered with roughly the same vertical spread around the horizontal line \\(y = 0\\) across all \\(x\\)-values, the assumption is reasonable. A funnel-shaped pattern suggests the spread is not constant. To check normality, we look at a histogram or a normal probability plot of the residuals. In a normal probability plot, the residuals should fall roughly along a straight line; strong curves or outliers suggest that the normality assumption may not hold.\n\n\n11.1.3 Example: Find the Equation of the Regression Line\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is one, we examine the sample of beer’s alcohol content and calories, portion of which is shown in Table 11.1 (Find Out How Many Calories in Beer?, 2019).\n\nFind the regression equation between alcohol content and calories.\nUse the regression equation to find the number of calories when the alcohol content is 7.00%.\nUse the regression equation to find the number of calories when the alcohol content is 14%.\n\n\n11.1.3.1 Solution\n\nFind the regression equation between alcohol content and calories.\n\nState random variables\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nClick to expand the box below to see how to obtain the regression equation.\n\n\n\n\n\n\n Finding the Regression Equation for the Beer Data\n\n\n\n\n\nBefore you begin: Make sure you have already imported the beer_data dataset into your Rguroo account, as was shown here.\n\nOpen the Analytics toolbox in Rguroo.\n\nOpen the Analysis dropdown and select Linear Regression → Simple Regression. This opens the Simple Linear Regression dialog.\n\nFrom the Dataset dropdown, choose the beer_data dataset.\n\nFrom the Predictor (x) dropdown, choose the alcohol variable.\n\nFrom the Response (y) dropdown, choose the calories variable.\n\nClick the preview icon  to view a preview of the regression results.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nSimple Linear Regression dialog in Rguroo\n\n\n\n\n\n\n\n\n\n\n\nRguroo output: Linear Regression for Beer Data\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nThe following is a portion of the output from Rguroo for this regression analysis.\n\n\n\n\nTable 11.2: Data and model summary for Alcohol Content versus Calories\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis table shows various information about the regression analysis. In particular, the last row, labeled “Equation of Least Squares Line,” gives the estimates for the slope and the \\(y\\)-intercept. Note that the equation states calories in place of \\(y\\) and alcohol in place of \\(x\\).\nFrom this, rounding the values, you can see that the y-intercept is 14.53 and the slope is 2672.36. So the regression equation is \\(\\hat{y}=2672x+14.5\\).\nRemember, this is an estimate for the true regression equation. A different sample would produce a different estimate.\nConditions check:\n\nRandomness: A random sample of alcohol content and calories was taken.\nCheck: There is no guarantee that this was a random sample. The data was collected off of a website, and the website does not say how the data was obtained. However, it is a collection of most popular beers from large breweries, so it may be alright that it isn’t a random sample.\nLinearity: For a given \\(x\\)-value, the distribution of \\(y\\)-values has a mean that lies on the least squares line.\n\nCheck:\nWe Examine two graphs to check this condition.\n\nFrom Figure 11.1, the scatter plot looks fairly linear. Note that this graph is also shown in the output of the Linear Regression function in Rguroo.\n\n\nknitr::include_graphics(\"Final_output/Rguroo_outputs/Regression/AlcoholCalorieResvsFit.png\")\n\n\n\n\n\n\n\nFigure 11.3: Residuals versus Fitted Values Plot for Beer Data\n\n\n\n\n\n\nFigure 11.3 shows a plot of residuals versus fitted values. To graph the residuals, the residuals must first be calculated. For each \\(x\\) value, compute the corresponding \\(y\\) value predicted by the regression equation; this predicted value is called the fitted value and is denoted by \\(\\hat{y}\\). Then subtract the observed \\(y\\) value from this fitted value to obtain the residual. Fortunately, Rguroo calculates these automatically for you.\n\nClick to expand the box below to see how to obtain the residuals in Rguroo.\n\n\n\n\n\n\n Obtaining Residuals in Rguroo\n\n\n\n\n\nBefore you begin: Continue with the steps shown here to obtain the regression equation in Rguroo.\n\nGo to the tab in Rguroo with your regression results.\nClick the  button to open the Simple Linear Regression dialog.\nSelect the checkbox Predictions & Residuals.\nClick the preview icon  to see a preview of the regression results with residuals.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nSimple Linear Regression dialog: Obtaining Residuals\n\n\n\n\n\n\n\n\nClick to expand the box below to see the residuals for the first 20 cases of the beer data.\n\n\n\n\n\n\nResiduals for Alcohol and Calorie Content in Beer\n\n\n\n\n\n\n\n\n\nTable 11.3\n\n\n\n\n\n\nResiduals for Alcohol and Calorie Content in Beer from Rguroo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 11.4: Residuals for Alcohol and Calorie Content in Beer\n\n\n\n\n\n\nx\n\n\n\n\n14.5785382\n\n\n10.5785382\n\n\n1.3021259\n\n\n34.1313626\n\n\n4.8549504\n\n\n-26.4214618\n\n\n-17.8686374\n\n\n-27.4214618\n\n\n6.8549504\n\n\n20.1313626\n\n\n28.8549504\n\n\n20.1313626\n\n\n14.8549504\n\n\n5.8549504\n\n\n14.8549504\n\n\n4.8549504\n\n\n22.8549504\n\n\n21.1313626\n\n\n18.1313626\n\n\n-18.1450496\n\n\n-51.8686374\n\n\n-33.1450496\n\n\n-11.4214618\n\n\n-5.4214618\n\n\n-37.8686374\n\n\n-3.1450496\n\n\n-28.8686374\n\n\n-22.4214618\n\n\n-12.9742863\n\n\n-15.1450496\n\n\n-5.8686374\n\n\n-26.4214618\n\n\n16.5785382\n\n\n-0.8686374\n\n\n0.8549504\n\n\n-17.4214618\n\n\n13.8549504\n\n\n5.1313626\n\n\n-11.5922251\n\n\n3.8549504\n\n\n-6.3158129\n\n\n9.8549504\n\n\n-9.8686374\n\n\n-22.2101640\n\n\n-24.8686374\n\n\n15.9605993\n\n\n32.2370115\n\n\n-10.1450496\n\n\n-4.5922251\n\n\n9.6841871\n\n\n-20.8686374\n\n\n-7.3158129\n\n\n-11.8686374\n\n\n-13.5922251\n\n\n-19.1450496\n\n\n6.2370115\n\n\n-17.1450496\n\n\n-0.1450496\n\n\n13.8549504\n\n\n-18.8686374\n\n\n-0.1450496\n\n\n13.8549504\n\n\n-42.8686374\n\n\n-25.8686374\n\n\n-18.4214618\n\n\n-4.1450496\n\n\n-4.1450496\n\n\n-11.4214618\n\n\n-10.4214618\n\n\n-18.4214618\n\n\n-32.8686374\n\n\n0.8549504\n\n\n14.8549504\n\n\n3.8549504\n\n\n21.8549504\n\n\n17.8549504\n\n\n16.8549504\n\n\n-16.4214618\n\n\n-11.4214618\n\n\n11.8549504\n\n\n11.8549504\n\n\n-17.8686374\n\n\n-3.1450496\n\n\n17.8549504\n\n\n6.8549504\n\n\n3.8549504\n\n\n-11.4214618\n\n\n26.8549504\n\n\n-8.4214618\n\n\n-26.4214618\n\n\n-17.8686374\n\n\n-17.8686374\n\n\n-11.4214618\n\n\n-5.1450496\n\n\n-11.4214618\n\n\n-30.6978741\n\n\n-5.1450496\n\n\n-11.4214618\n\n\n-25.4214618\n\n\n6.5785382\n\n\n-11.4214618\n\n\n-11.4214618\n\n\n-11.4214618\n\n\n6.5785382\n\n\n-23.4214618\n\n\n-30.8686374\n\n\n30.1313626\n\n\n-1.5922251\n\n\n-1.5922251\n\n\n-8.1450496\n\n\n11.8549504\n\n\n6.8549504\n\n\n-13.3158129\n\n\n-3.1450496\n\n\n55.4728892\n\n\n-7.4214618\n\n\n-2.1450496\n\n\n-14.8686374\n\n\n-26.3158129\n\n\n-8.3158129\n\n\n-2.1450496\n\n\n-4.1450496\n\n\n-27.6978741\n\n\n25.8549504\n\n\n-1.1450496\n\n\n4.1313626\n\n\n-13.5922251\n\n\n-6.1450496\n\n\n-28.1450496\n\n\n11.8549504\n\n\n11.8549504\n\n\n17.8549504\n\n\n46.8549504\n\n\n0.1313626\n\n\n-2.4214618\n\n\n-3.1450496\n\n\n-6.1450496\n\n\n-2.1450496\n\n\n-11.4214618\n\n\n10.1313626\n\n\n15.1313626\n\n\n48.2370115\n\n\n12.4077749\n\n\n8.8549504\n\n\n15.1313626\n\n\n13.4077749\n\n\n29.4077749\n\n\n0.1313626\n\n\n16.4077749\n\n\n19.1313626\n\n\n50.1313626\n\n\n9.8549504\n\n\n31.5785382\n\n\n4.8549504\n\n\n-25.1450496\n\n\n-6.3158129\n\n\n-6.3158129\n\n\n-14.8686374\n\n\n0.8549504\n\n\n-8.4214618\n\n\n20.1313626\n\n\n-1.1450496\n\n\n14.8549504\n\n\n7.8549504\n\n\n1.8549504\n\n\n13.5785382\n\n\n2.8549504\n\n\n12.8549504\n\n\n-1.1450496\n\n\n-3.1450496\n\n\n1.8549504\n\n\n13.5785382\n\n\n-23.4214618\n\n\n13.5785382\n\n\n-22.4214618\n\n\n-2.1450496\n\n\n-14.8686374\n\n\n-2.1450496\n\n\n-5.1450496\n\n\n-3.9742863\n\n\n-2.1450496\n\n\n6.5785382\n\n\n24.5785382\n\n\n7.8549504\n\n\n-0.1450496\n\n\n0.8549504\n\n\n7.8549504\n\n\n18.4077749\n\n\n7.8549504\n\n\n16.5785382\n\n\n11.8549504\n\n\n-6.1450496\n\n\n-3.1450496\n\n\n31.5785382\n\n\n-6.1450496\n\n\n1.8549504\n\n\n21.8549504\n\n\n-22.4214618\n\n\n-13.5922251\n\n\n4.8549504\n\n\n14.8549504\n\n\n4.8549504\n\n\n13.2370115\n\n\n4.8549504\n\n\n-4.8686374\n\n\n14.0257137\n\n\n4.8549504\n\n\n-38.1450496\n\n\n-6.1450496\n\n\n38.5785382\n\n\n4.8549504\n\n\n9.8549504\n\n\n25.8549504\n\n\n9.5785382\n\n\n-16.5922251\n\n\n-0.1450496\n\n\n1.8549504\n\n\n-10.1450496\n\n\n31.5785382\n\n\n6.5785382\n\n\n-41.5922251\n\n\n3.4077749\n\n\n-2.1450496\n\n\n4.8549504\n\n\n20.5785382\n\n\n11.8549504\n\n\n4.8549504\n\n\n\n\n\n\n\n\nlm_out_beer saves the linear model into a variable called lm_out_beer\nresiduals(lm_out_beer) finds the residuals for each \\(x\\) value based on the linear model, lm_out_beer, and displays them.\nNow graph the residuals to see if there is any pattern to the residuals. The command in rStudio is:\n\ngf_point(residuals(lm_out_beer)~alcohol, data=Beer, title=\"Residuals for Calories in beer\", xlab=\"Alcohol Content\", ylab=\"Residuals for Number of Calories\") \n\n\n\n\n\n\n\nFigure 11.4: Residual Plot of Beer Data\n\n\n\n\n\nThe residual versus the \\(x-\\)values plot Figure 11.4 looks fairly random.\nIt appears that the distribution for calories is a normal distribution.\n\nUse the regression equation to find the number of calories when the alcohol content is 7.00%.\n\n\\(\\hat{y}=2672*0.07+14.3=201.34\\) If you are drinking a beer that is 7.00% alcohol content, then it is probably close to 200 calories.\nThe mean number of calories is 154.5 calories. This value of 201 seems like a better estimate than the mean when looking at the data since all the beers with 7% alcohol have between 160 and 231 calories. The regression equation is a better estimate than just the mean.\n\nUse the regression equation to find the number of calories when the alcohol content is 14%.\n\n\\(\\hat{y}=2672*0.14+14.3=388.38\\)\nIf you are drinking a beer that is 14% alcohol content, then it has probably close to 389 calories. Since 12% alcohol beer has 330 calories you might think that 14% would have more calories than this. This estimate is what is called extrapolation. It is not a good idea to predict values that are far outside the range of the original data. This is because you can never be sure that the regression equation is valid for data outside the original data.\nNotice, that the 7.00% value falls into the range of the original \\(x\\)-values. The processes of predicting values using an \\(x\\) within the range of original \\(x\\)-values is called interpolating. The 14.00% value is outside the range of original \\(x\\)-values. Using an \\(x\\)-value that is outside the range of the original \\(x\\)-values is called extrapolating. When predicting values using interpolation, you can usually feel pretty confident that that value will be close to the true value. When you extrapolate, you are not really sure that the predicted value is close to the true value. This is because when you interpolate, you know the equation that predicts, but when you extrapolate, you are not really sure that your relationship is still valid. The relationship could in fact change for different \\(x\\)-values.\nAn example of this is when you use regression to come up with an equation to predict the growth of a city, like Flagstaff, AZ. Based on analysis it was determined that the population of Flagstaff would be well over 50,000 by 1995. However, when a census was undertaken in 1995, the population was less than 50,000. This is because they extrapolated and the growth factor they were using had obviously changed from the early 1990’s. Growth factors can change for many reasons, such as employment growth, employment stagnation, disease, articles saying great place to live, etc. Realize that when you extrapolate, your predicted value may not be anywhere close to the actual value that you observe.\nWhat does the slope mean in the context of this problem?\nThe calories increase 26.72 calories for every 1% increase in alcohol content.\nThe \\(y\\)-intercept in many cases is meaningless. In this case, it means that if a drink has 0 alcohol content, then it would have 14.3 calories. This may be reasonable, but remember this value is an extrapolation so it may be wrong.\nConsider the residuals again. According to the data, a beer with 7.0% alcohol has between 160 and 231 calories. The predicted value is 201 calories. This variation means that the actual value was between 40 calories below and 30 calories above the predicted value. That isn’t that far off. Some of the actual values differ by a large amount from the predicted value. This is due to variability in the response variable. The larger the residuals the less the model explains the variability in the response variable. There needs to be a way to calculate how well the model explains the variability in the response variable. This will be explored in the next section.\nOn last thing to look at here, is that you may wonder if import beer has a different amount of calories than domestic beer. The location is a co-variate, a third variable that affects the calories, and you can graph a scatter plot that separates based on the co-variate. Here is how to do this in r Studio.\n(ref:Beer10a-point-lm) Calories versus Alcohol Content Separated by Location of Beer Data\n\ngf_point(calories~alcohol, data=Beer, title=\"Calories versus Alcohol Content of Beer separated by Location\", color=~location, xlab=\"Alcohol Content\", ylab=\"Number of calories\") |&gt;\n  gf_lm() \n\n\n\n\n\n\n\nFigure 11.5: Calories versus Alcohol Content of Beer separated by Location\n\n\n\n\n\nLooking at the scatter plot, there doesn’t appear to be an affect from domestic or import. This is what is nice about scatter plots, You can visually see a possible relationships.\n\n11.1.4 Homework for Regression Section\nFor each problem, state the random variables. The Data Frame in this section are used in the homework for sections 10.2 and 10.3 also.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected and are in Table 3.15 (\\“Prediction of height,\\” 2013). Create a scatter plot and find a regression equation between the height of a person and the length of their metacarpal. Then use the regression equation to find the height of a person for a metacarpal length of 44 mm and for a metacarpal length of 55 mm. Which height that you calculated do you think is closer to the true height of the person? Why?\n\nCode book for Data Frame Metacarpal is below Table 3.15.\n\nTable 3.16 contains the value of the house and the amount of rental income in a year that the house brings in (\\“Capital and rental,\\” 2013). Create a scatter plot and find a regression equation between house value and rental income. Then use the regression equation to find the rental income of a house worth \\$230,000 and for a house worth \\$400,000. Which rental income that you calculated do you think is closer to the true rental income? Why?\n\nCode book for Data Frame House is below Table 3.16.\n\nThe World Bank collects information on the life expectancy of a person in each country (\\“Life expectancy at,\\” 2013) and the fertility rate per woman in the country (\\“Fertility rate,\\” 2013). The Data Frame for countries for the year 2011 are in Table 3.17. Create a scatter plot of the Data Frame and find a linear regression equation between fertility rate and life expectancy in 2011. Then use the regression equation to find the life expectancy for a country that has a fertility rate of 2.7 and for a country with fertility rate of 8.1. Which life expectancy that you calculated do you think is closer to the true life expectancy? Why?\n\nCode book for Data Frame Fertility is below Table 3.17.\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The Data Frame for the countries where this information is available in Table 3.18. Create a scatter plot of the Data Frame and find a regression equation between percentage spent on health expenditure and the percentage of women receiving prenatal care. Then use the regression equation to find the percent of women receiving prenatal care for a country that spends 5.0% of GDP on health expenditure and for a country that spends 12.0% of GDP. Which prenatal care percentage that you calculated do you think is closer to the true percentage? Why?\n\nCode book for Data Frame Fert_prenatal is below Table 3.18.\n\nThe height and weight of baseball players are in Table 11.5 (\\“MLB heightsweights,\\” 2013). Create a scatter plot and find a regression equation between height and weight of baseball players. Then use the regression equation to find the weight of a baseball player that is 75 inches tall and for a baseball player that is 68 inches tall. Which weight that you calculated do you think is closer to the true weight? Why?\n\n\nBaseball &lt;- read.csv( \"https://krkozak.github.io/MAT160/baseball.csv\") \nknitr::kable(head(Baseball))\n\n\n\nTable 11.5: Heights and Weights of Baseball Players\n\n\n\n\n\n\nplayer\nheight\nweight\n\n\n\n\n1\n65.78\n112.99\n\n\n2\n71.52\n136.49\n\n\n3\n69.40\n153.03\n\n\n4\n68.22\n142.34\n\n\n5\n67.79\n144.30\n\n\n6\n68.70\n123.30\n\n\n\n\n\n\n\n\nCode book for Data Frame Baseball\nDescription\nThe heights and weights of MLB players.\nFormat\nThis Data Frame contains the following columns:\nPlayer: Player in the sample\nheight: height of baseball player (inches)\nweight: weight of baseball player (pounds)\nSource MLB heightsweights. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights\nReferences SOCR Data Frame of MLB Heights Weights from UCLA.\n\nDifferent species have different body weights and brain weights are in Table 11.6. (\\“Brain2bodyweight,\\” 2013). Create a scatter plot and find a regression equation between body weights and brain weights. Then use the regression equation to find the brain weight for a species that has a body weight of 62 kg and for a species that has a body weight of 180,000 kg. Which brain weight that you calculated do you think is closer to the true brain weight? Why?\n\n\nBody &lt;- read.csv( \"https://krkozak.github.io/MAT160/body.csv\") \nknitr::kable(head(Body))\n\n\n\nTable 11.6: Body Weights and Brain Weights of Species\n\n\n\n\n\n\nspecies\nbodyweight\nbrainweight\nbrainbodyproportion\n\n\n\n\nNewborn_Human\n3.20\n0.3749848\n0.1171828\n\n\nAdult_Human\n73.00\n1.3499816\n0.0184929\n\n\nPithecanthropus_Man\n70.00\n0.9250109\n0.0132144\n\n\nSquirrel\n0.80\n0.0076204\n0.0095254\n\n\nHamster\n0.15\n0.0014061\n0.0093742\n\n\nChimpanzee\n50.00\n0.4199812\n0.0083996\n\n\n\n\n\n\n\n\nCode book for Data Frame Body\nDescription\nThe body weight, brain weight, and brain/body proportion of different species of animals.\nFormat\nThis Data Frame contains the following columns:\nspecies: species of animal\nbodyweight: the body weight of the species (kg)\nbrainweight: the brain weight of the species (kg)\nbrainbodyproportion: the ratio of brain weight to body weight of the species\nSource Brain2bodyweight. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Brain2BodyWeight\nReferences SOCR Data of species body weights and brain weights from UCLA.\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (\\“Data hotdogs,\\” 2013) The Data Frame are in Table 10.9. Create a scatter plot and find a regression equation between amount of calories and amount of sodium. Then use the regression equation to find the amount of sodium a hot dog has if it is 170 calories and if it is 120 calories. Which sodium level that you calculated do you think is closer to the true sodium level? Why?\n\nCode book for data frame Hotdog is below Table 10.9.\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in Table 11.7 (\\“OECD economic development,\\” 2013). Create a scatter plot and find a regression equation between percent of labor force in agriculture and per capita income. Then use the regression equation to find the per capita income in a country that has 21 percent of labor in agriculture and in a country that has 2 percent of labor in agriculture. Which per capita income that you calculated do you think is closer to the true income? Why?\n\n\nAgriculture &lt;- read.csv( \"https://krkozak.github.io/MAT160/agriculture.csv\") \nknitr::kable(head(Agriculture))\n\n\n\nTable 11.7: Percent of Labor in Agriculture and Per Capita Income for European Countries\n\n\n\n\n\n\ncountry\npercapita\nagriculture\nindustry\nservices\n\n\n\n\nSWEEDEN\n1644\n14\n53\n33\n\n\nSWITZERLAND\n1361\n11\n56\n33\n\n\nLUXEMBOURG\n1242\n15\n51\n34\n\n\nU. KINGDOM\n1105\n4\n56\n40\n\n\nDENMARK\n1049\n18\n45\n37\n\n\nW. GERMANY\n1035\n15\n60\n25\n\n\n\n\n\n\n\n\nCode book for Data Frame Agriculture\nDescription\nThe per capita income and percent in different industries in European countries\nFormat\nThis Data Frame contains the following columns:\ncountry: country in Europe\npercapita: per captia income\nagriculture: percentage of workforce in agriculture\nindustry: percentage of workforce in industry\nservices: percentage of workforce in services\nSource OECD economic development. (2013, December 04). Retrieved from http://lib.stat.cmu.edu/DASL/Datafiles/oecdat.html\nReferences Data And Story Library\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are in table \\#10.1.11 (\\“Smoking and cancer,\\” 2013). Create a scatter plot and find a regression equation between number of cigarettes smoked and number of deaths from bladder cancer. Then use the regression equation to find the number of deaths from bladder cancer when the cigarette sales were 20 per capita and when the cigarette sales were 6 per capita. Which number of deaths that you calculated do you think is closer to the true number? Why?\n\n\nCancer &lt;- read.csv( \"https://krkozak.github.io/MAT160/cancer_1.csv\") \nknitr::kable(head(Cancer))\n\n\n\nTable 11.8: Number of Cigarettes and Number of Bladder Cancer Deaths\n\n\n\n\n\n\nstate\ncig\nbladder\nlung\nkidney\nleukemia\n\n\n\n\nAL\n18.20\n2.90\n17.05\n1.59\n6.15\n\n\nAZ\n25.82\n3.52\n19.80\n2.75\n6.61\n\n\nAR\n18.24\n2.99\n15.98\n2.02\n6.94\n\n\nCA\n28.60\n4.46\n22.07\n2.66\n7.06\n\n\nCT\n31.10\n5.11\n22.83\n3.35\n7.20\n\n\nDE\n33.60\n4.78\n24.55\n3.36\n6.45\n\n\n\n\n\n\n\n\nCode book for Data Frame Cancer\nDescription This data frame contains the number of cigarette sales (per Capita), number of cancer Deaths (per 100 Thousand) from bladder, lung, kidney, and leukemia.\nFormat\nThis Data Frame contains the following columns:\nstate: state in US\ncig: the number of cigarette sales (per capita)\nbladder: number of deaths per 100 thousand from bladder cancer\nlung: number of deaths per 100 thousand from lung cancer\nkidney: number of deaths per 100 thousand from kidney cancer\nleukemia: number of deaths per 100 thousand from leukemia\nSource Smoking and cancer. (2013, December 04). Retrieved from http://lib.stat.cmu.edu/DASL/Datafiles/cigcancerdat.html\nReferences Data And Story Library\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in Table 11.9 (\\“us auto mileage,\\” 2019). Create a scatter plot and find a regression equation between weight of cars and mileage. Then use the regression equation to find the mileage on a car that weighs 3800 pounds and on a car that weighs 2000 pounds. Which mileage that you calculated do you think is closer to the true mileage? Why?\n\n\nCars&lt;- read.csv( \"https://krkozak.github.io/MAT160/cars.csv\") \nknitr::kable(head(Cars))\n\n\n\nTable 11.9: Weights and Mileages of Cars\n\n\n\n\n\n\nmake\nvol\nhp\nmpg\nsp\nwt\n\n\n\n\nGM/GeoMetroXF1\n89\n49\n65.4\n96\n17.5\n\n\nGM/GeoMetro\n92\n55\n56.0\n97\n20.0\n\n\nGM/GeoMetroLSI\n92\n55\n55.9\n97\n20.0\n\n\nSuzukiSwift\n92\n70\n49.0\n105\n20.0\n\n\nDaihatsuCharade\n92\n53\n46.5\n96\n20.0\n\n\nGM/GeoSprintTurbo\n89\n70\n46.2\n105\n20.0\n\n\n\n\n\n\n\n\nCode book for Data Frame Cars\nDescription Variation in gasoline mileage among makes and models of automobiles is influenced substantially by the weight and horsepower of the vehicles. When miles per gallon and horsepower are transformed to logarithms, the linearity of the regression is improved. A negative second order term is required to fit the logarithmic mileage to weight relation. If the logged variables are standardized, the coefficients of the first order terms indicate the standard units change in log mileage per one standard unit change in the predictor variable at the (logarithmic) mean. This change is constant in the case of mileage to horsepower, but not for mileage to weight. The coefficient of the second order weight term indicates the change in standardized slope associated with a one standard deviation increase in the logarithm of weight.\nFormat\nThis Data Frame contains the following columns:\nmake: the type of car\nvol: cubic feet of cab space\nhp: engine horsepower\nnpg: the average mileage of the car\nsp: top speed (mph)\nwt: the weight of the car (100 pounds)\nSource (n.d.). Retrieved July 21, 2019, from https://www3.nd.edu/~busiforc/handouts/Data and Stories/regression/us auto mileage/usautomileage.html\nReferences R.M. Heavenrich, J.D. Murrell, and K.H. Hellman, Light Duty Automotive Technology and Fuel Economy Trends Through 1991, U.S. Environmental Protection Agency, 1991 (EPA/AA/CTAB/91-02).\n**\\ **\n\n\n11.2 Correlation\nA correlation exists between two variables when the values of one variable are somehow associated with the values of the other variable. Correlation is a word used in every day life and many people think they understand what it means. What it really means is a measure of how strong the linear relationship is between two variables. However, it doesn’t give you a strong indication of how well the relationship explains the variability in the response variable, and it doesn’t work for more than two variables. There is a way to look at how much of the relationship explains the variability in the response variable.\nThere are many types of patterns one can see in the data. Common types of patterns are linear, exponential, logarithmic, or periodic. To see this pattern, you can draw a scatter plot of the data.This course will mostly deal with linear, but the other patterns exist.\nThere is some variability in the response variable values, such as calories. Some of the variation in calories is due to alcohol content and some is due to other factors. How much of the variation in the calories is due to alcohol content?\nWhen considering this question, you want to look at how much of the variation in calories is explained by alcohol content and how much is explained by other variables. Realize that some of the changes in calories have to do with other ingredients. You can have two beers at the same alcohol content, but one beer has higher calories because of the other ingredients. Some variability is explained by the model and some variability is not explained. Together, both of these give the total variability. This is (total variation)=(explained variation)+(unexplained variation)\nThe proportion of the variation that is explained by the model is \\(R^2=\\frac{\\text{explained variation}}{\\text{total variation}}\\)\nThis is known as the coefficient of determination.\nTo find the coefficient of determination, on r Studio, the command is to first name the linear model to be something and then you can find the summary of that named linear model. The next example shows how to find this.\n\n11.2.1 Example: Find the coefficient of determination \\(R^2\\)\nHow well does the alcohol content of a beer explain the variability in the number of calories in 12-ounce beer? To determine this a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), is in Table 11.1.\n\n11.2.1.1 Solution\nState random variables\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\nFirst find and name the linear model, how about lm_out_beer, so you can refer to it. Then use the command\n\nlm_out_beer&lt;-lm(calories~alcohol, data=Beer) #creates the linear model and calls it lm_out_beer \nrsquared(lm_out_beer) #finds the rsquared of the linear model created.\n\n[1] 0.8164314\n\n\nThe \\(R^2=0.8164\\). Thus, 81.64% of the variation in calories is explained to the linear relationship between alcohol content and calories. The other 18.36% of the variation is due to other factors. A really good coefficient of determination has a very small, unexplained part. So this means you probably don’t need to look for a co-variate.\nThis is all you have to do to find how much of the variability in the response variable is explained by the linear model.\nIf you do want to talk about correlation, correlation is another measure of the strength of the linear relationship. The symbol for the coefficient correlation is \\(r\\). To find \\(r\\), it is the square root of \\(R^2\\). So \\(r=\\sqrt{R^2}\\). So all you need to do is take the square root of the answer you already found. The only thing is that when you take a square root of both sides of an equation you need the \\(\\pm\\) symbol. This is because you don’t know if you want the positive or negative root. However, remember to read graphs from left to right, the same as you read words. If the graph goes up the correlation is positive and if the graph goes down the correlation is negative. Just affix the correct sign to the \\(r\\) value. You can also use this command and r will give you the correct sign.\ncor(respose_variable~explanatory_variable, data=Data_Frame)\nThe linear correlation coefficient is a number that describes the strength of the linear relationship between the two variables. It is also called the Pearson correlation coefficient after Karl Pearson who developed it.\n\n\n\n11.2.2 Interpretation of the correlation coefficient\n\\(r\\) is always between \\(-1\\) and 1. \\(r\\) = \\(-1\\) means there is a perfect negative linear correlation and \\(r\\) = 1 means there is a perfect positive correlation. The closer \\(r\\) is to 1 or \\(-1\\), the stronger the correlation. The closer \\(r\\) is to 0, the weaker the correlation. CAREFUL: \\(r\\) = 0 does not mean there is no correlation. It just means there is no linear correlation. There might be a very strong curved pattern.\n\n\n11.2.3 Example: Calculating the Linear Correlation Coefficient, \\(r\\)\nHow strong is the positive relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is a linear correlation, a random sample was taken of beer’s alcohol content and calories for several different beers (\\“Calories in beer,,\\” 2011), and the Data Frame are in Table 11.1. Find the correlation coefficient and interpret that value.\n\n11.2.3.1 Solution\nState random variables\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\nFrom example 10.2.1, \\(R^2=0.8164\\). The correlation coefficient = \\(r=\\sqrt{r^2}=\\sqrt{0.8164}=0.9035486\\), or use this command in r\n\ncor(calories~alcohol, data=Beer)\n\n[1] 0.9035659\n\n\nThe scatter graph went up from left to right, so the correlation coefficient is positive. Since 0.90 is close to 1, then that means there is a fairly strong positive correlation.\n\n\n\n11.2.4 Causation\nOne common mistake people make is to assume that because there is a correlation, then one variable causes the other. This is usually not the case. That would be like saying the amount of alcohol in the beer causes it to have a certain number of calories. However, fermentation of sugars is what causes the alcohol content. The more sugars you have, the more alcohol can be made, and the more sugar, the higher the calories. It is actually the amount of sugar that causes both. Do not confuse the idea of correlation with the concept of causation. Just because two variables are correlated does not mean one causes the other to happen. However, the new theory is showing that if you have a relationship between two variables and a strong correlation, and you can show that there are no other variables that could explain the change, then you can show causation. This is how doctors have shown that smoking causes kidney cancer. Just realize that proving that one caused the other is a difficult process, and causation should not be just assumed.\n\n\n11.2.5 Example: Correlation Versus Causation\n\nA study showed a strong linear correlation between per capita beer consumption and teacher’s salaries. Does giving a teacher a raise cause people to buy more beer? Does buying more beer cause teachers to get a raise?\nA study shows that there is a correlation between people who have had a root canal and those that have cancer. Does that mean having a root canal causes cancer?\n\n\n11.2.5.1 Solution\n\nA study showed a strong linear correlation between per capita beer consumption and teacher’s salaries. Does giving a teacher a raise cause people to buy more beer? Does buying more beer cause teachers to get a raise?\nThere is probably some other factor causing both of them to increase at the same time. Think about this: In a town where people have little extra money, they won’t have money for beer and they won’t give teachers raises. In another town where people have more extra money to spend it will be easier for them to buy more beer and they would be more willing to give teachers raises.\n\n\n\nA study shows that there is a correlation between people who have had a root canal and those that have cancer. Does that mean having a root canal causes cancer?\nJust because there is positive correlation doesn’t mean that one caused the other. It turns out that there is a positive correlation between eating carrots and cancer, but that doesn’t mean that eating carrots causes cancer. In other words, there are lots of relationships you can find between two variables, but that doesn’t mean that one caused the other.\n\nRemember a correlation only means a pattern exists. It does not mean that one variable causes the other variable to change.\n\n\n\n11.2.6 Homework for Correlation Section\nFor each problem, state the random variables.The Data Frame in this section are in section 10.1 and will be used in section 10.3.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected and are in Table 3.15 (\\“Prediction of height,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Metacarpal is below Table 3.15.\n\nTable 3.16 contains the value of the house and the amount of rental income in a year that the house brings in (\\“Capital and rental,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame House is below Table 3.16.\n\nThe World Bank collects information on the life expectancy of a person in each country (\\“Life expectancy at,\\” 2013) and the fertility rate per woman in the country (\\“Fertility rate,\\” 2013). The Data Frame for countries for the year 2011 are in Table 3.17. Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Fertility is below Table 3.17.\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The Data Frame for the countries where this information is available in Table 3.18. Find the coefficient of determination and the correlation coefficient between the percentage spent on health expenditure and the percentage of women receiving prenatal care, then interpret both.\n\nCode book for Data Frame Fert_prenatal is below Table 3.18.\n\nThe height and weight of baseball players are in Table 11.5 (\\“MLB heightsweights,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Baseball is below Table 11.5.\n\nDifferent species have different body weights and brain weights are in Table 11.6 (\\“Brain2bodyweight,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Body is below Table 11.6.\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (\\“Data hotdogs,\\” 2013) The Data Frame are in Table 10.9. Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for data frame Hotdog is below Table 10.9.\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in Table 11.7 (\\“OECD economic development,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Agriculture is Table 11.7.\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are in Table 12.15 (\\“Smoking and cancer,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Cancer is below Table 12.15.\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in Table 11.9 (\\“us auto mileage,\\” 2019). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Cars is below Table 11.9.\n\nThere is a correlation between police expenditure and crime rate. Does this mean that spending more money on police causes the crime rate to decrease? Explain your answer.\nThere is a correlation between tobacco sales and alcohol sales. Does that mean that using tobacco causes a person to also drink alcohol? Explain your answer.\nThere is a correlation between the average temperature in a location and the mortality rate from breast cancer. Does that mean that higher temperatures cause more women to die of breast cancer? Explain your answer.\nThere is a correlation between the length of time a tableware company polishes a dish and the price of the dish. Does that mean that the time a plate is polished determines the price of the dish? Explain your answer.\n\n\n\n\n11.3 Inference for Regression and Correlation\nThe idea behind regression is to find an equation that relates the response variable to the explanatory variables, and then use that equation to predict values of the response variable from values of the explanatory variables. But how do you now how good that estimate is? First you need a measure of the variability in the estimate, called the standard error of the estimate. The definition of the standard error of the estimate is how much error is in the estimate from the regression equation.\nStandard Error of the Estimate formula \\(S_e=\\sum{(\\hat{y}-y)^2}\\)\nThis formula is hard to work with but luckily r Studio will calculate it for you. In fact the command has already been used, so this information is already found. The command is\nsummary(lm_out) #lm_out is the name you gave the linear model\n\n11.3.1 Example: Finding the Standard Error of the Estimate\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), in Table 11.1. Find the standard error of the estimate.\nCode book for data frame Beer is below Table 11.1.\n\n11.3.1.1 Solution\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\nFirst save the linear model with a name like lm_out_beer. Then use summary(lm_out_beer) to find the standard error of the estimate.\n\nlm_out_beer=lm(calories~alcohol, data=Beer) \nsummary(lm_out_beer)\n\n\nCall:\nlm(formula = calories ~ alcohol, data = Beer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.869 -11.421  -0.145  11.855  55.473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   14.527      4.577   3.174  0.00171 ** \nalcohol     2672.359     84.478  31.634  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.62 on 225 degrees of freedom\nMultiple R-squared:  0.8164,    Adjusted R-squared:  0.8156 \nF-statistic:  1001 on 1 and 225 DF,  p-value: &lt; 2.2e-16\n\n\nThe standard error of the estimate is the residual standard error in the output. So the standard error of the estimate for the number of calories in a beer related to alcohol content is \\(S_e=17.62\\)\n\n\n\n11.3.2 Prediction Interval\nUsing the regression equation you can predict the number of calories from the alcohol content. However, you only find one value. The problem is that beers vary a bit in calories even if they have the same alcohol content. It would be nice to have a range instead of a single value. The range is called a prediction interval.\n\n\n11.3.3 Prediction Interval for an Individual \\(y\\)\nGiven the fixed value \\(x\\), the prediction interval for an individual \\(y\\) is \\(\\hat{y}\\pm E\\) where \\(E\\) is the error of the estimate.\nr will produce the prediction interval for you. The process follows\nTo calculate the linear model. Note this may already been done.\nlm_out = lm(explanatory_variable~ response_variable)\nThe following will compute a prediction interval. For the prediction variable set to a particular value (put that value in place of the word value), at a particular C level (given as a decimal).\npredict(lm_out, newdata=list(prediction_variable = value), interval=“prediction”, level=C)\n\n\n11.3.4 Example \\#10.3.2: Find the Prediction Interval\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), in Table 11.1. Find a 95% prediction interval for the number of calories when the alcohol content is 7.0%. content and calories (\\“Calories in beer,,\\” 2011). The Data Frame is in Table 11.1.\n\n11.3.4.1 Solution\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\nComputing the prediction interval using r Studio\n\npredict(lm_out_beer, newdata=list(alcohol=0.07), interval = \"prediction\", level=0.95) \n\n       fit      lwr     upr\n1 201.5922 166.6664 236.518\n\n\nthe fit = 201.5922 is the number of calories in the beer when the alcohol content is 7.0%. The prediction interval is between the lower (lwr) and upper (upr). The prediction interval is 166.6664 and 236.528. That means that 95% of all beer that has 7.0% alcohol has between 167 and 237 calories.\n\n\n\n11.3.5 Hypothesis Test for Correlation or Slope:\nHow do you really say you have a correlation or a linear relationship? Can you test to see if there really is a correlation or linear relationship? Of course, the answer is yes. The hypothesis test for correlation is as follows.\n\nState the random variables in words.\n\n\\(x\\) = explanatory variable\n\\(y\\) = response variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation or linear relationship}\\)\n\\(H_a: \\text{there is a correlation or linear relationship}\\)\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test are the same conditions for regression and correlation.\n\nFind the test statistic and p-value\n\nThe test statistic and p-value is already in the summary(lm_out) output.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n11.3.6 Example: Testing the Claim of a Linear Correlation\nIs there a linear relationship, or correlation, between the alcohol content and the number of calories in 12-ounce beer? To determine if there is a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), in Table 11.1. Test at the 5% level.\n\n11.3.6.1 Solution\n\nState the random variables in words.\n\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation or linear relationship}\\)\n\\(H_a: \\text{there is a correlation or linear relationship}\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test were already checked in Example: Find the Equation of the Regression Line\n\nFind the test statistic and p-value\n\nThe command on r Studio is\n\nlm_out_beer&lt;-lm(calories~alcohol, data=Beer) \nsummary(lm_out_beer)\n\n\nCall:\nlm(formula = calories ~ alcohol, data = Beer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.869 -11.421  -0.145  11.855  55.473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   14.527      4.577   3.174  0.00171 ** \nalcohol     2672.359     84.478  31.634  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.62 on 225 degrees of freedom\nMultiple R-squared:  0.8164,    Adjusted R-squared:  0.8156 \nF-statistic:  1001 on 1 and 225 DF,  p-value: &lt; 2.2e-16\n\n\nThe test statistic is the t value in the table for the explanatory variable. In this case that is 31.634. The p-value is the Pr(&gt;|t|) which is \\(2.2X10^{-16}\\).\n\nConclusion\n\nReject \\(H_o\\) since the p-value is less than 0.05.\n\nInterpretation\n\nThere is enough evidence to show that there is a correlation and linear relationship between alcohol content and number of calories in a 12-ounce bottle of beer.\n\n\n\n11.3.7 Homework for Inference for Regression and Correlation Section\nFor each problem, state the random variables. The Data Frame in this section are in the homework for section 10.1 and were also used in section 10.2.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected and are in Table 3.15 (\\“Prediction of height,\\” 2013).\n\nCode book for Data Frame Metacarpal is below Table 3.15.\n\nFind the standard error of the estimate.\nCompute a 99% prediction interval for height of a person with a metacarpal length of 44 mm.\nTest at the 1% level for a correlation or linear relationship between length of metacarpal bone 1 and height of a person.\n\n\n\nTable 3.16 contains the value of the house and the amount of rental income in a year that the house brings in (\\“Capital and rental,\\” 2013).\n\nCode book for Data Frame House is below Table 3.16.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the rental income on a house worth \\$230,000.\nTest at the 5% level for a correlation or linear relationship between house value and rental amount.\n\n\n\nThe World Bank collects information on the life expectancy of a person in each country (\\“Life expectancy at,\\” 2013) and the fertility rate per woman in the country (\\“Fertility rate,\\” 2013). The Data Frame for countries for the year 2011 is in Table 3.17.\n\nCode book for Data Frame Fertility is below Table 3.17.\n\nFind the standard error of the estimate.\nCompute a 99% prediction interval for the life expectancy for a country that has a fertility rate of 2.7.\nTest at the 1% level for a correlation or linear relationship between fertility rate and life expectancy.\n\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The Data Frame for the countries where this information is available in Table 3.18.\n\nCode book for Data Frame Fert_prenatal is below Table 3.18.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the percentage of woman receiving prenatal care for a country that spends 5.0 % of GDP on health expenditure.\nTest at the 5% level for a correlation or linear relationship between percentage spent on health expenditure and the percentage of women receiving prenatal care.\n\n\n\nThe height and weight of baseball players are in Table 11.5 (\\“MLB heightsweights,\\” 2013).\n\nCode book for Data Frame Baseball is below Table 11.5.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the weight of a baseball player that is 75 inches tall.\nTest at the 5% level for a correlation or linear relationship between height and weight of baseball players.\n\n\n\nDifferent species have different body weights and brain weights are in Table 11.6 (\\“Brain2bodyweight,\\” 2013).\n\nCode book for Data Frame Body is below Table 11.6.\n\nFind the standard error of the estimate.\nCompute a 99% prediction interval for the brain weight for a species that has a body weight of 62 kg.\nTest at the 1% level for a correlation or linear relationship between body weights and brain weights.\n\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (\\“Data hotdogs,\\” 2013) The Data Frame are in Table 10.9.\n\nCode book for data frame Hotdog is below Table 10.9.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the amount of sodium a beef hot dog has if it is 170 calories.\nTest at the 5% level for a correlation or linear relationship between amount of calories and amount of sodium.\n\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in Table 11.7 (\\“OECD economic development,\\” 2013).\n\nCode book for Data Frame Agriculture is below Table 11.7.\n\nFind the standard error of the estimate.\nCompute a 90% prediction interval for the per capita income in a country that has 21 percent of labor in agriculture.\nTest at the 5% level for a correlation or linear relationship between percent of labor force in agriculture and per capita income.\n\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are in Table 12.15 (“Smoking and cancer,” 2013).\n\nCode book for Data Frame Cancer is below Table 12.15.\n\nFind the standard error of the estimate.\nCompute a 99% prediction interval for the number of deaths from bladder cancer when the cigarette sales were 20 per capita.\nTest at the 1% level for a correlation or linear relationship between cigarette smoking and deaths of bladder cancer.\n\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in Table 11.9 (“us auto mileage,” 2019).\n\nCode book for Data Frame Cars is below Table 11.9.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the mileage on a car that weighs 3800 pounds.\nTest at the 5% level for a correlation or linear relationship between the weight of cars and mileage.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#correlation",
    "href": "Regression.html#correlation",
    "title": "11  Regression",
    "section": "",
    "text": "A correlation exists between two variables when the values of one variable are somehow associated with the values of the other variable. Correlation is a word used in every day life and many people think they understand what it means. What it really means is a measure of how strong the linear relationship is between two variables. However, it doesn’t give you a strong indication of how well the relationship explains the variability in the response variable, and it doesn’t work for more than two variables. There is a way to look at how much of the relationship explains the variability in the response variable.\nThere are many types of patterns one can see in the data. Common types of patterns are linear, exponential, logarithmic, or periodic. To see this pattern, you can draw a scatter plot of the data.This course will mostly deal with linear, but the other patterns exist.\nThere is some variability in the response variable values, such as calories. Some of the variation in calories is due to alcohol content and some is due to other factors. How much of the variation in the calories is due to alcohol content?\nWhen considering this question, you want to look at how much of the variation in calories is explained by alcohol content and how much is explained by other variables. Realize that some of the changes in calories have to do with other ingredients. You can have two beers at the same alcohol content, but one beer has higher calories because of the other ingredients. Some variability is explained by the model and some variability is not explained. Together, both of these give the total variability. This is (total variation)=(explained variation)+(unexplained variation)\nThe proportion of the variation that is explained by the model is \\(R^2=\\frac{\\text{explained variation}}{\\text{total variation}}\\)\nThis is known as the coefficient of determination.\nTo find the coefficient of determination, on r Studio, the command is to first name the linear model to be something and then you can find the summary of that named linear model. The next example shows how to find this.\n\n11.2.1 Example: Find the coefficient of determination \\(R^2\\)\nHow well does the alcohol content of a beer explain the variability in the number of calories in 12-ounce beer? To determine this a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), is in Table 11.1.\n\n11.2.1.1 Solution\nState random variables\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\nFirst find and name the linear model, how about lm_out_beer, so you can refer to it. Then use the command\n\nlm_out_beer&lt;-lm(calories~alcohol, data=Beer) #creates the linear model and calls it lm_out_beer \nrsquared(lm_out_beer) #finds the rsquared of the linear model created.\n\n[1] 0.8164314\n\n\nThe \\(R^2=0.8164\\). Thus, 81.64% of the variation in calories is explained to the linear relationship between alcohol content and calories. The other 18.36% of the variation is due to other factors. A really good coefficient of determination has a very small, unexplained part. So this means you probably don’t need to look for a co-variate.\nThis is all you have to do to find how much of the variability in the response variable is explained by the linear model.\nIf you do want to talk about correlation, correlation is another measure of the strength of the linear relationship. The symbol for the coefficient correlation is \\(r\\). To find \\(r\\), it is the square root of \\(R^2\\). So \\(r=\\sqrt{R^2}\\). So all you need to do is take the square root of the answer you already found. The only thing is that when you take a square root of both sides of an equation you need the \\(\\pm\\) symbol. This is because you don’t know if you want the positive or negative root. However, remember to read graphs from left to right, the same as you read words. If the graph goes up the correlation is positive and if the graph goes down the correlation is negative. Just affix the correct sign to the \\(r\\) value. You can also use this command and r will give you the correct sign.\ncor(respose_variable~explanatory_variable, data=Data_Frame)\nThe linear correlation coefficient is a number that describes the strength of the linear relationship between the two variables. It is also called the Pearson correlation coefficient after Karl Pearson who developed it.\n\n\n\n11.2.2 Interpretation of the correlation coefficient\n\\(r\\) is always between \\(-1\\) and 1. \\(r\\) = \\(-1\\) means there is a perfect negative linear correlation and \\(r\\) = 1 means there is a perfect positive correlation. The closer \\(r\\) is to 1 or \\(-1\\), the stronger the correlation. The closer \\(r\\) is to 0, the weaker the correlation. CAREFUL: \\(r\\) = 0 does not mean there is no correlation. It just means there is no linear correlation. There might be a very strong curved pattern.\n\n\n11.2.3 Example: Calculating the Linear Correlation Coefficient, \\(r\\)\nHow strong is the positive relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is a linear correlation, a random sample was taken of beer’s alcohol content and calories for several different beers (\\“Calories in beer,,\\” 2011), and the Data Frame are in Table 11.1. Find the correlation coefficient and interpret that value.\n\n11.2.3.1 Solution\nState random variables\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\nFrom example 10.2.1, \\(R^2=0.8164\\). The correlation coefficient = \\(r=\\sqrt{r^2}=\\sqrt{0.8164}=0.9035486\\), or use this command in r\n\ncor(calories~alcohol, data=Beer)\n\n[1] 0.9035659\n\n\nThe scatter graph went up from left to right, so the correlation coefficient is positive. Since 0.90 is close to 1, then that means there is a fairly strong positive correlation.\n\n\n\n11.2.4 Causation\nOne common mistake people make is to assume that because there is a correlation, then one variable causes the other. This is usually not the case. That would be like saying the amount of alcohol in the beer causes it to have a certain number of calories. However, fermentation of sugars is what causes the alcohol content. The more sugars you have, the more alcohol can be made, and the more sugar, the higher the calories. It is actually the amount of sugar that causes both. Do not confuse the idea of correlation with the concept of causation. Just because two variables are correlated does not mean one causes the other to happen. However, the new theory is showing that if you have a relationship between two variables and a strong correlation, and you can show that there are no other variables that could explain the change, then you can show causation. This is how doctors have shown that smoking causes kidney cancer. Just realize that proving that one caused the other is a difficult process, and causation should not be just assumed.\n\n\n11.2.5 Example: Correlation Versus Causation\n\nA study showed a strong linear correlation between per capita beer consumption and teacher’s salaries. Does giving a teacher a raise cause people to buy more beer? Does buying more beer cause teachers to get a raise?\nA study shows that there is a correlation between people who have had a root canal and those that have cancer. Does that mean having a root canal causes cancer?\n\n\n11.2.5.1 Solution\n\nA study showed a strong linear correlation between per capita beer consumption and teacher’s salaries. Does giving a teacher a raise cause people to buy more beer? Does buying more beer cause teachers to get a raise?\nThere is probably some other factor causing both of them to increase at the same time. Think about this: In a town where people have little extra money, they won’t have money for beer and they won’t give teachers raises. In another town where people have more extra money to spend it will be easier for them to buy more beer and they would be more willing to give teachers raises.\n\n\n\nA study shows that there is a correlation between people who have had a root canal and those that have cancer. Does that mean having a root canal causes cancer?\nJust because there is positive correlation doesn’t mean that one caused the other. It turns out that there is a positive correlation between eating carrots and cancer, but that doesn’t mean that eating carrots causes cancer. In other words, there are lots of relationships you can find between two variables, but that doesn’t mean that one caused the other.\n\nRemember a correlation only means a pattern exists. It does not mean that one variable causes the other variable to change.\n\n\n\n11.2.6 Homework for Correlation Section\nFor each problem, state the random variables.The Data Frame in this section are in section 10.1 and will be used in section 10.3.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected and are in Table 3.15 (\\“Prediction of height,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Metacarpal is below Table 3.15.\n\nTable 3.16 contains the value of the house and the amount of rental income in a year that the house brings in (\\“Capital and rental,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame House is below Table 3.16.\n\nThe World Bank collects information on the life expectancy of a person in each country (\\“Life expectancy at,\\” 2013) and the fertility rate per woman in the country (\\“Fertility rate,\\” 2013). The Data Frame for countries for the year 2011 are in Table 3.17. Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Fertility is below Table 3.17.\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The Data Frame for the countries where this information is available in Table 3.18. Find the coefficient of determination and the correlation coefficient between the percentage spent on health expenditure and the percentage of women receiving prenatal care, then interpret both.\n\nCode book for Data Frame Fert_prenatal is below Table 3.18.\n\nThe height and weight of baseball players are in Table 11.5 (\\“MLB heightsweights,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Baseball is below Table 11.5.\n\nDifferent species have different body weights and brain weights are in Table 11.6 (\\“Brain2bodyweight,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Body is below Table 11.6.\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (\\“Data hotdogs,\\” 2013) The Data Frame are in Table 10.9. Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for data frame Hotdog is below Table 10.9.\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in Table 11.7 (\\“OECD economic development,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Agriculture is Table 11.7.\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are in Table 12.15 (\\“Smoking and cancer,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Cancer is below Table 12.15.\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in Table 11.9 (\\“us auto mileage,\\” 2019). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Cars is below Table 11.9.\n\nThere is a correlation between police expenditure and crime rate. Does this mean that spending more money on police causes the crime rate to decrease? Explain your answer.\nThere is a correlation between tobacco sales and alcohol sales. Does that mean that using tobacco causes a person to also drink alcohol? Explain your answer.\nThere is a correlation between the average temperature in a location and the mortality rate from breast cancer. Does that mean that higher temperatures cause more women to die of breast cancer? Explain your answer.\nThere is a correlation between the length of time a tableware company polishes a dish and the price of the dish. Does that mean that the time a plate is polished determines the price of the dish? Explain your answer.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#inference-for-regression-and-correlation",
    "href": "Regression.html#inference-for-regression-and-correlation",
    "title": "11  Regression",
    "section": "11.3 Inference for Regression and Correlation",
    "text": "11.3 Inference for Regression and Correlation\nThe idea behind regression is to find an equation that relates the response variable to the explanatory variables, and then use that equation to predict values of the response variable from values of the explanatory variables. But how do you now how good that estimate is? First you need a measure of the variability in the estimate, called the standard error of the estimate. The definition of the standard error of the estimate is how much error is in the estimate from the regression equation.\nStandard Error of the Estimate formula \\(S_e=\\sum{(\\hat{y}-y)^2}\\)\nThis formula is hard to work with but luckily r Studio will calculate it for you. In fact the command has already been used, so this information is already found. The command is\nsummary(lm_out) #lm_out is the name you gave the linear model\n\n11.3.1 Example: Finding the Standard Error of the Estimate\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), in Table 11.1. Find the standard error of the estimate.\nCode book for data frame Beer is below Table 11.1.\n\n11.3.1.1 Solution\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\nFirst save the linear model with a name like lm_out_beer. Then use summary(lm_out_beer) to find the standard error of the estimate.\n\nlm_out_beer=lm(calories~alcohol, data=Beer) \nsummary(lm_out_beer)\n\n\nCall:\nlm(formula = calories ~ alcohol, data = Beer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.869 -11.421  -0.145  11.855  55.473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   14.527      4.577   3.174  0.00171 ** \nalcohol     2672.359     84.478  31.634  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.62 on 225 degrees of freedom\nMultiple R-squared:  0.8164,    Adjusted R-squared:  0.8156 \nF-statistic:  1001 on 1 and 225 DF,  p-value: &lt; 2.2e-16\n\n\nThe standard error of the estimate is the residual standard error in the output. So the standard error of the estimate for the number of calories in a beer related to alcohol content is \\(S_e=17.62\\)\n\n\n\n11.3.2 Prediction Interval\nUsing the regression equation you can predict the number of calories from the alcohol content. However, you only find one value. The problem is that beers vary a bit in calories even if they have the same alcohol content. It would be nice to have a range instead of a single value. The range is called a prediction interval.\n\n\n11.3.3 Prediction Interval for an Individual \\(y\\)\nGiven the fixed value \\(x\\), the prediction interval for an individual \\(y\\) is \\(\\hat{y}\\pm E\\) where \\(E\\) is the error of the estimate.\nr will produce the prediction interval for you. The process follows\nTo calculate the linear model. Note this may already been done.\nlm_out = lm(explanatory_variable~ response_variable)\nThe following will compute a prediction interval. For the prediction variable set to a particular value (put that value in place of the word value), at a particular C level (given as a decimal).\npredict(lm_out, newdata=list(prediction_variable = value), interval=“prediction”, level=C)\n\n\n11.3.4 Example \\#10.3.2: Find the Prediction Interval\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), in Table 11.1. Find a 95% prediction interval for the number of calories when the alcohol content is 7.0%. content and calories (\\“Calories in beer,,\\” 2011). The Data Frame is in Table 11.1.\n\n11.3.4.1 Solution\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\nComputing the prediction interval using r Studio\n\npredict(lm_out_beer, newdata=list(alcohol=0.07), interval = \"prediction\", level=0.95) \n\n       fit      lwr     upr\n1 201.5922 166.6664 236.518\n\n\nthe fit = 201.5922 is the number of calories in the beer when the alcohol content is 7.0%. The prediction interval is between the lower (lwr) and upper (upr). The prediction interval is 166.6664 and 236.528. That means that 95% of all beer that has 7.0% alcohol has between 167 and 237 calories.\n\n\n\n11.3.5 Hypothesis Test for Correlation or Slope:\nHow do you really say you have a correlation or a linear relationship? Can you test to see if there really is a correlation or linear relationship? Of course, the answer is yes. The hypothesis test for correlation is as follows.\n\nState the random variables in words.\n\n\\(x\\) = explanatory variable\n\\(y\\) = response variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation or linear relationship}\\)\n\\(H_a: \\text{there is a correlation or linear relationship}\\)\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test are the same conditions for regression and correlation.\n\nFind the test statistic and p-value\n\nThe test statistic and p-value is already in the summary(lm_out) output.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n11.3.6 Example: Testing the Claim of a Linear Correlation\nIs there a linear relationship, or correlation, between the alcohol content and the number of calories in 12-ounce beer? To determine if there is a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), in Table 11.1. Test at the 5% level.\n\n11.3.6.1 Solution\n\nState the random variables in words.\n\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation or linear relationship}\\)\n\\(H_a: \\text{there is a correlation or linear relationship}\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test were already checked in Example: Find the Equation of the Regression Line\n\nFind the test statistic and p-value\n\nThe command on r Studio is\n\nlm_out_beer&lt;-lm(calories~alcohol, data=Beer) \nsummary(lm_out_beer)\n\n\nCall:\nlm(formula = calories ~ alcohol, data = Beer)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-51.869 -11.421  -0.145  11.855  55.473 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   14.527      4.577   3.174  0.00171 ** \nalcohol     2672.359     84.478  31.634  &lt; 2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.62 on 225 degrees of freedom\nMultiple R-squared:  0.8164,    Adjusted R-squared:  0.8156 \nF-statistic:  1001 on 1 and 225 DF,  p-value: &lt; 2.2e-16\n\n\nThe test statistic is the t value in the table for the explanatory variable. In this case that is 31.634. The p-value is the Pr(&gt;|t|) which is \\(2.2X10^{-16}\\).\n\nConclusion\n\nReject \\(H_o\\) since the p-value is less than 0.05.\n\nInterpretation\n\nThere is enough evidence to show that there is a correlation and linear relationship between alcohol content and number of calories in a 12-ounce bottle of beer.\n\n\n\n11.3.7 Homework for Inference for Regression and Correlation Section\nFor each problem, state the random variables. The Data Frame in this section are in the homework for section 10.1 and were also used in section 10.2.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected and are in Table 3.15 (\\“Prediction of height,\\” 2013).\n\nCode book for Data Frame Metacarpal is below Table 3.15.\n\nFind the standard error of the estimate.\nCompute a 99% prediction interval for height of a person with a metacarpal length of 44 mm.\nTest at the 1% level for a correlation or linear relationship between length of metacarpal bone 1 and height of a person.\n\n\n\nTable 3.16 contains the value of the house and the amount of rental income in a year that the house brings in (\\“Capital and rental,\\” 2013).\n\nCode book for Data Frame House is below Table 3.16.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the rental income on a house worth \\$230,000.\nTest at the 5% level for a correlation or linear relationship between house value and rental amount.\n\n\n\nThe World Bank collects information on the life expectancy of a person in each country (\\“Life expectancy at,\\” 2013) and the fertility rate per woman in the country (\\“Fertility rate,\\” 2013). The Data Frame for countries for the year 2011 is in Table 3.17.\n\nCode book for Data Frame Fertility is below Table 3.17.\n\nFind the standard error of the estimate.\nCompute a 99% prediction interval for the life expectancy for a country that has a fertility rate of 2.7.\nTest at the 1% level for a correlation or linear relationship between fertility rate and life expectancy.\n\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The Data Frame for the countries where this information is available in Table 3.18.\n\nCode book for Data Frame Fert_prenatal is below Table 3.18.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the percentage of woman receiving prenatal care for a country that spends 5.0 % of GDP on health expenditure.\nTest at the 5% level for a correlation or linear relationship between percentage spent on health expenditure and the percentage of women receiving prenatal care.\n\n\n\nThe height and weight of baseball players are in Table 11.5 (\\“MLB heightsweights,\\” 2013).\n\nCode book for Data Frame Baseball is below Table 11.5.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the weight of a baseball player that is 75 inches tall.\nTest at the 5% level for a correlation or linear relationship between height and weight of baseball players.\n\n\n\nDifferent species have different body weights and brain weights are in Table 11.6 (\\“Brain2bodyweight,\\” 2013).\n\nCode book for Data Frame Body is below Table 11.6.\n\nFind the standard error of the estimate.\nCompute a 99% prediction interval for the brain weight for a species that has a body weight of 62 kg.\nTest at the 1% level for a correlation or linear relationship between body weights and brain weights.\n\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (\\“Data hotdogs,\\” 2013) The Data Frame are in Table 10.9.\n\nCode book for data frame Hotdog is below Table 10.9.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the amount of sodium a beef hot dog has if it is 170 calories.\nTest at the 5% level for a correlation or linear relationship between amount of calories and amount of sodium.\n\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in Table 11.7 (\\“OECD economic development,\\” 2013).\n\nCode book for Data Frame Agriculture is below Table 11.7.\n\nFind the standard error of the estimate.\nCompute a 90% prediction interval for the per capita income in a country that has 21 percent of labor in agriculture.\nTest at the 5% level for a correlation or linear relationship between percent of labor force in agriculture and per capita income.\n\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are in Table 12.15 (“Smoking and cancer,” 2013).\n\nCode book for Data Frame Cancer is below Table 12.15.\n\nFind the standard error of the estimate.\nCompute a 99% prediction interval for the number of deaths from bladder cancer when the cigarette sales were 20 per capita.\nTest at the 1% level for a correlation or linear relationship between cigarette smoking and deaths of bladder cancer.\n\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in Table 11.9 (“us auto mileage,” 2019).\n\nCode book for Data Frame Cars is below Table 11.9.\n\nFind the standard error of the estimate.\nCompute a 95% prediction interval for the mileage on a car that weighs 3800 pounds.\nTest at the 5% level for a correlation or linear relationship between the weight of cars and mileage.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Chi-Square and ANOVA Tests.html",
    "href": "Chi-Square and ANOVA Tests.html",
    "title": "12  Chi-squared and ANOVA Tests",
    "section": "",
    "text": "12.1 Chi-Square Test for Independence\nThis chapter presents material on three more hypothesis tests. One is used to determine significant relationship between two qualitative variables, the second is used to determine if the sample data has a particular distribution, and the last is used to determine significant relationships between means of 3 or more samples.\nRemember, qualitative data is where you collect data on individuals that are categories or names. Then you would count how many of the individuals had particular qualities. An example is that there is a theory that there is a relationship between breastfeeding and autism. To determine if there is a relationship, researchers could collect the time period that a mother breastfed her child and if that child was diagnosed with autism. Then you would have a table containing this information. Now you want to know if each cell is independent of each other cell. Remember, independence says that one event does not affect another event. Here it means that having autism is independent of being breastfed. What you really want is to see if they are not independent. In other words, does one affect the other? If you were to do a hypothesis test, this is your alternative hypothesis and the null hypothesis is that they are independent. There is a hypothesis test for this and it is called the Chi-Square Test for Independence. Technically it should be called the Chi-Square Test for Dependence, but for historical reasons it is known as the test for independence. Just as with previous hypothesis tests, all the steps are the same except for the conditions and the test statistic.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chi-squared and ANOVA Tests</span>"
    ]
  },
  {
    "objectID": "Chi-Square and ANOVA Tests.html#chi-square-test-for-independence",
    "href": "Chi-Square and ANOVA Tests.html#chi-square-test-for-independence",
    "title": "12  Chi-squared and ANOVA Tests",
    "section": "",
    "text": "12.1.1 Hypothesis Test for Chi-Square Test\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) the two variables are independent (this means that the one variable is not affected by the other)\n\\(H_a:\\) the two variables are dependent (this means that the one variable is affected by the other)\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample is taken. Check: describe the process used to collect sample.\nState: Expected frequencies for each cell are greater than or equal to 5, which means \\(E\\ge5\\). Check: The expected frequencies, \\(E\\), will be calculated later.\n\n3. Find the test statistic and p-value\nFinding the test statistic involves several steps. First the data is collected and counted, and then it is organized into a table (in a table each entry is called a cell). These values are known as the observed frequencies, which the symbol for an observed frequency is \\(O\\). Each table is made up of rows and columns. Then each row is totaled to give a row total and each column is totaled to give a column total.\nThe null hypothesis is that the variables are independent. Using the multiplication rule for independent events you can calculate the probability of being one value of the first variable, \\(A\\), and one value of the second variable, \\(B\\) (the probability of a particular cell). Remember in a hypothesis test, you assume that is true, the two variables are assumed to be independent.\nNow you want to find out how many individuals you expect to be in a certain cell. To find the expected frequencies, you just need to multiply the probability of that cell times the total number of individuals. Do not round the expected frequencies.\nIf the variables are independent the expected frequencies and the observed frequencies should be the same. The test statistic here will involve looking at the difference between the expected frequency and the observed frequency for each cell. Then you want to find the “total difference” of all of these differences. The larger the total, the smaller the chances that you could find that test statistic given that the condition of independence is true. That means that the condition of independence is not true. How do you find the test statistic? First find the differences between the observed and expected frequencies. Because some of these differences will be positive and some will be negative, you need to square these differences. These squares could be large just because the frequencies are large, you need to divide by the expected frequencies to scale them. Then finally add up all of these fractional values. This is the test statistic.\nTest Statistic:\nUsing r: See Example: Hypothesis Test with Chi-Square Test for the process\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value\\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n12.1.1.1 Example: Hypothesis Test with Chi-Square Test\nIs there a relationship between autism and breastfeeding? To determine if there is, a researcher asked mothers of autistic and non-autistic children to say what time period they breastfed their children. The data is in Autism Versus Breastfeeding (Schultz, Klonoff-Cohen, Wingard, Askhoomoff, Macera, Ji & Bacher, 2006). Do the data provide enough evidence to support that breastfeeding and autism are independent? Test at the 1% level.\n\n12.1.1.1.1 Autism Versus Breastfeeding\n\nBreast Feeding\n\n\n\n\n\n\n\n\n\nAutism\nNot Breast Feed\nBreast Feed less than 2 months\nBreast Feed 2 to 6 months\nBreast Feed more than 6 months\n\n\n\n\nyes\n241\n198\n164\n215\n\n\nno\n20\n25\n27\n44\n\n\n\nTo put this data into r, use the following commands:\nIf you have the data frame instead of the summary table as in this example, you can use the tally command to create the table in r. Just save the tally command with a name.\n\n\n\n12.1.1.2 Solution\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o\\): Breastfeeding and autism are independent\n\\(H_a\\): Breastfeeding and autism are dependent\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of breastfeeding time frames and autism incidence was taken. Check: this was stated in the problem.\nState: Expected frequencies for each cell are greater than or equal to 5, \\(E\\ge 5\\). Check: See step 3. All expected frequencies are more than 5.\n\n\n\nFind the test statistic and p-value On rStudio, the command is\n\n\nchisq.test(autism_table) #calculates the test statistics and p-value \n\n\n    Pearson's Chi-squared test\n\ndata:  autism_table\nX-squared = 11.217, df = 3, p-value = 0.01061\n\nchisq.test(autism_table)$expected # shows all the expected frequencies \n\n         [,1]      [,2]      [,3]      [,4]\nyes 228.58458 195.30407 167.27837 226.83298\nno   32.41542  27.69593  23.72163  32.16702\n\n\nThe test statistic is 11.217 and the p-value is 0.01061.\n\nConclusion\n\nFail to reject \\(H_o\\) since the p-value is more than 0.01.\n\nInterpretation\n\nThere is not enough evidence to support that breastfeeding and autism are dependent. This means that you cannot say whether a child is breastfed or not will indicate if that the child will be diagnosed with autism.\n\n\n\n12.1.2 Homework for Chi-Square Test for Independence Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nThe number of people who survived the Titanic based on class and sex is in Table 12.1 (“Encyclopedia Titanica,” 2013). Is there enough evidence to show that the class and the sex of a person who survived the Titanic are independent? Test at the 5% level.\n\n\n\n\n\nTable 12.1: Surviving the Titanic\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\nfirst\n134\n59\n\n\nsecond\n94\n25\n\n\nthird\n80\n58\n\n\n\n\n\n\n\n\n\nResearchers watched groups of dolphins off the coast of Ireland in 1998 to determine what activities the dolphins partake in at certain times of the day (“Activities of dolphin,” 2013). The numbers in Table 12.2 represent the number of groups of dolphins that were partaking in an activity at certain times of days. Is there enough evidence to show that the activity and the time period are independent for dolphins? Test at the 1% level.\n\n\nDolphin&lt;- read.csv( \"https://krkozak.github.io/MAT160/dolphins.csv\") \nDolphin_table&lt;-tally(~activity+period, data=Dolphin)\nknitr::kable(Dolphin_table)\n\n\n\nTable 12.2: Dolphin Activity\n\n\n\n\n\n\n\nAfternoon\nEvening\nMorning\nNoon\n\n\n\n\nFeed\n0\n56\n28\n4\n\n\nSocial\n9\n10\n38\n5\n\n\nTravel\n14\n13\n6\n6\n\n\n\n\n\n\n\n\n\nIs there a relationship between autism and what an infant is fed? To determine if there is, a researcher asked mothers of autistic and non-autistic children to say what they fed their infant. The data is in Table 12.3 (Schultz, Klonoff-Cohen, Wingard, Askhoomoff, Macera, Ji & Bacher, 2006). Do the data provide enough evidence to show that that what an infant is fed and autism are independent? Breast-feeding (BF), Formula with DHA/ARA (For with), and Formula without DHA/ARA (Form without)Test at the 1% level.\n\n\nFeeding&lt;- read.csv( \"https://krkozak.github.io/MAT160/Mothers.csv\") \nFeeding_table&lt;-tally(~autism+feeding, data=Feeding)\nknitr::kable(Feeding_table)\n\n\n\nTable 12.3: Autism Versus Breastfeeding\n\n\n\n\n\n\n\nbreast\nformula_with\nformula_without\n\n\n\n\nno\n6\n22\n10\n\n\nyes\n12\n39\n65\n\n\n\n\n\n\n\n\n\nStudents at multiple grade schools were asked what their personal goal (get good grades, be popular, be good at sports) was and how important good grades were to them (1 very important and 4 least important). The data is in Table 12.4 (“Popular kids datafile,” 2013). Do the data provide enough evidence to show that goal attainment and importance of grades are independent? Test at the 5% level.\n\n\nGoal&lt;- read.csv( \"https://krkozak.github.io/MAT160/Popular_Kids_clean.csv\") \nGoal_Grades_table&lt;-tally(~Goals+Grades, data=Goal)\nknitr::kable(Goal_Grades_table)\n\n\n\nTable 12.4: Personal Goal and Importance of Grades\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\nGrades\n70\n66\n55\n56\n\n\nPopular\n14\n33\n45\n49\n\n\nSports\n10\n24\n33\n23\n\n\n\n\n\n\n\n\n\nStudents at multiple grade schools were asked what their personal goal (get good grades, be popular, be good at sports) was and how important being good at sports were to them (1 very important and 4 least important). The data is in Table 12.5 (“Popular kids datafile,” 2013). Do the data provide enough evidence to show that goal attainment and importance of sports are independent? Test at the 5% level.\n\n\nGoal&lt;- read.csv( \"https://krkozak.github.io/MAT160/Popular_Kids_clean.csv\") \nGoal_Sports_table&lt;-tally(~Goals+Sports, data=Goal)\nknitr::kable(Goal_Sports_table)\n\n\n\nTable 12.5: Personal Goal and Importance of Sports\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\nGrades\n83\n81\n55\n28\n\n\nPopular\n32\n49\n43\n17\n\n\nSports\n50\n24\n14\n2\n\n\n\n\n\n\n\n\n\nStudents at multiple grade schools were asked what their personal goal (get good grades, be popular, be good at sports) was and how important having good looks were to them (1 very important and 4 least important). The data is in Table 12.6 (“Popular kids datafile,” 2013). Do the data provide enough evidence to show that goal attainment and importance of looks are independent? Test at the 5% level.\n\n\nGoal&lt;- read.csv( \"https://krkozak.github.io/MAT160/Popular_Kids_clean.csv\") \nGoal_Looks_table&lt;-tally(~Goals+Looks, data=Goal)\nknitr::kable(Goal_Looks_table)\n\n\n\nTable 12.6: Personal Goal and Importance of Looks\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\nGrades\n80\n66\n66\n35\n\n\nPopular\n81\n30\n18\n12\n\n\nSports\n24\n30\n17\n19\n\n\n\n\n\n\n\n\n\nStudents at multiple grade schools were asked what their personal goal (get good grades, be popular, be good at sports) was and how important having money were to them (1 very important and 4 least important). The data is in Table 12.7 (“Popular kids datafile,” 2013). Do the data provide enough evidence to show that goal attainment and importance of money are independent? Test at the 5% level.\n\n\nGoal&lt;- read.csv( \"https://krkozak.github.io/MAT160/Popular_Kids_clean.csv\") \nGoal_Money_table&lt;-tally(~Goals+Money, data=Goal)\nknitr::kable(Goal_Money_table)\n\n\n\nTable 12.7: Personal Goal and Importance of Money\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\nGrades\n14\n34\n71\n128\n\n\nPopular\n14\n29\n35\n63\n\n\nSports\n6\n12\n26\n46",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chi-squared and ANOVA Tests</span>"
    ]
  },
  {
    "objectID": "Chi-Square and ANOVA Tests.html#chi-square-goodness-of-fit",
    "href": "Chi-Square and ANOVA Tests.html#chi-square-goodness-of-fit",
    "title": "12  Chi-squared and ANOVA Tests",
    "section": "12.2 Chi-Square Goodness of Fit",
    "text": "12.2 Chi-Square Goodness of Fit\nIn probability, you calculated probabilities using both experimental and theoretical methods. There are times when it is important to determine how well the experimental values match the theoretical values. An example of this is if you wish to verify if a die is fair. To determine if observed values fit the expected values, you want to see if the difference between observed values and expected values is large enough to say that the test statistic is unlikely to happen if you assume that the observed values fit the expected values. The test statistic in this case is also the chi-square. The process is the same as for the chi-square test for independence.\n\n12.2.1 Hypothesis Test for Goodness of Fit Test\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) The data are consistent with a specific distribution\n\\(H_a:\\) The data are not consistent with a specific distribution\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample is taken. Check: State how the sample is collected.\nState: Expected frequencies for each cell are greater than or equal to 5. Check: The expected frequencies, *E*, will be calculated later.\n\n\n\nFind the test statistic and p-value\n\nUsing rStudio see example 11.2.1\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_0\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n12.2.2 Example: Goodness of Fit Test\nSuppose you have a die that you are curious if it is fair or not. If it is fair then the proportion for each value should be the same. You need to find the observed frequencies and to accomplish this you roll the die 500 times and count how often each side comes up. The data is in table Table 12.8. Do the data show that the die is fair? Test at the 5% level.\n\n\n\n\nTable 12.8: Observed Frequencies of Die for sides 1 through 6\n\n\n\n\n\n\nSides\nobserved….c.78..87..87..76..85..87.\n\n\n\n\n1\n78\n\n\n2\n87\n\n\n3\n87\n\n\n4\n76\n\n\n5\n85\n\n\n6\n87\n\n\n\n\n\n\n\n\n\n12.2.2.1 Solution\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) The observed frequencies are consistent with the distribution for fair die (the die is fair)\n\\(H_a:\\) The observed frequencies are not consistent with the distribution for fair die (the die is not fair)\n\\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample is taken. check: This is true since each throw of a die is a random event.\nState: Expected frequencies for each cell are greater than or equal to 5. Check: See step 3.\n\n\n\nFind the test statistic and p-value\n\nOn rStudio, this would be\n\nfair_die&lt;-c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6) \nobserved&lt;-c(78, 87, 87, 76, 85, 87) \nchisq.test(observed, p=fair_die) \n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 1.504, df = 5, p-value = 0.9126\n\nchisq.test(observed, p=fair_die)$expected \n\n[1] 83.33333 83.33333 83.33333 83.33333 83.33333 83.33333\n\n\nTest Statistic: The test statistic is 1.504. The p-value is 0.9126.\n\nConclusion\n\nFail to reject \\(H_o\\) since the p-value is greater than 0.05.\n\nInterpretation\n\nThere is not enough evidence to support that the die is not consistent with the distribution for a fair die. There is not enough evidence to support that the die is not fair.\n\n\n\n12.2.3 Homework for Chi-Square Goodness of Fit Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nAccording to the M&M candy company, the expected proportion can be found in Table 12.9. In addition, the table contains the number of M&M’s of each color that were found in a case of candy (Madison, 2013). At the 5% level, do the observed frequencies support the claim of M&M?\n\n\nMandM &lt;- data.frame(\n  Color = c(\"Blue\", \"Brown\", \"Green\", \"Orange\", \"Red\", \"Yellow\"),\n  Observed = c(78, 87, 87, 76, 85, 87),\n  Expected = c(0.24, 0.13, 0.16, 0.20, 0.13, 0.14)\n)\nknitr::kable(MandM)\n\n\n\nTable 12.9: M&M Observed and Expected\n\n\n\n\n\n\nColor\nObserved\nExpected\n\n\n\n\nBlue\n78\n0.24\n\n\nBrown\n87\n0.13\n\n\nGreen\n87\n0.16\n\n\nOrange\n76\n0.20\n\n\nRed\n85\n0.13\n\n\nYellow\n87\n0.14\n\n\n\n\n\n\n\n\n\nEyeglassomatic manufactures eyeglasses for different retailers. The data frame is in Table 3.4. They test to see how many defective lenses they made the time period of January 1 to March 31. Table 12.10 gives the defect and the number of defects.\nCode book for Data Frame Defects below Table 3.4.\n\n\nDefect_table &lt;- tally(~type, data=Defects)\nknitr::kable(Defect_table)\n\n\n\nTable 12.10: Number of Defective Lenses\n\n\n\n\n\n\ntype\nFreq\n\n\n\n\naxis\n1838\n\n\nbig\n1105\n\n\nchamfer\n1596\n\n\ncracks\n1546\n\n\nflaked\n1992\n\n\nheight\n1130\n\n\nintern\n976\n\n\nlost\n976\n\n\npd\n1398\n\n\nscratch\n5865\n\n\nshape\n1485\n\n\nsmall\n4613\n\n\nspot\n1371\n\n\n\n\n\n\n\n\nDo the data support the notion that each defect type occurs in the same proportion? Test at the 5% level.\n\nOn occasion, medical studies need to model the proportion of the population that has a disease and compare that to observed frequencies of the disease actually occurring. Suppose the end-stage renal failure in south-west Wales was collected for different age groups. Do the data in Table 12.11 show that the observed frequencies are in agreement with proportion of people in each age group (Boyle, Flowerdew & Williams, 1997)? Test at the 1% level.\nTable Renal Failure Frequencies\n\nRenal &lt;- data.frame(\n  Age_group = c(\"16-29\", \"30-44\", \"45-59\", \"60-75\", \"75+\"),\n  Observed = c(32, 66, 132, 218, 91),\n  Expected = c(0.23, 0.25, 0.22, 0.21, 0.09)\n)\nknitr::kable(Renal)\n\n\n\nTable 12.11: Renal Failure Frequencies\n\n\n\n\n\n\nAge_group\nObserved\nExpected\n\n\n\n\n16-29\n32\n0.23\n\n\n30-44\n66\n0.25\n\n\n45-59\n132\n0.22\n\n\n60-75\n218\n0.21\n\n\n75+\n91\n0.09\n\n\n\n\n\n\n\n\nIn Africa in 2011, the number of deaths of a female from cardiovascular disease for different age groups are in Table 12.12 (“Global health observatory,” 2013). In addition, the proportion of deaths of females from all causes for the same age groups are also in table Deaths of Females for Different Age Groups. Do the data show that the death from cardiovascular disease are in the same proportion as all deaths for the different age groups? Test at the 5% level.\n\nDeaths &lt;- data.frame(\n  Age = c(\"5-14\", \"14-29\", \"30-49\", \"50-69\"),\n  Observed = c(8, 16, 56, 433),\n  Expected = c(0.10, 0.12, 0.226, 0.52)\n)\nknitr::kable(Deaths)\n\n\n\nTable 12.12: Deaths of Females for Different Age Groups\n\n\n\n\n\n\nAge\nObserved\nExpected\n\n\n\n\n5-14\n8\n0.100\n\n\n14-29\n16\n0.120\n\n\n30-49\n56\n0.226\n\n\n50-69\n433\n0.520\n\n\n\n\n\n\n\n\n\n\n\nIn Australia in 1995, there was a question of whether indigenous people are more likely to die in prison than non-indigenous people. To figure out, the data in Table 12.13 was collected. (“Aboriginal deaths in,” 2013). Do the data show that indigenous people die in the same proportion as non-indigenous people? Test at the 1% level.\n\nPrisoners &lt;- data.frame(\n  Died = c(\"Yes\", \"No\"),\n  Observed = c(17, 3890),\n  Expected = c(0.003, 0.997))\nknitr::kable(Prisoners)\n\n\n\nTable 12.13: Death of Indigenous Prisoners\n\n\n\n\n\n\nDied\nObserved\nExpected\n\n\n\n\nYes\n17\n0.003\n\n\nNo\n3890\n0.997\n\n\n\n\n\n\n\n\nA project conducted by the Australian Federal Office of Road Safety asked people many questions about their cars. One question was the reason that a person chooses a given car, and that data is in Table 12.14 (“Car preferences,” 2013).\n\n\nCar_table &lt;- tally(~Reason, data=Car_pref)\nknitr::kable(Car_table)\n\n\n\nTable 12.14: Reason for Choosing a Car\n\n\n\n\n\n\nReason\nFreq\n\n\n\n\ncomfort\n47\n\n\ncost\n46\n\n\nlooks\n27\n\n\nperformance\n34\n\n\nreliability\n62\n\n\nsafety\n84\n\n\n\n\n\n\n\n\nDo the data show that the frequencies observed substantiate the claim that the reasons for choosing a car are equally likely? Test at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chi-squared and ANOVA Tests</span>"
    ]
  },
  {
    "objectID": "Chi-Square and ANOVA Tests.html#analysis-of-variance-anova",
    "href": "Chi-Square and ANOVA Tests.html#analysis-of-variance-anova",
    "title": "12  Chi-squared and ANOVA Tests",
    "section": "12.3 Analysis of Variance (ANOVA)",
    "text": "12.3 Analysis of Variance (ANOVA)\nThere are times where you want to compare three or more population means. One idea is to just test different combinations of two means. The problem with that is that your chance for a type I error increases. Instead you need a process for analyzing all of them at the same time. This process is known as analysis of variance (ANOVA). The test statistic for the ANOVA is fairly complicated, you will want to use technology to find the test statistic and p-value. The test statistic is distributed as an F-distribution, which is skewed right and depends on degrees of freedom. Since you will use technology to find these, the distribution and the test statistic will not be presented. Remember, all hypothesis tests are the same process. Note that to obtain a statistically significant result there need only be a difference between any two of the \\(k\\) means.\nBefore conducting the hypothesis test, it is helpful to look at the means and standard deviations for each data set. If the sample means with consideration of the sample standard deviations are different, it may mean that some of the population means are different. However, do realize that if they are different, it doesn’t provide enough evidence to show the population means are different. Calculating the sample statistics just gives you an idea that conducting the hypothesis test is a good idea.\n\n12.3.1 Hypothesis test using ANOVA to compare \\(k\\) means\n\nState the random variables and the parameters in words\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) all the means are the same\n\\(H_a:\\) at least two of the means are different\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\na. State: A random sample of size is taken from each population. Check: discuss how the samples were taken.\n\nState: All the samples are independent of each other. Check: Discuss how they are all independent.\nState: Each population is normally distributed. Check: Create density plots and normal quantile plot of each sample. Note: the ANOVA test is fairly robust to the condition especially if the sample sizes are fairly close to each other. Unless the populations are really not normally distributed and the sample sizes are close to each other, then this is a loose condition.\nState: The population variances are all equal. Check: See if the sample variances are close to each other. If the sample sizes are close to each other, then this is a loose condition.\n\n4. Find the test statistic and p-value\nThe test statistic is \\(F\\). To find the test statistic, use technology such r Studio.\nThe test statistic, \\(F\\), is distributed as an F-distribution, where both degrees of freedom are needed in this distribution. The p-value is also calculated r Studio.\n\nConclusion\n\nThis is where you write reject \\(H_O\\) or fail to reject \\(H_O\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\nIf you do in fact reject \\(H_o\\), then you know that at least two of the means are different. The next question you might ask is which are different? You can look at the sample means, but realize that these only give a preliminary result. To actually determine which means are different, you need to conduct other tests. Some of these tests are the range test, multiple comparison tests, Duncan test, Student-Newman-Keuls test, Tukey test, Scheffé test, Dunnett test, least significant different test, and the Bonferroni test. There is no consensus on which test to use.\n\n\n12.3.2 Example: Hypothesis Test Involving Several Means\nCancer is a terrible disease. Surviving may depend on the type of cancer the person has. To see if the mean survival time for several types of cancer are different, data was collected on the survival time in days of patients with one of these cancer in advanced stage. The data is in Table 12.15 (“Cancer survival story,” 2013). (Please realize that this data is from 1978. There have been many advances in cancer treatment, so do not use this data as an indication of survival rates from these cancers.) Do the data indicate that at least two of the mean survival time for these types of cancer are not all equal? Test at the 1% level.\n\nCancer &lt;- read.csv( \"https://krkozak.github.io/MAT160/cancer.csv\") \nknitr::kable(head(Cancer))\n\n\n\nTable 12.15: Survival Times in Days of Five Cancer Types\n\n\n\n\n\n\nsurvival\norgan\n\n\n\n\n124\nStomach\n\n\n42\nStomach\n\n\n25\nStomach\n\n\n45\nStomach\n\n\n412\nStomach\n\n\n51\nStomach\n\n\n\n\n\n\n\n\nCode book for data frame Cancer\nDescription Survival time for several types of cancer was collected.\nThis data frame contains the following columns:\nsurvival: survival times (months)\norgan: the organ that the cancer is in\nSource Cancer survival story. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/CancerSurvival.html&gt;\nReferences &lt;http://lib.stat.cmu.edu/DASL&gt;\n\n12.3.2.1 Solution\n\nState the random variables and the parameters in words\n\n\\(x_1:\\) survival time of patient with Stomach cancer\n\\(x_2:\\) survival time of patient with Bronchus (lung) cancer\n\\(x_3:\\) survival time of patient with Colon cancer\n\\(x_4:\\) survival time of patient with Ovarian cancer\n\\(x_5:\\) survival time of patient with Breast cancer\n\\(\\mu_1:\\) mean survival time of patient with Stomach cancer\n\\(\\mu_2:\\) mean survival time of patient with Bronchus (lung) cancer\n\\(\\mu_3:\\) mean survival time of patient with Colon cancer\n\\(\\mu_4:\\) mean survival time of patient with Ovarian cancer\n\\(\\mu_5:\\) mean survival time of patient with Brest cancer\nNow before conducting the hypothesis test, look at the means and standard deviations. There appears to be a difference between at least two of the means, but realize that the standard deviations are very different. The difference you see may not be significant.\nNotice the sample sizes are not the same.\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) all the means are equal\n\\(H_a:\\) some of the means are different\n\\(\\alpha=0.01\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of 13 survival times from stomach cancer was taken. A random sample of 17 survival times from bronchus cancer was taken. A random sample of 17 survival times from colon cancer was taken. A random sample of 6 survival times from ovarian cancer was taken. A random sample of 11 survival times from breast cancer was taken.\nCheck: These statements may not be true. This information was not shared as to whether the samples were random or not but it may be safe to assume that.\nState: The samples are all independent.\nCheck: Since the individuals have different cancers, then the samples are independent.\nState: Population of all survival times from stomach cancer is normally distributed. Population of all survival times from bronchus cancer is normally distributed. Population of all survival times from colon cancer is normally distributed. Population of all survival times from ovarian cancer is normally distributed. Population of all survival times from breast cancer is normally distributed.\nCheck: Looking at the density plots and normal quantile plots for each sample, it appears that none of the populations are normally distributed. The sample sizes are somewhat different for the problem. This condition may not be true.\n(ref:cancer-density–graphs-cap) Density Plot of Survival Times for Different Cancers\n\ngf_density(~survival|organ, data=Cancer, title=\"Survival times for Different Cancers\", xlab = \"Survival Times\")\n\n\n\n\n\n\n\nFigure 12.1: Density Plot of Survival Times for Different Cancers\n\n\n\n\n\n\ngf_qq(~survival|organ, data=Cancer, title=\"Survival times for Different Cancers\")\n\n\n\n\n\n\n\nFigure 12.2: Quantile Plot of Survival Times for Different Cancers\n\n\n\n\n\nState: The population variances are all equal.\nCheck: The sample standard deviations are approximately 346.3, 209.9, 427.2, 1098.6, and 1239.0 respectively. This condition does not appear to be met, since the sample standard deviations are very different. The sample sizes are somewhat different for the problem. This condition may not be true.\n\n4. Find the test statistic and p-value\nTo find the test statistic and p-value on r Studio, the commands would be:\n\nresults=aov(survival~organ, data=Cancer) \nsummary(results)\n\n            Df   Sum Sq Mean Sq F value   Pr(&gt;F)    \norgan        4 11535761 2883940   6.433 0.000229 ***\nResiduals   59 26448144  448274                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe test statistic is F = 6.433 and the p-value = 0.000229.\n\nConclusion\n\nReject \\(H_o:\\) since the p-value is less than 0.01.\n\nInterpretation\n\nThere is enough evidence to support that at least two of the mean survival times from different cancers are not equal.\n\ndf_stats(survival~organ, data=Cancer, mean)\n\n  response    organ      mean\n1 survival   Breast 1395.9091\n2 survival Bronchus  211.5882\n3 survival    Colon  457.4118\n4 survival    Ovary  884.3333\n5 survival  Stomach  286.0000\n\n\nBy examination of the means, it appears that the mean survival time for breast cancer is different from the mean survival times for both stomach and bronchus cancers. It may also be different for the mean survival time for colon cancer. The others may not be different enough to actually say for sure.\n\n\n\n12.3.3 Homework for Analysis of Variance (ANOVA) Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nCuckoo birds are in the habit of laying their eggs in other birds’ nest. The other birds adopt and hatch the eggs. The lengths (in cm) of cuckoo birds’ eggs in the other species nests were measured and are in Table 12.16 (“Cuckoo eggs in,” 2013). Do the data show that the mean length of cuckoo bird’s eggs is not all the same when put into different nests? Test at the 5% level.\n\n\nEggs &lt;- read.csv( \"https://krkozak.github.io/MAT160/Birds_eggs.csv\") \nknitr::kable(head(Eggs))\n\n\n\nTable 12.16: Lengths of Cuckoo Bird Eggs in Different Species Nest\n\n\n\n\n\n\nlength\nbird\n\n\n\n\n19.65\nMeadow\n\n\n20.05\nMeadow\n\n\n20.65\nMeadow\n\n\n20.85\nMeadow\n\n\n21.65\nMeadow\n\n\n21.65\nMeadow\n\n\n\n\n\n\n\n\nCode book for data frame Eggs\nDescription Cuckoo birds are in the habit of laying their eggs in other birds’ nest. The other birds adopt and hatch the eggs. The lengths (in cm) of cuckoo birds’ eggs in the other species nests were measured\nThis data frame contains the following columns:\nlength: length of cuckoo bird’s eggs in other species nets (cm)\nbird: bids where eggs were found in their nests. The birds are Meadow Pipit, Tree Pipit, Hedge Sparrow, Robin, Pied Wagtail, and Wren\nSource Cuckoo eggs in nest of other birds. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/cuckoo.html&gt;\nReferences SOCR Home page: &lt;http://www.socr.ucla.edu&gt;\n\nLevi-Strauss Co manufactures clothing. The quality control department measures weekly values of different suppliers for the percentage difference of waste between the layout on the computer and the actual waste when the clothing is made (called run-up). The data is in Table 12.17 (“Waste run up,” 2013). Do the data show that there is a difference between some of the suppliers? Test at the 1% level.\n\n\nLevi &lt;- read.csv( \"https://krkozak.github.io/MAT160/Levi_jeans.csv\") \nknitr::kable(head(Levi))\n\n\n\nTable 12.17: Run-ups for Different Plants Making Levi Strauss Clothing\n\n\n\n\n\n\nrun_up\nplant\n\n\n\n\n1.2\nPlant_1\n\n\n10.1\nPlant_1\n\n\n-2.0\nPlant_1\n\n\n1.5\nPlant_1\n\n\n-3.0\nPlant_1\n\n\n-0.7\nPlant_1\n\n\n\n\n\n\n\n\nCode book for data frame Levi\nDescription Levi-Strauss Co manufactures clothing. The quality control department measures weekly values of different suppliers for the percentage difference of waste between the layout on the computer and the actual waste when the clothing is made (called run-up).\nThis data frame contains the following columns:\nrun_up: percentage difference of waste between the layout on the computer and the actual waste when the clothing is made. There are some negative values because sometimes the supplier is able to layout the pattern better than the computer\nplant: Which suppliers\nSource Waste run up. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/wasterunup.html&gt;\nReferences &lt;http://lib.stat.cmu.edu/DASL&gt;\n\nSeveral magazines were grouped into three categories based on what level of education of their readers the magazines are geared towards: high, medium, or low level. Then random samples of the magazines were selected to determine the number of three-plus-syllable words were in the advertising copy, and the data is in Table 12.18 (“Magazine ads readability,” 2013). Is there enough evidence to show that the mean number of three-plus-syllable words in advertising copy is different for at least two of the education levels? Test at the 5% level.\n\n\nAdvertising &lt;- read.csv( \"https://krkozak.github.io/MAT160/three_syllable_words.csv\") \nknitr::kable(head(Advertising))\n\n\n\nTable 12.18: Number of Three Plus Syllable Words in Advertising Copy\n\n\n\n\n\n\nnumber\neducation\n\n\n\n\n34\nHigh\n\n\n21\nHigh\n\n\n37\nHigh\n\n\n31\nHigh\n\n\n10\nHigh\n\n\n24\nHigh\n\n\n\n\n\n\n\n\nCode book for data frame Advertising\nDescription Several magazines were grouped into three categories based on what level of education of their readers the magazines are geared towards: high, medium, or low level. Then random samples of the magazines were selected to determine the number of three-plus-syllable words were in the advertising copy\nThis data frame contains the following columns:\nnumber: number of three=plus-syllable words in advertising copy\neducation: level of education the magazine is geared towards: high, medium, or low\nSource Magazine ads readability. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/magadsdat.html&gt;\nReferences &lt;http://lib.stat.cmu.edu/DASL&gt;\n\nA study was undertaken to see how accurate food labeling for calories on food that is considered reduced calorie. The group measured the amount of calories for each item of food and then found the percent difference between measured and labeled food. The group also looked at food that was nationally advertised, regionally distributed, or locally prepared. The data is in Table 12.19 (“Calories datafile,” 2013). Do the data indicate that at least two of the mean percent differences between the three groups are different? Test at the 5% level.\n\n\nFood &lt;- read.csv( \"https://krkozak.github.io/MAT160/Food_calories_percent_diff.csv\") \nknitr::kable(head(Food))\n\n\n\nTable 12.19: Percent Differences Between Measured and Labeled Food\n\n\n\n\n\n\npercent_diff\nfood\n\n\n\n\n2\nnational\n\n\n-28\nnational\n\n\n-6\nnational\n\n\n8\nnational\n\n\n6\nnational\n\n\n-1\nnational\n\n\n\n\n\n\n\n\nCode book for data frame Food\nDescription A study was undertaken to see how accurate food labeling for calories on food that is considered reduced calorie. The group measured the amount of calories for each item of food and then found the percent difference between measured and labeled food. The group also looked at food that was nationally advertised, regionally distributed, or locally prepared.\nThis data frame contains the following columns:\npercent_diff: percent difference between the number of calories that are measured in the food and the amount that is labeled on the food.\nfood: Where the food is created: nationally advertised, regionally distributed, or locally prepared.\nSource Calories datafile. (2013, December 07). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/Calories.html&gt;\nReferences &lt;http://lib.stat.cmu.edu/DASL&gt;\n\nThe amount of sodium (in mg) in different types of hot dogs is in Table 12.19 (“Hot dogs story,” 2013). Is there sufficient evidence to show that the mean amount of sodium in the types of hot dogs are not all equal? Test at the 5% level.\n\nCode book for data frame Food is below Table 12.19.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Chi-squared and ANOVA Tests</span>"
    ]
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "13  References",
    "section": "",
    "text": "13.1 Data Sources:\n(n.d.). Retrieved July 18, 2019, from https://www.idvbook.com/teaching-aid/data-sets/the-breakfast-cereal-data-set/ The Best Kids’ Cereal. (n.d.). Retrieved July 18, 2019, from https://www.ranker.com/list/best-kids-cereal/ranker-food\n(n.d.). Retrieved from https://www3.nd.edu/~busiforc/handouts/Data and Stories/t test/Friday The Thirteenth/Friday The Thirteenth Data.html\n(n.d.). Retrieved July 21, 2019, from https://www3.nd.edu/~busiforc/handouts/Data and Stories/regression/us auto mileage/usautomileage.html\nhttp://apps.who.int/gho/athena/data/download.xsl?format=xml&target=GHO/WHOSIS_000001&profile=excel&filter=COUNTRY:;SEX:;REGION:EUR\nAboriginal deaths in custody. (2013, September 26). Retrieved from &lt;http://www.statsci.org/data/oz/custody.html\nActivities of Dolphin Groups. (n.d.). Retrieved July 12, 2019, from http://www.statsci.org/data/general/dolpacti.html\nAdvanced Solutions International, Inc. (n.d.). Retrieved July 16, 2019, from https://www.amstat.org/asa/News/ASA-Calls-Time-on-Statistically-Significant-in-Science-Research.aspx\nAnnual maximums of daily rainfall in Sydney. (2013, September 25). Retrieved from &lt;http://www.statsci.org/data/oz/sydrain.html&gt;\nAP exam scores. (2013, November 20). Retrieved from &lt;http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_030708_APExamScores&gt;\nAppliance life expectancy. (2013, November 8). Retrieved from &lt;http://www.mrappliance.com/expert/life-guide/&gt;\nAustralian Human Rights Commission, (1996). Indigenous deaths in custody 1989 - 1996. Retrieved from website: &lt;http://www.humanrights.gov.au/publications/indigenous-deaths-custody&gt;\nBhat, R., & Kushtagi, P. (2006). A re-look at the duration of human pregnancy. Singapore Med J., 47(12), 1044-8. Retrieved from &lt;http://www.ncbi.nlm.nih.gov/pubmed/17139400&gt;\nBoyle, P., Flowerdew, R., & Williams, A. (1997). Evaluating the goodness of fit in models of sparse medical data: A simulation approach. International Journal of Epidemiology, 26(3), 651-656. Retrieved from &lt;http://ije.oxfordjournals.org/content/26/3/651.full.pdf&gt; html\nBrain2bodyweight. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Brain2BodyWeight\nBuy sushi grade fish online. (2013, November 20). Retrieved from http://www.catalinaop.com/\nCancer survival story. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/CancerSurvival.html&gt;\nCalories datafile. (2013, December 07). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/Calories.html&gt;\nCapital and rental values of Auckland properties. (2013, September 26). Retrieved from http://www.statsci.org/data/oz/rentcap.html\nCar Preferences. (n.d.). Retrieved July 11, 2019, from http://www.statsci.org/data/oz/carprefs.html\nCDC-data and statistics, autism spectrum disorders - ncbdd. (2013, October 21). Retrieved from &lt;http://www.cdc.gov/ncbddd/autism/data.html&gt;\nCDC features - new data on autism spectrum disorders. (2013, November 26). Retrieved from &lt;http://www.cdc.gov/features/countingautism/&gt;\nCenter for Disease Control and Prevention, Prevalence of Autism Spectrum Disorders - Autism and Developmental Disabilities Monitoring Network. (2008). Autism and developmental disabilities monitoring network-2012. Retrieved from website: &lt;http://www.cdc.gov/ncbddd/autism/documents/ADDM-2012-Community-Report.pdf&gt;\nCO2 emissions (metric tons per capita). (n.d.). Retrieved July 18, 2019, from https://data.worldbank.org/indicator/EN.ATM.CO2E.PC\nCollege Board, SAT. (2012). Total group profile report. Retrieved from website: &lt;http://media.collegeboard.com/digitalServices/pdf/research/TotalGroup-2012.pdf&gt;\nConsumer Price Index Data from 1913 to 2019. (2019, June 12). Retrieved July 10, 2019, from https://www.usinflationcalculator.com/inflation/consumer-price-index-and-annual-percent-changes-from-1913-to-2008/\nCPS News Releases. (n.d.). Retrieved July 8, 2019, from https://www.bls.gov/cps/\nCuckoo eggs in nest of other birds. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/cuckoo.html&gt;\nCurrent health expenditure (% of GDP). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/SH.XPD.CHEX.GD.ZS\n11 little-known facts about left-handers. (2013, October 21). Retrieved from &lt;http://www.huffingtonpost.com/2012/10/29/left-handed-facts-lefties_n_2005864.html&gt;\nEducation by age datafile. (2013, December 05). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/Educationbyage.html&gt;\nEncyclopedia Titanica. (2013, November 09). Retrieved from &lt;http://www.encyclopedia-titanica.org/&gt;\nDeaths from firearms. (2013, September 26). Retrieved from http://www.statsci.org/data/oz/firearms.html\nFlanagan, R., Rooney, C., & Griffiths, C. (2005). Fatal poisoning in childhood, england & wales 1968-2000. Forensic Science International, 148:121-129, Retrieved from &lt;http://www.cdc.gov/nchs/data/ice/fatal_poisoning_child.pdf&gt;\nFederal Trade Commission, (2008). Consumer fraud and identity theft complaint data: January-December 2007. Retrieved from website: &lt;http://www.ftc.gov/opa/2008/02/fraud.pdf&gt;\nFertility rate, total (births per woman). (n.d.). Retrieved July 8, 2019, from https://data.worldbank.org/indicator/SP.DYN.TFRT.IN\nFind Out How Many Calories in Beer? (n.d.). Retrieved July 21, 2019, from https://www.beer100.com/beer-calories/\nGettler, L. T., McDade, T. W., Feranil, A. B., & Kuzawa, C. W. (2011). Longitudinal evidence that fatherhood decreases testosterone in human males. The Proceedings of the National Academy of Sciences, PNAS 2011, doi: 10.1073/pnas.1105403108\nGlobal health observatory data respository. (2013, October 09). Retrieved from [http://apps.who.int/gho/athena/data/download.xsl?format=xml&target=GHO/MORT\\\\\\_400&profile=excel&filter=AGEGROUP:YEARS05-14;AGEGROUP:YEARS15-29;AGEGROUP:YEARS30-49;AGEGROUP:YEARS50-69;AGEGROUP:YEARS70](http://apps.who.int/gho/athena/data/download.xsl?format=xml&target=GHO/MORT\\_400&profile=excel&filter=AGEGROUP:YEARS05-14;AGEGROUP:YEARS15-29;AGEGROUP:YEARS30-49;AGEGROUP:YEARS50-69;AGEGROUP:YEARS70){.uri} ;MGHEREG:REG6_AFR;GHECAUSES:\\;SEX:\\\nHealth Insurance Market Place Retrieved from website: http://aspe.hhs.gov/health/reports/2013/marketplacepremiums/ib_premiumslandscape.pdf\nHelmet Sizes for New Zealand Airforce. (n.d.). Retrieved July 20, 2019, from http://www.statsci.org/data/oz/nzhelmet.html\nHo, P. M., Bryson, C. L., & Rumsfeld, J. S. (2009). Medication adherence. Circulation, 119(23), 3028-3035. Retrieved from &lt;http://circ.ahajournals.org/content/119/23/3028&gt;\nHouseholds by age of householder and size of household: 1990 to 2010. (2013, October 19). Retrieved from &lt;http://www.census.gov/compendia/statab/2012/tables/12s0062.pdf&gt;\nJanssen, P. A., Thiessen, P., Klein, M. C., Whitfield, M. F., MacNab, Y. C., & Cullis-Kuhl, S. C. (2007). Standards for the measurement of birth weight, length and head circumference at term in neonates of european, chinese and south asian ancestry. Open Medicine, 1(2), e74-e88. Retrieved from &lt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2802014/&gt;\nJohn Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nKiama blowhole eruptions. (2013, September 25). Retrieved from &lt;http://www.statsci.org/data/oz/kiama.html&gt;\nKozak K (2019). Survey results form surveys collected in statistics class at Coconino Community College.\nKuulasmaa, K., Hense, H., & Tolonen, H. World Health Organization (WHO), WHO Monica Project. (1998). Quality assessment of data on blood pressure in the who monica project (ISSN 2242-1246). Retrieved from WHO MONICA Project e-publications website: &lt;http://www.thl.fi/publications/monica/bp/bpqa.htm&gt;\nLabor force participation rate, female (% of female population ages 15 ) (modeled ILO estimate). (n.d.). Retrieved July 20, 2019, from https://data.worldbank.org/indicator/SL.TLF.CACT.FE.ZS\nLange TL, Royals HE, Connor LL (1993) Influence of water chemistry on mercury concentration in largemouth bass from Florida lakes. Trans Am Fish Soc 122:74-84. Michael K. Saiki, Darell G. Slotton, Thomas W. May, Shaun M. Ayers, and Charles N. Alpers (2000) Summary of Total Mercury Concentrations in Fillets of Selected Sport Fishes Collected during 2000–2003 from Lake Natoma, Sacramento County, California (Raw data is included in appendix), U.S. Geological Survey Data Series 103, 1-21. NISER 081107 ID Data. (n.d.). Retrieved July 18, 2019, from http://wiki.stat.ucla.edu/socr/index.php/NISER_081107_ID_Data\nLawes, C., Hoorn, S., Law, M., & Rodgers, A. (2004). High cholesterol. In M. Ezzati, A. Lopez, A. Rodgers & C. Murray (Eds.), Comparative Quantification of Health Risks (1 ed., Vol. 1, pp. 391-496). Retrieved from &lt;http://www.who.int/publications/cra/chapters/volume1/0391-0496.pdf&gt;\nLee, A. (1994). Data analysis: An introduction based on r. Auckland. Retrieved from &lt;http://www.statsci.org/data/oz/nzrivers.html&gt;\nLife expectancy at birth. (2013, October 14). Retrieved from http://data.worldbank.org/indicator/SP.DYN.LE00.IN\nLife expectancy in southeast Asia. (2013, September 23). Retrieved from &lt;http://apps.who.int/gho/data/node.main.688&gt;\nMadison, J. (2013, October 15). M&M’s color distribution analysis. Retrieved from &lt;http://joshmadison.com/2007/12/02/mms-color-distribution-analysis/&gt;\nMagazine ads readability. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/magadsdat.html&gt;\nM&M’s Color Distribution Analysis. (n.d.). Retrieved July 11, 2019, from https://joshmadison.com/2007/12/02/mms-color-distribution-analysis/\nMaintaining Balance while Concentrating. (n.d.). Retrieved July 19, 2019, from http://www.statsci.org/data/general/balaconc.html\nMLB heightsweights. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights\nOECD economic development. (2013, December 04). Retrieved from http://lib.stat.cmu.edu/DASL/Datafiles/oecdat.html\nOlson, K., & Hanson, J. (1997). Using reiki to manage pain: a preliminary report. Cancer Prev Control, 1(2), 108-13. Retrieved from &lt;http://www.ncbi.nlm.nih.gov/pubmed/9765732&gt;\nOvegard, M., Berndt, K., & Lunneryd, S. (2012). Condition indices of Atlantic cod (gadus morhua) biased by capturing method. ICES Journal of Marine Science, doi: 10.1093/icesjms/fss145\nPopular kids datafile. (2013, December 05). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/PopularKids.html&gt;\nPopulation density (people per sq. km of land area). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/EN.POP.DNST\nPopulation reference bureau. (2013, October 8). Retrieved from &lt;http://www.prb.org/DataFinder/Topic/Rankings.aspx?ind=25&gt;\nPrediction of Height from Metacarpal Bone Length. (n.d.). Retrieved July 9, 2019, from http://www.statsci.org/data/general/stature.html\nPregnant women receiving prenatal care (%). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/SH.STA.ANVC.ZS\nPulse rates before and after exercise. (2013, September 25). Retrieved from &lt;http://www.statsci.org/data/oz/ms212.html&gt;\nReserve Bank of Australia. (2019, May 13). Statistical Tables. Retrieved July 10, 2019, from https://www.rba.gov.au/statistics/tables/\nRyan, B. F., Joiner, B. L., & Ryan, Jr, T. A. (1985). Cholesterol levels after heart attack. Retrieved from &lt;http://www.statsci.org/data/general/cholest.html&gt;\nSanchez, Y. W. (2016, October 20). Poll: Arizona voters still favor legalizing marijuana. Retrieved from https://www.azcentral.com/story/news/politics/elections/2016/10/20/poll-arizona-marijuana-legalization-proposition-205/92417690/\nSchultz, S. T., Klonoff-Cohen, H. S., Wingard, D. L., Askhoomoff, N. A., Macera, C. A., Ji, M., & Bacher, C. (2006). Breastfeeding, infant formula supplementation, and autistic disorder: the results of a parent survey. International Breastfeeding Journal, 1(16), doi: 10.1186/1746-4358-1-16\nSeafood online. (2013, November 20). Retrieved from http://www.allfreshseafood.com/\nSmoking and cancer. (2013, December 04). Retrieved from http://lib.stat.cmu.edu/DASL/Datafiles/cigcancerdat.html\nSOCR 012708 id data hotdogs. (2013, November 13). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_012708_ID_Data_HotDogs\nSOCR Data 2008 World CountriesRankings. (n.d.). Retrieved July 19, 2019, from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_2008_World_CountriesRankings#SOCR_Data_-_Ranking_of_the_top_100_Countries_based_on_Political.2C_Economic.2C_Health.2C_and_Quality-of-Life_Factors\nSOCR data Oct2009 id ni. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Oct2009_ID_NI\nSOCR data nips infantvitK shotdata. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_NIPS_InfantVitK_ShotData\nStaff nurse - RN salary. (2013, November 08). Retrieved from &lt;http://www1.salary.com/Staff-Nurse-RN-salary.html&gt;\nTime between nerve pulses. (2019, July 3). Retrieved from &lt;http://www.statsci.org/data/general/nerve.html&gt;\nTime of passages of play in rugby. (2013, September 25). Retrieved from &lt;http://www.statsci.org/data/oz/rugby.html&gt;\nTuition and Fees, 1998-99 Through 2018-19. (2018, December 31). Retrieved from https://www.chronicle.com/interactives/tuition-and-fees\nU.S. Census Bureau, Current Population Survey, Annual Social and Economic Supplements.\nUS Department of Agriculture, Agricultural Research Service. (2012). What we eat in America. Retrieved from website: &lt;http://www.ars.usda.gov/Services/docs.htm?docid=18349&gt;\nUS Department of Commerce, & Noaa. (2016, November 15). 1950 Oklahoma Tornadoes. Retrieved from https://www.weather.gov/oun/tornadodata-ok-1950\nUV radiation: Burden of disease by country. (2013, September 4). Retrieved from &lt;http://apps.who.int/gho/data/node.main.165?lang=en&gt;\nWaste run up. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/wasterunup.html&gt;\nWhat percentage of people have green eyes?. (2013, October 21). Retrieved from &lt;http://www.ask.com/question/what-percentage-of-people-have-green-eyes&gt;",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "Numerical-description-of-Data.html",
    "href": "Numerical-description-of-Data.html",
    "title": "4  Numerical Description of Data",
    "section": "",
    "text": "4.1 Measures of Center\nChapter 1 discussed what a population, sample, parameter, and statistic are, and how to take different types of samples. Chapter 2 discussed ways to graphically display data. There was also a discussion of important characteristics: center, variations, distribution, outliers, and changing characteristics of the data over time. Distributions and outliers can be answered using graphical means. Finding the center and variation can be done using numerical methods that will be discussed in this chapter. Both graphical and numerical methods are part of a branch of statistics known as descriptive statistics. Later descriptive statistics will be used to make decisions and/or estimate population parameters using methods that are part of the branch called inferential statistics.\nThis section focuses on measures of central tendency. Many times you are asking what to expect on average. Such as when you pick a major, you would probably ask how much you expect to earn in that field. If you are thinking of relocating to a new town, you might ask how much you can expect to pay for housing. If you are planting vegetables in the spring, you might want to know how long it will be until you can harvest. These questions, and many more, can be answered by knowing the center of the data set. There are three measures of the “center” of the data. They are the mode, median, and mean. Any of the values can be referred to as the “average.”\nThe mode is the data value that occurs the most frequently in the data. To find it, you count how often each data value occurs, and then determine which data value occurs most often. The mode is not the most useful measure of center. This is because, a data set can have more than one mode. If there is a tie between two values for the most number of times then both values are the mode and the data is called bimodal (two modes). If every data point occurs the same number of times, there is no mode. If there are more than two numbers that appear the most times, then usually there is no mode.\nThe median is the data value in the middle of a sorted list of data. To find it, you put the data in order, and then determine which data value is in the middle of the data set.\nThe mean is the arithmetic average of the numbers. This is the center that most people call the average, though all three – mean, median, and mode – really are averages.\nThere are no symbols for the mode and the median, but the mean is used a great deal, and statisticians gave it a symbol. There are actually two symbols, one for the population parameter and one for the sample statistic. In most cases you cannot find the population parameter, so you use the sample statistic to estimate the population parameter.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical-description-of-Data.html#measures-of-center",
    "href": "Numerical-description-of-Data.html#measures-of-center",
    "title": "4  Numerical Description of Data",
    "section": "",
    "text": "4.1.1 Population Mean\n\\(\\mu=\\frac{\\sum{x}}{N}\\), pronounced mu\nN is the size of the population.\nx represents a data value.\n\\(\\sum{x}\\) means to add up all of the data values.\n\n\n4.1.2 Sample Mean\n\\(\\bar{x}=\\frac{\\sum{x}}{n}\\), pronounced x bar.\nn is the size of the sample.\nx represents a data value.\n\\(\\sum{x}\\) means to add up all of the data values.\nThe value for \\(\\bar{x}\\) is used to estimate \\(\\mu\\) since \\(\\mu\\) can’t be calculated in most situations.\n\n\n4.1.3 Example: Finding the Mean and Median using Rguroo\nSuppose a vet wants to find the average weight of cats. The weights (in kg) of cats are in Table 4.1.\n\n\n\n\nTable 4.1: Head of Cats\n\n\n\n\n\n\nSex\nBwt\nHwt\n\n\n\n\nF\n2.0\n7.0\n\n\nF\n2.0\n7.4\n\n\nF\n2.0\n9.5\n\n\nF\n2.1\n7.2\n\n\nF\n2.1\n7.3\n\n\nF\n2.1\n7.6\n\n\n\n\n\n\n\n\n\nClick to expand the box below to see instructions to import and view the cats dataset in your Data Toolbox in Rguroo.\n\n\n\n\n\n\n Importing cats to Data Toolbox\n\n\n\n\n\n\nGo to the Data toolbox.\nFrom the Data Import dropdown, select Dataset Repository.\nIn the top search box, type MASS, then select the MASS repository.\nIn the middle search box, type cats, and select the cats dataset that appears in the lower panel.\n\nClick the Import button. The dataset will be imported into your Rguroo account.\nClick the Close button to close the Rguroo dialog.\nTo view the dataset, double-click cats under the Data toolbox list.\n\n\n\n\nFind the mean and median of the weight of a cat.\n\n4.1.3.1 Solution\nBefore starting any mathematics problem, it is always a good idea to define the unknown in the problem. In statistics, you want to define the variable. The symbol for the variable is *x*.\nThe variable is x = weight of a cat\nClick to expand the box below see how to calculate the mean and median in Rguroo.\n\n\n\n\n\n\n Calculating the Mean and Median Weight of Cats\n\n\n\n\n\nBefore you begin: Make sure you have already imported the cats dataset into your Data Toolbox, as was shown here.\n\nOpen the Data toolbox.\n\nClick on the Functions dropdown, and select Summary Statistic. This opens the Basic Summary Statistic dialog.\n\nFrom the Dataset dropdown, select the cats dataset.\n\nFrom the Numerical dropdown, select the Bwt variable.\n\nIn the Statistics section of the dialog, select the checkbox for Mean and Median.\n\nClick the preview icon  to see the summary statistics.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nBasic Summary Statistic dialog in Rguroo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.1: Mean and Median Weight of Cats\n\n\n\n\n\nThe mean weight is 2.72 kg.\n\nThe median weight is 2.7 kg also. It appears the average weight is 2.7 kg of all cats.\n\n\n\n4.1.4 Example: Finding Mean and Median with Factor\nLooking at the data frame for cats weights Table 4.1 you see that there are several variables You may want to know what the other variables are. A Code Book describes the data set, explains what the variables are including the units, and the source of the data frame. The code book for the cats is below.\nImage 3.1.1: Code book for cats data frame\n\n\n\nCode book for cats data frame\n\n\nSuppose you want to know if male cats weigh more than female cats. Looking at the variables, you notice that there is a variable for the sex of the cat. You can look at the weights of males and females separately. This looks like:\n\n4.1.4.1 Solution\nClick to expand the box below see how to calculate the mean and median, separated by sex, in Rguroo.\n\n\n\n\n\n\n Calculating the Mean and Median Weight of Cats by Sex\n\n\n\n\n\nBefore you begin: Make sure you have already imported the cats dataset into your Data Toolbox, as was shown here.\n\nOpen the Data toolbox.\n\nClick on the Functions dropdown, and select Summary Statistic. This opens the Basic Summary Statistic dialog.\n\nFrom the Dataset dropdown, select the cats dataset.\n\nFrom the Numerical dropdown, select the Bwt variable.\nFrom the Factor 1 dropdown, select the Sex factor.\nIn the Statistics section of the dialog, select the checkbox for Mean and Median.\n\nClick the preview icon  to see the summary statistics.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nBasic Summary Statistic dialog in Rguroo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.2: Mean and Median Weight of Cats by Sex\n\n\n\n\n\nNotice that the female cats’ mean weight is 2.4 kg and the male cats’ mean weight is 2.9 kg. The median weight of female cats is 2.3 kg and for males it is 2.9 kg. So it does appear that male cats weigh a bit more than the female cats.\nThere are many different summary statistics that can be found. An example is the minimum and maximum value. In this example, you will see how to find the min and max values and then filter them out of a data set to see what effect they have on the mean and median.\n\n\n\n4.1.5 Example: Effect of Extreme Values on Mean and Median\nFind the minimum and maximum values of cats weights.\n\n4.1.5.1 Solution\nThe steps to find the minimum and the maximum in Rguroo are similar to finding the mean and median. In the Statistics section of the dialog, select the checkbox for Minimum and Maximum.\nHere is the Basic Summary Statistic Dialog and Output from Rguroo for the minimum and maximum of cats body weight.\n\n\n\n\n\n\n\n\n\n\n\n(a) Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 4.3: Maximum and Minimum Weight of Cats\n\n\n\n\nThe minimum weight of a cat in this data frame is 2 kg and the maximum weight of a cat is 3.9 kg.\nYou can create two new data sets in Rguroo by subsetting (filtering). One data set will exclude the maximum value. Click to expand the box below to see how to subset a dataset in Rguroo:\n\n\n\n\n\n\n Subsetting the cats Dataset\n\n\n\n\n\nBefore you begin: Make sure you have already imported the cats dataset into your Data Toolbox, as was shown here.\n\nOpen the Data toolbox.\n\nClick on the Functions dropdown, and select Subset. This opens the Data Subset dialog.\n\nFrom the Dataset dropdown, select the cats dataset.\n\nSelect the Logical Expression button.\n\nTo create a new Logical Expression, click the green plus icon.\nFrom the Variable dropdown, select the Bwt. From the Op. (operations) dropwdown, select the less than symbol (&lt;). In the Value column, enter 3.9 (the maximum body weight of cats dataset). Click the Done.\nWhen you are done with your selection, click the preview icon  to see a preview of the scatterplot.\nIn Save As… name the new data set cats_nomax and click the Save As….\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n  \n\n\n\n\n\n\nThen create a data set that excludes the minimum value using the same steps shown here. In the Logical Expression dialog, choose the greater than (&gt;) operator in the Op. dropdown and enter 2 in the Value column. Save this new dataset as cats_nomin.\n\nNow you can find the mean and median of each new data set using the steps described here :\nThe mean without the maximum value is 2.70 kg, and the median is 2.7 kg.\nThe mean without the minimum value is 2.75 kg, and the median is 2.7 kg.\nFrom Example: Effect of Extreme Values on Mean and Median, the mean of the cats data set with all the values is 2.72 kg where the median is 2.7 kg. Notice that when the maximum value was excluded from the data set, the mean decreased a little but the median didn’t change, and when the minimum value was excluded from the data set, the mean increased a little but the median didn’t change. The mean is much higher than the median. Why is this? This is because the mean is affected by extreme values, while the median is not. We say the median is a resistant measure of center because it isn’t affected by extreme values as much.\nAn outlier is a data value that is very different from the rest of the data. It can be really high or really low. Extreme values may be an outlier if the extreme value is far enough from the center. If there are extreme values in the data, the median is a better measure of the center than the mean. If there are no extreme values, the mean and the median will be similar so most people use the mean. The mean is not a resistant measure because it is affected by extreme values. The median is a resistant measure because it not affected by extreme values.\nAs a consumer you need to be aware that people choose the measure of center that best supports their claim. When you read an article in the newspaper and it talks about the “average” it usually means the mean but sometimes it refers to the median. Some articles will use the word “median” instead of “average” to be more specific. If you need to make an important decision and the information says “average”, it would be wise to ask if the “average” is the mean or the median before you decide.\nAs an example, suppose that a company wants to use the mean salary as the average salary for the company. This is because the high salaries of the administrators will pull the mean higher. The company can say that the employees are paid well because the average is high. However, the employees want to use the median since it discounts the extreme values of the administration and will give a lower value of the average. This will make the salaries seem lower and that a raise is in order.\nWhy use the mean instead of the median? The reason is because when multiple samples are taken from the same population, the sample means tend to be more consistent than other measures of the center.\nTo understand how the different measures of center relate to skewed or symmetric distributions, see Figure 4.4. As you can see sometimes the mean is smaller than the median, sometimes the mean is larger than the median, and sometimes they are the same values.\nFigure 4.4: Mean, Median, Mode as Related to a Distribution\n\n\n\n\n\n\n\n\nFigure 4.4: Mean, median, mode as related to distribution\n\n\n\n\n\nOne last type of average is a weighted average. Weighted averages are used quite often in different situations. Some teachers use them in calculating a student’s grade in the course, or a grade on a project. Some employers use them in employee evaluations. The idea is that some activities are more important than others. As an example, a full time teacher at a community college may be evaluated on their service to the college, their service to the community, whether their paperwork is turned in on time, and their teaching. However, teaching is much more important than whether their paperwork is turned in on time. When the evaluation is completed, more weight needs to be given to the teaching and less to the paperwork. This is a weighted average.\n\n\n\n4.1.6 Weighted Average\n\\(\\text{weighted average}=\\frac{\\sum{x*w}}{\\sum{w}}\\)\nwhere w is the weight of the data value, x.\n\n\n4.1.7 Example: Weighted Average\nIn your biology class, your final grade is based on several things: a lab score, scores on two major tests, and your score on the final exam. There are 100 points available for each score. The lab score is worth 15% of the course, the two exams are worth 25% of the course each, and the final exam is worth 35% of the course. Suppose you earned scores of 95 on the labs, 83 and 76 on the two exams, and 84 on the final exam. Compute your weighted average for the course.\n\n4.1.7.1 Solution\nVariable: x = score\nA weighted average can be found using Rguroo. First, we will create a new dataset in Rguroo. Click to expand the box below to see how to create a new dataset in Rguroo:\n\n\n\n\n\n\n Creating a New Dataset\n\n\n\n\n\n\nOpen the Data toolbox.\n\nClick on the Data Import dropdown, and select Create New Dataset. This opens the Create New Dataset dialog. Specify the number of rows and columns for your new dataset.\n\nThe default variable names will be Var1, Var2, etc. To change the variable name, move your cursor over the variable name on the header. You will see the Variable Context Menu icon. Click on the icon to open the variable context menu. From the menu select the option Rename. This will bring up the Rename Variable dialog box. In the dialog box, type in the new name for your variable, and press enter. The new name will now appear on the header. Note that variable names cannot have spaces.\n\nEnter your data.\n\nEnter a name for your dataset in the Save As textbox on the top, and click the Save as button to save your dataset.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog and new dataset\n\n\n\n\n\n \n\n\n\n\n\n\n\nTo calculate the weighted average in Rguroo, you will follow the same steps described here. In the Basic Summary Statistics dialog box select the biology_grade from the dataset dropdown, select score in the Numerical dropdown, and select weight in the frequency dropdown as shown in figure Figure 4.5.\n\n\n\n\n\n\n\n\n\n\n\n(a) Basic Summary Statistics Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 4.5: Weighted Mean of Biology Grade\n\n\n\n\nYour weighted mean in the biology class is 83.4%. Using the traditional grading scale, you have a B in the class.\n\n\n\n4.1.8 Example: Weighted Average\nThe faculty evaluation process at John Jingle University rates a faculty member on the following activities: teaching, publishing, committee service, community service, and submitting paperwork in a timely manner. The process involves reviewing student evaluations, peer evaluations, and supervisor evaluation for each teacher and awarding him/her a score on a scale from 1 to 10 (with 10 being the best). The weights for each activity are 20 for teaching, 18 for publishing, 6 for committee service, 4 for community service, and 2 for paperwork.\n\nOne faculty member had the following ratings: 8 for teaching, 9 for publishing, 2 for committee work, 1 for community service, and 8 for paperwork. Compute the weighted average of the evaluation.\nAnother faculty member had ratings of 6 for teaching, 8 for publishing, 9 for committee work, 10 for community service, and 10 for paperwork. Compute the weighted average of the evaluation.\nWhich faculty member had the higher average evaluation?\n\n\n4.1.8.1 Solution\n\nOne faculty member had the following ratings: 8 for teaching, 9 for publishing, 2 for committee work, 1 for community service, and 8 for paperwork. Compute the weighted average of the evaluation.\n\nVariable: x = rating, w = weight\nTo find the weighted average using Rguroo, start by creating a new dataset as described here. The new dataset is shown in the figure Figure 4.6 below.\n\n\n\n\n\n\n\n\nFigure 4.6: Faculty Ratings Dataset\n\n\n\n\n\nCalculate the weighted average in Rguroo using the same steps described here. In the Basic Summary Statistics dialog box select the faculty_ratings from the dataset dropdown, select faculty_a in the Numerical dropdown, and select weight in the frequency dropdown as shown in figure Figure 4.7.\n\n\n\n\n\n\n\n\n\n\n\n(a) Basic Summary Statistics Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 4.7: Weighted Mean of Faculty Ratings\n\n\n\n\nThe weighted average is 7.08.\n\nAnother faculty member had ratings of 6 for teaching, 8 for publishing, 9 for committee work, 10 for community service, and 10 for paperwork. Compute the weighted average of the evaluation.\n\nThe weighted mean can be calculated in the same fasion as part (a). In the Basic Summary Statistics dialog box select the faculty_ratings from the dataset dropdown, select faculty_b in the Numerical dropdown, and select weight in the frequency dropdown as shown in figure\n\n\n\n\n\n\n\n\n\n\n\n(a) Basic Summary Statistics Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 4.8: Weighted Mean of Faculty Ratings\n\n\n\n\nThe weighted average for this employee is 7.56.\n\nWhich faculty member had the higher average evaluation?\nThe second faculty member has a higher average evaluation.\n\nThe last thing to mention is which average is used on which type of data.\nMode can be found on nominal, ordinal, interval, and ratio data, since the mode is just the data value that occurs most often. You are just counting the data values.\nMedian can be found on ordinal, interval, and ratio data, since you need to put the data in order. As long as there is order to the data you can find the median.\nMean can be found on interval and ratio data, since you must have numbers to add together.\n\n\n\n4.1.9 Homework for Measures of Center Section\nUse Rguroo on all problems. State the variable on all problems.\n\nBefore you begin: Make sure you have already imported the cholesterol dataset into your Data Toolbox, as was shown here by selecting the Statistics Using Technology - Kozak as shown in the figure Figure 4.9. Cholesterol levels were collected from patients a certain number of days after they had a heart attack and are in Table 4.2. Find the mean and median for cholesterol levels 2 days after the heart attack.\n\n\n\n\n\n\n\n\n\nFigure 4.9: Repository Search Dialog\n\n\n\n\n\n\n\n\n\nTable 4.2: Head of Cholesterol Levels of Patients After Heart Attack\n\n\n\n\n\n\npatient\nday2\nday4\nday14\n\n\n\n\n1\n270\n218\n156\n\n\n2\n236\n234\nNA\n\n\n3\n210\n214\n242\n\n\n4\n142\n116\nNA\n\n\n5\n280\n200\nNA\n\n\n6\n272\n276\n256\n\n\n\n\n\n\n\n\nCode book for Data Frame Cholesterol\nDescription Cholesterol levels were collected from patients a certain number of days after they had a heart attack\nThis data frame contains the following columns:\nPatient: Patient number\nday2: Cholesterol level of patient 2 days after heart attack. (mg/dL)\nday4: Cholesterol level of patient 4 days after heart attack. (mg/dL)\nday14: Cholesterol level of patient 14 days after heart attack. (mg/dL)\nSource Ryan, B. F., Joiner, B. L., & Ryan, Jr, T. A. (1985). Cholesterol levels after heart attack. Retrieved from here\nReferences Ryan, Joiner & Ryan, Jr, 1985\n\nBefore you begin: Make sure you have already imported the length dataset into your Data Toolbox located in the Statistics Using Technology - Kozak. The lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 4.3 (Lee, 1994). Find the mean and median length of rivers that flow into the Pacific Ocean and the mean and median length of rivers that flow into the Tasman Sea.\n\n\n\n\n\nTable 4.3: Head of Length of New zealand rivers (km)\n\n\n\n\n\n\nriver\nlength\nflowsto\n\n\n\n\nClarence\n209\nPacific\n\n\nConway\n48\nPacific\n\n\nWaiau\n169\nPacific\n\n\nHurunui\n138\nPacific\n\n\nWaipara\n64\nPacific\n\n\nAshley\n97\nPacific\n\n\n\n\n\n\n\n\nCode book for data frame Length\nDescription Rivers in New Zealand, the lengths of river and what body of water the river flows into\nThis data frame contains the following columns:\nRiver: Name of the river\nlength: how long the river is in kilometers\nflowsto: what body of water the river flows into Pacific Ocean is Pacific and the Tasman Sea is Tasman\nSource Lee, A. (1994). Data analysis: An introduction based on r. Auckland. Retrieved from here\nReferences Lee, A. (1994). Data analysis: An introduction based on r. Auckland.\n\nBefore you begin: Make sure you have already imported the pay dataset into your Data Toolbox, located in the Statistics Using Technology - Kozak. Print-O-Matic printing company’s employees have salaries that are contained in Table 4.4.\n\n\n\n\n\nTable 4.4: Head of Salaries of Print-O-Matic Printing Company Employees\n\n\n\n\n\n\nemployee\nsalary\n\n\n\n\nCEO\n272500\n\n\nDriver\n58456\n\n\nCD74\n100702\n\n\nCD65\n57380\n\n\nEmbellisher\n73877\n\n\nFolder\n65270\n\n\n\n\n\n\n\n\nCode book for data frame Pay\nDescription Salaries of Print-O-Matic printing company’s employees\nThis data frame contains the following columns:\nemployee:employees position in the company\nsalary: salary of that employee (Australian dollars (AUD))\nSource John Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nReferences John Matic (2013)\n\nFind the mean and median.\nFind the mean and median with the CEO’s salary removed.\nWhat happened to the mean and median when the CEO’s salary was removed? Why?\nIf you were the CEO, who is answering concerns from the union that employees are underpaid, which average (mean or median) using the complete data set of the complete data set would you prefer? Why?\nIf you were a platen worker, who believes that the employees need a raise, which average (mean or median) using the complete data set would you prefer? Why?\n\n\n\nBefore you begin: Make sure you have already imported the cost dataset into your Data Toolbox, located in the Statistics Using Technology - Kozak. Print-O-Matic printing company spends specific amounts on fixed costs every month. The costs of those fixed costs are in a Table 4.5.\n\n\n\n\n\nTable 4.5: Fixed Costs for Print-O-Matic Printing Company\n\n\n\n\n\n\ncharges\ncost\n\n\n\n\nBank charges\n482\n\n\nCleaning\n2208\n\n\nComputer expensive\n2471\n\n\nLease payments\n2656\n\n\nPostage\n2117\n\n\nUniforms\n2600\n\n\n\n\n\n\n\n\nCode book for data frame Cost\nDescription fixed monthly charges for Print-0-Matic printing company\nThis data frame contains the following columns:\ncharges: Categories of monthly fixed charges\ncost: fixed month costs (AUD)\nSource John Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nReferences John Matic (2013)\n\nFind the mean and median.\nFind the mean and median with the bank charges removed.\nWhat happened to the mean and median when the bank charges was removed? Why?\nIf it is your job to oversee the fixed costs, which average (mean or median) using the complete data set would you prefer to use when submitting a report to administration to show that costs are low? Why?\nIf it is your job to find places in the budget to reduce costs, which average (mean or median) using the complete data set would you prefer to use when submitting a report to administration to show that fixed costs need to be reduced? Why?\n\n\n\nLooking at graph 3.1.2, state if the graph is skewed left, skewed right, or symmetric and then state which is larger, the mean or the median?\n\nGraph 3.1.2: Skewed or Symmetric Graph\n\n\n\nGraph 3.1.2\n\n\n\nLooking at graph 3.1.3, state if the graph is skewed left, skewed right, or symmetric and then state which is larger, the mean or the median?\n\nGraph 3.1.3: Skewed or Symmetric Graph\n\n\n\nGraph 3.1.3\n\n\n\nAn employee at Coconino Community College (CCC) is evaluated based on goal setting and accomplishments toward the goals, job effectiveness, competencies, and CCC core values. Suppose for a specific employee, goal 1 has a weight of 30%, goal 2 has a weight of 20%, job effectiveness has a weight of 25%, competency 1 has a weight of 4%, competency 2 has a weight of 3%, competency 3 has a weight of 3%, competency 4 has a weight of 3%, competency 5 has a weight of 2%, and core values has a weight of 10%. Suppose the employee has scores of 3.0 for goal 1, 3.0 for goal 2, 2.0 for job effectiveness, 3.0 for competency 1, 2.0 for competency 2, 2.0 for competency 3, 3.0 for competency 4, 4.0 for competency 5, and 3.0 for core values. Find the weighted average score for this employee. If an employee has a score less than 2.5, they must have a Performance Enhancement Plan written. Does this employee need a plan?\nAn employee at Coconino Community College (CCC) is evaluated based on goal setting and accomplishments toward goals, job effectiveness, competencies, CCC core values. Suppose for a specific employee, goal 1 has a weight of 20%, goal 2 has a weight of 20%, goal 3 has a weight of 10%, job effectiveness has a weight of 25%, competency 1 has a weight of 4%, competency 2 has a weight of 3%, competency 3 has a weight of 3%, competency 4 has a weight of 5%, and core values has a weight of 10%. Suppose the employee has scores of 2.0 for goal 1, 2.0 for goal 2, 3.0 for goal 3, 2.0 for job effectiveness, 2.0 for competency 1, 3.0 for competency 2, 2.0 for competency 3, 3.0 for competency 4, and 4.0 for core values. Find the weighted average score for this employee. If an employee that has a score less than 2.5, they must have a Performance Enhancement Plan written. Does this employee need a plan?\nA statistics class has the following activities and weights for determining a grade in the course: test 1 worth 15% of the grade, test 2 worth 15% of the grade, test 3 worth 15% of the grade, homework worth 10% of the grade, semester project worth 20% of the grade, and the final exam worth 25% of the grade. If a student receives an 85 on test 1, a 76 on test 2, an 83 on test 3, a 74 on the homework, a 65 on the project, and a 79 on the final, what grade did the student earn in the course?\nA statistics class has the following activities and weights for determining a grade in the course: test 1 worth 15% of the grade, test 2 worth 15% of the grade, test 3 worth 15% of the grade, homework worth 10% of the grade, semester project worth 20% of the grade, and the final exam worth 25% of the grade. If a student receives a 92 on test 1, an 85 on test 2, a 95 on test 3, a 92 on the homework, a 55 on the project, and an 83 on the final, what grade did the student earn in the course?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical-description-of-Data.html#measures-of-spread",
    "href": "Numerical-description-of-Data.html#measures-of-spread",
    "title": "4  Numerical Description of Data",
    "section": "4.2 Measures of Spread",
    "text": "4.2 Measures of Spread\nVariability is an important idea in statistics. If you were to measure the height of everyone in your classroom, every observation gives you a different value. That means not every student has the same height. Thus there is variability in people’s heights. If you were to take a sample of the income level of people in a town, every sample gives you different information. There is variability between samples too. Variability describes how the data are spread out. If the data are very close to each other, then there is low variability. If the data are very spread out, then there is high variability. How do you measure variability? It would be good to have a number that measures it. This section will describe some of the different measures of variability, also known as variation.\nIn [Example: Finding the Mean and Median using Rguroo], the average weight of a cat was calculated to be 2.72 kg. How much does this tell you about the weight of all cats? Can you tell if most of the weights were close to 2.72 kg or were the weights really spread out? The highest weight and the lowest weight are known, but is there more that you can tell? All you know is that the center of the weights is 2.72 kg.\nYou need more information.\nThe range of a set of data is the difference between the highest and the lowest data values (or maximum and minimum values). The interval is the lowest and highest values. The range is one value while the interval is two.\n\n4.2.1 Example: Range\nFrom Example: Effect of Extreme Values on Mean and Median, the maximum is 3.9 kg and the minimum is 2 kg. So the range is \\(3.9-2=1.9 kg\\). But what does that tell you? You don’t know if the weights are really spread out, or if they are close together.\nUnfortunately, range doesn’t really provide a very accurate picture of the variability. A better way to describe how the data is spread out is needed. Instead of looking at the distance the highest value is from the lowest how about looking at the distance each value is from the mean. This distance is called the deviation. You might want to find the average of the deviation. Though the calculation for finding the average deviation is not very straight forward, you end up with a value called the variance. The symbol for the population variance is \\(\\sigma^2\\), and it is the average squared distance from the mean. Statisticians like the variance, but many other people who work with statistics use a descriptive statistic which is the square root of the variance. This gives you the average distance from the mean. This is called the standard deviation, and the population standard deviation is denoted with the letter \\(\\sigma\\).\nThe standard deviation is the average (mean) distance from a data point to the mean. It can be thought of as how much a typical data point differs from the mean.\nThe sample variance formula: \\(s^2=\\frac{\\sum\\left(x-\\bar{x}\\right)^2}{n-1}\\), where \\(\\bar{x}\\) is the sample mean, \\(n\\) is the sample size, and \\(\\sum{}\\) means to find the sum of the values. The \\(n-1\\) on the bottom has to do with a concept called degrees of freedom. Basically, it makes the sample variance a better approximation of the population variance.\nThe sample standard deviation formula: \\(s=\\sqrt{ \\frac{\\sum\\left(x-\\bar{x}\\right)^2}{n-1}}\\).\nThe population variance formula: \\(\\sigma^2 = \\frac{\\sum\\left(x-\\mu \\right)^2}{N}\\), where \\(\\sigma\\) is the Greek letter sigma and \\(\\sigma^2\\) represents the population variance, \\(\\mu\\) is the population mean, and \\(N\\) is the size of the population.\nThe population standard deviation formula: \\(\\sigma =\\sqrt{ \\frac{\\sum\\left(x-\\mu \\right)^2}{N}}\\)\nBoth the sample variance and sample standard deviation can be found using technology. If using Rguroo, you would perform the same steps as described here and select standard deviation as shown in the figure Figure 4.10.\n\n\n\n\n\n\n\n\nFigure 4.10: Finding Standard Deviation Using Basic Summary Statistic\n\n\n\n\n\n\nThe next example will demonstrate this command.\n\n\n4.2.2 Example: Finding the Standard Deviation\nFor the data frame Cats Table 4.1 find the variance and standard derivation for weight of cats. Then find the variance and standard deviation separated by sex of the cat.\n\n4.2.2.1 Solution\nThe variance and standard deviation for all cats is found by performing the command:\n\ndf_stats(~Bwt, data=cats, var, sd) \n\n  response       var        sd\n1      Bwt 0.2355225 0.4853066\n\n\nThe variance for all cats is 0.24 \\(kg^2\\) and the standard deviation is 0.49 kg.\nTo find out the mean, variance, and standard deviation for each sex of the cats, use the command:\n\ndf_stats(Bwt~Sex, data=cats, mean, var, sd) \n\n  response Sex     mean        var        sd\n1      Bwt   F 2.359574 0.07506938 0.2739879\n2      Bwt   M 2.900000 0.21854167 0.4674844\n\n\nYou can see that the mean weight of females cats is 2.36 kg, the variance is 0.075 \\(kg^2\\), and the standard deviation is 0.27 kg. For males cats, the mean is 2.9 kg, the variance is 0.22 \\(kg^2\\), and the standard deviation is 0.47 kg. This means that female cats weigh less than males and since the variance and standard deviations are much less for female cats than males cats, female cats’ weights are more consistent than male cats.\nIn general a “small” variance and standard deviation means the data is close together (more consistent) and a “large” variance and standard deviation means the data is spread out (less consistent). Sometimes you want consistent data and sometimes you don’t. As an example if you are making bolts, you want the lengths to be very consistent so you want a small standard deviation. If you are administering a test to see who can be a pilot, you want a large standard deviation so you can tell who are the good pilots and who are the not so good pilots.\nWhat do “small” and “large” standard deviation mean? To a bicyclist whose average speed is 20 mph, \\(s = 20 mph\\) is huge. To an airplane whose average speed is 500 mph, \\(s = 20 mph\\) is nothing. The “size” of the variation depends on the size of the numbers in the problem and the mean. Another situation where you can determine whether a standard deviation is small or large is when you are comparing two different samples such as in Example: Finding the Standard Deviation. A sample with a smaller standard deviation is more consistent than a sample with a larger standard deviation.\nMany other books and authors stress that there is a computational formula for calculating the standard deviation. However, this formula doesn’t give you an idea of what standard deviation is and what you are doing. It is only good for doing the calculations quickly. It goes back to the days when standard deviations were calculated by hand, and the person needed a quick way to calculate the standard deviation. It is an archaic formula that this author is trying to eradicate. It is not necessary anymore, computers will do the calculations for you with as much meaning as this formula gives. It is suggested that you never use it. If you want to understand what the standard deviation is doing, then you should use the definition formula. If you want an answer quickly, use a computer.\n\n\n\n4.2.3 Use of Standard Deviation\nOne of the uses of the standard deviation is to describe how a population is distributed. This describes where much of the data is for most distributions. A general rule is that about 95% of the data is within 2 standard deviations of the mean. This is not perfect, but is works for many distributions. There are rules like the empirical rule and Chebyshev’s theorem that give you more detailed percentages, but 95% in 2 standard deviations is a very good approximation.\n\n\n4.2.4 Example: the general rule\nThe U.S. Weather Service has provided the information in Table 4.6 about the total monthly/annual number of reported tornadoes in Oklahoma for the years 1950 to 2018. (US Department of Commerce & Noaa, 2016)\n\nTornado&lt;-read.csv(\"https://krkozak.github.io/MAT160/Tornado_OK.csv\") \nknitr::kable(head(Tornado))\n\n\n\nTable 4.6: Monthly/Annual Number of tornadoes in Oklahoma\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\n\n\n1950\n0\n1\n1\n5\n12\n1\n0\n0\n2\n1\n0\n0\n23\n\n\n1951\n0\n2\n0\n11\n11\n11\n4\n2\n1\n1\n0\n0\n43\n\n\n1952\n0\n0\n0\n7\n5\n5\n4\n1\n0\n0\n0\n0\n22\n\n\n1953\n0\n4\n7\n9\n8\n13\n4\n2\n0\n0\n5\n2\n54\n\n\n1954\n0\n0\n7\n13\n19\n4\n4\n2\n3\n1\n0\n0\n53\n\n\n1955\n1\n1\n0\n15\n32\n22\n4\n2\n0\n0\n0\n0\n77\n\n\n\n\n\n\n\n\nCode book for data frame Tornado\nDescription The U.S. Weather Service has collected data on the monthly and annual number of tornadoes in Oklahoma.\nThis data frame contains the following columns:\nYear: Year from 1950-2018\nJan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec: Tornado numbers in each moth of the year\nAnnual: Total number of tornadoes for each year\nSource US Department of Commerce, & Noaa. (2016, November 15). 1950 Oklahoma Tornadoes. Retrieved from https://www.weather.gov/oun/tornadodata-ok-1950\nReferences The data was supplied by The U.S. Weather Service\nFind the general interval that contains about 95% of the data.\n\n4.2.4.1 Solution\nVariable: \\(x\\) = number of annual tornadoes in Oklahoma\nFind the mean and standard deviation:\n\ndf_stats(~Annual, data=Tornado, mean, sd)\n\n  response     mean       sd\n1   Annual 56.02899 27.56061\n\n\nThe mean is \\(\\mu=56\\) tornadoes and the standard deviation is \\(\\sigma=27.6\\) tornadoes. The interval will be \\(\\mu\\pm2*\\sigma=56\\pm2*27.6=(0.8,111.2)\\)\nAbout 95% of the years have between 0.8 or 1 and 111 tornadoes in Oklahoma.\nThe general rule says that about 95% of the data is within two standard deviations of the mean. That percentage is fairly high. There isn’t much data outside two standard deviations. A rule that can be followed is that if a data value is within two standard deviations, then that value is a common data value. If the data value is outside two standard deviations of the mean, either above or below, then the number is uncommon. It could even be called unusual. An easy calculation that you can do to figure it out is to find the difference between the data point and the mean, and then divide that answer by the standard deviation. As a formula this would be\n\\(z=\\frac{x-\\mu}{\\sigma}\\)\nIf you don’t know the population mean, \\(\\mu\\), and the population standard deviation, \\(\\sigma\\), then use the sample mean, \\(\\bar{x}\\), and the sample standard deviation, \\(s\\), to estimate the population parameter values. Realize that using the sample standard deviation may not actually be very accurate.\n\n\n\n4.2.5 Example: Determining If a Value Is Unusual\n\nIn 1974, there were 45 tornadoes in Oklahoma. Is this value unusual? Why or why not?\nIn 1999, there were 145 tornadoes in the Oklahoma. Is this value unusual? Why or why not?\n\n\n4.2.5.1 Solution\n\nIn 1974, there were 45 tornadoes in Oklahoma. Is this value unusual? Why or why not?\nVariable: \\(x\\) = number of tornadoes in Oklahoma\n\nTo answer this question, first find how many standard deviations 45 is from the mean. From 3.2.4 example, we know \\(\\mu=56\\) and \\(\\sigma=27.6\\). For \\(x\\)=45, \\(z=\\frac{45-56}{27.6}=-0.399\\)\nSince this value is between -2 and 2, then it is not unusual to have 45 tornadoes in a year in Oklahoma. The z value is negative, so that means that 45 is less than the mean number of tornadoes.\n\nIn 1999, there were 145 tornadoes in the Oklahoma. Is this value unusual? Why or why not?\nVariable: \\(x\\) = number of tornadoes in Oklahoma\n\nFor this question the \\(x\\) = 145, \\(z=\\frac{145-56}{27.6}=3.22\\)\nSince this value is more than 2, then it is unusual to have only 145 tornadoes in a year in Oklahoma.\n\n\n\n4.2.6 Homework for Measures of Spread Section\nUse Technology on all problems. State the variable on all problems.\n\nCholesterol levels were collected from patients certain days after they had a heart attack and are in Table 4.2. Find the mean, median, range, variance, and standard deviation for cholesterol levels 2 days after the heart attack.\n\nCode book for Data Frame Cholesterol is below Table 4.2.\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 4.3 (Lee, 1994). Find the mean, median, range, variance, and standard deviation of the length of rivers that flow into the Pacific Ocean and the mean, median, range, variance, and standard deviation of the length of rivers that flow into the Tasman Sea. Compare and contrast the length of rivers that flow to the Pacific Ocean versus the ones that flow into the Tasman Sea using both measures of spread and measures of variability.\n\nCode book for data frame Length is below Table 4.3.\n\nPrint-O-Matic printing company’s employees have salaries that are contained in Table 4.4. Find the mean, median, range, variance, and standard deviation for the salaries of all employees.\n\nCode book for data frame Pay below Table 4.4.\n\nPrint-O-Matic printing company spends specific amounts on fixed costs every month. The costs of those fixed costs are in Table 4.5. Find the mean, median, range, variance, and standard deviation for the fixed costs.\n\nCode book for Data frame Cost is below Table 4.5.\n\nThe data frame Pulse Table 4.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute.\n\n\nPulse&lt;-read.csv(\"https://krkozak.github.io/MAT160/pulse.csv\")\nknitr::kable(head(Pulse))\n\n\n\nTable 4.7: Head of Pulse Rates of people Before and After Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\n\n\n\n\n170\n68\n22\nmale\nyes\nyes\nmoderate\nsat\n70\n71\n93\n\n\n182\n75\n26\nmale\nyes\nyes\nmoderate\nsat\n80\n76\n93\n\n\n180\n85\n19\nmale\nyes\nyes\nmoderate\nran\n68\n125\n95\n\n\n182\n85\n20\nmale\nyes\nyes\nlow\nsat\n70\n68\n95\n\n\n167\n70\n22\nmale\nyes\nyes\nlow\nsat\n92\n84\n96\n\n\n178\n86\n21\nmale\nyes\nyes\nlow\nsat\n76\n80\n98\n\n\n\n\n\n\n\n\nCode book for data frame Pulse\nDescription Students in an introductory statistics class (MS212 taught by Professor John Eccleston and Dr Richard Wilson at The University of Queensland) participated in a simple experiment. The students took their own pulse rate. They were then asked to flip a coin. If the coin came up heads, they were to run in place for one minute. Otherwise they sat for one minute. Then everyone took their pulse again. The pulse rates and other physiological and lifestyle data are given in the data.\nFive class groups between 1993 and 1998 participated in the experiment. The lecturer, Richard Wilson, was concerned that some students would choose the less strenuous option of sitting rather than running even if their coin came up heads, In the years 1995-1998 a different method of random assignment was used. In these years, data forms were handed out to the class before the experiment. The forms were pre-assigned to either running or non-running and there were an equal number of each. In 1995 and 1998 not all of the forms were returned so the numbers running and sitting was still not entirely controlled.\nThis data frame contains the following columns:\nheight: height of subject in cm\nweight: weight of subject in kg\nage: age of subject in years\ngender: sex of subject, male, female\nSmokes: whether a subject regularly smokes, yes means does smoke, no means does not smoke\nalcohol: whether a subject regularly drinks alcohol, yes means the person does, no means the person does not\nexercise: whether a subject exercises, low, moderate, high\nran: whether a subject ran one minute between pulse measurements (ran) or sat between pulse measurement (sat)\npulse_before: the pulse rate before a subject either ran or sat (bpm)\npulse_after: the pulse rate after a subject either ran or sat (bpm)\nyear: what year the data was collected (93-98)\nSource Pulse rates before and after exercise. (2013, September 25). Retrieved from http://www.statsci.org/data/oz/ms212.html\nReferences The data was supplied by Dr Richard J. Wilson, Department of Mathematics, University of Queensland.\nCreate a data frame that contains only males, who drink alcohol, but do not smoke. Then compare the pulse before and the pulse after using the mean and standard deviation. Discuss whether pulse before or pulse after has a higher mean and larger spread. The following command creates a new data frame with just males, who drink alcohol, but do not smoke, use the following command, where the new name is Males in Table 4.8.\n\nMales&lt;- Pulse |&gt; \n  filter(gender==\"male\", smokes == \"no\", alcohol == \"yes\")\nknitr::kable(head(Males))\n\n\n\nTable 4.8: Head of Pulse Rates of Nonsmoking Males Before and After Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\n\n\n\n\n195\n84\n18\nmale\nno\nyes\nhigh\nsat\n71\n73\n93\n\n\n184\n74\n22\nmale\nno\nyes\nlow\nran\n78\n141\n93\n\n\n168\n60\n23\nmale\nno\nyes\nmoderate\nran\n88\n150\n93\n\n\n170\n75\n20\nmale\nno\nyes\nhigh\nran\n76\n88\n93\n\n\n187\n59\n18\nmale\nno\nyes\nhigh\nsat\n78\n82\n93\n\n\n180\n72\n18\nmale\nno\nyes\nmoderate\nsat\n69\n67\n93\n\n\n\n\n\n\n\n\n\nThe data frame Pulse Table 4.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute. Create a data frame that contains females, who do not smoke but do drink alcohol. Compare the pulse rate before and after exercise using the mean and standard deviation. Discuss whether pulse before or pulse after has a higher mean and larger spread.\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997) and the data is in Table 4.9.\n\n\nReiki&lt;- read.csv( \"https://krkozak.github.io/MAT160/reki.csv\") \nknitr::kable(head(Reiki))\n\n\n\nTable 4.9: Head of Pain Measurements Before and After Reiki Treatment\n\n\n\n\n\n\nvas.before\nvas.after\nlikert_before\nlikert_after\n\n\n\n\n6\n3\n2\n1\n\n\n2\n1\n2\n1\n\n\n2\n0\n3\n0\n\n\n9\n1\n3\n1\n\n\n3\n0\n2\n0\n\n\n3\n2\n2\n2\n\n\n\n\n\n\n\n\nCode book for data frame Reiki\nDescription The purpose of this study was to explore the usefulness of Reiki as an adjuvant to opioid therapy in the management of pain. Since no studies in this area could be found, a pilot study was carried out involving 20 volunteers experiencing pain at 55 sites for a variety of reasons, including cancer. All Reiki treatments were provided by a certified second-degree Reiki therapist. Pain was measured using both a visual analogue scale (VAS) and a Likert scale immediately before and after the Reiki treatment. Both instruments showed a highly significant (p &lt; 0.0001) reduction in pain following the Reiki treatment.\nThis data frame contains the following columns:\nvas.before: pain measured using a visual analogue scale (VAS) before Reiki treatment\nvas.after: pain measured using a visual analogue scale (VAS) after Reiki treatment\nlikert_before: pain measured using a likert before Reiki treatment\nlikert_after: pain measured using a likert after Reiki treatment\nSource Olson, K., & Hanson, J. (1997). Using reiki to manage pain: a preliminary report. Cancer Prev Control, 1(2), 108-13. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/9765732\nReferences** Using Reiki to manage pain: a preliminary report. Olson K1, Hanson J., Cancer Prev Control 1997, Jun; 1(2): 108-13.\nSince the data was collected both before and after the treatment for all of the units of observations, you want to look at the effect size of the treatment. You want to find the difference between before and after for the pain scale. First you must create a new data frame that adds a column for the difference in before and after. This data is known as paired data. To create the new column in a new data frame called Newreiki use the following commands, Table 4.10.\n\nNewreiki&lt;-Reiki |&gt;\n  mutate(vas.diff=vas.before-vas.after) \nknitr::kable(head(Newreiki))\n\n\n\nTable 4.10: Head of Pain Measurements Before and After Reiki Treatment with Difference column\n\n\n\n\n\n\nvas.before\nvas.after\nlikert_before\nlikert_after\nvas.diff\n\n\n\n\n6\n3\n2\n1\n3\n\n\n2\n1\n2\n1\n1\n\n\n2\n0\n3\n0\n2\n\n\n9\n1\n3\n1\n8\n\n\n3\n0\n2\n0\n3\n\n\n3\n2\n2\n2\n1\n\n\n\n\n\n\n\n\nNow find the mean and standard deviation of the vas.diff variable in Newreiki. Perform similar commands to create the likert.diff variable. Then find the mean and standard deviation for likert.diff, and compare and contrast the vas and likert methods for describing pain.\n8.Yearly rainfall amounts (in millimeters) in Sydney, Australia, are in Table 4.11 (Annual maximums of, 2013). a. Calculate the mean and standard deviation. b. Suppose Sydney, Australia received 300 mm of rainfall in a year. Would this be unusual?\n\nRainfall&lt;-read.csv(\"https://krkozak.github.io/MAT160/rainfall.csv\") \nknitr::kable(head(Rainfall))\n\n\n\nTable 4.11: Head of Yearly rainfall amounts in Sydney, Australia\n\n\n\n\n\n\namount\n\n\n\n\n146.8\n\n\n383.0\n\n\n90.9\n\n\n178.1\n\n\n267.5\n\n\n95.5\n\n\n\n\n\n\n\n\nCode book for data frame Rainfall\nDescription Daily rainfall (in millimeters) was recorded over a 47-year period in Turramurra, Sydney, Australia. For each year, the wettest day was identified (that having the greatest rainfall). The data show the rainfall recorded for the 47 annual maxima.\nThis data frame contains the following columns:\namount: daily rainfall (mm)\nSource Annual maximums of daily rainfall in Sydney. (2013, September 25). Retrieved from http://www.statsci.org/data/oz/sydrain.html\nReferences Rayner J.C.W. and Best D.J. (1989) Smooth tests of goodness of fit. Oxford: Oxford University Press. Hand D.J., Daly F., Lunn A.D., McConway K.J., Ostrowski E. (1994). A Handbook of Small Data Sets. London: Chapman & Hall. Data set 157. Thanks to Jim Irish of the University of Technology, Sydney, for assistance in identifying the correct units for this data.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical-description-of-Data.html#ranking",
    "href": "Numerical-description-of-Data.html#ranking",
    "title": "4  Numerical Description of Data",
    "section": "4.3 Ranking",
    "text": "4.3 Ranking\nAlong with the center and the variability, another useful numerical measure is the ranking of a number. A percentile is a measure of ranking. It represents a location measurement of a data value to the rest of the values. Many standardized tests give the results as a percentile. Doctors also use percentiles to track a child’s growth.\nThe \\(k^{th}\\) percentile is the data value that has k% of the data at or below that value.\n\n4.3.1 Example: Interpreting Percentile\n\nWhat does a score of the \\(90^{th}\\) percentile mean?\nWhat does a score of the \\(70^{th}\\) percentile mean?\n\n\n4.3.1.1 Solution\n\nWhat does a score of the \\(90^{th}\\) percentile mean?\nThis means that 90% of the scores were at or below this score. (A person did the same as or better than 90% of the test takers.)\nWhat does a score of the \\(70^{th}\\) percentile mean?\nThis means that 70% of the scores were at or below this score.\n\n\n\n\n4.3.2 Example: Percentile Versus Score\nIf the test was out of 100 points and you scored at the \\(80^{th}\\) percentile, what was your score on the test?\n\n4.3.2.1 Solution\nYou don’t know! All you know is that you scored the same as or better than 80% of the people who took the test. If all the scores were really low, you could have still failed the test. On the other hand, if many of the scores were high you could have gotten a 95% or more.\nThere are special percentiles called quartiles. Quartiles are numbers that divide the data into fourths. One fourth (or a quarter) of the data falls between consecutive quartiles.\n\n\n\n4.3.3 To find the quartiles:\nThe command in rStudio is\ndf_stats(~variable, data=data_frame, summary)\nIf you record the quartiles together with the maximum and minimum you have five numbers. This is known as the five-number summary. The five-number summary consists of the minimum, the first quartile (\\(Q1\\)), the median, the third quartile (\\(Q3\\)), and the maximum (in that order).\nThe interquartile range, \\(IQR\\), is the difference between the first and third quartiles, \\(Q1\\) and $Q3$. Half of the data (50%) falls in the interquartile range. If the \\(IQR\\) is “large” the data is spread out and if the \\(IQR\\) is “small” the data is closer together.\nInterquartile Range (\\(IQR\\))\nDetermining probable outliers from \\(IQR\\): fences\nA value that is less than \\(Q1-1.5*IQR\\) (this value is often referred to as a low fence) is considered an outlier.\nSimilarly, a value that is more than \\(Q3+1.5*IQR\\) (the high fence) is considered an outlier.\nA boxplot (or box-and-whisker plot) is a graphical display of the five-number summary. It can be drawn vertically or horizontally. The basic format is a box from \\(Q1\\) to \\(Q3\\), a vertical line across the box for the median and horizontal lines as whiskers extending out each end to the minimum and maximum. The minimum and maximum can be represented with dots. Don’t forget to label the tick marks on the number line and give the graph a title.\nAn alternate form of a Boxplot, known as a modified box plot, only extends the left line to the smallest value greater than the low fence, and extends the left line to the largest value less than the high fence, and displays markers (dots, circles or asterisks) for each outlier.\nIf the data are symmetrical, then the box plot will be visibly symmetrical. If the data distribution has a left skew or a right skew, the line on that side of the box plot will be visibly long. If the plot is symmetrical, and the four quartiles are all about the same length, then the data are likely a near uniform distribution. If a box plot is symmetrical, and both outside lines are noticeably longer than the \\(Q1\\) to median and median to \\(Q3\\) distance, the distribution is then probably bell-shaped.\n\n\n4.3.4 Example: Five-number Summary and Boxplot\nFind the five-number summary, the interquartile range (*IQR*), and draw a box-and-whiskers plot for the weight of cats Table 4.1.\n\n4.3.4.1 Solution\nVariable: \\(x\\) = weight of cats To compute the five-number summary on RStudio, use the command:\n\ndf_stats(~Bwt, data=cats, summary)\n\n  response Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1      Bwt    2     2.3    2.7 2.723611   3.025  3.9\n\n\nNote rStudio also calculates the mean as part of the summary command, but the five-number summary is just the five numbers:\nMinimum: 2 kg \\(Q1\\): 2.3 kg Median: 2.7 kg \\(Q3\\): 3.025 kg Maximum: 3.9 kg\nTo find the interquartile range, \\(IQR\\) find $Q3-Q1$, so \\(IQR=3.025-2.3=0.725 kg\\)\nTo create a boxplot use the command\ngf_boxplot(~variable, data=data_frame)\nThis is a modified boxplot which shows the outliers in the data.\n\ngf_boxplot(~Bwt, data=cats, title=\"Weight of Cats\", xlab=\"Body Weight (kg)\")\n\n\n\n\n\n\n\nFigure 4.11: Weight of Cats\n\n\n\n\n\nThere are no outliers since there are no dots outside of the fences.\n\n\n\n4.3.5 Example: Separating based on a factor\nFind the five-number summary of the weights of cats separated by the sex of the cat. Then create a box plot of the weights of cats for each sex of the cat.\n\n4.3.5.1 Solution\nVariable: \\(x_1\\) = weight of female cat\nVariable: \\(x_2\\) = weight of male cat\nTo find the five-number summary separated based on gender use the following command:\n\ndf_stats(~Bwt|Sex, data=cats, summary)\n\n  response Sex Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1      Bwt   F    2    2.15    2.3 2.359574     2.5  3.0\n2      Bwt   M    2    2.50    2.9 2.900000     3.2  3.9\n\n\nThe five-number summary for female cats is (in kg)\nMinimum: 2 \\(Q1\\): 2.15 Median: 2.3 \\(Q3\\): 2.5 Maximum: 3.0\nThe five-number summary for male cats is (in kg)\nMinimum: 2 \\(Q1\\): 2.50 Median: 2.9 \\(Q3\\): 3.2 Maximum: 3.9\n\ngf_boxplot(~Bwt|Sex, data=cats, title=\"Weights of Cats\", xlab=\"Body Weight in (kg)\") \n\n\n\n\n\n\n\nFigure 4.12: Weight of Cats Faceted by Sex\n\n\n\n\n\nNotice that the weights of female cats has a median less than male cats, and in fact it can be seen that the \\(Q1\\) to \\(Q3\\) of the female cats is less than the \\(Q1\\) to \\(Q3\\) of the male cats.\n\n\n\n4.3.6 Example: Putting it all together\nThe time (in 1/50 seconds) between successive pulses along a nerve fiber (“Time between nerve,” 2013) are given in Table 4.12.\n\nNerve&lt;-read.csv( \"https://krkozak.github.io/MAT160/Nerve_pulse.csv\") \nknitr::kable(head(Nerve))\n\n\n\nTable 4.12: Head of Successive pulses along a nerve fiber\n\n\n\n\n\n\ntime\n\n\n\n\n10.5\n\n\n1.5\n\n\n2.5\n\n\n5.5\n\n\n29.5\n\n\n3.0\n\n\n\n\n\n\n\n\nCode book for data frame Nerve\nDescription The data gives the time between 800 successive pulses along a nerve fiber. There are 799 observations rounded to the nearest half in units of 1/50 second.\nThis data frame contains the following columns:\ntime: time between successive Pulses along a nerve fiber, 1/50 second.\nSource Time between nerve pulses. (2019, July 3). Retrieved from &lt;http://www.statsci.org/data/general/nerve.html\nReferences Fatt, P., and Katz, B. (1952). Spontaneous subthreshold activity at motor nerve endings. Journal of Physiology 117, 109-128.\nCox, D. R., and Lewis, P. A. W. (1966). The Statistical Analysis of Series of Events. Methuen, London.\nJorgensen, B. (1982). The Generalized Inverse-Gaussian Distribution. Springer-Verlag.\n\n4.3.6.1 Solution\nFirst, it might be useful to look at a visualization of the data, so create a density plot\n\ngf_density(~time, data=Nerve, title=\"Time between Successive Nerve Pulses\", xlab=\"Time (1/50 second)\") \n\n\n\n\n\n\n\nFigure 4.13: Weight of Cats Faceted by Sex\n\n\n\n\n\nFrom the graph Figure 4.13 the data appears to be skewed right. Most of the time between successive nerve pulses appear to be around 5 or 10 1/50 second, but there are some times that are 60 1/50 second.\n\ndf_stats(~time, data=Nerve, mean, median, sd, summary)\n\n  response     mean median       sd Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1     time 10.95119    7.5 10.45956  0.5     3.5    7.5 10.95119      15   69\n\n\nNumerical descriptions might also be useful. Using technology, the mean is 11 1/50 second,the median is 7.5 1/50 second, the standard deviation is 10.5 1/50 second, and the five-number summary is minimum = 3.5, Q1 = 3.5, median = 7.5, Q3 = 15, and maximum = 69 1/50 second.\nTo visualize the five-number summary, create a box plot.\n\ngf_boxplot(~time, data=Nerve, title=\"Nerve Pulses\", xlab=\"Time (1/50 second)\")\n\n\n\n\n\n\n\nFigure 4.14: Boxplot of Nerve Pulses\n\n\n\n\n\nSince there are many dots outside the upper fence the data has many outliers. From all of this information, one could say that nerve pulses between successive pulses is around 11 1/50 second, with a spread of 19.5 1/50 second. Most of the values are round 11 1/50 second, but they are not very consistent. The density plot and boxplot show that there is a great deal of spread of the data and it is skewed to the right. This means mostly the speed is around 11 1/50 second, but there is a great deal of variability in the values.\n\n\n\n4.3.7 Homework for Ranking Section\nUse Technology on all problems. State the variable on all problems.\n\nSuppose you take a standardized test and you are in the \\(10^{th}\\) percentile. What does this percentile mean? Can you say that you failed the test? Explain.\nSuppose your child takes a standardized test in mathematics and scores in the \\(96^{th}\\) percentile. What does this percentile mean? Can you say your child passed the test? Explain.\nSuppose your child is in the \\(83^{rd}\\) percentile in height and \\(24^{th}\\) percentile in weight. Describe what this tells you about your child’s stature.\nSuppose your work evaluates the employees and places them on a percentile ranking. If your evaluation is in the \\(65^{th}\\) percentile, do you think you are working hard enough? Explain.\nCholesterol levels were collected from patients certain days after they had a heart attack and are in table Table 4.2.\n\nCode book for Data Frame Cholesterol below Table 4.2.\nFind the five-number summary and interquartile range (IQR) for the cholesterol level on day 2, and draw a boxplot\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in table Table 4.3 (Lee, 1994).\n\nCode book for data frame Length below Table 4.3.\nFind the five-number summary and interquartile range (IQR) for the lengths of rivers that go to the Pacific Ocean and ones that go to the Tasman Sea, and draw a boxplot of both.\n\nPrint-O-Matic printing company’s employees have salaries that are contained in Table 4.4 Find the five number summary and draw a boxplot for the salaries of all employees.\n\nCode book for data frame Pay below Table 4.4.\n\nThe data frame Pulse Table 4.7 contains various variables about a person including their pulse rates before the subject exercised and after after the subject ran in place for one minute.\n\nCode book for data frame Pulse below Table 4.7.\nCreate a data frame that contains only people who drink alcohol, but do not smoke. Then find the five number summary and draw a boxplot for both males and females separately.\n\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997) and the data is in Table 4.9.\n\nCode book for data frame Reiki below Table 4.9.\nFind the five number summary for both the before and after VAS scores and draw boxplots of before and after VAS scores. To draw two boxplots at the same time, after the command to create the first box plot type the piping symbol |&gt; (base r) or %&gt;% (magrittr package) before pressing enter. (Note: |&gt; and %&gt;% are piping symbols that can be thought of as “and then.”) Then type the command for the second boxplot after the + symbol or on the next line in the r chunk if using an rmd or qmd file. Then press enter. You may want to graph each boxplot as a different color. To do this, the command would be\ngf_boxplot(~variable, data=data_frame, color=“red”, xlab=“type a label”)\nYou can pick any color you want. Just replace the word red with the color you want to use. Now compare and contrast the before and after VAS scores.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  }
]