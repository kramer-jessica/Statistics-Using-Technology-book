% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{5}
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

% _preamble.tex
\usepackage[most]{tcolorbox}
% Keep only color defs, inline macros, and the stepbox. No custom collapsible box needed.

% Colors: Okabe–Ito palette
\definecolor{OIblue}{HTML}{0072B2}
\definecolor{OIsky}{HTML}{56B4E9}
\definecolor{OIgreen}{HTML}{009E73}
\definecolor{OIorange}{HTML}{E69F00}
\definecolor{OIverm}{HTML}{D55E00}
\definecolor{OIpurple}{HTML}{CC79A7}
\definecolor{OIblack}{HTML}{000000}

% Inline macros (match your .var/.def/.fun/... classes)
\newcommand{\Var}[1]{\textcolor{OIblue}{\textit{#1}}}           % .var
\newcommand{\Def}[1]{\textcolor{OIpurple}{\textit{#1}}}         % .def
\newcommand{\Des}[1]{\textcolor{OIorange}{#1}}                  % .des
\newcommand{\Data}[1]{\textcolor{OIsky}{\textit{#1}}}           % .data
\newcommand{\Dpd}[1]{\textcolor{OIgreen}{\textit{#1}}}          % .dpd
\newcommand{\Fun}[1]{\textcolor{OIverm}{\textit{#1}}}           % .fun
\newcommand{\Dialog}[1]{\textcolor{OIblue}{\textbf{#1}}}        % .dialog
\newcommand{\Repo}[1]{\textcolor{OIgreen}{\textbf{\textit{#1}}}}% .repo
\newcommand{\Ans}[1]{\textcolor{OIblack}{\textbf{#1}}}          % .ans
% Text typed into a textbox / input field (PDF equivalent of .typein)
% ==========================
% Typein style for PDF (matches .typein in HTML)
% ==========================
\newcommand{\Typein}[1]{%
  \colorbox{blue!2}{\texttt{\textcolor{black}{#1}}}%
}
% Subtle inline button for PDF (very light gray)
\newtcbox{\Button}{on line,
  colback=gray!5,        % very light gray background
  coltext=black!80,      % medium–dark gray text
  colframe=gray!40,      % light gray border
  boxrule=0.4pt,
  left=2pt,right=2pt,top=1pt,bottom=1pt,
  arc=1mm, boxsep=0pt, nobeforeafter}
% Step box for PDF
\usepackage[most]{tcolorbox}
\tcbset{enhanced, breakable}
\newtcolorbox{stepbox}{
  colback=OIsky!8,
  colframe=OIblue,
  arc=2mm,
  boxrule=0.8pt,
  left=6pt,right=6pt,top=6pt,bottom=6pt
}
\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{tcolorbox}{}{\usepackage[skins,breakable]{tcolorbox}}
\@ifpackageloaded{fontawesome5}{}{\usepackage{fontawesome5}}
\definecolor{quarto-callout-color}{HTML}{909090}
\definecolor{quarto-callout-note-color}{HTML}{0758E5}
\definecolor{quarto-callout-important-color}{HTML}{CC1914}
\definecolor{quarto-callout-warning-color}{HTML}{EB9113}
\definecolor{quarto-callout-tip-color}{HTML}{00A047}
\definecolor{quarto-callout-caution-color}{HTML}{FC5300}
\definecolor{quarto-callout-color-frame}{HTML}{acacac}
\definecolor{quarto-callout-note-color-frame}{HTML}{4582ec}
\definecolor{quarto-callout-important-color-frame}{HTML}{d9534f}
\definecolor{quarto-callout-warning-color-frame}{HTML}{f0ad4e}
\definecolor{quarto-callout-tip-color-frame}{HTML}{02b875}
\definecolor{quarto-callout-caution-color-frame}{HTML}{fd7e14}
\makeatother
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Statistics Using Technology: Rguroo Edition},
  pdfauthor={Kathryn Kozak; John Doe; Jane Smith},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Statistics Using Technology: Rguroo Edition}
\author{Kathryn Kozak \and John Doe \and Jane Smith}
\date{2025-12-31}

\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter{Home}\label{home}

\section*{Preface}\label{preface}
\addcontentsline{toc}{section}{Preface}

\markright{Preface}

This is test of my macros: Inline examples (should be colored): - The
variable is \Var{x}. - The defined term is \Def{standard deviation}. -
The descriptor is \Des{Frequency}. - The dataset is \Data{Cars}. - The
dropdown selection is \Dpd{Mean}. - The function is \Fun{lm}. - The
dialog name is \Dialog{Descriptive Statistics}. - The repository is
\Repo{Rguroo Datasets}. - The answer is \Ans{42}.

I hope you find this book useful in teaching statistics. When writing
this book, I tried to follow the
\href{Retrieved\%20from\%20http://www.amstat.org/education/gaise/GAISECollege_\%20Recommendations.pdf}{GAISE
Standards (GAISE recommendations}.

\begin{itemize}
\item
  Teach statistical thinking.
\item
  Focus on conceptual understanding.
\item
  Integrate real data with a context and a purpose.
\item
  Foster active learning.
\item
  Use technology to explore concepts and analyze data.
\item
  Use assessments to improve and evaluate student learning
\end{itemize}

To this end, I ask students to interpret the results of their
calculations. I incorporated the use of technology (R Studio) for most
calculations. Because of that you will not find me using any of the
computational formulas for standard deviations or correlation and
regression since I prefer students understand the concept of these
quantities. Also, because I utilize technology you will not find the
standard normal table, Student's t-table, binomial table, chi-square
distribution table, and F-distribution table in the book. Another
difference between this book and other statistics books is the order of
hypothesis testing and confidence intervals. Most books present
confidence intervals first and then hypothesis tests. I find that
presenting hypothesis testing first and then confidence intervals is
more understandable for students. Lastly, I have de-emphasized the use
of the z-test. In fact, I only use it to introduce hypothesis testing,
and never utilize it again. Two samples should be emphasized over one
sample test. Lastly, to aid student understanding and interest, most of
the homework and examples utilize real data with multiple variables. The
beauty of multiple variables, is that you can ask the students to
investigate different analysis with different variables. This way
students can work with data and come up with connections of asking
questions and using data to answer the questions. Again, I hope you find
this book useful for your introductory statistics class.

\section*{Mathematical Knowledge
Assumed}\label{mathematical-knowledge-assumed}
\addcontentsline{toc}{section}{Mathematical Knowledge Assumed}

\markright{Mathematical Knowledge Assumed}

I want to make a comment about the mathematical knowledge that I assumed
the students possess. The course for which I wrote this book has a
higher prerequisite than most introductory statistics books. However, I
do feel that students can read and understand this book as long as they
can read critically. I do not show how to create most of the graphs, but
all graphs are created with R Studio. So I hope the mathematical level
is appropriate for your course.

\section*{Technology Used}\label{technology-used}
\addcontentsline{toc}{section}{Technology Used}

\markright{Technology Used}

The technology that I utilized for creating the graphs and statistical
analysis is R Studio. This is a statistical software that are used by
statisticians and so using it gives students skills they may need in the
future. Please feel free to use any other technology that is more
appropriate for your students. Do make sure that you use some
technology. I worked on the \href{https://www.statprep.org/}{StatPREP
project} and there are Little Apps that can be used to explore data.
There are also activities that can be used in your classes that utilize
the Little Apps on the website.

\section*{Acknowledgments}\label{acknowledgments}
\addcontentsline{toc}{section}{Acknowledgments}

\markright{Acknowledgments}

I would like to thank the following people for taking their valuable
time to review the book. Their comments and insights improved this book
immensely.

\begin{itemize}
\item
  Daniel Kaplan, Macalester College
\item
  Jane Tanner, Onondaga Community College
\item
  Rob Farinelli, College of Southern Maryland
\item
  Carrie Kinnison, retired engineer
\item
  Sean Simpson, Westchester Community College
\item
  Kim Sonier, Coconino Community College
\item
  Jim Ham, Delta College
\item
  Brian Birgen, Wartburg College
\item
  Christopher Cunningham, Elgin Community College
\item
  Kendra Feinstein, Tacoma Community College
\item
  David Straayer, Tacoma Community College
\item
  Students of Coconino Community College
\item
  Students of Elgin Community College
\item
  Students of Tacoma Community College
\item
  Students of Wartburg College
\end{itemize}

I also want to thank Coconino Community College for granting me a
sabbatical so that I would have the time to write the book. On a
personal note, I wanted to thank my brother, John Matic, his wife
Jenelle, and their children Hannah and Eli for their hospitality when
writing the first edition. In addition to allowing my family access to
their home, John provided numerous examples and data sets for business
applications in this book. I inadvertently left this thank you out of
the first edition of the book, His help and his family's hospitality
were invaluable to me. Lastly, I want to thank my husband Rich and my
son Dylan for supporting me in this project. Without their love and
support, I would not have been able to complete the book.

\section*{New to the Fourth Edition}\label{new-to-the-fourth-edition}
\addcontentsline{toc}{section}{New to the Fourth Edition}

\markright{New to the Fourth Edition}

The additions to this edition mostly involve format changes and other
edits to make the textbook more accessible for students with visual
disabilities. Have a textbook that is accessible to all is very
important to me, so please let me know if more changes need to be made.
Minor changes and corrections were also made. One change is that every
hypothesis test and confidence interval has assumptions that must be
true to make the inference valid. Instead of calling them assumptions
though, I decided to call them conditions to remove confusion about
other assumptions.

\section*{Packages Needed for r
studio}\label{packages-needed-for-r-studio}
\addcontentsline{toc}{section}{Packages Needed for r studio}

\markright{Packages Needed for r studio}

You will need the following packages installed and loaded in r Studio:
arm, HNANES, MASS, mosaic, Weighted.Desc.Stat.

\section*{License}\label{license}
\addcontentsline{toc}{section}{License}

\markright{License}

Creative Commons Attribution Sharealike.

2025 Kathryn Kozak

\includegraphics{Creative_commons.png}

ISBN:

\bookmarksetup{startatroot}

\chapter{Statistical Basics}\label{statistical-basics}

You are exposed to statistics regularly. If you are a sports fan, then
you have the statistics for your favorite player. If you are interested
in politics, then you look at the polls to see how people feel about
certain issues or candidates. If you are an environmentalist, then you
research arsenic levels in the water of a town or analyze the global
temperatures. If you are in the business profession, then you may track
the monthly sales of a store or use quality control processes to monitor
the number of defective parts manufactured. If you are in the health
profession, then you may look at how successful a procedure is or the
percentage of people infected with a disease. There are many other
examples from other areas. To understand how to collect data and analyze
it, you need to understand what the field of statistics is and the basic
definitions.

\section{What is Statistics?}\label{what-is-statistics}

\textbf{Statistics} is the study of how to collect, organize, analyze,
and interpret data collected from a group.

There are two branches of statistics. One is called descriptive
statistics, which is where you collect and organize data. The other is
called inferential statistics, which is where you analyze and interpret
data. First you need to look at descriptive statistics since you will
use the descriptive statistics when making inferences.

To understand how to create descriptive statistics and then conduct
inferences, there are a few definitions that you need to look at. Note,
many of the words that are defined have common definitions that are used
in non-statistical terminology. In statistics, some have slightly
different definitions. It is important that you notice the difference
and utilize the statistical definitions.

The first thing to decide in a statistical study is whom you want to
measure and what you want to measure. You always want to make sure that
you can answer the question of whom you measured and what you measured.
The who is known as the observation and the what is the variable(s).

\textbf{observation, or simply observations}: a person or object that
you are interested in finding out information about.

\textbf{Variable}: the measurement or observation of the observation

Having the observation and the variables is part of picture of a
\textbf{data set} or \textbf{data frame}. To make a data set or data
frame into what is called tidy data, it should be organized in a way
that each row of the data frame is an observation, and the variables
should be well defined and are easily identified. An example of a data
frame that is tidy data is:

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.1955}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0602}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0301}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0376}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0677}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0602}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0301}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0526}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0451}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0451}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0526}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0526}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0677}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0451}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0526}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0376}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 32\tabcolsep) * \real{0.0677}}@{}}

\caption{\label{tbl-dataframe_example}Example of a Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
chidren
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
mfr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
calories
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
protein
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sodium
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fiber
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
carbo
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sugars
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
potass
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
vitamins
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
shelf
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
cups
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
rating
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
100\%\_Bran & N & N & C & 70 & 4 & 1 & 130 & 10.0 & 5.0 & 6 & 280 & 25 &
3 & 1 & 0.33 & 68.40297 \\
100\%\_Natural\_Bran & N & Q & C & 120 & 3 & 5 & 15 & 2.0 & 8.0 & 8 &
135 & 0 & 3 & 1 & 1.00 & 33.98368 \\
All-Bran & N & K & C & 70 & 4 & 1 & 260 & 9.0 & 7.0 & 5 & 320 & 25 & 3 &
1 & 0.33 & 59.42551 \\
All-Bran\_with\_Extra\_Fiber & N & K & C & 50 & 4 & 0 & 140 & 14.0 & 8.0
& 0 & 330 & 25 & 3 & 1 & 0.50 & 93.70491 \\
Almond\_Delight & N & R & C & 110 & 2 & 2 & 200 & 1.0 & 14.0 & 8 & -1 &
25 & 3 & 1 & 0.75 & 34.38484 \\
Apple\_Cinnamon\_Cheerios & Y & G & C & 110 & 2 & 2 & 180 & 1.5 & 10.5 &
10 & 70 & 25 & 1 & 1 & 0.75 & 29.50954 \\

\end{longtable}

Collecting multiple variables from one observation makes sense. If you
wanted to figure out the diameter of breast height of Ponderosa Pine
trees in the Coconino National Forest, you need to physically measure a
bunch of trees. While you are measuring the diameter, you might also
want to measure the height of the tree, if the tree has a bark beetle
infestation, the estimated age of the tree, the color of the bark, and
how many branches it has. You may only want to estimate the average
diameter at breast height, but now you have the ability to estimate
other quantities too. No sense walking all over the forest and only
measure one thing.

A large data frame is one that has at least 5 variables and at least
1000 units of observations. If a data frame only has 3 variables and 500
rows, that doesn't make it not usable. The 1000 observations and 5
variables is just a guideline to work with.

If you put the observation and the variable into one statement, then you
obtain a population.

\textbf{Population}: set of all values of the variable for the entire
group of units of observations

Notice, the population answers who you want to measure and what you want
to measure. Make sure that your population always answers both of these
questions. If it doesn't, then you haven't given someone who is reading
your study the entire picture. As an example, if you just say that you
are going to collect data from the senators in the U.S. Congress, you
haven't told your reader want you are going to collect. Do you want to
know their income, their highest degree earned, their voting record,
their age, their political party, their gender, their marital status, or
how they feel about a particular issue? Without telling what you want to
measure, your reader has no idea what your study is actually about.

Sometimes the population is very easy to collect. Such as if you are
interested in finding the average age of all of the current senators in
the U.S. Congress, there are only 100 senators. This wouldn't be hard to
find. However, if instead you were interested in knowing the average age
that a senator in the U.S. Congress first took office for all senators
that ever served in the U.S. Congress, then this would be a bit more
work. It is still doable, but it would take a bit of time to collect.
But what if you are interested in finding the average diameter of breast
height of all of the Ponderosa Pine trees in the Coconino National
Forest? This would be impossible to actually collect. What do you do in
these cases? Instead of collecting the entire population, you take a
smaller group of the population, kind of a snap shot of the population.
This smaller group is called a sample.

\textbf{Sample}: a subset from the population. It looks just like the
population, but contains less data.

In today of big data, there is some confusion between really large data
frames and populations. The population is a theoretical concept and even
if you have a very large data frame, that doesn't mean you have the
population. Most populations are not actually able to be collected. They
are considered an ideal that you are trying to make decisions about.

How you collect your sample can determine how accurate the results of
your study are. There are many ways to collect samples. Some of them
create better samples than others. No sampling method is perfect, but
some are better than others. Sampling techniques will be discussed
later. For now, realize that every time you take a sample you will find
different data values. The sample is a snapshot of the population, and
there is more information than is in the picture. The idea is to try to
collect a sample that gives you an accurate picture, but you will never
know for sure if your picture is the correct picture. Unlike previous
mathematics classes where there was always one right answer, in
statistics there can be many answers, and you don't know which are
right.

Once you have your data frame, either from a population or a sample, you
need to know how you want to summarize the data. As an example, suppose
you are interested in finding the proportion of people who like a
candidate, the average height a plant grows to using a new fertilizer,
or the variability of the test scores. Understanding how you want to
summarize the data helps to determine the type of data you want to
collect. Since the population is what we are interested in, then you
want to calculate a number from the population. This is known as a
parameter. As mentioned already, you can't really collect the entire
population. Even though this is the number you are interested in, you
can't really calculate it. Instead you use a number calculated from the
sample, called a statistic, to estimate the parameter. Since no sample
is exactly the same, the statistic values are going to be different from
sample to sample. They estimate the value of the parameter, but again,
you do not know for sure if your answer is correct.

\textbf{Parameter}: a number calculated from the population. Usually
denoted with a Greek letter. This number is a fixed, unknown number that
you want to find.

\textbf{Statistic}: a number calculated from the sample. Usually denoted
with letters from the Latin alphabet, though sometimes there is a Greek
letter with a \textasciicircum (called a hat) above it. Since you can
find samples, it is readily known, though it changes depending on the
sample taken. It is used to estimate the parameter value.

One last concept to mention is that there are two different types of
variables -- qualitative (categorical) and quantitative (numerical).
Each type of variable has different parameters and statistics that you
find. It is important to know the difference between them.

\textbf{Qualitative} or \textbf{categorical variable:} answer is a word
or name that describes a quality of the observation

\textbf{Quantitative} or \textbf{numerical variable:} answer is a
number, something that can be counted or measured from the observation

\subsection{Example: Stating Definitions for Qualitative
Variable}\label{example-stating-definitions-for-qualitative-variable}

In 2010, the Pew Research Center questioned 1500 adults in the U.S. to
estimate the proportion of the population favoring marijuana use for
medical purposes. It was found that 73\% are in favor of using marijuana
for medical purposes. State the observation, variable, population, and
sample.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution}

Observation: a U.S. adult

Variable: the response to the question ``should marijuana be used for
medical purposes?'' This is qualitative data since you are recording a
person's response --- yes or no.

Population: set of responses of all adults in the U.S.

Sample: set of responses of 1500 adults in the U.S.

Parameter: proportion of all U.S. Adults who favor marijuana for medical
purposes

Statistic --- proportion of 1500 U.S. Adults who favor marijuana for
medical purposes

\subsection{Example: Stating Definitions for Qualitative
Variable}\label{example-stating-definitions-for-qualitative-variable-1}

A parking control officer records the manufacturer of every \(5^{th}\)
car in the college parking lot in order to determine the most common
manufacturer. State the observation, variable, population, and sample.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-1}

Observation: a car in the college parking lot

Variable: the name of the manufacturer. This is qualitative data since
you are recording a car type.

Population: set of names of the manufacturer of all cars in the college
parking lot.

Sample: set of names of the manufacturer of the a particular number of
cars in college parking lot

Parameter: proportion of each car type of all cars in the college
parking lot

Statistic: proportion of each car type a particular number of cars in
the college parking lot

\subsection{Example: Stating Definitions for Quantitative
Variable}\label{example-stating-definitions-for-quantitative-variable}

A biologist wants to estimate the average height of a plant that is
given a new plant food. She gives 10 plants the new plant food and
measures the plant height on day 50. State the observation, variable,
population, and sample.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-2}

Observation: a plant given the new plant food

Variable: the height of the plant on day 50 (Note: it is not the average
height since you cannot measure an average -- it is calculated from
data.) This is quantitative data since you will have a number.

Population: set of heights on day 50 of all plants when the new plant
food is used

Sample: set of heights on day 50 of 10 plants when the new plant food is
used

Parameter: average height on day 50 of all plants when the new plant
food is used

Statistic: average height on day 50 of 10 plants when the new plant food
is used

Note: in
\hyperref[example-stating-definitions-for-qualitative-variable-1]{Example:
Stating Definitions for Qualitative Variable}, you most likely will be
comparing the new plant food to an old plant food. So you would have
more units of observations, but the plants given the new plant food are
what you are interested in in this case. You may also want to have
measurements on other days after you give the plant food. In your data
frame you would need to have many variables besides just the height of
the plant on day 50. Examples of variables would be plant\_number,
fertilizer (yes or no), height on day 20, height on day 30, height on
day 50, and so forth. One other comment, you variable names should make
sense to your reader, and be one word for ease in analyzing by a
computer program.

\subsection{Example: Stating Definitions for Quantitative
Variable}\label{example-stating-definitions-for-quantitative-variable-1}

A doctor wants to see if a new treatment for cancer extends the life
expectancy of a patient versus the old treatment. She gives one group of
25 cancer patients the new treatment and another group of 25 the old
treatment. She then measures the life expectancy of each of the
patients. State the units of observations, variables, populations, and
samples.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-3}

In this example there are two observations, two variables, two
populations, and two samples.

Observation 1: cancer patient given new treatment

Observation 2: cancer patient given old treatment

Variable 1: life expectancy when given new treatment. This is
quantitative data since you will have a number.

Variable 2: life expectancy when given old treatment. This is
quantitative data since you will have a number.

Population 1: set of life expectancies of all cancer patients given new
treatment

Population 2: set of life expectancies of all cancer patients given old
treatment

Sample 1: set of life expectancies of 25 cancer patients given new
treatment

Sample 2: set of life expectancies of 25 cancer patients given old
treatment

Parameter 1: average life expectancy of all cancer patients given new
treatment

Parameter 2: average life expectancy of all cancer patients given old
treatment

Statistic 1: average life expectancy of 25 cancer patients given new
treatment

Statistic 2: average life expectancy of 25 cancer patients given old
treatment

There are different types of quantitative variables, called discrete or
continuous. The difference is in how many values can the data have. If
you can actually count the number of data values (even if you are
counting to infinity), then the variable is called discrete. If it is
not possible to count the number of data values, then the variable is
called continuous.

\textbf{Discrete} data can only take on particular values like integers.
Discrete data are usually things you count.

\textbf{Continuous} data can take on any value. Continuous data are
usually things you measure.

\subsection{Example: Discrete or
Continuous}\label{example-discrete-or-continuous}

Classify the quantitative variable as discrete or continuous.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  The weight of a cat.
\item
  The number of fleas on a cat.
\item
  The size of a shoe.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-4}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  The weight of a cat.

  This is continuous since it is something you measure.
\item
  The number of fleas on a cat.

  This is discrete since it is something you count.
\item
  The size of a shoe.

  This is discrete since you can only be certain values, such as 7, 7.5,
  8, 8.5, 9. You can't buy a 9.73 shoe.
\end{enumerate}

There are also are four measurement scales for different types of data
with each building on the ones below it. They are:

\subsection{Measurement Scales:}\label{measurement-scales}

\textbf{Nominal}: data is just a name or category. There is no order to
any data and since there are no numbers, you cannot do any arithmetic on
this level of data. Examples of this are gender, car name, ethnicity,
and race.

\textbf{Ordinal}: data that is nominal, but you can now put the data in
order, since one value is more or less than another value. You cannot do
arithmetic on this data, but you can now put data values in order.
Examples of this are grades (A, B, C, D, F), place value in a race (1st,
2nd, 3rd), and size of a drink (small, medium, large).

\textbf{Interval}: data that is ordinal, but you can now subtract one
value from another and that subtraction makes sense. You can do
arithmetic on this data, but only addition and subtraction. Examples of
this are temperature and time on a clock.

\textbf{Ratio}: data that is interval, but you can now divide one value
by another and that ratio makes sense. You can now do all arithmetic on
this data. Examples of this are height, weight, distance, and length of
time.

Nominal and ordinal data come from qualitative variables. Interval and
ratio data come from quantitative variables.

Most people have a hard time deciding if the data are nominal, ordinal,
interval, or ratio. First, if the variable is qualitative (words instead
of numbers) then it is either nominal or ordinal. Now ask yourself if
you can put the data in a particular order. If you can it is ordinal.
Otherwise, it is nominal. If the variable is quantitative (numbers),
then it is either interval or ratio. For ratio data, a value of 0 means
there is no measurement. This is known as the absolute zero. If there is
an absolute zero in the data, then it means it is ratio. If there is no
absolute zero, then the data are interval. An example of an absolute
zero is if you have \textbackslash\$0 in your bank account, then you are
without money. The amount of money in your bank account is ratio data.
\textbf{Word of caution}: sometimes ordinal data is displayed using
numbers, such as 5 being strongly agree, and 1 being strongly disagree.
These numbers are not really numbers. Instead they are used to assign
numerical values to ordinal data. In reality you should not perform any
computations on this data, though many people do. If there are numbers,
make sure the numbers are inherent numbers, and not numbers that were
assigned.

\subsection{Example: Measurement Scale}\label{example-measurement-scale}

State which measurement scale each is.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Time of first class
\item
  Hair color
\item
  Length of time to take a test
\item
  Age groupings (baby, toddler, adolescent, teenager, adult, elderly)
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-5}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Time of first class

  This is interval since it is a number, but 0 o'clock means midnight
  and not the absence of time.
\item
  Hair color

  This is nominal since it is not a number, and there is no specific
  order for hair color.
\item
  Length of time to take a test.
\end{enumerate}

This is ratio since it is a number, and if you take 0 minutes to take a
test, it means you didn't take any time to complete it.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\item
  Age groupings (baby, toddler, adolescent, teenager, adult, elderly)

  This is ordinal since it is not a number, but you could put the data
  in order from youngest to oldest or the other way around.
\end{enumerate}

\subsection{Homework for What is Statistics
Section}\label{homework-for-what-is-statistics-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Suppose you want to know how Arizona workers age 16 or older travel to
  work. To estimate the percentage of people who use the different modes
  of travel, you take a sample containing 500 Arizona workers age 16 or
  older. State the observation, variable, population, sample, parameter,
  and statistic.
\item
  You wish to estimate the mean cholesterol levels of patients two days
  after they had a heart attack. To estimate the mean you collect data
  from 28 heart patients. State the observation, variable, population,
  sample, parameter, and statistic.
\item
  Print-O-Matic would like to estimate their mean salary of all
  employees. To accomplish this they collect the salary of 19 employees.
  State the observation, variable, population, sample, parameter, and
  statistic.
\item
  To estimate the percentage of households in Connecticut which use fuel
  oil as a heating source, a researcher collects information from 1000
  Connecticut households about what fuel is their heating source. State
  the observation, variable, population, sample, parameter, and
  statistic.
\item
  The U.S. Census Bureau needs to estimate the median income of males in
  the U.S., they collect incomes from 2500 males. State the observation,
  variable, population, sample, parameter, and statistic.
\item
  The U.S. Census Bureau needs to estimate the median income of females
  in the U.S., they collect incomes from 3500 females. State the
  observation, variable, population, sample, parameter, and statistic.
\item
  Eyeglassmatic manufactures eyeglasses and they would like to know the
  percentage of each defect type made. They review 25,891 defects and
  classify each defect that is made. State the observation, variable,
  population, sample, parameter, and statistic.
\item
  The World Health Organization wishes to estimate the mean density of
  people per square kilometer, they collect data on 56 countries. State
  the observation, variable, population, sample, parameter, and
  statistic
\item
  State the measurement scale for each.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Cholesterol level
\item
  Defect type
\item
  Time of first class
\item
  Opinion on a 5 point scale, with 5 being strongly agree and 1 being
  strongly disagree
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  State the measurement scale for each.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Temperature in degrees Celsius
\item
  Ice cream flavors available
\item
  Pain levels on a scale from 1 to 10, 10 being the worst pain ever
\item
  Salary of employees
\end{enumerate}

\section{Sampling Methods}\label{sampling-methods}

As stated before, if you want to know something about a population, it
is often impossible or impractical to examine the whole population. It
might be too expensive in terms of time or money. It might be
impractical --- you can't test all batteries for their length of
lifetime because there wouldn't be any batteries left to sell. You need
to look at a sample. Hopefully the sample behaves the same as the
population.

When you choose a sample you want it to be as similar to the population
as possible. If you want to test a new painkiller for adults you would
want the sample to include people who are fat, skinny, old, young,
healthy, not healthy, male, female, etc.

There are many ways to collect a sample. None are perfect, and you are
not guaranteed to collect a representative sample. That is unfortunately
the limitations of sampling. However, there are several techniques that
can result in samples that give you a semi-accurate picture of the
population. Just remember to be aware that the sample may not be
representative. As an example, you can take a random sample of a group
of people that are equally males and females, yet by chance everyone you
choose is female. If this happens, it may be a good idea to collect a
new sample if you have the time and money. There are many sampling
techniques, though only four will be presented here.

The simplest, and the type that is desired for is a \textbf{simple
random sample}. This is where you pick the sample such that every sample
has the same chance of being chosen. This type of sample is actually
hard to collect, since it is sometimes difficult to obtain a complete
list of all observations. There are many cases where you cannot conduct
a truly random sample. However, you can get as close as you can.

Now suppose you are interested in what type of music people like. It
might not make sense to try to find the most popular type of music
preferred by everyone in the U.S. You probably don't like the same music
as your parents. The answers vary so much you probably couldn't find an
answer for everyone all at once. It might make sense to look at people
in different age groups, or people of different ethnicities. This is
called a \textbf{stratified sample}. The issue with this sample type is
that sometimes people subdivide the population too much. It is best to
just have one stratification. Also, a stratified sample has similar
problems that a simple random sample has.

If your population has some order in it, then you could do a
\textbf{systematic sample}. This is popular in manufacturing. The
problem is that it is possible to miss a manufacturing mistake because
of how this sample is taken.

If you are collecting polling data based on location, then a
\textbf{cluster sample} that divides the population based on
geographical means would be the easiest sample to conduct. The problem
is that if you are looking for opinions of people, and people who live
in the same region may have similar opinions. As you can see each of the
sampling techniques have pluses and minuses.

One last type of sample that is sometimes conducted is called a
\textbf{convenience sample}. This sample is not one that should be
conducted since the idea of a convenience sample is that the sample is
collected using the most convenient process for the researcher. The
researcher may ask people who they know or who are easy to get a old of,
and it is in no way representative of the population.

A \textbf{simple random sample (SRS)} of size \textbf{n} is a sample
that is selected from a population in a way that ensures that every
different possible sample of size \textbf{n} has the same chance of
being selected. Also, every observation associated with the population
has the same chance of being selected.

Ways to select a simple random sample:

\begin{itemize}
\item
  Put all names in a hat and draw a certain number of names out.
\item
  Assign each observation a number and use a random number table or a
  calculator or computer to randomly select the observations that will
  be measured.
\end{itemize}

\subsection{Example: Choosing a Simple Random
Sample}\label{example-choosing-a-simple-random-sample}

Describe how to take a simple random sample from a classroom.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-6}

Give each student in the class a number. Using a random number generator
you could then pick the number of students you want to pick.

\subsection{Example: How Not to Choose a Simple Random
Sample}\label{example-how-not-to-choose-a-simple-random-sample}

You want to choose 5 students out of a class of 20. Give some examples
of samples that are *not* simple random samples.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-7}

Choose 5 students from the front row. The people in the last row have no
chance of being selected. Choose the 5 shortest students. The tallest
students have no chance of being selected. Ask your friend to pick
numbers that have been assigned to each student. Your friend may prefer
certain numbers and picks those. This is not known by your friend, but
this happens.

\subsection{Example: How to Choose a Simple Random Sample using
R}\label{example-how-to-choose-a-simple-random-sample-using-r}

You want to take a simple random sample of size 10 from a data frame
known as NHANES Table~\ref{tbl-random-sample}, use these steps:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"NHANES"}\NormalTok{) }\CommentTok{\# turns on the package NHANES in R}
\NormalTok{sample\_NHANES}\OtherTok{\textless{}{-}} \CommentTok{\# gives the new sample a name}
\NormalTok{  NHANES }\SpecialCharTok{|\textgreater{}} \CommentTok{\# states the dataframe to collect from}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n=}\DecValTok{10}\NormalTok{) }\CommentTok{\# creates a random sample and saves it as Sample\_NHANES}
\FunctionTok{options}\NormalTok{(}\AttributeTok{width =} \DecValTok{60}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(sample\_NHANES) }\CommentTok{\#displays the sample just created}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0050}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0212}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}@{}}

\caption{\label{tbl-random-sample}Random Sample of size 10 from NHANES}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SurveyYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AgeDecade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeMonths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MaritalStatus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HHIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HHIncomeMid
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Poverty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HomeRooms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HomeOwn
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Work
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HeadCirc
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BMI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMICatUnder20yrs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMI\_WHO
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Pulse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSysAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDiaAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Testosterone
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DirectChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TotChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diabetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DiabetesAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HealthGen
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysPhysHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysMentHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LittleInterest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Depressed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nPregnancies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nBabies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age1stBaby
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SleepHrsNight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SleepTrouble
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PhysActive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PhysActiveDays
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TVHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CompHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TVHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
CompHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alcohol12PlusYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SmokeNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SmokeAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marijuana
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeFirstMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RegularMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeRegMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HardDrugs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexEver
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartnLife
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SameSex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexOrientation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PregnantNow
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
63555 & 2011\_12 & male & 6 & 0-9 & NA & Hispanic & Hispanic & NA & NA &
75000-99999 & 87500 & 2.44 & 7 & Own & NA & 20.5 & NA & NA & 115.6 &
15.30 & NormWeight & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA
& NA & 0.83 & 1.40 & 4.09 & 120 & 1.290 & NA & NA & No & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & 3\_hr & 0\_to\_1\_hr &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA \\
70574 & 2011\_12 & female & 36 & 30-39 & NA & Black & Black & High
School & NeverMarried & 0-4999 & 2500 & 0.27 & 5 & Rent & NotWorking &
92.6 & NA & NA & 161.7 & 35.40 & NA & 30.0\_plus & 62 & 143 & 86 & 148 &
90 & 148 & 84 & 138 & 88 & 45.76 & 1.45 & 3.59 & 92 & 0.238 & NA & NA &
No & NA & Excellent & 0 & 0 & None & None & 4 & 2 & 19 & 8 & No & No & 3
& 2\_hr & 1\_hr & NA & NA & Yes & 3 & 104 & NA & No & Non-Smoker & NA &
No & NA & No & NA & No & Yes & 15 & 10 & 1 & No & Heterosexual & No \\
60910 & 2009\_10 & male & 36 & 30-39 & 441 & Hispanic & NA & College
Grad & Married & 75000-99999 & 87500 & 2.40 & 6 & Own & Working & 73.5 &
NA & NA & 163.0 & 27.66 & NA & 25.0\_to\_29.9 & 64 & 105 & 64 & 104 & 62
& 106 & 66 & 104 & 62 & NA & 1.11 & 6.44 & 151 & 0.677 & NA & NA & No &
NA & Good & 2 & 1 & None & None & NA & NA & NA & 7 & No & No & NA & NA &
NA & NA & NA & Yes & 6 & 52 & No & Yes & Smoker & 13 & Yes & 17 & No &
NA & Yes & Yes & 14 & 12 & 1 & No & Heterosexual & NA \\
58153 & 2009\_10 & female & 1 & 0-9 & 23 & White & NA & NA & NA &
65000-74999 & 70000 & 3.55 & 4 & Rent & NA & 14.0 & 88.4 & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
64383 & 2011\_12 & female & 26 & 20-29 & NA & Black & Black & Some
College & Separated & NA & NA & NA & 5 & Rent & Working & 83.9 & NA & NA
& 159.4 & 33.00 & NA & 30.0\_plus & 76 & 110 & 76 & 110 & 76 & NA & NA &
NA & NA & 22.09 & 0.96 & 4.03 & 109 & 0.908 & NA & NA & No & NA & Vgood
& 0 & 0 & None & None & 1 & 1 & NA & 6 & No & Yes & 2 & 0\_to\_1\_hr &
More\_4\_hr & NA & NA & Yes & 1 & 52 & NA & No & Non-Smoker & NA & No &
NA & No & NA & No & Yes & 17 & 5 & 5 & No & Heterosexual & No \\
70869 & 2011\_12 & male & 38 & 30-39 & NA & Mexican & Mexican & 9 - 11th
Grade & Married & 35000-44999 & 40000 & 1.04 & 6 & Own & Working & 99.4
& NA & NA & 167.6 & 35.40 & NA & 30.0\_plus & 62 & 109 & 59 & 108 & 66 &
108 & 62 & 110 & 56 & 306.00 & 0.80 & 5.43 & 96 & 1.247 & NA & NA & No &
NA & Good & 0 & 0 & None & None & NA & NA & NA & 8 & No & Yes & NA &
2\_hr & 0\_hrs & NA & NA & Yes & 8 & 12 & NA & No & Non-Smoker & NA & No
& NA & No & NA & No & Yes & 15 & 8 & 1 & No & Heterosexual & NA \\
52017 & 2009\_10 & male & 31 & 30-39 & 372 & White & NA & Some College &
NeverMarried & 35000-44999 & 40000 & 3.97 & 4 & Rent & Working & 108.9 &
NA & NA & 175.8 & 35.24 & NA & 30.0\_plus & 86 & 148 & 82 & 150 & 86 &
148 & 82 & 148 & 82 & NA & 0.54 & 4.40 & 383 & 1.765 & NA & NA & No & NA
& Good & 0 & 0 & None & None & NA & NA & NA & 7 & No & Yes & 2 & NA & NA
& NA & NA & Yes & 2 & 24 & NA & No & Non-Smoker & NA & No & NA & No & NA
& No & No & NA & 0 & 0 & No & Heterosexual & NA \\
69498 & 2011\_12 & male & 31 & 30-39 & NA & White & White & College Grad
& Married & more 99999 & 100000 & 5.00 & 4 & Own & Working & 102.6 & NA
& NA & 192.8 & 27.60 & NA & 25.0\_to\_29.9 & 60 & 114 & 76 & 114 & 82 &
114 & 76 & 114 & 76 & 445.99 & 1.29 & 5.02 & 120 & 1.818 & NA & NA & No
& NA & Vgood & 0 & 10 & Several & None & NA & NA & NA & 7 & No & Yes &
NA & 2\_hr & 1\_hr & NA & NA & Yes & 2 & 104 & NA & No & Non-Smoker & NA
& Yes & 20 & No & NA & No & Yes & 19 & 3 & 1 & No & Heterosexual & NA \\
59880 & 2009\_10 & female & 45 & 40-49 & 547 & Hispanic & NA & Some
College & Married & 25000-34999 & 30000 & 1.45 & 9 & Own & Working &
98.3 & NA & NA & 163.2 & 36.91 & NA & 30.0\_plus & 80 & 106 & 54 & 104 &
50 & 108 & 44 & 104 & 64 & NA & 1.22 & 5.48 & 129 & 0.313 & NA & NA &
Yes & NA & Good & 0 & 9 & None & Several & 3 & 2 & 22 & 5 & Yes & No &
NA & NA & NA & NA & NA & Yes & 1 & 24 & No & Yes & Smoker & 19 & No & NA
& No & NA & No & Yes & 19 & 2 & 1 & No & Heterosexual & NA \\
63426 & 2011\_12 & male & 58 & 50-59 & NA & White & White & 9 - 11th
Grade & Divorced & 10000-14999 & 12500 & 1.19 & 3 & Rent & Working &
83.3 & NA & NA & 175.9 & 26.90 & NA & 25.0\_to\_29.9 & 76 & 142 & 82 &
134 & 78 & 148 & 84 & 136 & 80 & 669.82 & 1.53 & 4.16 & 62 & 0.667 & NA
& NA & No & NA & Fair & 20 & 0 & None & None & NA & NA & NA & 6 & No &
No & 2 & 1\_hr & 0\_hrs & NA & NA & Yes & 2 & 312 & Yes & Yes & Smoker &
9 & No & NA & No & NA & No & Yes & 14 & 15 & 1 & No & Heterosexual &
NA \\

\end{longtable}

\textbf{Stratified sampling} is where you break the population into
groups called strata, then take a simple random sample from each strata.

For example:

\begin{itemize}
\item
  If you want to look at musical preference, you could divide the
  observations into age groups and then conduct simple random samples
  inside each group.
\item
  If you want to calculate the average price of textbooks, you could
  divide the observations into groups by major and then conduct simple
  random samples inside each group.
\end{itemize}

\subsection{Example: How to Choose a Stratified Sample using
R}\label{example-how-to-choose-a-stratified-sample-using-r}

To take a stratified sample using rStudio of size 20 from NHANES
Table~\ref{tbl-stratified-sample} using race as the strata, use these
steps:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(}\StringTok{"NHANES"}\NormalTok{) }\CommentTok{\# turns on the package NHANES in R}
\NormalTok{sample\_NHANES}\OtherTok{\textless{}{-}} \CommentTok{\# gives the new sample a name}
\NormalTok{  NHANES }\SpecialCharTok{|\textgreater{}} \CommentTok{\# states the dataframe to collect from}
  \FunctionTok{group\_by}\NormalTok{(Race1) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# tells what variable is the strata}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n=}\DecValTok{20}\NormalTok{) }\CommentTok{\# takes the random sample within each strata}
\FunctionTok{options}\NormalTok{(}\AttributeTok{width =} \DecValTok{60}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(sample\_NHANES) }\CommentTok{\#displays the sample just created}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0050}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0212}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}@{}}

\caption{\label{tbl-stratified-sample}Stratafied Sample of size 100 from
NHANES with Race as the Strata}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SurveyYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AgeDecade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeMonths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MaritalStatus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HHIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HHIncomeMid
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Poverty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HomeRooms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HomeOwn
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Work
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HeadCirc
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BMI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMICatUnder20yrs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMI\_WHO
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Pulse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSysAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDiaAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Testosterone
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DirectChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TotChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diabetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DiabetesAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HealthGen
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysPhysHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysMentHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LittleInterest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Depressed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nPregnancies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nBabies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age1stBaby
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SleepHrsNight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SleepTrouble
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PhysActive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PhysActiveDays
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TVHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CompHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TVHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
CompHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alcohol12PlusYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SmokeNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SmokeAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marijuana
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeFirstMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RegularMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeRegMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HardDrugs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexEver
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartnLife
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SameSex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexOrientation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PregnantNow
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
52884 & 2009\_10 & female & 44 & 40-49 & 539 & Black & NA & High School
& NeverMarried & 35000-44999 & 40000 & 2.18 & 5 & Own & Working & 78.7 &
NA & NA & 158.8 & 31.21 & NA & 30.0\_plus & 90 & 140 & 72 & 142 & 72 &
138 & 76 & 142 & 68 & NA & 1.24 & 5.38 & 62 & 1.550 & NA & NA & No & NA
& NA & NA & NA & NA & NA & NA & NA & NA & 6 & No & No & NA & NA & NA &
NA & NA & NA & NA & NA & Yes & Yes & Smoker & 16 & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & No \\
65346 & 2011\_12 & female & 49 & 40-49 & NA & Black & Black & College
Grad & Married & 75000-99999 & 87500 & 3.06 & 7 & Own & Working & 62.5 &
NA & NA & 155.1 & 26.00 & NA & 25.0\_to\_29.9 & 66 & 140 & 74 & 138 & 80
& 142 & 78 & 138 & 70 & NA & 2.30 & 5.72 & 148 & 2.508 & NA & NA & No &
NA & Good & 3 & 15 & Several & None & 3 & 3 & 24 & 6 & No & Yes & 1 &
1\_hr & 1\_hr & NA & NA & Yes & 1 & 3 & NA & No & Non-Smoker & NA & Yes
& 18 & Yes & 18 & No & Yes & 18 & 6 & 1 & No & Heterosexual & NA \\
55851 & 2009\_10 & female & 24 & 20-29 & 289 & Black & NA & High School
& NeverMarried & NA & NA & NA & 5 & Own & Working & 102.3 & NA & NA &
177.4 & 32.51 & NA & 30.0\_plus & 72 & 115 & 47 & 118 & 52 & 114 & 50 &
116 & 44 & NA & 1.37 & 3.57 & 53 & 0.331 & NA & NA & No & NA & NA & NA &
NA & NA & NA & NA & NA & NA & 7 & No & Yes & 7 & NA & NA & NA & NA & NA
& NA & NA & NA & No & Non-Smoker & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & Yes \\
54875 & 2009\_10 & male & 60 & 60-69 & 726 & Black & NA & Some College &
Divorced & 25000-34999 & 30000 & 1.86 & 6 & Rent & NotWorking & 90.0 &
NA & NA & 182.7 & 26.96 & NA & 25.0\_to\_29.9 & 54 & 135 & 80 & 146 & 80
& 136 & 80 & 134 & 80 & NA & 1.86 & 5.30 & 256 & 1.480 & NA & NA & No &
NA & Excellent & 0 & 0 & None & None & NA & NA & NA & 8 & No & No & NA &
NA & NA & NA & NA & No & NA & NA & No & Yes & Smoker & 15 & NA & NA & NA
& NA & Yes & Yes & 14 & 8 & NA & No & NA & NA \\
67957 & 2011\_12 & male & 50 & 50-59 & NA & Black & Black & 9 - 11th
Grade & Divorced & NA & NA & NA & 5 & Own & Working & 56.2 & NA & NA &
171.0 & 19.20 & NA & 18.5\_to\_24.9 & 66 & 221 & 88 & 224 & 82 & 226 &
86 & 216 & 90 & NA & NA & NA & 276 & 1.516 & NA & NA & No & NA & Good &
0 & 5 & None & Several & NA & NA & NA & 8 & No & No & 4 & 2\_hr &
0\_to\_1\_hr & NA & NA & Yes & 5 & 364 & Yes & Yes & Smoker & 20 & Yes &
18 & Yes & 18 & Yes & Yes & 15 & 1000 & 5 & No & Heterosexual & NA \\
69308 & 2011\_12 & male & 12 & 10-19 & NA & Black & Black & NA & NA &
45000-54999 & 50000 & 1.61 & 6 & Rent & NA & 61.9 & NA & NA & 167.0 &
22.20 & OverWeight & 18.5\_to\_24.9 & 96 & 118 & 16 & 112 & 34 & 118 &
32 & 118 & 0 & 106.86 & 1.19 & 3.28 & 64 & 2.370 & NA & NA & No & NA &
Excellent & 0 & 0 & NA & NA & NA & NA & NA & NA & NA & Yes & 4 & 2\_hr &
0\_to\_1\_hr & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA \\
55796 & 2009\_10 & female & 72 & 70+ & 867 & Black & NA & Some College &
Married & more 99999 & 100000 & 5.00 & 4 & Own & NotWorking & 67.7 & NA
& NA & 152.1 & 29.26 & NA & 25.0\_to\_29.9 & 60 & 158 & 60 & 168 & 72 &
162 & 58 & 154 & 62 & NA & 1.68 & 4.76 & 96 & 1.684 & NA & NA & No & NA
& Good & 0 & 10 & None & None & 3 & 2 & 16 & 6 & Yes & Yes & 2 & NA & NA
& NA & NA & Yes & NA & 0 & NA & No & Non-Smoker & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA \\
58775 & 2009\_10 & female & 69 & 60-69 & 833 & Black & NA & High School
& Divorced & 45000-54999 & 50000 & 4.16 & 5 & Own & Working & 83.6 & NA
& NA & 157.8 & 33.57 & NA & 30.0\_plus & 66 & 122 & 85 & 126 & 92 & 122
& 86 & 122 & 84 & NA & 1.97 & 5.82 & 201 & 3.045 & NA & NA & No & NA &
Good & 0 & 0 & None & None & 2 & 2 & 22 & 4 & Yes & Yes & 5 & NA & NA &
NA & NA & Yes & 2 & 15 & NA & No & Non-Smoker & NA & NA & NA & NA & NA &
No & Yes & 18 & 4 & NA & No & NA & NA \\
68592 & 2011\_12 & male & 34 & 30-39 & NA & Black & Black & Some College
& LivePartner & 75000-99999 & 87500 & 3.25 & 5 & Rent & Working & 73.5 &
NA & NA & 172.9 & 24.60 & NA & 18.5\_to\_24.9 & 76 & 127 & 75 & 126 & 74
& 128 & 72 & 126 & 78 & 449.46 & 1.37 & 5.30 & 123 & 2.929 & NA & NA &
No & NA & Good & 0 & 0 & None & None & NA & NA & NA & 6 & No & No & NA &
1\_hr & 1\_hr & NA & NA & Yes & 4 & 260 & No & Yes & Smoker & 18 & Yes &
18 & Yes & 18 & No & Yes & 16 & 25 & 1 & No & Heterosexual & NA \\
55894 & 2009\_10 & male & 7 & 0-9 & 93 & Black & NA & NA & NA &
35000-44999 & 40000 & 2.18 & 7 & Own & NA & 26.2 & NA & NA & 123.7 &
17.12 & NA & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & 1.29 & 4.76 & 247 & 1.123 & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & 2 & 2 & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA \\
62148 & 2009\_10 & female & 74 & 70+ & 897 & Black & NA & Some College &
Widowed & 10000-14999 & 12500 & 1.25 & 4 & Rent & NotWorking & 97.6 & NA
& NA & 166.1 & 35.38 & NA & 30.0\_plus & 62 & 131 & 72 & 144 & 72 & 130
& 70 & 132 & 74 & NA & NA & NA & 42 & 0.296 & NA & NA & Yes & 52 & Good
& 0 & 0 & Most & None & 5 & 5 & 16 & 9 & No & No & NA & NA & NA & NA &
NA & Yes & NA & 0 & No & Yes & Smoker & 13 & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA \\
68589 & 2011\_12 & male & 54 & 50-59 & NA & Black & Black & Some College
& Married & 65000-74999 & 70000 & 3.04 & 7 & Own & Working & 105.5 & NA
& NA & 184.7 & 30.90 & NA & 30.0\_plus & 64 & 134 & 83 & 134 & 84 & 136
& 84 & 132 & 82 & 386.91 & 1.11 & 4.09 & 69 & 0.932 & NA & NA & No & NA
& Vgood & 2 & 0 & None & None & NA & NA & NA & 3 & No & No & 7 &
0\_to\_1\_hr & 0\_hrs & NA & NA & No & NA & NA & NA & No & Non-Smoker &
NA & No & NA & No & NA & No & Yes & 28 & 3 & 0 & No & Heterosexual &
NA \\
57966 & 2009\_10 & male & 43 & 40-49 & 523 & Black & NA & Some College &
Married & more 99999 & 100000 & 5.00 & 7 & Rent & NotWorking & 131.3 &
NA & NA & 188.3 & 37.03 & NA & 30.0\_plus & 74 & 118 & 82 & 120 & 80 &
116 & 80 & 120 & 84 & NA & 2.35 & 5.77 & 95 & 0.127 & NA & NA & No & NA
& Fair & 21 & 30 & Several & Most & NA & NA & NA & 6 & Yes & No & NA &
NA & NA & NA & NA & Yes & 1 & 312 & NA & No & Non-Smoker & NA & Yes & 13
& No & NA & Yes & Yes & 18 & 11 & 1 & Yes & Heterosexual & NA \\
56527 & 2009\_10 & female & 49 & 40-49 & 596 & Black & NA & College Grad
& Divorced & more 99999 & 100000 & 5.00 & 6 & Own & Working & 69.0 & NA
& NA & 162.3 & 26.19 & NA & 25.0\_to\_29.9 & 66 & 134 & 67 & 136 & 70 &
138 & 66 & 130 & 68 & NA & 1.40 & 6.83 & 308 & 1.913 & NA & NA & No & NA
& Vgood & 1 & 2 & Several & None & 2 & 2 & 27 & 5 & No & Yes & 1 & NA &
NA & NA & NA & No & NA & NA & NA & No & Non-Smoker & NA & No & NA & No &
NA & No & Yes & 21 & 2 & 0 & No & Heterosexual & NA \\
55820 & 2009\_10 & male & 16 & 10-19 & 194 & Black & NA & NA & NA &
20000-24999 & 22500 & 1.09 & 5 & Own & NotWorking & 62.2 & NA & NA &
176.5 & 19.97 & NA & 18.5\_to\_24.9 & 82 & 115 & 53 & 112 & 48 & 114 &
48 & 116 & 58 & NA & 1.34 & 2.92 & 89 & 0.146 & NA & NA & No & NA &
Vgood & 0 & 1 & NA & NA & NA & NA & NA & 6 & Yes & Yes & 5 & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA \\
66720 & 2011\_12 & male & 58 & 50-59 & NA & Black & Black & High School
& Married & more 99999 & 100000 & 4.89 & 8 & Own & Working & 96.3 & NA &
NA & 182.0 & 29.10 & NA & 25.0\_to\_29.9 & 58 & 125 & 82 & 130 & 76 &
128 & 82 & 122 & 82 & 415.71 & 1.24 & 4.09 & 27 & 0.284 & 152 & 0.809 &
No & NA & Excellent & 0 & 0 & None & None & NA & NA & NA & 6 & No & No &
3 & 2\_hr & 0\_to\_1\_hr & NA & NA & Yes & 1 & 6 & NA & No & Non-Smoker
& NA & No & NA & No & NA & No & Yes & 12 & 7 & 2 & No & Heterosexual &
NA \\
62555 & 2011\_12 & male & 35 & 30-39 & NA & Black & Black & Some College
& LivePartner & 35000-44999 & 40000 & 1.83 & 4 & Rent & Working & 73.3 &
NA & NA & 190.5 & 20.20 & NA & 18.5\_to\_24.9 & 60 & 128 & 63 & 128 & 70
& 130 & 64 & 126 & 62 & 487.20 & 1.19 & 4.29 & 249 & 1.791 & NA & NA &
No & NA & Vgood & 0 & 2 & None & None & NA & NA & NA & 8 & No & Yes & 1
& More\_4\_hr & 4\_hr & NA & NA & No & NA & NA & NA & No & Non-Smoker &
NA & Yes & 19 & No & NA & No & Yes & 15 & 10 & 1 & No & Heterosexual &
NA \\
64043 & 2011\_12 & male & 14 & 10-19 & NA & Black & Black & NA & NA &
45000-54999 & 50000 & 2.17 & 4 & Rent & NA & 68.1 & NA & NA & 183.3 &
20.30 & NormWeight & 18.5\_to\_24.9 & 64 & 116 & 56 & 120 & 62 & 116 &
56 & 116 & 56 & 154.05 & 1.34 & 3.57 & 242 & 0.883 & NA & NA & No & NA &
Vgood & 7 & 10 & NA & NA & NA & NA & NA & NA & NA & Yes & 3 &
More\_4\_hr & 3\_hr & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
71632 & 2011\_12 & male & 18 & 10-19 & NA & Black & Black & NA & NA & NA
& NA & NA & NA & NA & NotWorking & 71.6 & NA & NA & 172.8 & 24.00 &
NormWeight & 18.5\_to\_24.9 & 72 & 101 & 54 & 106 & 44 & 102 & 52 & 100
& 56 & 471.00 & 1.03 & 4.71 & 35 & 0.065 & NA & NA & No & NA & Vgood & 0
& 0 & None & None & NA & NA & NA & 8 & No & Yes & NA & More\_4\_hr &
2\_hr & NA & NA & No & 3 & 3 & NA & NA & NA & NA & Yes & 16 & Yes & 16 &
No & Yes & 14 & 5 & 1 & No & Heterosexual & NA \\
60592 & 2009\_10 & male & 21 & 20-29 & 262 & Black & NA & 9 - 11th Grade
& NeverMarried & 20000-24999 & 22500 & 1.20 & 5 & Own & NotWorking &
75.4 & NA & NA & 188.4 & 21.24 & NA & 18.5\_to\_24.9 & 54 & 104 & 49 &
108 & 58 & 104 & 52 & 104 & 46 & NA & 1.58 & 4.63 & 74 & 0.389 & NA & NA
& No & NA & Fair & 0 & 3 & None & None & NA & NA & NA & 8 & No & Yes & 5
& NA & NA & NA & NA & Yes & 3 & 104 & No & Yes & Smoker & 18 & Yes & 18
& No & NA & No & Yes & 14 & 13 & 3 & No & Heterosexual & NA \\
65611 & 2011\_12 & female & 19 & 10-19 & NA & Hispanic & Hispanic & NA &
NA & NA & NA & NA & 4 & Rent & Working & 59.3 & NA & NA & 160.0 & 23.20
& NormWeight & 18.5\_to\_24.9 & 80 & 108 & 53 & 106 & 56 & 108 & 52 &
108 & 54 & 10.29 & 1.16 & 3.23 & 168 & 0.346 & NA & NA & No & NA & Good
& 0 & 0 & None & None & NA & NA & NA & 12 & No & No & 4 & 1\_hr & 0\_hrs
& NA & NA & No & NA & NA & NA & NA & NA & NA & No & NA & No & NA & No &
Yes & 17 & 1 & 1 & No & Heterosexual & NA \\
68270 & 2011\_12 & female & 48 & 40-49 & NA & Hispanic & Hispanic & 9 -
11th Grade & LivePartner & 55000-64999 & 60000 & 4.21 & 4 & Rent &
Working & 88.1 & NA & NA & 161.0 & 34.00 & NA & 30.0\_plus & 74 & 118 &
87 & 124 & 88 & 114 & 92 & 122 & 82 & 21.67 & 0.98 & 5.74 & 57 & 0.460 &
NA & NA & No & NA & Good & 0 & 0 & None & Several & 2 & 2 & 20 & 8 & No
& Yes & NA & 1\_hr & 0\_hrs & NA & NA & No & NA & NA & NA & No &
Non-Smoker & NA & No & NA & No & NA & No & Yes & 20 & 3 & 3 & No &
Heterosexual & NA \\
71812 & 2011\_12 & male & 3 & 0-9 & NA & Hispanic & Hispanic & NA & NA &
75000-99999 & 87500 & 2.91 & 5 & Own & NA & 17.1 & 102.8 & NA & 100.7 &
16.90 & NormWeight & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & 1\_hr & 0\_hrs & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA \\
66188 & 2011\_12 & male & 7 & 0-9 & NA & Hispanic & Hispanic & NA & NA &
20000-24999 & 22500 & 0.87 & 4 & Rent & NA & 45.8 & NA & NA & 133.1 &
25.90 & Obese & 25.0\_to\_29.9 & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & 2\_hr & 3\_hr & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA \\
69838 & 2011\_12 & male & 31 & 30-39 & NA & Hispanic & Hispanic & High
School & Married & 25000-34999 & 30000 & 1.35 & 4 & Rent & Working &
155.4 & NA & NA & 181.3 & 47.30 & NA & 30.0\_plus & 96 & 115 & 85 & 116
& 86 & 116 & 84 & 114 & 86 & 162.85 & 1.19 & 6.03 & 88 & 0.331 & NA & NA
& No & NA & Fair & 0 & 0 & None & None & NA & NA & NA & 4 & Yes & No & 7
& 3\_hr & 2\_hr & NA & NA & Yes & 3 & 9 & NA & No & Non-Smoker & NA &
Yes & 23 & No & NA & No & Yes & 17 & 25 & 3 & No & Heterosexual & NA \\
70661 & 2011\_12 & female & 65 & 60-69 & NA & Hispanic & Hispanic & Some
College & Married & more 99999 & 100000 & 5.00 & 7 & Own & NotWorking &
57.2 & NA & NA & 159.5 & 22.50 & NA & 18.5\_to\_24.9 & 80 & 147 & 67 &
150 & 54 & 150 & 72 & 144 & 62 & 21.20 & 1.40 & 5.56 & 97 & 0.924 & NA &
NA & No & NA & Vgood & 2 & 4 & None & Several & 4 & 2 & 29 & 5 & Yes &
Yes & NA & 3\_hr & 0\_hrs & NA & NA & Yes & 3 & 52 & Yes & Yes & Smoker
& 26 & NA & NA & NA & NA & No & Yes & 20 & 3 & NA & No & NA & NA \\
63899 & 2011\_12 & female & 75 & 70+ & NA & Hispanic & Hispanic & 8th
Grade & Divorced & 10000-14999 & 12500 & 1.14 & 7 & Own & NotWorking &
80.5 & NA & NA & 159.6 & 31.60 & NA & 30.0\_plus & 56 & 108 & 71 & 102 &
72 & 112 & 74 & 104 & 68 & NA & NA & NA & 20 & 0.256 & NA & NA & No & NA
& NA & NA & NA & NA & NA & NA & NA & NA & 5 & Yes & No & 4 & More\_4\_hr
& 0\_hrs & NA & NA & NA & NA & NA & NA & No & Non-Smoker & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
55613 & 2009\_10 & male & 26 & 20-29 & 317 & Hispanic & NA & Some
College & NeverMarried & 75000-99999 & 87500 & 5.00 & 5 & Rent & Working
& 82.1 & NA & NA & 175.6 & 26.63 & NA & 25.0\_to\_29.9 & 66 & 110 & 66 &
118 & 70 & 106 & 66 & 114 & 66 & NA & 1.58 & 4.32 & 146 & 0.652 & NA &
NA & No & NA & Excellent & 0 & 30 & Several & Most & NA & NA & NA & 3 &
No & No & NA & NA & NA & NA & NA & Yes & 3 & 208 & Yes & Yes & Smoker &
14 & Yes & 16 & Yes & 17 & Yes & Yes & 16 & 5 & 0 & Yes & Bisexual &
NA \\
56779 & 2009\_10 & female & 5 & 0-9 & 61 & Hispanic & NA & NA & NA &
35000-44999 & 40000 & 1.63 & 4 & Own & NA & 26.4 & NA & NA & 111.7 &
21.16 & NA & 18.5\_to\_24.9 & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & 2 & 1 & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA \\
51944 & 2009\_10 & male & 42 & 40-49 & 505 & Hispanic & NA & Some
College & Married & 15000-19999 & 17500 & 0.82 & 8 & Own & Working &
76.6 & NA & NA & 172.8 & 25.65 & NA & 25.0\_to\_29.9 & 58 & 110 & 76 &
110 & 80 & 108 & 76 & 112 & 76 & NA & 1.32 & 5.35 & 198 & 3.736 & NA &
NA & No & NA & NA & NA & NA & NA & NA & NA & NA & NA & 7 & No & No & NA
& NA & NA & NA & NA & NA & NA & NA & NA & No & Non-Smoker & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
55768 & 2009\_10 & male & 48 & 40-49 & 584 & Hispanic & NA & Some
College & Married & 45000-54999 & 50000 & 3.09 & 4 & Own & Working &
108.0 & NA & NA & 178.5 & 33.90 & NA & 30.0\_plus & 90 & 126 & 96 & 126
& 94 & 126 & 96 & NA & NA & NA & 0.91 & 4.68 & 62 & 0.620 & NA & NA &
Yes & 42 & Fair & 2 & 3 & Most & Several & NA & NA & NA & 5 & Yes & Yes
& 4 & NA & NA & NA & NA & No & NA & 0 & NA & No & Non-Smoker & NA & No &
NA & No & NA & No & Yes & 18 & 12 & 1 & No & Heterosexual & NA \\
52923 & 2009\_10 & male & 9 & 0-9 & 116 & Hispanic & NA & NA & NA & more
99999 & 100000 & 5.00 & 12 & Own & NA & 47.2 & NA & NA & 152.1 & 20.40 &
NA & 18.5\_to\_24.9 & 92 & 102 & 35 & 106 & 38 & 100 & 34 & 104 & 36 &
NA & 1.01 & 3.93 & 116 & 0.164 & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & 2 & 0 & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA \\
69636 & 2011\_12 & male & 2 & 0-9 & NA & Hispanic & Hispanic & NA & NA &
75000-99999 & 87500 & 3.58 & 5 & Own & NA & 13.1 & 93.2 & NA & 91.7 &
15.60 & NormWeight & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & 5 & 3\_hr & 0\_hrs & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA \\
66886 & 2011\_12 & male & 5 & 0-9 & NA & Hispanic & Hispanic & NA & NA &
25000-34999 & 30000 & 0.83 & 6 & Rent & NA & 19.5 & NA & NA & 111.8 &
15.60 & NormWeight & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & 3\_hr & 1\_hr & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA \\
62481 & 2011\_12 & female & 39 & 30-39 & NA & Hispanic & Hispanic &
College Grad & Separated & 65000-74999 & 70000 & 3.13 & 5 & Rent &
Working & 80.5 & NA & NA & 170.5 & 27.70 & NA & 25.0\_to\_29.9 & 78 & 95
& 58 & 98 & 56 & 96 & 62 & 94 & 54 & 48.93 & 1.19 & 3.83 & 282 & 1.300 &
NA & NA & No & NA & Vgood & 2 & 30 & None & None & 4 & 3 & 23 & 6 & No &
No & NA & 1\_hr & 3\_hr & NA & NA & Yes & 2 & 6 & NA & No & Non-Smoker &
NA & No & NA & No & NA & No & Yes & 20 & 25 & 2 & No & Heterosexual &
No \\
63037 & 2011\_12 & female & 41 & 40-49 & NA & Hispanic & Hispanic & 8th
Grade & Separated & 20000-24999 & 22500 & 1.36 & 3 & Rent & Working &
78.9 & NA & NA & 156.5 & 32.20 & NA & 30.0\_plus & 66 & 99 & 69 & 110 &
70 & 102 & 70 & 96 & 68 & NA & NA & NA & 74 & 0.569 & NA & NA & No & NA
& Fair & 8 & 5 & Several & None & 5 & 5 & 15 & 6 & No & No & 6 &
0\_to\_1\_hr & 0\_hrs & NA & NA & Yes & 2 & 1 & NA & No & Non-Smoker &
NA & No & NA & No & NA & No & No & NA & 0 & 0 & No & Heterosexual &
No \\
52709 & 2009\_10 & female & 15 & 10-19 & 185 & Hispanic & NA & NA & NA &
more 99999 & 100000 & 4.99 & 4 & Own & NA & 54.5 & NA & NA & 172.4 &
18.34 & NA & 12.0\_18.5 & 72 & 115 & 71 & 122 & 76 & 112 & 74 & 118 & 68
& NA & NA & NA & 113 & 0.419 & NA & NA & No & NA & Good & 0 & 0 & NA &
NA & NA & NA & NA & NA & NA & Yes & 6 & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA \\
61670 & 2009\_10 & male & 4 & 0-9 & 51 & Hispanic & NA & NA & NA &
10000-14999 & 12500 & 0.64 & 7 & Rent & NA & 16.2 & NA & NA & 102.1 &
15.54 & NA & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & 2 & 1 & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
68664 & 2011\_12 & male & 6 & 0-9 & NA & Hispanic & Hispanic & NA & NA &
more 99999 & 100000 & 5.00 & 4 & Own & NA & 25.2 & NA & NA & 118.4 &
18.00 & OverWeight & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA
& NA & 1.11 & 1.34 & 3.93 & 53 & 0.373 & NA & NA & No & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & 1\_hr & 0\_to\_1\_hr &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA \\
68644 & 2011\_12 & female & 1 & 0-9 & 13 & Hispanic & Hispanic & NA & NA
& 20000-24999 & 22500 & 0.80 & 4 & Rent & NA & 9.8 & 75.7 & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & 2 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
69262 & 2011\_12 & female & 2 & 0-9 & NA & Mexican & Mexican & NA & NA &
75000-99999 & 87500 & 3.58 & 5 & Rent & NA & 12.1 & 85.3 & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & More\_4\_hr & 1\_hr & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
60992 & 2009\_10 & female & 5 & 0-9 & 64 & Mexican & NA & NA & NA &
25000-34999 & 30000 & 0.85 & 6 & Own & NA & 19.8 & NA & NA & 110.5 &
16.22 & NA & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & 2 & 0 & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
63596 & 2011\_12 & male & 74 & 70+ & NA & Mexican & Mexican & 9 - 11th
Grade & Married & 35000-44999 & 40000 & 3.58 & 4 & Own & NotWorking &
99.3 & NA & NA & 175.2 & 32.40 & NA & 30.0\_plus & 68 & 124 & 77 & 126 &
82 & 118 & 74 & 130 & 80 & 475.35 & 0.96 & 5.20 & 69 & 0.359 & NA & NA &
No & NA & Good & 0 & 14 & None & None & NA & NA & NA & 6 & No & Yes & NA
& More\_4\_hr & 0\_hrs & NA & NA & Yes & 2 & 36 & No & Yes & Smoker & 19
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
64542 & 2011\_12 & female & 5 & 0-9 & NA & Mexican & Mexican & NA & NA &
NA & NA & NA & 7 & Other & NA & 26.8 & NA & NA & 123.9 & 17.50 &
OverWeight & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & 2 & 1\_hr & 0\_hrs & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA \\
56740 & 2009\_10 & female & 56 & 50-59 & 673 & Mexican & NA & 8th Grade
& Widowed & 10000-14999 & 12500 & 0.66 & 4 & Rent & Working & 75.0 & NA
& NA & 144.4 & 35.97 & NA & 30.0\_plus & 82 & 146 & 65 & 156 & 70 & 148
& 68 & 144 & 62 & NA & 1.32 & 5.02 & 52 & 0.468 & NA & NA & No & NA & NA
& NA & NA & NA & NA & NA & NA & NA & 8 & No & No & NA & NA & NA & NA &
NA & NA & NA & NA & No & Yes & Smoker & 39 & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA \\
66424 & 2011\_12 & female & 14 & 10-19 & NA & Mexican & Mexican & NA &
NA & 45000-54999 & 50000 & 1.42 & 5 & Rent & NA & 44.8 & NA & NA & 155.2
& 18.60 & NormWeight & 18.5\_to\_24.9 & 82 & 105 & 59 & 104 & 68 & 104 &
58 & 106 & 60 & 15.95 & 1.89 & 4.47 & 158 & NA & NA & NA & No & NA &
Good & 0 & 0 & NA & NA & NA & NA & NA & NA & NA & Yes & 6 & 3\_hr &
1\_hr & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA \\
70494 & 2011\_12 & male & 25 & 20-29 & NA & Mexican & Mexican & High
School & NeverMarried & 35000-44999 & 40000 & 1.97 & 5 & Own & Looking &
104.9 & NA & NA & 168.1 & 37.10 & NA & 30.0\_plus & 74 & 132 & 84 & 140
& 92 & 134 & 82 & 130 & 86 & 217.00 & 0.93 & 5.74 & 181 & 1.403 & NA &
NA & No & NA & Good & 0 & 0 & None & Several & NA & NA & NA & 7 & No &
No & NA & 1\_hr & 0\_to\_1\_hr & NA & NA & Yes & 6 & 52 & NA & No &
Non-Smoker & NA & Yes & 16 & No & NA & No & Yes & 15 & 4 & 1 & No &
Heterosexual & NA \\
54860 & 2009\_10 & female & 13 & 10-19 & 162 & Mexican & NA & NA & NA &
more 99999 & 100000 & 3.39 & 10 & Own & NA & 50.3 & NA & NA & 158.7 &
19.97 & NA & 18.5\_to\_24.9 & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & 1.40 & 3.83 & 66 & 1.000 & NA & NA & No & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA \\
62714 & 2011\_12 & male & 75 & 70+ & NA & Mexican & Mexican & 8th Grade
& Married & 10000-14999 & 12500 & 0.68 & 5 & Own & NotWorking & 67.6 &
NA & NA & 158.2 & 27.00 & NA & 25.0\_to\_29.9 & 66 & 125 & 72 & 126 & 64
& 124 & 72 & 126 & 72 & 314.00 & 1.22 & 6.65 & 32 & 0.471 & NA & NA &
Yes & 55 & Fair & 20 & 30 & NA & NA & NA & NA & NA & 6 & Yes & No & 2 &
More\_4\_hr & 0\_hrs & NA & NA & NA & NA & NA & NA & No & Non-Smoker &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
58710 & 2009\_10 & female & 43 & 40-49 & 527 & Mexican & NA & 9 - 11th
Grade & Married & 35000-44999 & 40000 & 1.44 & 10 & Own & NotWorking &
76.0 & NA & NA & 163.7 & 28.36 & NA & 25.0\_to\_29.9 & 66 & 114 & 74 &
114 & 76 & 116 & 76 & 112 & 72 & NA & 1.32 & 6.54 & 29 & NA & NA & NA &
No & NA & Good & 0 & 0 & Most & Several & 3 & 3 & 22 & 8 & No & No & NA
& NA & NA & NA & NA & No & 1 & 24 & NA & No & Non-Smoker & NA & No & NA
& No & NA & No & Yes & 21 & 1 & 1 & No & Heterosexual & No \\
69311 & 2011\_12 & female & 16 & 10-19 & NA & Mexican & Mexican & NA &
NA & 55000-64999 & 60000 & 2.46 & 7 & Own & NotWorking & 62.6 & NA & NA
& 167.2 & 22.40 & NormWeight & 18.5\_to\_24.9 & 72 & 101 & 61 & 104 & 68
& 104 & 60 & 98 & 62 & 28.30 & 1.68 & 4.63 & 291 & NA & NA & NA & No &
NA & Good & 0 & 0 & NA & NA & NA & NA & NA & 7 & No & No & 5 & 3\_hr &
0\_to\_1\_hr & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA \\
60210 & 2009\_10 & female & 20 & 20-29 & 240 & Mexican & NA & College
Grad & NeverMarried & 55000-64999 & 60000 & 4.12 & 4 & Rent & Working &
59.1 & NA & NA & 160.6 & 22.91 & NA & 18.5\_to\_24.9 & 78 & 98 & 67 & 94
& 68 & 96 & 68 & 100 & 66 & NA & 1.01 & 3.00 & 88 & NA & NA & NA & No &
NA & Vgood & 0 & 0 & None & None & NA & NA & NA & 8 & No & Yes & 4 & NA
& NA & NA & NA & Yes & 3 & 36 & NA & No & Non-Smoker & NA & Yes & 17 &
No & NA & No & Yes & 15 & 11 & 1 & No & Heterosexual & No \\
60307 & 2009\_10 & male & 8 & 0-9 & 102 & Mexican & NA & NA & NA & more
99999 & 100000 & 3.97 & 9 & Own & NA & 23.8 & NA & NA & 123.3 & 15.65 &
NA & 12.0\_18.5 & 70 & 98 & 0 & 90 & 0 & 96 & 0 & 100 & 0 & NA & 1.34 &
3.88 & 103 & 0.880 & NA & NA & No & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & 3 & 1 & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
62098 & 2009\_10 & male & 38 & 30-39 & 459 & Mexican & NA & Some College
& Married & NA & NA & NA & NA & NA & Working & 72.9 & NA & NA & 174.0 &
24.08 & NA & 18.5\_to\_24.9 & 66 & 107 & 74 & 110 & 70 & 108 & 72 & 106
& 76 & NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & 8 & No & No & NA & NA & NA & NA & NA & NA & NA & NA
& Yes & Yes & Smoker & 16 & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA \\
57227 & 2009\_10 & female & 27 & 20-29 & 332 & Mexican & NA & 8th Grade
& Married & 20000-24999 & 22500 & 0.81 & 4 & Rent & NotWorking & 65.6 &
NA & NA & 156.7 & 26.72 & NA & 25.0\_to\_29.9 & 86 & 95 & 42 & 96 & 42 &
94 & 42 & 96 & 42 & NA & 1.97 & 4.11 & 302 & 3.823 & NA & NA & No & NA &
NA & NA & NA & NA & NA & NA & NA & NA & 5 & No & No & NA & NA & NA & NA
& NA & NA & NA & NA & NA & No & Non-Smoker & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & Yes \\
62557 & 2011\_12 & male & 10 & 10-19 & NA & Mexican & Mexican & NA & NA
& 15000-19999 & 17500 & 0.46 & 5 & Rent & NA & 35.7 & NA & NA & 141.5 &
17.80 & NormWeight & 12.0\_18.5 & 66 & 97 & 64 & 96 & 62 & 98 & 64 & 96
& 64 & NA & NA & NA & 57 & 0.187 & NA & NA & No & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & 3 & 3\_hr & 1\_hr & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA \\
61770 & 2009\_10 & male & 40 & 40-49 & 485 & Mexican & NA & 8th Grade &
Married & 65000-74999 & 70000 & 1.74 & 6 & Own & Working & 81.5 & NA &
NA & 170.4 & 28.07 & NA & 25.0\_to\_29.9 & 74 & 142 & 85 & 140 & 90 &
138 & 86 & 146 & 84 & NA & 0.96 & 5.72 & 333 & 2.250 & NA & NA & No & NA
& Good & 0 & 0 & None & None & NA & NA & NA & 7 & No & No & NA & NA & NA
& NA & NA & Yes & 1 & 24 & NA & No & Non-Smoker & NA & No & NA & No & NA
& No & Yes & 14 & 5 & 1 & No & Heterosexual & NA \\
61106 & 2009\_10 & male & 53 & 50-59 & 641 & Mexican & NA & 8th Grade &
Married & 25000-34999 & 30000 & 1.36 & 5 & Own & Working & 90.9 & NA &
NA & 171.5 & 30.91 & NA & 30.0\_plus & 74 & 102 & 71 & 114 & 74 & 104 &
74 & 100 & 68 & NA & 1.01 & 5.46 & 164 & 0.845 & NA & NA & No & NA &
Good & 0 & 0 & None & None & NA & NA & NA & 9 & No & No & NA & NA & NA &
NA & NA & Yes & NA & 0 & Yes & Yes & Smoker & 20 & No & NA & No & NA &
No & Yes & 18 & 2 & 1 & No & Heterosexual & NA \\
52089 & 2009\_10 & male & 2 & 0-9 & 32 & Mexican & NA & NA & NA &
15000-19999 & 17500 & 0.87 & 4 & Rent & NA & 14.2 & 93.6 & NA & 93.7 &
16.17 & NA & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & 0 & 6 & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
65578 & 2011\_12 & female & 1 & 0-9 & 18 & Mexican & Mexican & NA & NA &
45000-54999 & 50000 & 1.91 & 7 & Own & NA & 12.2 & 82.8 & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & 4 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
63352 & 2011\_12 & male & 64 & 60-69 & NA & White & White & High School
& Married & more 99999 & 100000 & 5.00 & 3 & Own & Working & 78.1 & NA &
NA & 184.8 & 22.90 & NA & 18.5\_to\_24.9 & 82 & 138 & 89 & 136 & 86 &
138 & 90 & 138 & 88 & 933.36 & 1.11 & 5.35 & 231 & 0.724 & NA & NA & Yes
& 60 & Good & 0 & 7 & None & None & NA & NA & NA & 6 & No & No & 3 &
3\_hr & 0\_to\_1\_hr & NA & NA & Yes & 5 & 364 & Yes & Yes & Smoker & 17
& NA & NA & NA & NA & Yes & Yes & 23 & 1 & NA & No & NA & NA \\
52964 & 2009\_10 & female & 80 & NA & NA & White & NA & College Grad &
Married & more 99999 & 100000 & 5.00 & 8 & Own & NotWorking & 45.3 & NA
& NA & 148.0 & 20.68 & NA & 18.5\_to\_24.9 & 66 & 146 & 70 & 146 & 74 &
148 & 68 & 144 & 72 & NA & 2.40 & 5.61 & 99 & 1.650 & NA & NA & No & NA
& NA & NA & NA & NA & NA & NA & NA & NA & 7 & No & Yes & 3 & NA & NA &
NA & NA & NA & NA & NA & No & Yes & Smoker & 20 & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA \\
54654 & 2009\_10 & male & 17 & 10-19 & 204 & White & NA & NA & NA &
35000-44999 & 40000 & 1.55 & 7 & Own & Working & 53.5 & NA & NA & 173.8
& 17.71 & NA & 12.0\_18.5 & 72 & 97 & 59 & 98 & 66 & 100 & 60 & 94 & 58
& NA & 0.96 & 3.98 & NA & NA & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & 10 & No & Yes & 1 & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA \\
52335 & 2009\_10 & female & 20 & 20-29 & 250 & White & NA & High School
& NeverMarried & 5000-9999 & 7500 & 0.24 & 8 & Rent & NotWorking & 109.7
& NA & NA & 160.9 & 42.37 & NA & 30.0\_plus & 86 & 107 & 54 & 104 & 54 &
106 & 52 & 108 & 56 & NA & 1.11 & 2.38 & 168 & 1.159 & NA & NA & No & NA
& Fair & 0 & 0 & Several & None & 4 & 2 & 16 & 4 & Yes & No & NA & NA &
NA & NA & NA & No & NA & NA & NA & No & Non-Smoker & NA & No & NA & No &
NA & No & Yes & 15 & 3 & 0 & No & Heterosexual & No \\
71744 & 2011\_12 & female & 40 & 40-49 & NA & White & White & Some
College & Married & more 99999 & 100000 & 4.17 & 8 & Own & NotWorking &
57.9 & NA & NA & 154.4 & 24.30 & NA & 18.5\_to\_24.9 & 72 & 114 & 79 &
NA & NA & 116 & 80 & 112 & 78 & 25.25 & 1.32 & 4.60 & 74 & 0.587 & NA &
NA & No & NA & Vgood & 0 & 0 & None & None & 7 & 3 & 17 & 8 & No & No &
NA & 1\_hr & 0\_to\_1\_hr & NA & NA & No & NA & 0 & NA & No & Non-Smoker
& NA & Yes & 15 & Yes & 15 & No & Yes & 15 & 5 & 1 & No & Heterosexual &
No \\
59327 & 2009\_10 & female & 44 & 40-49 & 529 & White & NA & College Grad
& Married & 75000-99999 & 87500 & 5.00 & 8 & Own & Working & 51.0 & NA &
NA & 165.4 & 18.64 & NA & 18.5\_to\_24.9 & 62 & 110 & 70 & NA & NA & 112
& 68 & 108 & 72 & NA & 2.46 & 6.05 & 61 & 0.452 & NA & NA & No & NA &
Vgood & 0 & 30 & None & Several & 1 & NA & NA & 6 & Yes & Yes & 3 & NA &
NA & NA & NA & Yes & 3 & 260 & No & Yes & Smoker & NA & Yes & 18 & No &
NA & Yes & Yes & 20 & 30 & 0 & No & Heterosexual & No \\
66915 & 2011\_12 & male & 51 & 50-59 & NA & White & White & 9 - 11th
Grade & Married & 15000-19999 & 17500 & 0.99 & 4 & Rent & NotWorking &
109.4 & NA & NA & 184.4 & 32.20 & NA & 30.0\_plus & 60 & 136 & 92 & 130
& 90 & 138 & 88 & 134 & 96 & 213.69 & 0.88 & 6.28 & 238 & 1.935 & NA &
NA & No & NA & Poor & 30 & 0 & None & Most & NA & NA & NA & 8 & Yes & No
& 4 & More\_4\_hr & 0\_hrs & NA & NA & Yes & NA & 0 & Yes & Yes & Smoker
& 14 & Yes & 14 & Yes & 14 & Yes & Yes & 14 & 20 & 0 & No & Heterosexual
& NA \\
56846 & 2009\_10 & male & 32 & 30-39 & 385 & White & NA & Some College &
Married & more 99999 & 100000 & 4.67 & 6 & Own & Working & 98.5 & NA &
NA & 171.7 & 33.41 & NA & 30.0\_plus & 88 & 122 & 94 & 124 & 100 & 126 &
94 & 118 & 94 & NA & 0.80 & 5.82 & 361 & 3.223 & NA & NA & No & NA &
Vgood & 0 & 0 & None & None & NA & NA & NA & 7 & No & Yes & 1 & NA & NA
& NA & NA & Yes & 2 & 104 & NA & No & Non-Smoker & NA & Yes & 22 & No &
NA & No & Yes & 15 & 3 & 1 & No & Heterosexual & NA \\
51646 & 2009\_10 & male & 8 & 0-9 & 101 & White & NA & NA & NA &
55000-64999 & 60000 & 2.33 & 7 & Own & NA & 35.2 & NA & NA & 130.6 &
20.64 & NA & 18.5\_to\_24.9 & 72 & 107 & 37 & 114 & 46 & 108 & 36 & 106
& 38 & NA & 1.55 & 4.09 & 238 & 1.322 & NA & NA & No & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & 1 & 6 & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA \\
57815 & 2009\_10 & male & 53 & 50-59 & 642 & White & NA & High School &
Married & more 99999 & 100000 & 5.00 & 8 & Own & Working & 75.4 & NA &
NA & 173.2 & 25.13 & NA & 25.0\_to\_29.9 & 72 & 114 & 81 & 136 & 82 &
116 & 82 & 112 & 80 & NA & 1.16 & 4.81 & 57 & 0.588 & NA & NA & No & NA
& Vgood & 0 & 0 & None & None & NA & NA & NA & 8 & No & No & NA & NA &
NA & NA & NA & Yes & 2 & 12 & NA & No & Non-Smoker & NA & No & NA & No &
NA & No & Yes & 19 & 2 & 1 & No & Heterosexual & NA \\
70234 & 2011\_12 & male & 63 & 60-69 & NA & White & White & 8th Grade &
NeverMarried & 5000-9999 & 7500 & 0.87 & 3 & Rent & Working & 87.3 & NA
& NA & 172.1 & 29.50 & NA & 25.0\_to\_29.9 & 66 & 103 & 65 & 104 & 62 &
104 & 66 & 102 & 64 & 597.48 & 1.14 & 5.20 & 266 & 4.222 & NA & NA & No
& NA & Vgood & 0 & 5 & NA & NA & NA & NA & NA & 9 & No & No & NA & 2\_hr
& 0\_hrs & NA & NA & No & NA & NA & NA & No & Non-Smoker & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
67536 & 2011\_12 & female & 14 & 10-19 & NA & White & White & NA & NA &
more 99999 & 100000 & 5.00 & 13 & Own & NA & 61.9 & NA & NA & 165.2 &
22.70 & NormWeight & 18.5\_to\_24.9 & 92 & 111 & 67 & 116 & 80 & 112 &
66 & 110 & 68 & 14.03 & 1.19 & 3.36 & 65 & 0.330 & NA & NA & No & NA &
Good & 0 & 2 & NA & NA & NA & NA & NA & NA & NA & Yes & NA & 2\_hr &
1\_hr & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA \\
71855 & 2011\_12 & female & 62 & 60-69 & NA & White & White & High
School & Married & 10000-14999 & 12500 & 0.98 & 4 & Own & NotWorking &
99.0 & NA & NA & 166.1 & 35.90 & NA & 30.0\_plus & 118 & 143 & 75 & 162
& 74 & 150 & 76 & 136 & 74 & 155.79 & 1.03 & 6.75 & 35 & 0.114 & NA & NA
& Yes & 32 & Good & 30 & 7 & Most & Several & NA & NA & NA & 10 & No &
No & 5 & 2\_hr & 0\_to\_1\_hr & NA & NA & Yes & NA & 0 & No & Yes &
Smoker & 21 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA \\
56041 & 2009\_10 & female & 62 & 60-69 & 753 & White & NA & Some College
& Married & 35000-44999 & 40000 & 2.40 & 9 & Own & Working & 91.1 & NA &
NA & 170.2 & 31.45 & NA & 30.0\_plus & 66 & 130 & 67 & 140 & 66 & 134 &
64 & 126 & 70 & NA & 1.14 & 5.17 & 122 & 0.731 & NA & NA & No & NA &
Vgood & 0 & 0 & None & None & 1 & 1 & NA & 6 & No & No & NA & NA & NA &
NA & NA & Yes & 1 & 180 & No & Yes & Smoker & 18 & NA & NA & NA & NA &
No & Yes & 16 & 5 & NA & No & NA & NA \\
52293 & 2009\_10 & female & 56 & 50-59 & 672 & White & NA & College Grad
& Married & more 99999 & 100000 & 5.00 & 7 & Own & Working & 63.0 & NA &
NA & 167.8 & 22.37 & NA & 18.5\_to\_24.9 & 70 & 148 & 81 & 162 & 88 &
150 & 82 & 146 & 80 & NA & 3.44 & 5.51 & 358 & 5.042 & NA & NA & No & NA
& Vgood & 0 & 0 & Several & None & 1 & 1 & NA & 5 & Yes & Yes & 3 & NA &
NA & NA & NA & Yes & 2 & 260 & NA & No & Non-Smoker & NA & Yes & 21 & No
& NA & No & Yes & 19 & 20 & 1 & No & Heterosexual & NA \\
66473 & 2011\_12 & male & 56 & 50-59 & NA & White & White & College Grad
& NeverMarried & more 99999 & 100000 & 5.00 & 6 & Own & Working & 74.5 &
NA & NA & 179.1 & 23.20 & NA & 18.5\_to\_24.9 & 76 & 132 & 94 & 140 &
100 & 132 & 94 & 132 & 94 & 358.53 & 1.32 & 5.43 & 256 & 0.859 & NA & NA
& No & NA & Vgood & 0 & 30 & Most & Most & NA & NA & NA & 4 & Yes & Yes
& 2 & 1\_hr & 3\_hr & NA & NA & Yes & NA & 0 & No & Yes & Smoker & 18 &
Yes & 24 & No & NA & No & Yes & 20 & 1 & 0 & No & Heterosexual & NA \\
60687 & 2009\_10 & female & 5 & 0-9 & 60 & White & NA & NA & NA & 0-4999
& 2500 & 0.07 & 7 & Rent & NA & 19.6 & NA & NA & 110.5 & 16.05 & NA &
12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & 2 & 0 & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
59059 & 2009\_10 & male & 58 & 50-59 & 706 & White & NA & Some College &
Married & 75000-99999 & 87500 & 5.00 & 7 & Own & NotWorking & 103.4 & NA
& NA & 186.3 & 29.79 & NA & 25.0\_to\_29.9 & 74 & 119 & 62 & 120 & 58 &
120 & 62 & 118 & 62 & NA & 1.37 & 5.07 & 67 & 0.807 & NA & NA & No & NA
& Good & 0 & 0 & None & None & NA & NA & NA & 6 & No & Yes & 3 & NA & NA
& NA & NA & Yes & 1 & 260 & No & Yes & Smoker & 15 & Yes & 19 & No & NA
& No & Yes & 17 & 20 & 1 & No & Heterosexual & NA \\
68294 & 2011\_12 & female & 11 & 10-19 & NA & White & White & NA & NA &
75000-99999 & 87500 & 3.30 & 6 & Own & NA & 78.8 & NA & NA & 161.3 &
30.30 & Obese & 30.0\_plus & 88 & 98 & 71 & NA & NA & 100 & 72 & 96 & 70
& 11.39 & 1.01 & 3.52 & 18 & 0.171 & 29 & 0.397 & No & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & 4\_hr & 1\_hr & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA \\
63312 & 2011\_12 & male & 74 & 70+ & NA & White & White & High School &
Married & 35000-44999 & 40000 & 2.92 & 6 & Own & NotWorking & 83.6 & NA
& NA & 188.8 & 23.50 & NA & 18.5\_to\_24.9 & 96 & 102 & 56 & 114 & 68 &
108 & 60 & 96 & 52 & 608.64 & 1.55 & 4.16 & 9 & 0.040 & NA & NA & No &
NA & Good & 0 & 0 & Several & None & NA & NA & NA & 7 & No & No & NA &
More\_4\_hr & 0\_to\_1\_hr & NA & NA & Yes & 2 & 3 & Yes & Yes & Smoker
& 17 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
53068 & 2009\_10 & female & 55 & 50-59 & 670 & Other & NA & Some College
& LivePartner & NA & NA & 5.00 & 5 & Own & Working & 58.9 & NA & NA &
152.1 & 25.46 & NA & 25.0\_to\_29.9 & 74 & 127 & 68 & 134 & 72 & 128 &
70 & 126 & 66 & NA & 1.24 & 4.89 & 7 & 0.101 & 20 & 0.333 & No & NA &
Vgood & 4 & 0 & None & None & 1 & 1 & NA & 8 & No & No & NA & NA & NA &
NA & NA & Yes & 3 & 3 & No & Yes & Smoker & 19 & Yes & 19 & Yes & 22 &
No & Yes & 18 & 6 & 1 & No & Heterosexual & NA \\
66065 & 2011\_12 & female & 2 & 0-9 & NA & Other & Asian & NA & NA &
75000-99999 & 87500 & 4.50 & 8 & Own & NA & 9.6 & 85.4 & NA & 85.0 &
13.30 & UnderWeight & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & 6 & 0\_to\_1\_hr & 0\_hrs & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA \\
61277 & 2009\_10 & female & 16 & 10-19 & 196 & Other & NA & NA & NA &
15000-19999 & 17500 & 0.87 & 7 & Own & NotWorking & 84.3 & NA & NA &
159.0 & 33.35 & NA & 30.0\_plus & 68 & 108 & 79 & 98 & 76 & 110 & 76 &
106 & 82 & NA & 1.34 & 3.75 & 382 & 1.598 & NA & NA & No & NA & Good & 0
& 5 & NA & NA & NA & NA & NA & 8 & No & No & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA \\
64440 & 2011\_12 & male & 40 & 40-49 & NA & Other & Asian & College Grad
& NeverMarried & NA & NA & NA & 8 & Own & Working & 83.0 & NA & NA &
178.0 & 26.20 & NA & 25.0\_to\_29.9 & 62 & 147 & 93 & 152 & 92 & 144 &
92 & 150 & 94 & 264.81 & 1.66 & 5.28 & 99 & 0.647 & NA & NA & No & NA &
Vgood & 0 & 0 & None & None & NA & NA & NA & 6 & No & Yes & NA & 2\_hr &
1\_hr & NA & NA & No & NA & NA & NA & No & Non-Smoker & NA & No & NA &
No & NA & No & Yes & 32 & 6 & 2 & No & Heterosexual & NA \\
66402 & 2011\_12 & male & 60 & 60-69 & NA & Other & Other & Some College
& LivePartner & more 99999 & 100000 & 2.30 & 6 & Own & Working & 82.6 &
NA & NA & 177.4 & 26.20 & NA & 25.0\_to\_29.9 & 68 & 101 & 68 & 106 & 60
& 98 & 70 & 104 & 66 & 371.37 & 1.37 & 4.29 & 136 & 1.071 & NA & NA & No
& NA & Good & 0 & 0 & None & None & NA & NA & NA & 8 & No & No & NA &
3\_hr & 0\_to\_1\_hr & NA & NA & Yes & 4 & 2 & Yes & Yes & Smoker & 17 &
NA & NA & NA & NA & Yes & Yes & 14 & 12 & NA & No & NA & NA \\
69402 & 2011\_12 & female & 59 & 50-59 & NA & Other & Asian & College
Grad & NeverMarried & 75000-99999 & 87500 & 5.00 & 4 & Rent & Working &
65.7 & NA & NA & 158.4 & 26.20 & NA & 25.0\_to\_29.9 & 70 & 104 & 62 &
104 & 56 & 104 & 62 & NA & NA & 17.99 & 1.58 & 5.53 & 172 & 0.217 & NA &
NA & No & NA & Excellent & 21 & 0 & None & None & NA & NA & NA & 5 & No
& Yes & 2 & 3\_hr & 0\_hrs & NA & NA & No & NA & NA & NA & No &
Non-Smoker & NA & No & NA & No & NA & No & No & NA & 0 & 0 & No & NA &
NA \\
68448 & 2011\_12 & female & 28 & 20-29 & NA & Other & Other & College
Grad & NeverMarried & 35000-44999 & 40000 & 3.22 & 2 & Rent & Working &
61.8 & NA & NA & 160.9 & 23.90 & NA & 18.5\_to\_24.9 & 74 & 104 & 62 &
102 & 62 & 104 & 60 & 104 & 64 & NA & NA & NA & 34 & 0.183 & 77 & 0.510
& No & NA & Vgood & 0 & 0 & None & None & NA & NA & NA & 7 & No & Yes &
3 & 0\_hrs & 1\_hr & NA & NA & Yes & 1 & 5 & NA & No & Non-Smoker & NA &
No & NA & No & NA & No & Yes & 21 & 5 & 2 & No & Heterosexual & No \\
59005 & 2009\_10 & female & 20 & 20-29 & 242 & Other & NA & Some College
& NeverMarried & 0-4999 & 2500 & 0.28 & 2 & Rent & Working & 54.6 & NA &
NA & 148.9 & 24.63 & NA & 18.5\_to\_24.9 & 72 & 105 & 60 & 102 & 58 &
106 & 58 & 104 & 62 & NA & 2.04 & 4.09 & 298 & 1.187 & NA & NA & No & NA
& Fair & 0 & 0 & None & None & NA & NA & NA & 7 & No & Yes & 1 & NA & NA
& NA & NA & Yes & 1 & 6 & NA & No & Non-Smoker & NA & No & NA & No & NA
& No & Yes & 18 & 2 & 1 & No & Heterosexual & No \\
62612 & 2011\_12 & female & 38 & 30-39 & NA & Other & Asian & College
Grad & Married & 55000-64999 & 60000 & 2.68 & 5 & Rent & Working & 69.6
& NA & NA & 151.0 & 30.50 & NA & 30.0\_plus & 78 & 108 & 74 & 104 & 74 &
108 & 74 & 108 & 74 & 21.80 & 1.24 & 5.77 & 32 & 0.227 & 116 & 0.699 &
No & NA & NA & NA & NA & NA & NA & NA & NA & NA & 7 & No & No & 3 &
0\_to\_1\_hr & 0\_to\_1\_hr & NA & NA & NA & NA & NA & NA & No &
Non-Smoker & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
No \\
51857 & 2009\_10 & female & 44 & 40-49 & 538 & Other & NA & College Grad
& LivePartner & more 99999 & 100000 & 4.62 & 7 & Own & Working & 57.8 &
NA & NA & 155.1 & 24.03 & NA & 18.5\_to\_24.9 & 94 & 93 & 64 & 94 & 64 &
92 & 66 & 94 & 62 & NA & 1.71 & 4.19 & 57 & 0.210 & NA & NA & No & NA &
Vgood & 0 & 2 & None & None & NA & NA & NA & 7 & No & Yes & 2 & NA & NA
& NA & NA & No & 1 & 12 & Yes & Yes & Smoker & 20 & Yes & 16 & No & NA &
No & Yes & 20 & 4 & 1 & No & Heterosexual & No \\
71833 & 2011\_12 & female & 30 & 30-39 & NA & Other & Asian & College
Grad & Married & 65000-74999 & 70000 & 4.76 & 4 & Rent & NotWorking &
48.8 & NA & NA & 158.2 & 19.50 & NA & 18.5\_to\_24.9 & 84 & 83 & 54 & 88
& 56 & 86 & 60 & 80 & 48 & 17.49 & 1.27 & 5.12 & 23 & 0.793 & 68 & 0.791
& No & NA & Good & 7 & 0 & None & None & NA & NA & NA & 8 & No & No & NA
& 1\_hr & 2\_hr & NA & NA & Yes & 1 & 12 & NA & No & Non-Smoker & NA &
No & NA & No & NA & No & Yes & 20 & 8 & 1 & No & Heterosexual & No \\
62549 & 2011\_12 & male & 53 & 50-59 & NA & Other & Asian & High School
& Married & 45000-54999 & 50000 & 2.01 & 6 & Own & Working & 65.9 & NA &
NA & 174.9 & 21.50 & NA & 18.5\_to\_24.9 & 78 & 120 & 73 & 114 & 74 &
118 & 72 & 122 & 74 & 459.85 & 1.27 & 5.17 & 50 & 0.581 & NA & NA & No &
NA & NA & NA & NA & NA & NA & NA & NA & NA & 7 & No & No & NA & 2\_hr &
0\_to\_1\_hr & NA & NA & NA & NA & NA & NA & No & Non-Smoker & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
65625 & 2011\_12 & male & 23 & 20-29 & NA & Other & Asian & College Grad
& NeverMarried & 5000-9999 & 7500 & 0.73 & 3 & Rent & Working & 113.6 &
NA & NA & 171.9 & 38.40 & NA & 30.0\_plus & 94 & 114 & 75 & 114 & 70 &
112 & 72 & 116 & 78 & 325.70 & 1.14 & 3.83 & 50 & 0.581 & NA & NA & Yes
& 15 & Vgood & 0 & 0 & None & None & NA & NA & NA & 8 & No & Yes & 2 &
3\_hr & 4\_hr & NA & NA & Yes & 4 & 52 & NA & No & Non-Smoker & NA & Yes
& 18 & Yes & 19 & No & No & NA & 0 & 0 & No & Heterosexual & NA \\
56213 & 2009\_10 & male & 29 & 20-29 & 352 & Other & NA & Some College &
NeverMarried & 65000-74999 & 70000 & 3.23 & 4 & Rent & NotWorking &
116.9 & NA & NA & 185.8 & 33.86 & NA & 30.0\_plus & 84 & 108 & 79 & 106
& 70 & 110 & 78 & 106 & 80 & NA & 1.03 & 5.25 & 189 & 0.344 & NA & NA &
No & NA & NA & NA & NA & NA & NA & NA & NA & NA & 7 & No & Yes & 4 & NA
& NA & NA & NA & NA & NA & NA & No & Yes & Smoker & 17 & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA \\
67839 & 2011\_12 & male & 61 & 60-69 & NA & Other & Asian & College Grad
& Married & more 99999 & 100000 & 5.00 & 7 & Own & Working & NA & NA &
NA & NA & NA & NA & NA & 66 & 111 & 69 & 110 & 74 & 108 & 66 & 114 & 72
& 274.38 & 1.50 & 5.46 & 97 & 0.522 & NA & NA & No & NA & NA & NA & NA &
NA & NA & NA & NA & NA & 6 & No & Yes & 5 & 0\_to\_1\_hr & 0\_to\_1\_hr
& NA & NA & NA & NA & NA & No & Yes & Smoker & 23 & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA \\
58340 & 2009\_10 & male & 27 & 20-29 & 331 & Other & NA & College Grad &
NeverMarried & more 99999 & 100000 & 5.00 & 3 & Rent & Working & 90.3 &
NA & NA & 174.8 & 29.55 & NA & 25.0\_to\_29.9 & 70 & 111 & 64 & 114 & 68
& 110 & 64 & 112 & 64 & NA & 0.88 & 4.60 & 102 & 1.259 & NA & NA & No &
NA & Vgood & 0 & 0 & None & None & NA & NA & NA & 8 & No & Yes & 4 & NA
& NA & NA & NA & Yes & 3 & 52 & Yes & Yes & Smoker & 23 & No & NA & No &
NA & No & No & NA & 0 & 0 & No & Heterosexual & NA \\
63152 & 2011\_12 & female & 62 & 60-69 & NA & Other & Asian & College
Grad & NeverMarried & 20000-24999 & 22500 & 0.00 & 6 & Other &
NotWorking & 51.8 & NA & NA & 153.5 & 22.00 & NA & 18.5\_to\_24.9 & 72 &
128 & 63 & 136 & 62 & 128 & 66 & 128 & 60 & 13.95 & 2.33 & 6.05 & 140 &
0.787 & NA & NA & No & NA & Good & 0 & 0 & None & None & NA & NA & NA &
9 & No & No & 1 & More\_4\_hr & 3\_hr & NA & NA & Yes & NA & 0 & NA & No
& Non-Smoker & NA & NA & NA & NA & NA & No & No & NA & 0 & NA & No & NA
& NA \\
70537 & 2011\_12 & male & 26 & 20-29 & NA & Other & Other & Some College
& NeverMarried & 25000-34999 & 30000 & NA & 12 & Rent & Working & 86.9 &
NA & NA & 182.5 & 26.10 & NA & 25.0\_to\_29.9 & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & 31 & 0.378 & 29 & 0.475 & No & NA &
Good & 0 & 4 & None & None & NA & NA & NA & 8 & No & Yes & 5 & 2\_hr &
1\_hr & NA & NA & Yes & 6 & 104 & Yes & Yes & Smoker & 18 & Yes & 12 &
Yes & 14 & Yes & Yes & 17 & NA & 5 & No & Heterosexual & NA \\
63317 & 2011\_12 & male & 32 & 30-39 & NA & Other & Asian & High School
& NeverMarried & NA & NA & 2.20 & 4 & Own & Working & 77.5 & NA & NA &
173.9 & 25.60 & NA & 25.0\_to\_29.9 & 62 & 118 & 68 & 110 & 68 & 118 &
70 & 118 & 66 & 716.62 & 1.32 & 4.27 & 218 & 0.724 & NA & NA & No & NA &
Good & 0 & 10 & Most & Several & NA & NA & NA & 4 & Yes & Yes & 7 &
0\_to\_1\_hr & 0\_to\_1\_hr & NA & NA & Yes & 1 & 364 & No & Yes &
Smoker & 18 & Yes & 22 & No & NA & No & Yes & 19 & 5 & 4 & No &
Heterosexual & NA \\
59615 & 2009\_10 & male & 70 & 70+ & 850 & Other & NA & College Grad &
Divorced & 10000-14999 & 12500 & 0.92 & 2 & Rent & NotWorking & 84.1 &
NA & NA & 169.6 & 29.24 & NA & 25.0\_to\_29.9 & 74 & 129 & 62 & 136 & 62
& 132 & 60 & 126 & 64 & NA & 1.81 & 4.60 & 85 & NA & NA & NA & No & NA &
Good & 0 & 0 & None & None & NA & NA & NA & 8 & Yes & Yes & 7 & NA & NA
& NA & NA & Yes & 8 & 364 & NA & No & Non-Smoker & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA \\

\end{longtable}

\textbf{Systematic sampling} is where you randomly choose a starting
place then select every \(k^{th}\) observation to measure.

For example:

\begin{itemize}
\item
  You select every \(5^{th}\) item on an assembly line
\item
  You select every \(10^{th}\) name on the list
\end{itemize}

You select every \(3^{rd}\) customer that comes into the store.

Make sure you randomly select the starting point. Also, if you want a
sample with 100 units of observations, and you have a population that
has 10,000 units of observation, then you would want to select every
10,000/100=100 units of observations.

\textbf{Cluster sampling} is where you break the population into groups
called clusters. Randomly pick some clusters then poll all observations
in those clusters.

For example:

\begin{itemize}
\item
  A large city wants to poll all businesses in the city. They divide the
  city into sections (clusters), maybe a square block for each section,
  and use a random number generator to pick some of the clusters. Then
  they poll all businesses in each chosen cluster.
\item
  You want to measure whether a tree in the forest is infected with bark
  beetles. Instead of having to walk all over the forest, you divide the
  forest up into sectors (clusters), and then randomly pick the sectors
  (clusters) that you will travel to. Then record whether a tree is
  infected or not for every tree in that sector (cluster).
\end{itemize}

Many people confuse stratified sampling and cluster sampling. In
stratified sampling you use \textbf{all} the groups and \textbf{some} of
the members in each group. Cluster sampling is the other way around. It
uses \textbf{some} of the groups and \textbf{all} the members in each
group.

The four sampling techniques that were presented all have advantages and
disadvantages. There is another sampling technique that is sometimes
utilized because either the researcher doesn't know better, or it is
easier to do. This sampling technique is known as a convenience sample.
This sample will not result in a representative sample, and should be
avoided.

\textbf{Convenience sample} is one where the researcher picks
observations to be included that are easy for the researcher to collect.

\begin{itemize}
\tightlist
\item
  An example of a convenience sample is if you want to know the opinion
  of people about the criminal justice system, and you stand on a street
  corner near the county court house, and questioning the first 10
  people who walk by. The people who walk by the county court house are
  most likely involved in some fashion with the criminal justice system,
  and their opinion would not represent the opinions of all
  observations.
\end{itemize}

On a rare occasion, you do want to collect the entire population. In
which case you conduct a census.

A \textbf{census} is when every observation is measured.

\subsection{Example: Sampling type}\label{example-sampling-type}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Banner Health is a several state nonprofit chain of hospitals.
  Management wants to assess the incident of complications after
  surgery. They wish to use a sample of surgery patients. Several
  sampling techniques are described below. Categorize each technique as
  simple random sample, stratified sample, systematic sample, cluster
  sample, or convenience sampling.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Obtain a list of patients who had surgery at all Banner Health
  facilities. Divide the patients according to type of surgery. Draw
  simple random samples from each group.
\item
  Obtain a list of patients who had surgery at all Banner Health
  facilities. Number these patients, and then use a random number table
  to obtain the sample.
\item
  Randomly select some Banner Health facilities from each of the seven
  states, and then include all the patients on the surgery lists of the
  states.
\item
  At the beginning of the year, instruct each Banner Health facility to
  record any complications from every 100\^{}th\^{} surgery.
\item
  Instruct each Banner Health facilities to record any complications
  from 20 surgeries this week and send in the results.

  \subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-8}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Obtain a list of patients who had surgery at all Banner Health
  facilities. Divide the patients according to type of surgery. Draw
  simple random samples from each group.

  This is a stratified sample since the patients where separated into
  different stratum and then random samples were taken from each strata.
  The problem with this is that some types of surgeries may have more
  chances for complications than others. Of course, the stratified
  sample would show you this.
\item
  Obtain a list of patients who had surgery at all Banner Health
  facilities. Number these patients, and then use a random number table
  to obtain the sample.

  This is a random sample since each patient has the same chance of
  being chosen. The problem with this one is that it will take a while
  to collect the data.
\item
  Randomly select some Banner Health facilities from each of the seven
  states, and then include all the patients on the surgery lists of the
  states.

  This is a cluster sample since all patients are questioned in each of
  the selected hospitals. The problem with this is that you could have
  by chance selected hospitals that have no complications.
\item
  At the beginning of the year, instruct each Banner Health facility to
  record any complications from every 100\^{}th\^{} surgery.

  This is a systematic sample since they selected every \(100^{th}\)
  surgery. The problem with this is that if every \(90^{th}\) surgery
  has complications, you wouldn't see this come up in the data.
\item
  Instruct each Banner Health facilities to record any complications
  from 20 surgeries this week and send in the results.

  This is a convenience sample since they left it up to the facility how
  to do it. The problem with convenience samples is that the person
  collecting the data will probably collect data from surgeries that had
  no complications.
\end{enumerate}

\subsection{Homework for Sampling Methods
Section}\label{homework-for-sampling-methods-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Researchers want to collect cholesterol levels of U.S. patients who
  had a heart attack two days prior. The following are different
  sampling techniques that the researcher could use. Classify each as
  simple random sample, stratified sample, systematic sample, cluster
  sample, or convenience sample.

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    The researchers randomly select 5 hospitals in the U.S. then measure
    the cholesterol levels of all the heart attack patients in each of
    those hospitals.
  \item
    The researchers list all of the heart attack patients and measure
    the cholesterol level of every \(25^{th}\) person on the list.
  \item
    The researchers go to one hospital on a given day and measure the
    cholesterol level of the heart attack patients at that time.
  \item
    The researchers list all of the heart attack patients. They then
    measure the cholesterol levels of randomly selected patients.
  \item
    The researchers divide the heart attack patients based on race, and
    then measure the cholesterol levels of randomly selected patients in
    each race grouping.
  \end{enumerate}
\item
  The quality control officer at a manufacturing plant needs to
  determine what percentage of items in a batch are defective. The
  following are different sampling techniques that could be used by the
  officer. Classify each as simple random sample, stratified sample,
  systematic sample, cluster sample, or convenience sample.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  The officer lists all of the batches in a given month. The number of
  defective items is counted in randomly selected batches.
\item
  The officer takes the first 10 batches and counts the number of
  defective items.
\item
  The officer groups the batches made in a month into which shift they
  are made. The number of defective items is counted in randomly
  selected batches in each shift.
\item
  The officer chooses every \(15^{th}\) batch off the line and counts
  the number of defective items in each chosen batch.
\end{enumerate}

The officer divides the batches made in a month into which day they were
made. Then certain days are picked and every batch made that day is
counted to determine the number of defective items.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  You wish to determine the GPA of students at your school. Describe
  what process you would go through to collect a sample if you use a
  simple random sample.
\item
  You wish to determine the GPA of students at your school. Describe
  what process you would go through to collect a sample if you use a
  stratified sample.
\item
  You wish to determine the GPA of students at your school. Describe
  what process you would go through to collect a sample if you use a
  systematic sample.
\item
  You wish to determine the GPA of students at your school. Describe
  what process you would go through to collect a sample if you use a
  cluster sample.
\item
  You wish to determine the GPA of students at your school. Describe
  what process you would go through to collect a sample if you use a
  convenience sample.
\end{enumerate}

\section{Experimental Design}\label{experimental-design}

The section is an introduction to experimental design. This is how to
actually design an experiment or a survey so that they are statistical
sound. Experimental design is a very involved process, so this is just a
small introduction.

\subsection{\texorpdfstring{\textbf{Guidelines for planning a
statistical
study}}{Guidelines for planning a statistical study}}\label{guidelines-for-planning-a-statistical-study}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Identify the observations that you are interested in. Realize that you
  can only make conclusions for these observations. As an example, if
  you use a fertilizer on a certain genus of plant, you can't say how
  the fertilizer will work on any other types of plants. However, if you
  diversify too much, then you may not be able to tell if there really
  is an improvement since you have too many factors to consider.
\item
  Specify the variable. You want to make sure this is something that you
  can measure, and make sure that you control for all other factors too.
  As an example, if you are trying to determine if a fertilizer works by
  measuring the height of the plants on a particular day, you need to
  make sure you can control how much fertilizer you put on the plants
  (which would be your treatment), and make sure that all the plants
  receive the same amount of sunlight, water, and temperature.
\item
  Specify the population. This is important in order for you know what
  conclusions you can make and what observations you are making the
  conclusions about.
\item
  Specify the method for taking measurements or making observations.
\item
  Determine if you are taking a census or sample. If taking a sample,
  decide on the sampling method.
\item
  Collect the data.
\item
  Use appropriate descriptive statistics methods and make decisions
  using appropriate inferential statistics methods.
\item
  Note any concerns you might have about your data collection methods
  and list any recommendations for future.
\end{enumerate}

There are two types of studies:

An \textbf{observational study} is when the investigator collects data
merely by watching or asking questions. Nothing is change or controlled

An \textbf{experiment} is when the investigator changes a variable or
imposes a treatment to determine its effect.

\subsection{Example: Observational Study or
Experiment}\label{example-observational-study-or-experiment}

State if the following is an observational study or an experiment.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Poll students to see if they favor increasing tuition.
\item
  Give some students a tutor to see if grades improve.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-9}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Poll students to see if they favor increasing tuition.

  This is an observational study. You are only asking a question.
\item
  Give some students a tutor to see if grades improve.

  This is an experiment. The tutor is the treatment.
\end{enumerate}

\subsection{Survey}\label{survey}

Many observational studies involve surveys. A \textbf{survey} uses
questions to collect the data and needs to be written so that there is
no bias.

\subsection{Experiment Options}\label{experiment-options}

In an experiment, there are different options.

\textbf{Randomized two-treatment experiment}: in this experiment, there
are two treatments, and observations are randomly placed into the two
groups. Either both groups get a treatment, or one group gets a
treatment and the other gets either nothing or a placebo. The group
getting either an old treatment, no treatment or a placebo is called the
control group. The group getting the treatment is called the treatment
group. The idea of the placebo is that a person thinks they are
receiving a treatment, but in reality they are receiving a sugar pill or
fake treatment. Doing this helps to account for the placebo effect,
which is where a person's mind makes their body respond to a treatment
because they think they are taking the treatment when they are not
really taking the treatment. Note, not every experiment needs a placebo,
such when using animals or plants. Also, you can't always use a placebo
or no treatment. As an example, if you are testing a new blood pressure
medication you can't give a person with high blood pressure a placebo or
no treatment because of moral reasons.

\textbf{Randomized Block Design}: a block is a group of subjects that
are similar, but the blocks differ from each other. Then randomly assign
treatments to subjects inside each block. An example would be separating
students into full-time versus part-time, and then randomly picking a
certain number full-time students to get the treatment and a certain
number part-time students to get the treatment. This way some of each
type of student gets the treatment and some do not.

\textbf{Rigorously Controlled Design}: carefully assign subjects to
different treatment groups, so that those given each treatment are
similar in ways that are important to the experiment. An example would
be if you want to have a full-time student who is male, takes only night
classes, has a full-time job, and has children in one treatment group,
then you need to have the same type of student getting the other
treatment. This type of design is hard to implement since you don't know
how many differentiation you would use, and should be avoided.

\textbf{Matched Pairs Design}: the treatments are given to two groups
that can be matched up with each other in some ways. One example would
be to measure the effectiveness of a muscle relaxer cream on the right
arm and the left arm of observations, and then for each observation you
can match up their right arm measurement with their left arm. Another
example of this would be before and after experiments, such as weight
before and weight after a diet.

No matter which experiment type you conduct, you should also consider
the following:

\textbf{Replication}: repetition of an experiment on more than one
observation so you can make sure that the sample is large enough to
distinguish true effects from random effects. It is also the ability for
someone else to duplicate the results of the experiment.

\textbf{Blind study} is where the subject used in the study does not
know which treatment they are getting or if they are getting the
treatment or a placebo.

\textbf{Double-blind study} is where neither the subject used in the
study nor the researcher knows who is getting which treatment or who is
getting the treatment and who is getting the placebo. This is important
so that there can be no bias created by either the subject or the
researcher.

One last consideration is the time period that you are collecting the
data over. There are three types of time periods that you can consider.

\textbf{Cross-sectional study}: data observed, measured, or collected at
one point in time.

\textbf{Retrospective} (or \textbf{case-control}) \textbf{study}: data
collected from the past using records, interviews, and other similar
artifacts.

\textbf{Prospective} (or \textbf{longitudinal} or \textbf{cohort})
\textbf{study}: data collected in the future from groups sharing common
factors.

\subsection{Homework for Experimental Design
Section}\label{homework-for-experimental-design-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You want to determine if cinnamon reduces a person's insulin
  sensitivity. You give patients who are insulin sensitive a certain
  amount of cinnamon and then measure their glucose levels. Is this an
  observation or an experiment? Why?
\item
  You want to determine if eating more fruits reduces a person's chance
  of developing cancer. You watch people over the years and ask them to
  tell you how many servings of fruit they eat each day. You then record
  who develops cancer. Is this an observation or an experiment? Why?
\item
  A researcher wants to evaluate whether countries with lower fertility
  rates have a higher life expectancy. They collect the fertility rates
  and the life expectancies of countries around the world. Is this an
  observation or an experiment? Why?
\item
  To evaluate whether a new fertilizer improves plant growth more than
  the old fertilizer, the fertilizer developer gives some plants the new
  fertilizer and others the old fertilizer. Is this an observation or an
  experiment? Why?
\item
  A researcher designs an experiment to determine if a new drug lowers
  the blood pressure of patients with high blood pressure. The patients
  are randomly selected to be in the study and they randomly pick which
  group to be in. Is this a randomized experiment? Why or why not?
\item
  Doctors trying to see if a new stent works longer for kidney patients,
  asks patients if they are willing to have one of two different stents
  put in. During the procedure the doctor decides which stent to put in
  based on which one is on hand at the time. Is this a randomized
  experiment? Why or why not?
\item
  A researcher wants to determine if diet and exercise together helps
  people lose weight over just exercising. The researcher solicits
  volunteers to be part of the study, randomly picks which volunteers
  are in the study, and then lets each volunteer decide if they want to
  be in the diet and exercise group or the exercise only group. Is this
  a randomized experiment? Why or why not?
\item
  To determine if lack of exercise reduces flexibility in the knee
  joint, physical therapists ask for volunteers to join their trials.
  They then randomly select the volunteers to be in the group that
  exercises and to be in the group that doesn't exercise. Is this a
  randomized experiment? Why or why not?
\item
  You collect the weights of tagged fish in a tank. You then put an
  extra protein fish food in water for the fish and then measure their
  weight a month later. Are the two samples matched pairs or not? Why or
  why not?
\item
  A mathematics instructor wants to see if a computer homework system
  improves the scores of the students in the class. The instructor
  teaches two different sections of the same course. One section
  utilizes the computer homework system and the other section completes
  homework with paper and pencil. Are the two samples matched pairs or
  not? Why or why not?
\item
  A business manager wants to see if a new procedure improves the
  processing time for a task. The manager measures the processing time
  of the employees then trains the employees using the new procedure.
  Then each employee performs the task again and the processing time is
  measured again. Are the two samples matched pairs or not? Why or why
  not?
\item
  The prices of generic items are compared to the prices of the
  equivalent named brand items. Are the two samples matched pairs or
  not? Why or why not?
\item
  A doctor gives some of the patients a new drug for treating acne and
  the rest of the patients receive the old drug. Neither the patient nor
  the doctor knows who is getting which drug. Is this a blind
  experiment, double blind experiment, or neither? Why?
\item
  One group is told to exercise and one group is told to not exercise.
  Is this a blind experiment, double blind experiment, or neither? Why?
\item
  The researchers at a hospital want to see if a new surgery procedure
  has a better recovery time than the old procedure. The patients are
  not told which procedure that was used on them, but the surgeons
  obviously did know. Is this a blind experiment, double blind
  experiment, or neither? Why?
\item
  To determine if a new medication reduces headache pain, some patients
  are given the new medication and others are given a placebo. Neither
  the researchers nor the patients know who is taking the real
  medication and who is taking the placebo. Is this a blind experiment,
  double blind experiment, or neither? Why?
\item
  A new study is underway to track the eating and exercise patterns of
  people at different time periods in the future, and see who is
  afflicted with cancer later in life. Is this a cross-sectional study,
  a retrospective study, or a prospective study? Why?
\item
  To determine if a new medication reduces headache pain, some patients
  are given the new medication and others are given a placebo. The pain
  levels of a patient are then recorded. Is this a cross-sectional
  study, a retrospective study, or a prospective study? Why?
\item
  To see if there is a link between smoking and bladder cancer, patients
  with bladder cancer are asked if they currently smoke or if they
  smoked in the past. Is this a cross-sectional study, a retrospective
  study, or a prospective study? Why?
\item
  The Nurses Health Survey was a survey where nurses were asked to
  record their eating habits over a period of time, and their general
  health was recorded. Is this a cross-sectional study, a retrospective
  study, or a prospective study? Why?
\item
  Consider a question that you would like to answer. Describe how you
  would design your own experiment. Make sure you state the question you
  would like to answer, then determine if an experiment or an
  observation is to be done, decide if the question needs one or two
  samples, if two samples are the samples matched, if this is a
  randomized experiment, if there is any blinding, and if this is a
  cross-sectional, retrospective, or prospective study.
\end{enumerate}

\section{How Not to Do Statistics}\label{how-not-to-do-statistics}

Many studies are conducted and conclusions are made. However, there are
occasions where the study is not conducted in the correct manner or the
conclusion is not correctly made based on the data. There are many
things that you should question when you read a study. There are many
reasons for the study to have bias in it. Bias is where a study may have
a certain slant or preference for a certain result. The following are a
list of some of the questions or issues you should consider to help
decide if there is bias in a study.

One of the first issues you should ask is who funded the study. If the
entity that sponsored the study stands to gain either profits or
notoriety from the results, then you should question the results. It
doesn't mean that the results are wrong, but you should scrutinize them
on your own to make sure they are sound. As an example if a study says
that genetically modified foods are safe, and the study was funded by a
company that sells genetically modified food, then one may question the
validity of the study. Since the company funds the study and their
profits rely on people buying their food, there may be bias.

An experiment could have \textbf{lurking or confounding variables} when
you cannot rule out the possibility that the observed effect is due to
some other variable rather than the factor being studied. An example of
this is when you give fertilizer to some plants and no fertilizer to
others, but the no fertilizer plants also are placed in a location that
doesn't receive direct sunlight. You won't know if the plants that
received the fertilizer grew taller because of the fertilizer or the
sunlight. Make sure you design experiments to eliminate the effects of
confounding variables by controlling all the factors that you can.

\textbf{Over generalization} is where you do a study on one group and
then try to say that it will happen on all groups. An example is doing
cancer treatments on rats. Just because the treatment works on rats does
not mean it will work on humans. Another example is that until recently
most FDA medication testing had been done on white males of a particular
age. There is no way to know how the medication affects other genders,
ethnic groups, age groups, and races. The new FDA guidelines stresses
using subjects from different groups.

\textbf{Cause and effect} is where people decide that one variable
causes the other just because the variables are related. Unless the
study was done as an experiment where a variable was controlled, you
cannot say that one variable caused the other. There is the possibility
that another variable caused both to change. As an example, there is a
relationship between number of drownings at the beach and ice cream
sales. This does not mean that ice cream sales increasing causes people
to drown. Most likely the cause for both increasing is the heat.

\textbf{Sampling error}: This is the difference between the sample
results and the true population results. This is unavoidable, and
results in the fact that samples are different from each other. As an
example, if you take a sample of 5 people's height in your class, you
will get 5 numbers. If you take another sample of 5 people's heights in
your class, you will likely get 5 different numbers.

\textbf{Non-sampling error}: This is where the sample is collected
poorly either through a biased sample or through error in measurements.
Care should be taken to avoid this error.

Lastly, there should be care taken in considering the difference between
\textbf{statistical significance versus practical significance}. This is
a major issue in statistics. Something could be statistically
significance, which means that a statistical test shows there is
evidence to show what you are trying to prove. However, in practice it
doesn't mean much or there are other issues to consider. As an example,
suppose you find that a new drug for high blood pressure does reduce the
blood pressure of patients. When you look at the improvement it actually
doesn't amount to a large difference. Even though statistically there is
a change, it may not be worth marketing the product because it really
isn't that big of a change. Another consideration is that you find the
blood pressure medication does improve a person's blood pressure, but it
has serious side effects or it costs a great deal for a prescription. In
this case, it wouldn't be practical to use it. In both cases, the study
is shown to be statistically significant, but practically you don't want
to use the medication. The main thing to remember in a statistical study
is that the statistics is only part of the process. You also want to
make sure that there is practical significance. One more comment on
statistical significance, the American Statistical Association (ASA)
recently came out with a statement, ``Based on our review of the
articles in this special issue and the broader literature, we conclude
that it is time to stop using the term `statistically significant'
entirely.'' (Advanced Solutions International, Inc, 2019) Though the ASA
suggests not using this term anymore, there are many studies that have
been done in the past that uses this term, so it is presented here.
However, it is not a term that should be use and will be down played in
the rest of this book.

\textbf{Surveys} have their own areas of bias that can occur. A few of
the issues with surveys are in the wording of the questions, the
ordering of the questions, the manner the survey is conducted, and the
response rate of the survey.

The wording of the questions can cause \textbf{hidden bias}, which is
where the questions are asked in a way that makes a person respond a
certain way. An example is that a poll was done where people were asked
if they believe that there should be an amendment to the constitution
protecting a woman's right to choose. About 60\% of all people
questioned said yes. Another poll was done where people were asked if
they believe that there should be an amendment to the constitution
protecting the life of an unborn child. About 60\% of all people
questioned said yes. These two questions deal with the same issue,
though giving different results, but how the question was asked affected
the outcome.

The ordering of the question can also cause \textbf{hidden bias}. An
example of this is if you were asked if there should be a fine for
texting while driving, but proceeding that question is the question
asking if you text while drive. By asking a person if they actually
partake in the activity, that person now personalizes the question and
that might affect how they answer the next question of creating the
fine.

\textbf{Non-response} is where you send out a survey but not everyone
returns the survey. You can calculate the response rate by dividing the
number of returns by the number of surveys sent. Most response rates are
around 30-50\%. A response rate less than 30\% is very poor and the
results of the survey are not valid. To reduce non-response, it is
better to conduct the surveys in person, though these are very
expensive. Phones are the next best way to conduct surveys, emails can
be effective, and physical mailings are the least desirable way to
conduct surveys.

\textbf{Voluntary response} is where people are asked to respond via
phone, email or online. The problem with these is that only people who
really care about the topic are likely to call or email. These surveys
are not scientific and the results from these surveys are not valid.
Note: all studies involve volunteers. The difference between a voluntary
response survey and a scientific study is that in a scientific study the
researchers ask the subjects to be involved, while in a voluntary
response survey the subjects become involved on their own choosing.

\subsection{Example: Bias in a Study}\label{example-bias-in-a-study}

Suppose a mathematics department at a community college would like to
assess whether computer-based homework improves students' test scores.
They use computer-based homework in one classroom with one teacher and
use traditional paper and pencil homework in a different classroom with
a different teacher. The students using the computer-based homework had
higher test scores. What is wrong with this experiment?

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-10}

Since there were different teachers, you do not know if the better test
scores are because of the teacher or the computer-based homework. A
better design would be have the same teacher teach both classes. The
control group would utilize traditional paper and pencil homework and
the treatment group would utilize the computer-based homework. Both
classes would have the same teacher, and the students would be split
between the two classes randomly. The only difference between the two
groups should be the homework method. Of course, there is still
variability between the students, but utilizing the same teacher will
reduce any other confounding variables.

\subsection{Example: Cause and Effect}\label{example-cause-and-effect}

Determine if the one variable did cause the change in the other
variable.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Cinnamon was giving to a group of people who have diabetes, and then
  their blood glucose levels were measured a time period later. All
  other factors for each person were kept the same. Their glucose levels
  went down. Did the cinnamon cause the reduction?
\item
  There is a link between spray on tanning products and lung cancer.
  Does that mean that spray on tanning products cause lung cancer?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-11}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Cinnamon was giving to a group of people who have diabetes, and then
  their blood glucose levels were measured a time period later. All
  other factors for each person were kept the same. Their glucose levels
  went down. Did the cinnamon cause the reduction?

  Since this was a study where the use of cinnamon was controlled, and
  all other factors were kept constant from person to person, then any
  changes in glucose levels can be attributed to the use of cinnamon.
\item
  There is a link between spray on tanning products and lung cancer.
  Does that mean that spray on tanning products cause lung cancer?

  Since there is only a link, and not a study controlling the use of the
  tanning spray, then you cannot say that increased use causes lung
  cancer. You can say that there is a link, and that there could be a
  cause, but you cannot say for sure that the spray causes the cancer.
\end{enumerate}

\subsection{Example: Generalizations}\label{example-generalizations}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  A researcher conducts a study on the use of ibuprofen on humans and
  finds that it is safe. Does that mean that all species can use
  ibuprofen?
\item
  Aspirin has been used for years to bring down fevers in humans.
  Originally it was tested on white males between the ages of 25 and 40
  and found to be safe. Is it safe to give to everyone?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-12}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  A researcher conducts a study on the use of ibuprofen on humans and
  finds that it is safe. Does that mean that all species can use
  ibuprofen?

  No.~Just because a drug is safe to use on one species doesn't mean it
  is safe to use for all species. In fact, ibuprofen is toxic to cats.
\item
  Aspirin has been used for years to bring down fevers in humans.
  Originally it was tested on white males between the ages of 25 and 40
  and found to be safe. Is it safe to give to everyone?

  No.~Just because one age group can use it doesn't mean it is safe to
  use for all age groups. In fact, there has been a link between giving
  a child under the age of 19 aspirin when they have a fever and Reye's
  syndrome.
\end{enumerate}

\subsection{Homework for How Not to Do Statistics
Section}\label{homework-for-how-not-to-do-statistics-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Suppose there is a study where a researcher conducts an experiment to
  show that deep breathing exercises helps to lower blood pressure. The
  researcher takes two groups of people and has one group to perform
  deep breathing exercises and a series of aerobic exercises every day
  and the other group was asked to refrain from any exercises. The
  researcher found that the group performing the deep breathing
  exercises and the aerobic exercises had lower blood pressure. Discuss
  any issue with this study.
\item
  Suppose a car dealership offers a low interest rate and a longer
  payoff period to customers or a high interest rate and a shorter
  payoff period to customers, and most customers choose the low interest
  rate and longer payoff period, does that mean that most customers want
  a lower interest rate? Explain.
\item
  Over the years it has been said that coffee is bad for you. When
  looking at the studies that have shown that coffee is linked to poor
  health, you will see that people who tend to drink coffee don't sleep
  much, tend to smoke, don't eat healthy, and tend to not exercise. Can
  you say that the coffee is the reason for the poor health or is there
  a lurking variable that is the actual cause? Explain.
\item
  When researchers were trying to figure out what caused polio, they saw
  a connection between ice cream sales and polio. As ice cream sales
  increased so did the incident of polio. Does that mean that eating ice
  cream causes polio? Explain your answer.
\item
  There is a positive correlation between having a discussion of gun
  control, which usually occur after a mass shooting, and the sale of
  guns. Does that mean that the discussion of gun control increases the
  likelihood that people will buy more guns? Explain.
\item
  There is a study that shows that people who are obese have a vitamin D
  deficiency. Does that mean that obesity causes a deficiency in vitamin
  D? Explain.
\item
  A study was conducted that shows that polytetrafluoroethylene (PFOA)
  (Teflon is made from this chemical) has an increase risk of tumors in
  lab mice. Does that mean that PFOA's have an increased risk of tumors
  in humans? Explain.
\item
  Suppose a telephone poll is conducted by contacting U.S. citizens via
  landlines about their view of gay marriage. Suppose over 50\% of those
  called do not support gay marriage. Does that mean that you can say
  over 50\% of all people in the U.S. do not support gay marriage?
  Explain.
\item
  Suppose that it can be shown to be statistically significant that a
  smaller percentage of the people are satisfied with your business. The
  percentage before was 87\% and is now 85\%. Do you change how you
  conduct business? Explain?
\item
  You are testing a new drug for weight loss. You find that the drug
  does in fact statistically show a weight loss. Do you market the new
  drug? Why or why not?
\item
  There was an online poll conducted about whether the mayor of
  Auckland, New Zealand, should resign due to an affair. The majority of
  people participating said he should. Should the mayor resign due to
  the results of this poll? Explain.
\item
  An online poll showed that the majority of Americans believe that the
  government covered up events of 9/11. Does that really mean that most
  Americans believe this? Explain.
\item
  A survey was conducted at a college asking all employees if they were
  satisfied with the level of security provided by the security
  department. Discuss how the results of this question could be biased.
\item
  An employee survey says, ``Employees at this institution are very
  satisfied with working here. Please rate your satisfaction with the
  institution.'' Discuss how this question could create bias.
\item
  A survey has a question that says, ``Most people are afraid that they
  will lose their house due to economic collapse. Choose what you think
  is the biggest issue facing the nation today. a) Economic collapse, b)
  Foreign policy issues, c) Environmental concerns.'' Discuss how this
  question could create bias.
\item
  A survey says, ``Please rate the career of Roberto Clemente, one of
  the best right field baseball players in the world.'' Discuss how this
  question could create bias.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Graphical Description of
Data}\label{graphical-description-of-data}

In chapter 1, you were introduced to the concepts of population, which
again is a collection of all the measurements from the individuals of
interest. Remember, in most cases you can't collect the entire
population, so you have to take a sample. Thus, you collect data either
through a sample or a census. Now you have a large number of data
values. What can you do with them? No one likes to look at just a set of
numbers. One thing is to organize the data into a table or graph.
Ultimately though, you want to be able to use that graph to interpret
the data, to describe the distribution of the data set, and to explore
different characteristics of the data. The characteristics that will be
discussed in this chapter and the next chapter are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Center: middle of the data set, also known as the average.
\item
  Variation: how much the data varies.
\item
  Distribution: shape of the data (symmetric, uniform, or skewed).
\item
  Qualitative data: analysis of the data
\item
  Outliers: data values that are far from the majority of the data.
\item
  Time: changing characteristics of the data over time.
\end{enumerate}

This chapter will focus mostly on using the graphs to understand aspects
of the data, and not as much on how to create the graphs. There is
technology that will create most of the graphs, though it is important
for you to understand the basics of how to create them.

This textbook uses RStudio to perform all graphical and descriptive
statistics, and all statistical inference. When using RStudio, every
command is performed the same way. You start off with a goal(explanatory
variable \textasciitilde{} response variable, data=data
frame\_name,\ldots)

RStudio uses packages to make calculations easier. For this textbook,
you will mostly need the package mosaic. There will be others that you
will need on occasion, but you will be told that at the time. Most
likely, mosaic is already installed in your RStudio. If you wish to
install other packages you use the command

install.packages(``name of package'')

where you replace the name of package with the package you wish to
install.

Once the package is installed, then you will need to tell RStudio you
want to use it every time you start RStudio. The command to tell RStudio
you want to use a package is

library(``name of package'')

You will need to turn on the package mosaic. The NHANES package contains
a data frame that is useful. Both are accessed by running the command
library(``name of package'').

Back to the basic command

goal(explanatory variable \textasciitilde{} response variable, data=data
frame\_name,\ldots)

The goal depends on what you want to do. If you want to create a graph
then you would need

gf\_graph\_type(explanatory\_variable \textasciitilde{}
response\_variable, data=data\_frame\_name, \ldots)

As an example if you want to create a density plot of cholesterol levels
on day 2 from a data frame called Cholesterol, then your command would
be

gf\_density(\textasciitilde day2, data=Cholesterol)

You will see more on what the different commands are that you would use.
A word about the \ldots{} at the end of the command. That means there
are other things you can do, but that is up to you if you want to
actually do them. They do not need to be used if you don't want to. The
following sections will show you how to create the different graphs that
are usually completed in an introductory statistics course.

\section{Qualitative Data}\label{qualitative-data}

Remember, qualitative data are words describing a characteristic of the
individual. There are several different graphs that are used for
qualitative data. These graphs include bar graphs, Pareto charts, and
pie charts. Bar graphs can be created using a statistical program like
RStudio.

\textbf{Bar graphs or charts} consist of the frequencies on one axis and
the categories on the other axis. Drawing the bar graph using r is
performed using the following command.

gf\_bar(\textasciitilde explanatory variable, data=Dataframe)

\subsection{Example: Drawing a Bar
Chart}\label{example-drawing-a-bar-chart}

Data was collected for two semesters in a statistics class. The data
frame is in Table~\ref{tbl-Class}. The command

head(data frame)

shows the variables and the first few lines of the data set. The data
sets are usually larger than what is shown. The head command allows one
to see the structure of the data frame.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Class}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/class\_survey.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Class))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0684}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0598}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1368}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1624}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0427}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.3504}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0598}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1197}}@{}}

\caption{\label{tbl-Class}Head of Statistics Class Survey}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
vehicle
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
distance\_campus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ice\_cream
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
rent
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
major
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
winter
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
None & Female & 1.5 & Cookie Dough & 724 & Environmental and
Sustainability Studies & 61 & Liked it \\
Mercury & Female & 14.7 & Sherbet & 200 & Administrative Justice & 60 &
Don't like it \\
Ford & Female & 2.4 & Chocolate Brownie. & 600 & Bio Chem & 68 & Liked
it \\
Toyota & Female & 5.2 & coffee & 0 & & 66 & Loved it \\
Jeep & Male & 2.0 & Cookie Dough & 600 & Pre-health Careers & 71 & Loved
it \\
Subaru & Male & 5.0 & none & 500 & Finance & 72 & No opinion \\

\end{longtable}

Every data frame has a code book that describes the data set, the source
of the data set, and a listing and description of the variables in the
data frame.

\textbf{Code book for data frame class}

Description Survey results from two semesters of statistics classes at
Coconino Community College in the years 2018-2019.

Format

This data frame contains the following columns:

vehicle: Type of car a student drives

gender: Self declared gender of a student

distance\_campus: how far a student lives from the Lone Tree Campus of
Coconino Community College (miles)

ice\_cream: favorite ice cream flavor

rent: How much a student pays in rent

major: Students declared major

height: height of the student (inches)

winter: Student's opinion of winter (Love it, Like it, Don't like, No
opinion)

Source

Kozak K (2019). Survey results form surveys collected in statistics
class at Coconino Community College.

References

Kozak, 2019

Create a bar graph of vehicle type. To do this in RStudio, use the
command

gf\_bar(\textasciitilde variable, data=Data\_Frame, \ldots)

where gf\_bar is the goal, vehicle is the name of the response variable
(there is no explanatory variable), the data frame is Class, and a title
was added to the graph.

\subsubsection{Solution}\label{solution-13}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_bar}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{vehicle, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{title=}\StringTok{"Bar Chart of Cars driven by students in statistics class"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Vehicle"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-class_data-1.pdf}

}

\caption{\label{fig-class_data}Cars driven by students in statistics
class}

\end{figure}%

Description of Figure~\ref{fig-class_data} is a Bar graph with bars for
Audi, Buick, Honda, Hyundai, Mercury, Nissan with height of 1, Dodge and
None with height of 2, Jeep, Subaru, Toyota with heights of 3, and
Chevrolet and Ford at height of 4.

Notice from Figure~\ref{fig-class_data}, you can see that Chevrolet and
Ford are the more popular car, with Jeep, Subaru, and Toyota not far
behind. Many types seems to be the lesser used, and tied for last place.
However, more data would help to figure this out.

\textbf{All graphs should have labels on each axis and a title for the
graph.}

The beauty of data frames with multiple variables is that you can answer
many questions from the data. Suppose you want to see if gender makes a
difference for the type of car a person drives. If you are a car
manufacturer, if you knew that certain genders like certain cars, then
you would advertise to the different genders. To create a bar graph that
separates based on gender, perform the following command in RStudio.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_bar}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{vehicle, }\AttributeTok{fill=}\SpecialCharTok{\textasciitilde{}}\NormalTok{gender, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{title=}\StringTok{"Cars driving by students in statistics class"}\NormalTok{,}\AttributeTok{xlab=}\StringTok{"Vehicle"}\NormalTok{, }\AttributeTok{position=}\FunctionTok{position\_dodge}\NormalTok{())}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-class_data_dodge-1.pdf}

}

\caption{\label{fig-class_data_dodge}Bar graph of Cars driven by
students in statistics class}

\end{figure}%

Description of Figure~\ref{fig-class_data_dodge} is a bar graph of
number of vehicles separated by female and male. Audi and male has
height of 1, Buick and female has a height of 1, Chevrolet and male and
Chevrolet and female have heights of 2, Dodge and male and Dodge and
female has heights of 1, Ford and female has a height of 4, Honda and
female has a height of 1, Hyundai and male has a height of 1, Jeep and
male has a height of 2 while Jeep and female has a height of 1, Mercury
and female has a height of 1, Nissan and female has a height of 1, no
car and female has a height of 2, Subaru and female has a height of 1,
Subaru and male has a height of 2, Toyota and female has a height of 1,
and Toyota and male has a height of 2.

Notice a Ford is driven by females more than any other car, while
Chevrolet, Mercury, and Subaru cars are equally driven by males.
Obviously a larger sample would be needed to make any conclusions from
this data.

There are other types of graphs that can be created for quantitative
variables. Another type is known as a dot plot. The command for this
graph is as follows.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_dotplot}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{vehicle, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{title=}\StringTok{"Cars driven by students in statistics class"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Vehicle"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-class_data_dot-car-1.pdf}

}

\caption{\label{fig-class_data_dot-car}Cars driven by students in
statistics class}

\end{figure}%

Description of Figure~\ref{fig-class_data_dot} is a dot plot of number
of vehicles with Audi, Buick, Honda, Hyundai, Mercury, Nissan with
height of 1, Dodge and None with height of 2, Jeep, Subaru, Toyota with
heights of 3, and Chevrolet and Ford at height of 4. Very similar to bar
graph.

Notice a dot plot is like a bar chart. Both give you the same
information. You can also divide a dot plot by gender.

Another type of graph that is also useful and similar to the dot plot is
a point plot (scatter plot). In this plot you can graph the explanatory
variable versus the response variable. The command for this in rStudio
is as follows.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_point}\NormalTok{(vehicle}\SpecialCharTok{\textasciitilde{}}\NormalTok{gender, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{title=}\StringTok{"Cars driving by students in statistics class"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Gender"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Vehicle"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-class_data_point-1.pdf}

}

\caption{\label{fig-class_data_point}Cars driven by students in
statistics class}

\end{figure}%

Description of Figure~\ref{fig-class_data_point} is a scatter plot of
type of vehicles separated by female and male with females owning
Toyota, Subaru, none, Nissan, Mercury, Jeep, Honda, Ford, Dodge,
Chevrolet, and Buick, while males own Toyota, Subaru, Jeep, Hyundai,
Dodge, Chevrolet, and Audi.

The problem with Figure~\ref{fig-class_data_point} is that if there are
multiple females who drive a Ford, only one dot is shown. So it is best
to spread the dots out using a plot known as a jitter plot. In a jitter
plot the dots are randomly moved off the center line. The command for a
jitter plot is as follows:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_jitter}\NormalTok{(vehicle}\SpecialCharTok{\textasciitilde{}}\NormalTok{gender, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{title=}\StringTok{"Cars driving by students in statistics class"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Gender"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Vehicle"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-class_data_jitter-1.pdf}

}

\caption{\label{fig-class_data_jitter}Cars driven by students in
statistics class}

\end{figure}%

Description of Figure~\ref{fig-class_data_jitter} is a jitter plot of
number of vehicles separated by female and male with females owning 1
Toyota, 1 Subaru, 2 with none, 1 Nissan, 1 Mercury, 1 Jeep, 1 Honda, 4
Fords, 1 Dodge, 2 Chevrolets, and 1 Buick, while males own 2 Toyotas, 2
Subarus, 2 Jeeps, 1 Hyundai, 1 Dodge, 1 Chevrolets, and 1 Audi.

Now you can observe that there are 4 females who drive a Ford. There is
one female who drives a Honda. Other information about other cars and
genders can be seen better than in the point plot and the bar graph.
Jitter plots are useful to see how many data values are for each
qualitative data values.

There are many other types of graphs that can be used on qualitative
data. There are spreadsheet software packages that will create most of
them, and it is better to look at them to see how to create then. It
depends on your data as to which may be useful, but the bar, dot, and
jitter plots are really the most useful.

\subsection{Homework for Qualitative Data
Section}\label{homework-for-qualitative-data-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Eyeglassomatic manufactures eyeglasses for different retailers. The
  number of lenses for different activities is in
  Table~\ref{tbl-eyeglasses}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Eyeglasses}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/eyglasses.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Eyeglasses))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}l@{}}

\caption{\label{tbl-eyeglasses}Head of Eyeglasses Data frame}

\tabularnewline

\toprule\noalign{}
activity \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Grind \\
Grind \\
Grind \\
Grind \\
Grind \\
Grind \\

\end{longtable}

\textbf{Code book for Data Frame Eyeglasses}

\textbf{Description} Activities that an Eyeglass company performs when
making eyeglasses, Grind means ground the lenses and put them in frames,
multicoat means put tinting or coatings on lenses and then put them in
frames, assemble means received frames and lenses from other sources and
put them together, make frames means made the frames and put lenses in
from other sources, receive finished means received glasses from other
source unknown means do not know where the lenses came from.

Format

This data frame contains the following columns:

activity: The activity that is completed to make the eyeglasses by
Eyeglassomatic

Source John Matic provided the data from a company he worked with. The
company's name is fictitious, but the data is from an actual company.

References John Matic (2013)

Make a bar chart of this data. State any findings you can see from the
graph.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Data was collected for two semesters in a statistics class drive. The
  data frame is in Table~\ref{tbl-Class}.
\end{enumerate}

\textbf{Code book for the Data Frame Class is found below
Table~\ref{tbl-Class}.}

Create a bar graph of the variable ice cream. State any findings you can
see from the graphs.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The number of deaths in the US due to carbon monoxide (CO) poisoning
  from generators from the years 1999 to 2011 are in
  Table~\ref{tbl-Area} (Hinatov, 2012). Create a bar chart of this data.
  State any findings you see from the graph.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Area}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/area.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Area))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}l@{}}

\caption{\label{tbl-Area}Head of Area Data frame}

\tabularnewline

\toprule\noalign{}
deaths \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Urban \\
Urban \\
Urban \\
Urban \\
Urban \\
Urban \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Data was collected for two semesters in a statistics class drive. The
  data frame is in Table~\ref{tbl-Class}. Create a bar graph and dot
  plot of the variable major. Create a jitter plot of major and gender.
  State any findings you can see from the graphs.

  \textbf{Code book for the Data Frame Class is found below
  Table~\ref{tbl-Class}.}
\item
  Eyeglassomatic manufactures eyeglasses for different retailers. They
  test to see how many defective lenses they made during the time period
  of January 1 to March 31. The table Table~\ref{tbl-Defects} gives the
  defect and the number of defects. Create a bar chart of the data and
  then describe what this tells you about what causes the most defects.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Defects}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/defects.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Defects))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}l@{}}

\caption{\label{tbl-Defects}Head of Defects Data frame}

\tabularnewline

\toprule\noalign{}
type \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
small \\
small \\
pd \\
flaked \\
scratch \\
spot \\

\end{longtable}

\textbf{Code book for Data Frame Defects}

\textbf{Description} Types of defects that an Eyeglass company sees in
the lenses they make into eyeglasses.

Format

This data frame contains the following columns:

type: The type of defect that is Seen when making eyeglasses by
Eyeglassomatic

Source John Matic provided the data from a company he worked with. The
company's name is fictitious, but the data is from an actual company.

References John Matic (2013)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  American National Health and Nutrition Examination (NHANES) surveys is
  collected every year by the US National Center for Health Statistics
  (NCHS). The data frame is in Table~\ref{tbl-NHANES}. Create a bar
  chart of MartialStatus. Create a jitter plot of MaritalStatus versus
  Education. Describe any findings from the graphs.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(NHANES))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0051}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0165}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0178}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0102}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0102}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0140}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0216}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0165}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0140}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0102}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0140}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0140}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0203}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0203}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0190}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0165}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0102}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0140}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0178}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0165}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0140}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0190}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0140}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0178}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0203}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0203}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0140}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0178}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0165}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0127}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0102}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0089}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0203}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0190}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0102}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0190}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}@{}}

\caption{\label{tbl-NHANES}NHANES Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SurveyYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AgeDecade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeMonths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MaritalStatus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HHIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HHIncomeMid
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Poverty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HomeRooms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HomeOwn
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Work
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HeadCirc
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BMI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMICatUnder20yrs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMI\_WHO
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Pulse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSysAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDiaAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Testosterone
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DirectChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TotChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diabetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DiabetesAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HealthGen
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysPhysHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysMentHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LittleInterest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Depressed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nPregnancies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nBabies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age1stBaby
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SleepHrsNight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SleepTrouble
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PhysActive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PhysActiveDays
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TVHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CompHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TVHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
CompHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alcohol12PlusYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SmokeNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SmokeAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marijuana
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeFirstMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RegularMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeRegMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HardDrugs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexEver
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartnLife
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SameSex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexOrientation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PregnantNow
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
51624 & 2009\_10 & male & 34 & 30-39 & 409 & White & NA & High School &
Married & 25000-34999 & 30000 & 1.36 & 6 & Own & NotWorking & 87.4 & NA
& NA & 164.7 & 32.22 & NA & 30.0\_plus & 70 & 113 & 85 & 114 & 88 & 114
& 88 & 112 & 82 & NA & 1.29 & 3.49 & 352 & NA & NA & NA & No & NA & Good
& 0 & 15 & Most & Several & NA & NA & NA & 4 & Yes & No & NA & NA & NA &
NA & NA & Yes & NA & 0 & No & Yes & Smoker & 18 & Yes & 17 & No & NA &
Yes & Yes & 16 & 8 & 1 & No & Heterosexual & NA \\
51624 & 2009\_10 & male & 34 & 30-39 & 409 & White & NA & High School &
Married & 25000-34999 & 30000 & 1.36 & 6 & Own & NotWorking & 87.4 & NA
& NA & 164.7 & 32.22 & NA & 30.0\_plus & 70 & 113 & 85 & 114 & 88 & 114
& 88 & 112 & 82 & NA & 1.29 & 3.49 & 352 & NA & NA & NA & No & NA & Good
& 0 & 15 & Most & Several & NA & NA & NA & 4 & Yes & No & NA & NA & NA &
NA & NA & Yes & NA & 0 & No & Yes & Smoker & 18 & Yes & 17 & No & NA &
Yes & Yes & 16 & 8 & 1 & No & Heterosexual & NA \\
51624 & 2009\_10 & male & 34 & 30-39 & 409 & White & NA & High School &
Married & 25000-34999 & 30000 & 1.36 & 6 & Own & NotWorking & 87.4 & NA
& NA & 164.7 & 32.22 & NA & 30.0\_plus & 70 & 113 & 85 & 114 & 88 & 114
& 88 & 112 & 82 & NA & 1.29 & 3.49 & 352 & NA & NA & NA & No & NA & Good
& 0 & 15 & Most & Several & NA & NA & NA & 4 & Yes & No & NA & NA & NA &
NA & NA & Yes & NA & 0 & No & Yes & Smoker & 18 & Yes & 17 & No & NA &
Yes & Yes & 16 & 8 & 1 & No & Heterosexual & NA \\
51625 & 2009\_10 & male & 4 & 0-9 & 49 & Other & NA & NA & NA &
20000-24999 & 22500 & 1.07 & 9 & Own & NA & 17.0 & NA & NA & 105.4 &
15.30 & NA & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & 4 & 1 & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
51630 & 2009\_10 & female & 49 & 40-49 & 596 & White & NA & Some College
& LivePartner & 35000-44999 & 40000 & 1.91 & 5 & Rent & NotWorking &
86.7 & NA & NA & 168.4 & 30.57 & NA & 30.0\_plus & 86 & 112 & 75 & 118 &
82 & 108 & 74 & 116 & 76 & NA & 1.16 & 6.70 & 77 & 0.094 & NA & NA & No
& NA & Good & 0 & 10 & Several & Several & 2 & 2 & 27 & 8 & Yes & No &
NA & NA & NA & NA & NA & Yes & 2 & 20 & Yes & Yes & Smoker & 38 & Yes &
18 & No & NA & Yes & Yes & 12 & 10 & 1 & Yes & Heterosexual & NA \\
51638 & 2009\_10 & male & 9 & 0-9 & 115 & White & NA & NA & NA &
75000-99999 & 87500 & 1.84 & 6 & Rent & NA & 29.8 & NA & NA & 133.1 &
16.82 & NA & 12.0\_18.5 & 82 & 86 & 47 & 84 & 50 & 84 & 50 & 88 & 44 &
NA & 1.34 & 4.86 & 123 & 1.538 & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & 5 & 0 & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA \\

\end{longtable}

To view the code book for NHANES, type help(``NHANES'') in rStudio after
you load the NHANES packages using library(``NHANES'')

\section{Quantitative Data}\label{quantitative-data}

There are several different graphs for quantitative data. With
quantitative data, you can talk about how the data is distributed,
called a distribution. The shape of the distribution can be described
from the graphs.

\textbf{Histogram:} a graph of frequencies (counts) on the vertical axis
and classes on the horizontal axis. The height of the rectangles is the
frequency and the width is the class width. The width depends on how
many classes (bins) are in the histogram. The shape of a histogram is
dependent on the number of bins. In RStudio the command to create a
histogram is

gf\_histogram(\textasciitilde response variable, data=Data\_Frame,
title=``title of the graph'')

The last part of the command puts a title on the graph. You type in what
ever you want for the title in the quotes.

\textbf{Density Plot:} Similar to a histogram, except smoothing is
created to smooth out the graph. The shape is not dependent on the
number of bins so the distribution is easier to determine from the
density plot. In RStudio the command to create a density plot is

gf\_density(\textasciitilde response variable, data=Data\_Frame,
title=``title of the graph'', xlab=``Label'', ylab=``Label'')

The last part of the command puts a title on the graph and labels on the
axes. You type in what every you want for the title and labels in the
quotes.

The last part of the command puts a title on the graph and labels on the
axes. You type in what every you want for the title and labels in the
quotes.

\subsection{Example: Drawing a Histogram and Density
plot}\label{example-drawing-a-histogram-and-density-plot}

Data was collected for two semesters in a statistics class drive. The
data frame is in Table~\ref{tbl-Class} and the code book is below the
data frame

Draw a histogram, density plot, and a dot plot for the variable the
distance a student lives from the Lone Tree Campus of Coconino Community
College. Describe the story the graphs tell.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-14}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_histogram}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{distance\_campus, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{title=}\StringTok{"Distance in miles from the Lone Tree Campus"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Distance (miles)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-class_data_histogram-1.pdf}

}

\caption{\label{fig-class_data_histogram}Distance in miles from the Lone
Tree Campus}

\end{figure}%

Description of the graph is histogram with high part on left and low
part on right with several gaps. The graph contains bars.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{distance\_campus, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{title=}\StringTok{"Distance in miles from the Lone Tree Campus"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Distance (miles)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-class_data_density-1.pdf}

}

\caption{\label{fig-class_data_density}Distance in miles from the Lone
Tree Campus}

\end{figure}%

Description of the graph is density graph with high part on left and low
part on right with several gaps. The graph is smooth.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_dotplot}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{distance\_campus, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{title=}\StringTok{"Distance in miles from the Lone Tree Campus"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Distance (miles)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-class_data_dot-1.pdf}

}

\caption{\label{fig-class_data_dot}Distance in miles from the Lone Tree
Campus}

\end{figure}%

Description of the graph of dot plot with high part on left and low part
on right with several gaps. The graph is with dots that represent each
data value.

Notice the histogram, density plot, and dot plot are all very similar,
but the density plot is smoother. They all tell you similar ideas of the
shape of the distribution. Reviewing the graphs you can see that most of
the students live within 10 miles of the Lone Tree Campus, in fact most
live within 5 miles from the campus. However, there is a student who
lives around 50 miles from the Lone Tree Campus. This is a great deal
farther from the rest of the data. This value could be considered an
outlier. An outlier is a data value that is far from the rest of the
values. It may be an unusual value or a mistake. It is a data value that
should be investigated. In this case, the student lived really far from
campus, thus the value is not a mistake, and is just very unusual. The
density plot is probably the best plot for most data frames.

There are other aspects that can be discussed, but first some other
concepts need to be introduced.

\subsection{Shapes of the
distribution:}\label{shapes-of-the-distribution}

When you look at a distribution, look at the basic shape. There are some
basic shapes that are seen in histograms. Realize though that some
distributions have no shape. The common shapes are symmetric, skewed,
and uniform. Another interest is how many peaks a graph may have. This
is known as modal.

Symmetric means that you can fold the graph in half down the middle and
the two sides will line up. You can think of the two sides as being
mirror images of each other. Skewed means one ``tail'' of the graph is
longer than the other. The graph is skewed in the direction of the
longer tail (backwards from what you would expect). A uniform graph has
all the bars the same height.

Modal refers to the number of peaks. Unimodal has one peak and bimodal
has two peaks. Usually if a graph has more than two peaks, the modal
information is not longer of interest.

Other important features to consider are gaps between bars, a repetitive
pattern, how spread out is the data, and where the center of the graph
is.

\subsection{Examples of graphs:}\label{examples-of-graphs}

This graph is roughly symmetric and unimodal:

\textbf{Graph: Symmetric Distribution}

\begin{figure}[H]

{\centering \includegraphics{symmetric.png}

}

\caption{symmetric Graph}

\end{figure}%

This graph is symmetric and bimodal:

\textbf{Graph: Symmetric and Bimodal Distribution}

\begin{figure}[H]

{\centering \includegraphics{bimodal_symmetric.png}

}

\caption{Bimodal and symmetric graph}

\end{figure}%

This graph is skewed to the right:

\textbf{Graph: Skewed Right Distribution}

\begin{figure}[H]

{\centering \includegraphics{skewed_right.png}

}

\caption{Skewed right graph}

\end{figure}%

This graph is skewed to the left and has a gap:

\textbf{Graph: Skewed Left Distribution}

\begin{figure}[H]

{\centering \includegraphics{skewed_left.png}

}

\caption{Skewed Left graph}

\end{figure}%

This graph is uniform since all the bars are the same height:

\textbf{Graph: Uniform Distribution}

\begin{figure}[H]

{\centering \includegraphics{Uniform.png}

}

\caption{Uniform graph}

\end{figure}%

\subsection{Example: Drawing a Histogram and Density
plot}\label{example-drawing-a-histogram-and-density-plot-1}

Data was collected from the Chronicle of Higher Education for tuition
from public four year colleges, private four year colleges, and for
profit four year colleges. The data frame is in Table~\ref{tbl-Tuition}.
Draw a density plot of instate tuition levels for all four year
institutions, and then separate the density plot for instate tuition
based on type of institution. Describe any findings from the graph.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tuition}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Tuition\_4\_year.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Tuition))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.3797}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0949}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0380}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0696}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1013}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.0886}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1203}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 14\tabcolsep) * \real{0.1076}}@{}}

\caption{\label{tbl-Tuition}Head of Tuition Data Frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
INSTITUTION
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TYPE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
STATE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ROOM\_BOARD
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
INSTATE\_TUITION
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
INSTATE\_TOTAL
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
OUTOFSTATE\_TUITION
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
OUTOFSTATE\_TOTAL
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
University of Alaska AnchoragePublic 4-year & Public\_4 year & AK &
12200 & 7688 & 19888 & 23858 & 36058 \\
University of Alaska FairbanksPublic 4-year & Public\_4 year & AK & 8930
& 8087 & 17017 & 24257 & 33187 \\
University of Alaska SoutheastPublic 4-year & Public\_4 year & AK & 9200
& 7092 & 16292 & 19404 & 28604 \\
Alaska Bible CollegePrivate 4-year & Private\_4\_year & AK & 5700 & 9300
& 15000 & 9300 & 15000 \\
Alaska Pacific UniversityPrivate 4-year & Private\_4\_year & AK & 7300 &
20830 & 28130 & 20830 & 28130 \\
Alabama Agricultural and Mechanical UniversityPublic 4-year & Public\_4
year & AL & 8379 & 9698 & 18077 & 17918 & 26297 \\

\end{longtable}

\textbf{Code book for Data Frame Tuition}

\textbf{Description} Cost of four year institutions.

Format

This data frame contains the following columns:

INSTITUTION: Name of four year institution

TYPE: Type of four year institution, Public\_4\_year, Private\_4\_year,
For\_profit\_4\_year.

STATE: What state the institution resides

ROOM\_BOARD: The cost of room and board at the institution
(\textbackslash\$)

INSTATE\_TUTION: The cost of instate tuition (\textbackslash\$)

INSTATE\_TOTAL: The cost of room and board and instate tuition
(\textbackslash\$ per year)

OUTOFSTATE\_TUTION: The cost of out of state tuition (\textbackslash\$
per year)

OUTOFSTATE\_TOTAL: The cost of room and board and out of state tuition
(\textbackslash\$ per year)

Source Tuition and Fees, 1998-99 Through 2018-19. (2018, December 31).
Retrieved from https://www.chronicle.com/interactives/tuition-and-fees

References Chronicle of Higher Education *, December 31, 2018.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-15}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{INSTATE\_TUITION, }\AttributeTok{data=}\NormalTok{Tuition, }\AttributeTok{title=}\StringTok{"Instate Tuition at all Four Year institutions"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Instate Tutition ($ per year)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

{\centering \includegraphics{Graphical-Description-of-Data_files/figure-pdf/tuition-instate-1.pdf}

}

\caption{Density Plot for Instate Tuition Levels at all Four-Year
Colleges}

\end{figure}%

Description of the graph is a density with high part on left, then a dip
and up to peak in the middle that is lower than the left peak and then
the lowest peak on the right .

(ref:tuition-instate-type-cap) Density Plot for Instate Tuition Levels
at all Four-Year Colleges

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{INSTATE\_TUITION}\SpecialCharTok{|}\NormalTok{TYPE, }\AttributeTok{data=}\NormalTok{Tuition, }\AttributeTok{title=}\StringTok{"Instate Tuition at all Four Year institions"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Instate Tuition ($/year)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-tuition_data_density-1.pdf}

}

\caption{\label{fig-tuition_data_density}Instate Tuition at all Four
Year institions}

\end{figure}%

Description of Figure~\ref{fig-tuition_data_density} is a density plots
separated by for profit 4 year with peak on left, private 4 year with
peak in the middle, and public 4 year colleges with peak on the left.
Public 4 year has the highest peak, with for profit 4 year is lower, and
then private 4 year with the lowest peak.

The distribution is skewed right, with no gaps. Most institutions in
state is less than \textbackslash\$ 20,000 per year though some go as
high as \textbackslash\$ 60,000 per year. When separated by public
versus private and for profit, most public are much less than
\textbackslash\$ 20,000 per year while private four year cost around
\textbackslash\$ 30,000 per year, and for profit are around
\textbackslash\$ 20,000 per year.

There are other types of graphs for quantitative data. They will be
explored in the next section.

\subsection{Homework for Quantitative Data
Section}\label{homework-for-quantitative-data-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The weekly median incomes of males and females for specific
  occupations, are given in Table~\ref{tbl-wages} (CPS News Releases.
  (n.d.). Retrieved July 8, 2019, from https://www.bls.gov/cps/). Create
  a density plot for males and females. Discuss any findings from the
  graph. Note: to put two graphs on the same axis, type the piping
  symbol \textbar\textgreater{} (base r) or \%\textgreater\% (magrittr
  package) (Note: \textbar\textgreater{} and \%\textgreater\% are piping
  symbols that can be thought of as ``and then'') at the end of the
  first command and then type the command for the second graph on the
  next line. Also, use fill=``pick a color'' in the command to plot the
  graphs with different colors so the two graphs can be easier to
  distinguish.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Wages}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/wages.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Wages))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.4538}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0846}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0923}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0923}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0769}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1077}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.0923}}@{}}

\caption{\label{tbl-wages}Head of Wages Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Occupation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Numworkers
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
median\_wage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
male\_worker
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
male\_wage
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
female\_worker
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
female\_wage
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Management, professional, and related occupations & 48808 & 1246 & 23685
& 1468 & 25123 & 1078 \\
Management, business, and financial operations occupations & 19863 &
1355 & 10668 & 1537 & 9195 & 1168 \\
Management occupations & 13477 & 1429 & 7754 & 1585 & 5724 & 1236 \\
Chief executives & 1098 & 2291 & 790 & 2488 & 307 & 1736 \\
General and operations managers & 939 & 1338 & 656 & 1427 & 283 &
1139 \\
Legislators & 14 & NA & 10 & NA & 4 & NA \\

\end{longtable}

\textbf{Code book for Data Frame Wages}

\textbf{Description} Median weekly earnings of full-time wage and salary
workers by detailed occupation and sex. The Current Population Survey
(CPS) is a monthly survey of households conducted by the Bureau of
Census for the Bureau of Labor Statistics. It provides a comprehensive
body of data on the labor force, employment, unemployment, persons not
in the labor force, hours of work, earnings, and other demographic and
labor force characteristics.

Format

This data frame contains the following columns:

Occupation: Occupations of workers.

Numworkers: The number of workers in each occupation (in thousands of
workers)

median\_wage: Median weekly wage (\textbackslash\$)

male\_worker: number of male workers (in thousands of workers)

male\_wage: Median weekly wage of male workers (\textbackslash\$)

female\_worker: number of female workers (in thousands of workers)

female\_wage: Median weekly wage of female workers (\textbackslash\$)

Source CPS News Releases. (n.d.). Retrieved July 8, 2019, from
https://www.bls.gov/cps/

References Current Population Survey (CPS) retrieved July 8, 2019.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The density of people per square kilometer for certain countries is in
  Table~\ref{tbl-Density} (World Bank, 2019). Create density plot of
  density in 2018 for just Sub-Saharan Africa. Describe what story the
  graph tells.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Density}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/density.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Density))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0190}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0190}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0381}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0293}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0161}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0146}}@{}}

\caption{\label{tbl-Density}Head of Density Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Country\_Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Country\_Code
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
IncomeGroup
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1961
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1962
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1963
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1964
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1965
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1966
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1967
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1968
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1969
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1970
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1971
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1972
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1973
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1974
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1975
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1976
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1977
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1978
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1979
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1980
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1981
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1982
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1983
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1984
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1985
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1986
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1987
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1988
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1989
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1990
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1991
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1992
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1993
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1994
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1995
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1996
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1997
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1998
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1999
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2001
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2002
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2003
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2004
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2005
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2006
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2007
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2008
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2009
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2010
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2012
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2013
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2014
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2015
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2016
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2017
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2018
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Aruba & ABW & Latin America \& Caribbean & High income & 307.988889 &
312.361111 & 314.972222 & 316.844444 & 318.666667 & 320.638889 &
322.527778 & 324.366667 & 326.255556 & 328.127778 & 330.222222 &
332.444444 & 334.683333 & 336.266667 & 336.983333 & 336.588889 &
335.366667 & 333.905556 & 333.222222 & 333.866667 & 336.483333 &
340.805556 & 345.561111 & 349.088889 & 350.144444 & 348.022222 &
343.516667 & 339.327778 & 339.066667 & 345.272222 & 359.011111 &
379.08333 & 402.80000 & 426.11111 & 446.24444 & 462.22222 & 474.72778 &
484.87222 & 494.47222 & 504.73889 & 516.10000 & 527.73333 & 538.98333 &
548.53889 & 555.72778 & 560.18889 & 562.34444 & 563.10000 & 563.63889 &
564.82778 & 566.92222 & 569.77778 & 573.10556 & 576.52222 & 579.67222 &
582.62222 & 585.36667 & 588.02778 \\
Afghanistan & AFG & South Asia & Low income & 14.044987 & 14.323808 &
14.617537 & 14.926295 & 15.250314 & 15.585020 & 15.929795 & 16.293023 &
16.686236 & 17.114913 & 17.577191 & 18.060863 & 18.547565 & 19.013188 &
19.436265 & 19.825220 & 20.174779 & 20.435006 & 20.542009 & 20.458461 &
20.175341 & 19.732451 & 19.204316 & 18.693582 & 18.286015 & 17.976563 &
17.774920 & 17.795553 & 18.179820 & 19.012205 & 20.370396 & 22.18783 &
24.22664 & 26.15527 & 27.74049 & 28.87822 & 29.64973 & 30.23277 &
30.89612 & 31.82911 & 33.09590 & 34.61810 & 36.27251 & 37.87440 &
39.29522 & 40.48808 & 41.51049 & 42.46282 & 43.49296 & 44.70408 &
46.13150 & 47.73056 & 49.42804 & 51.11478 & 52.71207 & 54.19711 &
55.59599 & 56.93776 \\
Angola & AGO & Sub-Saharan Africa & Lower middle income & 4.436891 &
4.498708 & 4.555593 & 4.600180 & 4.628676 & 4.637213 & 4.631622 &
4.629544 & 4.654892 & 4.724765 & 4.845414 & 5.012073 & 5.211328 &
5.423422 & 5.634074 & 5.839022 & 6.042941 & 6.249063 & 6.463517 &
6.690695 & 6.930654 & 7.181319 & 7.442124 & 7.712163 & 7.990693 &
8.277943 & 8.574036 & 8.877878 & 9.188078 & 9.503799 & 9.825059 &
10.15270 & 10.48773 & 10.83159 & 11.18570 & 11.55107 & 11.92875 &
12.32021 & 12.72709 & 13.15110 & 13.59249 & 14.05263 & 14.53556 &
15.04624 & 15.58803 & 16.16259 & 16.76856 & 17.40245 & 18.05910 &
18.73446 & 19.42782 & 20.13951 & 20.86771 & 21.61047 & 22.36655 &
23.13506 & 23.91654 & 24.71305 \\
Albania & ALB & Europe \& Central Asia & Upper middle income & 60.576642
& 62.456898 & 64.329234 & 66.209307 & 68.058066 & 69.874927 & 71.737153
& 73.805548 & 75.974270 & 77.937190 & 79.848650 & 81.865912 & 83.823066
& 85.770949 & 87.767555 & 89.727226 & 91.735255 & 93.659343 & 95.541314
& 97.518139 & 99.491095 & 101.615985 & 103.794161 & 106.001058 &
108.202993 & 110.315146 & 112.540329 & 114.683796 & 117.808139 &
119.946788 & 119.225912 & 118.50507 & 117.78420 & 117.06336 & 116.34248
& 115.62164 & 114.90077 & 114.17993 & 113.45905 & 112.73821 & 111.68515
& 111.35073 & 110.93489 & 110.47223 & 109.90828 & 109.21704 & 108.39478
& 107.56620 & 106.84376 & 106.31463 & 106.02901 & 105.85405 & 105.66029
& 105.44175 & 105.13515 & 104.96719 & 104.87069 & 104.61226 \\
Andorra & AND & Europe \& Central Asia & High income & 30.585106 &
32.702128 & 34.919149 & 37.168085 & 39.465957 & 41.802128 & 44.165957 &
46.574468 & 49.059574 & 51.651064 & 54.380851 & 57.217021 & 60.068085 &
62.808511 & 65.329787 & 67.610638 & 69.725532 & 71.780851 & 74.080851 &
76.738298 & 79.787234 & 83.221277 & 86.951064 & 90.863830 & 94.893617 &
98.972340 & 103.095745 & 107.306383 & 111.591489 & 115.976596 &
120.576596 & 125.29362 & 129.72553 & 133.35532 & 135.85106 & 136.93617 &
136.86596 & 136.47234 & 136.95745 & 139.12766 & 143.27872 & 149.04043 &
155.70638 & 162.22128 & 167.80213 & 172.32553 & 175.92340 & 178.42979 &
179.70851 & 179.67872 & 178.18511 & 175.37660 & 171.85957 & 168.53830 &
165.98085 & 164.46170 & 163.83191 & 163.84255 \\
Arab World & ARB & & & 8.430860 & 8.663154 & 8.903441 & 9.152526 &
9.410965 & 9.679951 & 9.959490 & 10.247580 & 10.541383 & 10.839409 &
11.140162 & 11.445801 & 11.762925 & 12.100336 & 12.464221 & 12.856964 &
13.276051 & 13.716559 & 14.171137 & 14.634158 & 15.103942 & 15.581254 &
16.065812 & 16.557944 & 17.057705 & 17.563945 & 18.075438 & 18.592082 &
19.114029 & 19.817110 & 20.358106 & 20.73408 & 21.29364 & 21.84602 &
22.52760 & 23.05216 & 23.57027 & 24.08237 & 24.60020 & 25.12980 &
25.67166 & 26.22642 & 26.80081 & 27.40153 & 28.03371 & 28.69994 &
29.39751 & 30.11889 & 30.85858 & 31.59402 & 32.33012 & 33.06767 &
33.80379 & 34.53398 & 35.25690 & 35.96876 & 36.66980 & 37.37237 \\

\end{longtable}

\textbf{Code book for Data Frame Density}

\textbf{Description} Population density of all countries in the world

Format

This data frame contains the following columns:

Country\_Name: The name of countries or regions around the world

Country\_Code: The 3 letter code for a country or region

Region: World Banks classification of where the country is in the world

Incomegroup: World Banks classification of what income level the country
is considered to be

y1961-y2018: population density for the years 1961 through 2018, people
per sq. km of land area, population density is midyear population
divided by land area in square kilometers. Population is based on the de
facto definition of population, which counts all residents regardless of
legal status or citizenship--except for refugees not permanently settled
in the country of asylum, who are generally considered part of the
population of their country of origin. Land area is a country's total
area, excluding area under inland water bodies, national claims to
continental shelf, and exclusive economic zones. In most cases the
definition of inland water bodies includes major rivers and lakes.

Source Population density (people per sq. km of land area). (n.d.).
Retrieved July 9, 2019, from
https://data.worldbank.org/indicator/EN.POP.DNST

References Food and Agriculture Organization and World Bank population
estimates.

Since the Density data frame is for all countries, a new data frame must
be created with just Sub-Saharan Africa Table~\ref{tbl-Africa}. This is
created by using the following command

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Africa }\OtherTok{\textless{}{-}}\NormalTok{ Density }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(Region }\SpecialCharTok{==} \StringTok{"Sub{-}Saharan Africa"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Africa))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0347}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0181}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0264}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0278}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0167}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0167}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0167}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0167}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0167}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 122\tabcolsep) * \real{0.0153}}@{}}

\caption{\label{tbl-Africa}Head of Africa Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Country\_Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Country\_Code
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
IncomeGroup
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1961
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1962
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1963
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1964
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1965
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1966
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1967
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1968
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1969
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1970
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1971
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1972
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1973
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1974
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1975
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1976
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1977
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1978
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1979
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1980
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1981
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1982
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1983
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1984
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1985
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1986
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1987
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1988
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1989
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1990
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1991
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1992
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1993
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1994
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1995
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1996
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1997
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1998
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1999
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2001
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2002
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2003
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2004
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2005
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2006
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2007
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2008
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2009
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2010
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2012
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2013
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2014
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2015
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2016
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2017
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2018
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Angola & AGO & Sub-Saharan Africa & Lower middle income & 4.4368910 &
4.4987078 & 4.5555932 & 4.6001797 & 4.6286757 & 4.637213 & 4.631622 &
4.629544 & 4.654892 & 4.724765 & 4.845414 & 5.012073 & 5.211328 &
5.423422 & 5.634074 & 5.839022 & 6.042941 & 6.249063 & 6.463517 &
6.690695 & 6.930654 & 7.181319 & 7.442124 & 7.712163 & 7.990693 &
8.277943 & 8.574036 & 8.877878 & 9.188078 & 9.503799 & 9.825059 &
10.152696 & 10.487727 & 10.831593 & 11.185695 & 11.551070 & 11.928748 &
12.320206 & 12.727095 & 13.151097 & 13.592487 & 14.052633 & 14.535557 &
15.046238 & 15.588034 & 16.162590 & 16.768559 & 17.402450 & 18.059101 &
18.734456 & 19.427818 & 20.139513 & 20.867715 & 21.610475 & 22.366553 &
23.135064 & 23.916538 & 24.713052 \\
Burundi & BDI & Sub-Saharan Africa & Low income & 111.0762461 &
113.2134346 & 115.4371885 & 117.8461838 & 120.4976246 & 123.461449 &
126.682944 & 129.942640 & 132.940187 & 135.477959 & 137.460942 &
139.005685 & 140.386527 & 141.994977 & 144.115265 & 146.840771 &
150.095210 & 153.787617 & 157.758333 & 161.888551 & 166.141744 &
170.550000 & 175.137578 & 179.949494 & 185.001441 & 190.293731 &
195.760826 & 201.273287 & 206.661565 & 211.797391 & 216.702726 &
221.400506 & 225.780880 & 229.710553 & 233.140304 & 235.985631 &
238.400701 & 240.870794 & 244.046885 & 248.398403 & 254.110008 &
261.063590 & 269.048053 & 277.713902 & 286.793692 & 296.255802 &
306.160981 & 316.436994 & 327.011994 & 337.834969 & 348.847586 &
360.046262 & 371.506581 & 383.344899 & 395.639797 & 408.411137 &
421.613084 & 435.178271 \\
Benin & BEN & Sub-Saharan Africa & Low income & 21.8682778 & 22.1966655
& 22.5510731 & 22.9333540 & 23.3447677 & 23.786440 & 24.257778 &
24.756917 & 25.280782 & 25.827776 & 26.397410 & 26.991548 & 27.613294 &
28.267222 & 28.956767 & 29.684046 & 30.449087 & 31.251667 & 32.090511 &
32.965280 & 33.878397 & 34.832512 & 35.827856 & 36.864305 & 37.943429 &
39.060890 & 40.220495 & 41.440688 & 42.745796 & 44.151259 & 45.667781 &
47.284525 & 48.969165 & 50.675949 & 52.372810 & 54.046284 & 55.708044 &
57.380853 & 59.099840 & 60.889952 & 62.759250 & 64.698421 & 66.695238 &
68.730082 & 70.789509 & 72.870672 & 74.980428 & 77.127714 & 79.325186 &
81.582645 & 83.902359 & 86.282795 & 88.724619 & 91.227758 & 93.791699 &
96.417763 & 99.106101 & 101.853920 \\
Burkina Faso & BFA & Sub-Saharan Africa & Low income & 17.8895468 &
18.1298465 & 18.3765387 & 18.6362939 & 18.9139985 & 19.211853 &
19.528578 & 19.861261 & 20.205314 & 20.557748 & 20.918790 & 21.290837 &
21.675742 & 22.076173 & 22.494682 & 22.931422 & 23.387920 & 23.869953 &
24.384708 & 24.937292 & 25.530556 & 26.163213 & 26.830793 & 27.526469 &
28.245274 & 28.986455 & 29.751729 & 30.542050 & 31.359002 & 32.204072 &
33.077792 & 33.980676 & 34.914020 & 35.879342 & 36.878209 & 37.912080 &
38.982259 & 40.090365 & 41.237942 & 42.426689 & 43.657116 & 44.930921 &
46.252270 & 47.626349 & 49.056762 & 50.545234 & 52.090720 & 53.690515 &
55.340271 & 57.036612 & 58.778914 & 60.567420 & 62.400493 & 64.276378 &
66.193801 & 68.151966 & 70.150892 & 72.191283 \\
Botswana & BWA & Sub-Saharan Africa & Upper middle income & 0.9046371 &
0.9242108 & 0.9452208 & 0.9667267 & 0.9881143 & 1.009235 & 1.030635 &
1.053318 & 1.078644 & 1.107609 & 1.140485 & 1.177090 & 1.217356 &
1.261116 & 1.308127 & 1.358635 & 1.412540 & 1.468895 & 1.526432 &
1.584296 & 1.641713 & 1.699001 & 1.757680 & 1.819983 & 1.887287 &
1.960269 & 2.037842 & 2.117529 & 2.195903 & 2.270492 & 2.340307 &
2.406003 & 2.468742 & 2.530410 & 2.592370 & 2.655109 & 2.718093 &
2.780555 & 2.841325 & 2.899677 & 2.954984 & 3.007856 & 3.060360 &
3.115288 & 3.174489 & 3.239476 & 3.309264 & 3.380162 & 3.446964 &
3.506264 & 3.556194 & 3.598805 & 3.639363 & 3.685377 & 3.742022 &
3.811240 & 3.890967 & 3.977425 \\
Central African Republic & CAF & Sub-Saharan Africa & Low income &
2.4496228 & 2.4911073 & 2.5351857 & 2.5821310 & 2.6320363 & 2.685510 &
2.742146 & 2.799759 & 2.855406 & 2.907227 & 2.954377 & 2.998141 &
3.041595 & 3.089005 & 3.143547 & 3.205583 & 3.274453 & 3.351091 &
3.436349 & 3.530380 & 3.634855 & 3.748648 & 3.865801 & 3.978269 &
4.080659 & 4.169895 & 4.248676 & 4.324333 & 4.407419 & 4.505336 &
4.620548 & 4.750130 & 4.889642 & 5.032288 & 5.172969 & 5.310336 &
5.445497 & 5.578818 & 5.711281 & 5.843570 & 5.974539 & 6.103130 &
6.230025 & 6.356344 & 6.482362 & 6.610275 & 6.738595 & 6.859556 &
6.962703 & 7.041587 & 7.092741 & 7.121280 & 7.139783 & 7.165840 &
7.212382 & 7.283841 & 7.377489 & 7.490412 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The Affordable Care Act created a market place for individuals to
  purchase health care plans. In 2014, the premiums for a 27 year old
  for the different levels health insurance are given in
  Table~\ref{tbl-Insurance} (\textbackslash{}``Health insurance
  marketplace,\textbackslash{}'' 2013). Create a density plot of
  bronze\_lowest, then silver\_lowest, and gold\_lowest all on the same
  aces. Use \textbar\textgreater{} or \%\textgreater\% at the end of
  each command. Describe the story the graphs tells.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Insurance}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/insurance.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Insurance))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0299}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0597}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0697}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0697}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0597}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0647}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1045}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1095}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1095}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1045}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1095}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1095}}@{}}

\caption{\label{tbl-Insurance}Head of Insurance Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
state
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
average\_QHP
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
bronze\_lowest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
silver\_lowest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
gold\_lowest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
catastrophic
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
second\_silver\_pretax
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
second\_silver\_posttax
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
lowest\_bronze\_posttax
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
silver\_family\_pretax
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
silver\_family\_posttax
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
bronze\_family\_posttax
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
AK & 34 & 254 & 312 & 401 & 236 & 312 & 107 & 48 & 1131 & 205 & 0 \\
AL & 7 & 162 & 200 & 248 & 138 & 209 & 145 & 98 & 757 & 282 & 112 \\
AR & 28 & 181 & 231 & 263 & 135 & 241 & 145 & 85 & 873 & 282 & 64 \\
AZ & 106 & 141 & 164 & 187 & 107 & 166 & 145 & 120 & 600 & 282 & 192 \\
DE & 19 & 203 & 234 & 282 & 137 & 237 & 145 & 111 & 859 & 282 & 158 \\
FL & 102 & 169 & 200 & 229 & 132 & 218 & 145 & 96 & 789 & 282 & 104 \\

\end{longtable}

\textbf{Code book for Data Frame Insurance}

\textbf{Description} The Affordable Care Act created a market place for
individuals to purchase health care plans.The data is from 2014.

Format

This data frame contains the following columns:

state: state of insured.

average\_QHP: The number of qualified health plans

bronze\_lowest: premium for the lowest bronze level of insurance for a
single person (\textbackslash\$)

silver\_lowest: premium for the lowest silver level of insurance for a
single person (\textbackslash\$)

gold\_lowest: premium for the lowest gold level of insurance for a
single person (\textbackslash\$)

catastrophic: premium for the catastrophic level of insurance for a
single person (\textbackslash\$)

second\_silver\_pretax: premium for the second silver level of insurance
for a single person pretax (\textbackslash\$)

second\_silver\_posttax: premium for the second silver level of
insurance for a single person posttax (\textbackslash\$)

second\_bronze\_posttax: premium for the lowest bronze level of
insurance for a single person posttax (\textbackslash\$)

silver\_family\_pretax: premium for the silver level of insurance for a
family pretax (\textbackslash\$)

silver\_family\_posttax: premium for the silver level of insurance for a
family posttax (\textbackslash\$)

bronze\_family\_posttax: premium for the bronze level of insurance for a
family posttax (\textbackslash\$)

Source Health Insurance Market Place Retrieved from website:
http://aspe.hhs.gov/health/reports/2013/marketplacepremiums/ib\_premiumslandscape.pdf
premiums for 2014.

References Department of Health and Human Services, ASPE. (2013). Health
insurance marketplace

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Students in a statistics class took their first test. In
  Table~\ref{tbl-first-test-1} are the scores they earned. Create a
  density plot for grades. Describe the shape of the distribution.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Firsttest\_1}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/firsttest\_1.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Firsttest\_1))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-first-test-1}Head of First Test Data frame}

\tabularnewline

\toprule\noalign{}
grades \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
80 \\
79 \\
89 \\
74 \\
73 \\
67 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Students in a statistics class took their first test. The scores they
  earned are in Table~\ref{tbl-first-test-2}. Create a density plot for
  grades. Describe the shape of the distribution. Compare to the graph
  in question 4.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Firsttest\_2}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/firsttest\_2.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Firsttest\_2))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-first-test-2}Head of First Test Data frame}

\tabularnewline

\toprule\noalign{}
grades \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
67 \\
67 \\
76 \\
47 \\
85 \\
70 \\

\end{longtable}

\section{Other Graphical Representations of
Data}\label{other-graphical-representations-of-data}

There are many other types of graphs. Some of the more common ones are
the point plot (scatter plot), and a time-series plot. There are also
many different graphs that have emerged lately for qualitative data.
Many are found in publications and websites. The following is a
description of the point plot (scatter plot), and the time-series plot.

\subsection{Point Plots or Scatter
Plot}\label{point-plots-or-scatter-plot}

Sometimes you have two different variables and you want to see if they
are related in any way. A scatter plot helps you to see what the
relationship would look like. A scatter plot is just a plotting of the
ordered pairs.

\subsection{Example: Scatter Plot}\label{example-scatter-plot}

Is there a relationship between systolic blood pressure and weight? To
answer this question some data is needed. The data frame NHANES contains
this data, but given the size of the data frame, it may be not be very
useful to look at the graph of all the data. It makes sense to take a
sample from the data frame. A random sample is the better type of sample
to take. Once the sample is taken, then a scatter plot can be created.
The rStudio command for a scatter plot is

gf\_point(response\_variable \textasciitilde{} explanatory\_variable,
data= Data\_Frame)

The sample is Table~\ref{tbl-sample_NHANES}.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-16}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_NHANES }\OtherTok{\textless{}{-}}\NormalTok{ NHANES }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{100}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(sample\_NHANES))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0050}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0188}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0212}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0188}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0188}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0188}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0188}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0150}}@{}}

\caption{\label{tbl-sample_NHANES}Head of NHANES Sample Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SurveyYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AgeDecade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeMonths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MaritalStatus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HHIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HHIncomeMid
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Poverty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HomeRooms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HomeOwn
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Work
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HeadCirc
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BMI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMICatUnder20yrs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMI\_WHO
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Pulse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSysAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDiaAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Testosterone
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DirectChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TotChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diabetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DiabetesAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HealthGen
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysPhysHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysMentHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LittleInterest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Depressed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nPregnancies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nBabies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age1stBaby
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SleepHrsNight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SleepTrouble
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PhysActive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PhysActiveDays
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TVHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CompHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TVHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
CompHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alcohol12PlusYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SmokeNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SmokeAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marijuana
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeFirstMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RegularMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeRegMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HardDrugs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexEver
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartnLife
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SameSex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexOrientation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PregnantNow
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
64458 & 2011\_12 & male & 4 & 0-9 & NA & Hispanic & Hispanic & NA & NA &
25000-34999 & 30000 & 0.78 & 5 & Own & NA & 19.7 & NA & NA & 110.1 &
16.30 & NormWeight & 12.0\_18.5 & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & No & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & 2 & 2\_hr & 0\_hrs & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA \\
60186 & 2009\_10 & male & 34 & 30-39 & 411 & Black & NA & Some College &
NeverMarried & 45000-54999 & 50000 & 1.39 & 3 & Rent & Working & 84.3 &
NA & NA & 182.3 & 25.37 & NA & 25.0\_to\_29.9 & 66 & 118 & 69 & 124 & 74
& 118 & 70 & 118 & 68 & NA & NA & NA & 18 & 0.667 & 101 & 1.485 & No &
NA & Good & 0 & 0 & None & None & NA & NA & NA & 6 & Yes & Yes & 4 & NA
& NA & NA & NA & Yes & 5 & 52 & NA & No & Non-Smoker & NA & No & NA & No
& NA & No & Yes & 15 & 5 & 1 & No & Heterosexual & NA \\
53700 & 2009\_10 & female & 37 & 30-39 & 448 & Mexican & NA & 9 - 11th
Grade & Divorced & 20000-24999 & 22500 & 0.72 & 4 & Rent & Working &
70.2 & NA & NA & 164.3 & 26.01 & NA & 25.0\_to\_29.9 & 60 & 107 & 68 &
106 & 62 & 112 & 68 & 102 & 68 & NA & 0.91 & 4.53 & 65 & 0.320 & NA & NA
& No & NA & Excellent & 0 & 0 & None & None & 3 & 3 & 20 & 5 & Yes & Yes
& 1 & NA & NA & NA & NA & Yes & 2 & 104 & No & Yes & Smoker & NA & Yes &
16 & Yes & 18 & Yes & Yes & 18 & 6 & 1 & Yes & Heterosexual & No \\
70867 & 2011\_12 & female & 29 & 20-29 & NA & Mexican & Mexican & Some
College & LivePartner & 35000-44999 & 40000 & 1.34 & 5 & Own &
NotWorking & 58.5 & NA & NA & 160.9 & 22.60 & NA & 18.5\_to\_24.9 & 80 &
90 & 53 & 90 & 46 & 92 & 50 & 88 & 56 & 59.02 & 1.42 & 4.42 & 261 &
1.591 & NA & NA & No & NA & Fair & 3 & 0 & Several & None & 6 & 3 & 20 &
7 & No & Yes & 5 & 2\_hr & 0\_to\_1\_hr & NA & NA & Yes & 2 & 1 & NA &
No & Non-Smoker & NA & No & NA & No & NA & No & Yes & 18 & 1 & 1 & No &
Heterosexual & Yes \\
58972 & 2009\_10 & male & 42 & 40-49 & 513 & Mexican & NA & 8th Grade &
Divorced & 25000-34999 & 30000 & 1.72 & 4 & Own & Working & 59.1 & NA &
NA & 170.5 & 20.33 & NA & 18.5\_to\_24.9 & 76 & 151 & 85 & 150 & 86 &
148 & 82 & 154 & 88 & NA & 3.72 & 6.39 & 51 & 0.378 & NA & NA & No & NA
& Excellent & 0 & 0 & None & None & NA & NA & NA & 8 & No & Yes & 1 & NA
& NA & NA & NA & Yes & 2 & 96 & Yes & Yes & Smoker & 18 & No & NA & No &
NA & No & No & NA & 0 & 0 & No & Heterosexual & NA \\
60377 & 2009\_10 & male & 60 & 60-69 & 726 & Black & NA & Some College &
Married & more 99999 & 100000 & 5.00 & 6 & Own & Working & 89.1 & NA &
NA & 172.3 & 30.01 & NA & 30.0\_plus & 58 & 120 & 87 & 124 & 86 & 122 &
84 & 118 & 90 & NA & 1.16 & 5.33 & 307 & 6.020 & NA & NA & No & NA &
Vgood & 10 & 0 & None & None & NA & NA & NA & 8 & No & No & NA & NA & NA
& NA & NA & Yes & 1 & 52 & NA & No & Non-Smoker & NA & NA & NA & NA & NA
& No & Yes & 16 & 12 & NA & No & NA & NA \\

\end{longtable}

Preliminary: State the explanatory variable and the response variable

Let x=explanatory variable = Weight of a person (Weight)

y=response variable = Systolic blood pressure (BPSys1)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_point}\NormalTok{(BPSys1}\SpecialCharTok{\textasciitilde{}}\NormalTok{Weight, }\AttributeTok{data=}\NormalTok{sample\_NHANES, }\AttributeTok{xlab=}\StringTok{"Weight (kg)"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Systolic Blood Pressure"}\NormalTok{, }\AttributeTok{title=}\StringTok{"Blood Pressure versus Weight"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-blood-data-1.pdf}

}

\caption{\label{fig-blood-data}Blood Pressure versus Weight}

\end{figure}%

Description of Figure~\ref{fig-blood-data} is a scatter plot with dots
all over the plot though a line could be thought of fitting the dots
with lower on the left and higher on the right.

Looking at the graph Figure~\ref{fig-blood-data}, it appears that there
is a linear relationship between weight and systolic blood pressure
though it looks somewhat weak. It also appears to be a positive
relationship, thus as weight increases, the systolic blood pressure
increases.

\subsection{\texorpdfstring{\textbf{Time-Series}}{Time-Series}}\label{time-series}

A time-series plot is a graph showing the data measurements in
chronological order, the data being quantitative data. For example, a
time-series plot is used to show profits over the last 5 years. To
create a time-series plot on RStudio, use the command

gf\_line(response\_variable \textasciitilde{} explanatory\_variable,
data=Data\_Frame)

The purpose of a time-series graph is to look for trends over time.
Caution, you must realize that the trend may not continue. Just because
you see an increase, doesn't mean the increase will continue forever. As
an example, prior to 2007, many people noticed that housing prices were
increasing. The belief at the time was that housing prices would
continue to increase. However, the housing bubble burst in 2007, and
many houses lost value, and haven't recovered.

\subsection{Example: Time-Series Plot}\label{example-time-series-plot}

The bank assets (in billions of Australia dollars (AUD)) of the Reserve
Bank of Australia (RBA) and other financial organizations for the time
period of September 1 1969, through March 1 2019, are contained in table
Table~\ref{tbl-Australian} (Reserve Bank of Australia, 2019). Create a
time-series plot of the total assets of Authorized Deposit-taking
Institutions (ADIs) and interpret any findings.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Australian}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Australian\_financial.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Australian))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0185}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0106}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0290}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0475}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0554}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0396}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0475}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0396}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0528}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0475}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0528}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0475}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0475}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0712}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0660}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0686}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0607}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0818}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0580}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 38\tabcolsep) * \real{0.0580}}@{}}

\caption{\label{tbl-Australian}Head of Australian Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Date
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Day
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_RBA
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_ADIs\_Banks
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_ADIs\_Building
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_ADIs\_CU
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_ADIs\_Total
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_RFCs\_MM
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_RFCs\_Finance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_RFCs\_Total
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Life.offices
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Life\_funds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Life\_Total
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Other\_Public\_trusts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Other\_Cash\_trusts
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Other\_Common\_funds
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Others\_Friendly
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Other\_General\_insurance
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Other\_vehicles
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Assets\_Unconsolidated
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Sep-69 & 0 & 2.7 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA \\
Dec-69 & 90 & 2.9 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA \\
Mar-70 & 180 & 3.0 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA \\
Jun-70 & 270 & 3.0 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA \\
Sep-70 & 360 & 3.0 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA \\
Dec-70 & 450 & 3.0 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA \\

\end{longtable}

\textbf{Code book for Data frame Australian}

\textbf{Description} The data is a range of economic and financial data
produced by the Reserve Bank of Australia and other organizations.

Format

This data frame contains the following columns:

Date: quarters from September 1, 1969, to March 1, 2019

Day: The number of days since September 1, 1969, using 90 days between
starts of a quarter. This column is to make it easier to graph in
rStudio, and has no other purpose.

Assets\_RBA: The assets for the Royal Bank of Australia

Assets\_ADIs\_Banks: The assets for Authorized Deposit-taking
Institutions (ADIs), Banks

Assets\_ADIs\_Building: The assets for Authorized Deposit-taking
Institutions (ADIs), Building societies

Assets\_ADIs\_CU: The assets for Authorized Deposit-taking Institutions
(ADIs), Credit Unions

Assets\_ADIs\_Total: The assets for Authorized Deposit-taking
Institutions (ADIs), total

Assets\_RFCs\_MM: The assets for Registered Financial Corporations
(RFCs), Money Market Corporations

Assets\_RFCs\_Finance: The assets for Registered Financial Corporations
(RFCs), Finance companies and general financiers

Assets\_RFCs\_Total: The assets for Registered Financial Corporations
(RFCs) total

Assets\_Life offices: The Assets of Life offices and superannuation
funds; Life insurance offices

Assets\_Life\_funds: The Assets of Life offices and superannuation
funds; Superannuation funds

Assets\_Life\_Total: The Assets of Life offices and superannuation;
Total

Assets\_Other\_Public\_trusts: The Assets of Other managed funds; Public
unit trusts

Assets\_Other\_Cash\_trusts: The Assets of Other managed funds; Cash
management trusts

Assets\_Other\_Common\_funds: The Assets of Other managed funds; Common
funds

Assets\_Others\_Friendly: The Assets of Other managed funds; Friendly
societies

Assets\_Other\_General\_insurance: The Assets of Other financial
institutions; General insurance offices

Assets\_Other\_vehicles: The Assets Other financial institutions;
Securitisation vehicles

Assets\_Unconsolidated: The Assets of Unconsolidated; Statutory funds of
life insurance offices; Superannuation

Source Reserve Bank of Australia. (2019, May 13). Statistical Tables.
Retrieved July 10, 2019, from https://www.rba.gov.au/statistics/tables/

References Reserve Bank of Australia and other organizations

\subsubsection{Solution}\label{solution-17}

variable, x=total assets of Authorized Deposit-taking Institutions
(ADIs)

Looking at the code book, one can see that the variable
Assets\_ADIs\_Total is the variable in the data frame that is of
interest here. With a time series plot, the other variable is time. In
this case the variable in the data frame that represents time is Date.
The problem with Date is that the units are every quarter. This is not
easily interpreted by rStudio, so a column was created called Day. From
the code book, this is the number of days since September 1, 1969, using
90 days between starts of a quarter. Even though this isn't perfect, it
will work for determining trends. So create a time series plot of
Assets\_ADIs\_Total versus Day. The command is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_line}\NormalTok{(Assets\_ADIs\_Total}\SpecialCharTok{\textasciitilde{}}\NormalTok{Day, }\AttributeTok{data=}\NormalTok{Australian, }\AttributeTok{title=}\StringTok{"Total Assets of Authorized Deposit{-}taking Institutions (ADIs)"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Day since September 1, 1969"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"ADI (AUD)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Graphical-Description-of-Data_files/figure-pdf/fig-Australian_data-1.pdf}

}

\caption{\label{fig-Australian_data}Total Assets of Authorized
Deposit-taking Institutions}

\end{figure}%

Description of Figure~\ref{fig-Australian_data} is an increasing time
series Graph of Total Assets of Authorized Deposit-taking Institutions
from day 7500 to 17500. The first number starts at 0 and goes up to
about 4500.

From the graph, total assets of Authorized Deposit-taking Institutions
(ADIs) appear to be increasing with a slight dip around 14000 days since
September 1, 1969. That would be around the year 2008 (14000 days /360
days per year + 1969).

Be careful when making a graph. If the vertical axis doesn't start at 0,
then the change can look much more dramatic than it really is. For a
graph to be useful to the reader, it needs to have a title that explains
what the graph contains, the axes should be labeled so the reader knows
what each axes represents, each axes should have a scale marked, and it
is best if the vertical axis contains 0 to show the relationship.

\subsection{Homework for Other Graphical Representations of Data
Section}\label{homework-for-other-graphical-representations-of-data-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When an anthropologist finds skeletal remains, they need to figure out
  the height of the person. The height of a person (in cm) and the
  length of one of their metacarpal bone (in cm) were collected and are
  in Table~\ref{tbl-Metacarpal} (Prediction of height, 2013). Create a
  scatter plot of length and height and state if there is a relationship
  between the height of a person and the length of their metacarpal.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Metacarpal}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/metacarpal.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Metacarpal))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}

\caption{\label{tbl-Metacarpal}Head of Metacarpal Data frame}

\tabularnewline

\toprule\noalign{}
length & height \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
45 & 171 \\
51 & 178 \\
39 & 157 \\
41 & 163 \\
48 & 172 \\
49 & 183 \\

\end{longtable}

\textbf{Code book for Data frame Metacarpal}

\textbf{Description} When anthropologists analyze human skeletal
remains, an important piece of information is living stature. Since
skeletons are commonly based on statistical methods that utilize
measurements on small bones. The following data was presented in a paper
in the American Journal of Physical Anthropology to validate one such
method.

Format

This data frame contains the following columns:

length: length of Metacarpal I bone in mm

height: stature of skeleton in cm

Source Prediction of Height from Metacarpal Bone Length. (n.d.).
Retrieved July 9, 2019, from
http://www.statsci.org/data/general/stature.html

References Musgrave, J., and Harneja, N. (1978). The estimation of adult
stature from metacarpal bone length. Amer. J. Phys. Anthropology 48,
113-120.

Devore, J., and Peck, R. (1986). Statistics. The Exploration and
Analysis of Data. West Publishing, St Paul, Minnesota.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The value of the house and the amount of rental income in a year that
  the house brings in are in Table~\ref{tbl-House} (Capital and rental
  2013). Create a scatter plot and state if there is a relationship
  between the value of the house and the annual rental income.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{House}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/house.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(House))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}

\caption{\label{tbl-House}Head of House Data frame}

\tabularnewline

\toprule\noalign{}
capital & rental \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
61500 & 6656 \\
67500 & 6864 \\
75000 & 4992 \\
75000 & 7280 \\
76000 & 6656 \\
77000 & 4576 \\

\end{longtable}

\textbf{Code book for Data frame House}

\textbf{Description} The data show the capital value and annual rental
value of domestic properties in Auckland in 1991.

Format

This data frame contains the following columns:

Capital: Selling price of house in Australian dollar (AUD)

rental: rental price of a house in Australian dollar (AUD)

Source Capital and rental values of Auckland properties. (2013,
September 26). Retrieved from
http://www.statsci.org/data/oz/rentcap.html

References Lee, A. (1994) Data Analysis: An introduction based on R.
Auckland: Department of Statistics, University of Auckland. Data
courtesy of Sage Consultants Ltd.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The World Bank collects information on the life expectancy of a person
  in each country (\textbackslash{}``Life expectancy
  at,\textbackslash{}'' 2013) and the fertility rate per woman in the
  country (\textbackslash{}``Fertility rate,\textbackslash{}'' 2013).
  The data for countries for the year 2011 are in
  Table~\ref{tbl-Fertility}. Create a scatter plot of the data and state
  if there appears to be a relationship between life expectancy and the
  number of births per woman in 2011.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fertility}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/fertility.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Fertility))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.2000}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1143}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1524}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1143}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1524}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1143}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 12\tabcolsep) * \real{0.1524}}@{}}

\caption{\label{tbl-Fertility}Head of Fertility Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
country
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
lifexp\_2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fertilrate\_2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
lifexp\_2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fertilrate\_2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
lifexp\_1990
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fertilrate\_1990
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Macao SAR, China & 79.91 & 1.03 & 77.62 & 0.94 & 75.28 & 1.69 \\
Hong Kong SAR, China & 83.42 & 1.20 & 80.88 & 1.04 & 77.38 & 1.27 \\
Singapore & 81.89 & 1.20 & 78.05 & NA & 76.03 & 1.87 \\
Hungary & 74.86 & 1.23 & 71.25 & 1.32 & 69.32 & 1.84 \\
Korea, Rep. & 80.87 & 1.24 & 75.86 & 1.47 & 71.29 & 1.59 \\
Romania & 74.51 & 1.25 & 71.16 & 1.31 & 69.74 & 1.84 \\

\end{longtable}

\textbf{Code book for Data frame Fertility}

\textbf{Description} Data is from the World Bank on the life expectancy
of countries and the fertility rates in those countries.

Format

This data frame contains the following columns:

Country: Countries in the World

lifexp\_2011: Life expectancy of a person born in 2011

fertilrate\_2011: Fertility rate in the country in 2011

lifexp\_2000: Life expectancy of a person born in 2000

fertilrate\_2000: Fertility rate in the country in 2000

lifexp\_1990: Life expectancy of a person born in 1990

fertilrate\_1990: Fertility rate in the country in 1990

Source Life expectancy at birth. (2013, October 14). Retrieved from
http://data.worldbank.org/indicator/SP.DYN.LE00.IN

References Data from World Bank, Life expectancy at birth, total (years)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The World Bank collected data on the percentage of gross domestic
  product (GDP) that a country spends on health expenditures (Current
  health expenditure (\% of GDP), 2019), the fertility rate of the
  country (Fertility rate, total (births per woman), 2019), and the
  percentage of women receiving prenatal care (Pregnant women receiving
  prenatal care (\%), 2019). The data for the countries where this
  information is available in Table~\ref{tbl-Fert_prenatal}. Create a
  scatter plot of the health expenditure and percentage of women
  receiving prenatal care in the year 2000, and state if there appears
  to be a relationship between percentage spent on health expenditure
  and the percentage of women receiving prenatal care.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Fert\_prenatal}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/fertility\_prenatal.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Fert\_prenatal))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0181}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0168}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0336}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0258}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0078}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0129}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 222\tabcolsep) * \real{0.0116}}@{}}

\caption{\label{tbl-Fert_prenatal}Head of Fert\_prenatal Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Country.Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Country.Code
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
IncomeGroup
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1960
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1961
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1962
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1963
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1964
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1965
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1966
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1967
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1968
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1969
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1970
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1971
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1972
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1973
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1974
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1975
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1976
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1977
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1978
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1979
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1980
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1981
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1982
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1983
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1984
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1985
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1986
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1987
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1988
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1989
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1990
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1991
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1992
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1993
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1994
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1995
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1996
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1997
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1998
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f1999
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2001
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2002
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2003
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2004
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2005
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2006
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2007
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2008
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2009
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2010
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2012
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2013
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2014
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2015
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2016
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
f2017
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1986
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1987
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
p1988
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1989
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1990
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1991
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1992
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1993
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1994
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1995
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1996
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1997
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1998
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p1999
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2001
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2002
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2003
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2004
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2005
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2006
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2007
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2008
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2009
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2010
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2012
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2013
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2014
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2015
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2016
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
p2017
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
p2018
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2001
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2002
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2003
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2004
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2005
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2006
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2007
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2008
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2009
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2010
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2012
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2013
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2014
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2015
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
e2016
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Angola & AGO & Sub-Saharan Africa & Lower middle income & 7.478 & 7.524
& 7.563 & 7.592 & 7.611 & 7.619 & 7.618 & 7.613 & 7.608 & 7.604 & 7.601
& 7.603 & 7.606 & 7.611 & 7.614 & 7.615 & 7.609 & 7.594 & 7.571 & 7.540
& 7.504 & 7.469 & 7.438 & 7.413 & 7.394 & 7.380 & 7.366 & 7.349 & 7.324
& 7.291 & 7.247 & 7.193 & 7.130 & 7.063 & 6.992 & 6.922 & 6.854 & 6.791
& 6.734 & 6.683 & 6.639 & 6.602 & 6.568 & 6.536 & 6.502 & 6.465 & 6.420
& 6.368 & 6.307 & 6.238 & 6.162 & 6.082 & 6.000 & 5.920 & 5.841 & 5.766
& 5.694 & 5.623 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & 65.6 & NA & NA & NA & NA & NA & 79.8 & NA & NA & NA
& NA & NA & NA & NA & NA & 81.6 & NA & NA & 2.334435 & 5.483823 &
4.072288 & 4.454100 & 4.757211 & 3.734836 & 3.366183 & 3.211438 &
3.495036 & 3.578677 & 2.736684 & 2.840603 & 2.692890 & 2.990929 &
2.798719 & 2.950431 & 2.877825 \\
Armenia & ARM & Europe \& Central Asia & Upper middle income & 4.786 &
4.670 & 4.521 & 4.345 & 4.150 & 3.950 & 3.758 & 3.582 & 3.429 & 3.302 &
3.199 & 3.114 & 3.035 & 2.956 & 2.875 & 2.792 & 2.712 & 2.641 & 2.582 &
2.538 & 2.510 & 2.499 & 2.503 & 2.517 & 2.538 & 2.559 & 2.578 & 2.591 &
2.592 & 2.578 & 2.544 & 2.484 & 2.400 & 2.297 & 2.179 & 2.056 & 1.938 &
1.832 & 1.747 & 1.685 & 1.648 & 1.635 & 1.637 & 1.648 & 1.665 & 1.681 &
1.694 & 1.702 & 1.706 & 1.703 & 1.693 & 1.680 & 1.664 & 1.648 & 1.634 &
1.622 & 1.612 & 1.604 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & 82 & NA & NA & 92.4 & NA & NA & NA & NA & 93.0 & NA & NA & NA &
NA & 99.1 & NA & NA & NA & NA & NA & 99.6 & NA & NA & 6.505224 &
6.536263 & 5.690812 & 5.610725 & 8.227844 & 7.034880 & 5.588461 &
5.445144 & 4.346749 & 4.689046 & 5.264181 & 3.777260 & 6.711859 &
8.269840 & 10.178299 & 10.117627 & 9.927321 \\
Belize & BLZ & Latin America \& Caribbean & Upper middle income & 6.500
& 6.480 & 6.460 & 6.440 & 6.420 & 6.400 & 6.379 & 6.358 & 6.337 & 6.316
& 6.299 & 6.288 & 6.284 & 6.285 & 6.287 & 6.278 & 6.250 & 6.195 & 6.109
& 5.992 & 5.849 & 5.684 & 5.510 & 5.336 & 5.170 & 5.019 & 4.886 & 4.771
& 4.671 & 4.584 & 4.508 & 4.436 & 4.363 & 4.286 & 4.201 & 4.109 & 4.010
& 3.908 & 3.805 & 3.703 & 3.600 & 3.496 & 3.390 & 3.282 & 3.175 & 3.072
& 2.977 & 2.893 & 2.821 & 2.762 & 2.715 & 2.676 & 2.642 & 2.610 & 2.578
& 2.544 & 2.510 & 2.475 & NA & NA & NA & NA & NA & 96 & NA & NA & NA &
NA & NA & NA & 98 & 95.9 & 100.0 & NA & 98 & NA & NA & 94.0 & 94.0 &
99.2 & NA & NA & NA & 96.2 & NA & NA & NA & 97.2 & 97.2 & NA & NA &
3.942030 & 4.228792 & 3.864327 & 4.260178 & 4.091610 & 4.216728 &
4.163924 & 4.568384 & 4.646109 & 5.311070 & 5.764874 & 5.575126 &
5.322589 & 5.727331 & 5.652458 & 5.884248 & 6.121374 \\
Cote d'Ivoire & CIV & Sub-Saharan Africa & Lower middle income & 7.691 &
7.720 & 7.750 & 7.781 & 7.811 & 7.841 & 7.868 & 7.893 & 7.912 & 7.927 &
7.936 & 7.941 & 7.942 & 7.939 & 7.929 & 7.910 & 7.877 & 7.828 & 7.763 &
7.682 & 7.590 & 7.488 & 7.383 & 7.278 & 7.176 & 7.078 & 6.984 & 6.892 &
6.801 & 6.710 & 6.622 & 6.536 & 6.454 & 6.374 & 6.298 & 6.224 & 6.152 &
6.079 & 6.006 & 5.932 & 5.859 & 5.787 & 5.717 & 5.651 & 5.589 & 5.531 &
5.476 & 5.423 & 5.372 & 5.321 & 5.269 & 5.216 & 5.160 & 5.101 & 5.039 &
4.976 & 4.911 & 4.846 & NA & NA & NA & NA & NA & NA & NA & NA & 83.2 &
NA & NA & NA & NA & 84.3 & 87.6 & NA & NA & NA & NA & 87.3 & 84.8 & NA &
NA & NA & NA & NA & 90.6 & NA & NA & NA & 93.2 & NA & NA & 5.672228 &
4.850694 & 4.476869 & 4.645306 & 5.213588 & 5.353556 & 5.808850 &
6.259154 & 6.121605 & 6.223329 & 6.146566 & 5.978840 & 6.019660 &
5.074942 & 5.043462 & 5.262711 & 4.403621 \\
Ethiopia & ETH & Sub-Saharan Africa & Low income & 6.880 & 6.877 & 6.875
& 6.872 & 6.867 & 6.864 & 6.867 & 6.880 & 6.903 & 6.937 & 6.978 & 7.020
& 7.060 & 7.094 & 7.121 & 7.143 & 7.167 & 7.195 & 7.230 & 7.271 & 7.316
& 7.360 & 7.397 & 7.424 & 7.437 & 7.435 & 7.418 & 7.387 & 7.347 & 7.298
& 7.246 & 7.193 & 7.143 & 7.094 & 7.046 & 6.995 & 6.935 & 6.861 & 6.769
& 6.659 & 6.529 & 6.380 & 6.216 & 6.044 & 5.867 & 5.690 & 5.519 & 5.355
& 5.201 & 5.057 & 4.924 & 4.798 & 4.677 & 4.556 & 4.437 & 4.317 & 4.198
& 4.081 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & 26.7 & NA & NA & NA & NA & 27.6 & NA & NA & NA & NA & NA & 33.9 &
NA & NA & 41.2 & NA & 62.4 & NA & NA & 4.365290 & 4.713670 & 4.705820 &
4.885341 & 4.304562 & 4.100981 & 4.226696 & 4.801925 & 4.280639 &
4.412473 & 5.466372 & 4.468978 & 4.539596 & 4.075065 & 4.033651 &
3.975932 & 3.974016 \\
Guinea & GIN & Sub-Saharan Africa & Low income & 6.114 & 6.127 & 6.138 &
6.147 & 6.154 & 6.160 & 6.168 & 6.177 & 6.189 & 6.205 & 6.225 & 6.249 &
6.277 & 6.306 & 6.337 & 6.369 & 6.402 & 6.436 & 6.468 & 6.500 & 6.529 &
6.557 & 6.581 & 6.602 & 6.619 & 6.631 & 6.637 & 6.637 & 6.631 & 6.618 &
6.598 & 6.570 & 6.535 & 6.493 & 6.444 & 6.391 & 6.334 & 6.273 & 6.211 &
6.147 & 6.082 & 6.015 & 5.947 & 5.877 & 5.804 & 5.729 & 5.653 & 5.575 &
5.496 & 5.417 & 5.336 & 5.256 & 5.175 & 5.094 & 5.014 & 4.934 & 4.855 &
4.777 & NA & NA & NA & NA & NA & NA & 57.6 & NA & NA & NA & NA & NA & NA
& 70.7 & NA & NA & NA & 84.3 & NA & 82.2 & NA & 88.4 & NA & NA & NA & NA
& 85.2 & NA & NA & NA & 84.3 & NA & NA & 3.697726 & 3.884610 & 4.384152
& 3.651081 & 3.365547 & 2.949490 & 2.960601 & 3.013074 & 2.762090 &
2.936868 & 3.067742 & 3.789550 & 3.503983 & 3.461137 & 4.780977 &
5.827122 & 5.478273 \\

\end{longtable}

\textbf{Code book for Data frame Fert\_prenatal}

\textbf{Description} Data is from the World Bank on money spent on
expenditure of countries and the percentage of women receiving prenatal
care in those countries.

Format

This data frame contains the following columns:

Country.Name: Countries around the world

Country.Code: Three letter country code for countries around the world

Region: Location of a country around the world as classified by the
World Bank

IncomeGroup: The income level of a country as classified by the World
Bank

f1960-f2017: Fertility rate of a country from 1960-2017

p1986-p2018: Percentage of women receiving prenatal care in the country
in 1986-2018

e200-2016: Expenditure amounts of the countries for medical care in
2000-2016 (\% of GDP)

Source Fertility rate, total (births per woman). (n.d.). Retrieved July
8, 2019, from https://data.worldbank.org/indicator/SP.DYN.TFRT.IN
Pregnant women receiving prenatal care (\%). (n.d.). Retrieved July 9,
2019, from https://data.worldbank.org/indicator/SH.STA.ANVC.ZS Current
health expenditure (\% of GDP). (n.d.). Retrieved July 9, 2019, from
https://data.worldbank.org/indicator/SH.XPD.CHEX.GD.ZS

References Data from World Bank, fertility rate, expenditure on health,
and pregnant woman rate of prenatal care.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The Australian Institute of Criminology gathered data on the number of
  deaths (per 100,000 people) due to firearms during the period 1983 to
  1997 (\textbackslash{}``Deaths from firearms,\textbackslash{}'' 2013).
  The data is in Table~\ref{tbl-firearm}. Create a time-series plot of
  the data and state any findings you can from the graph.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Firearm}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/rate.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Firearm))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}

\caption{\label{tbl-firearm}Head of Firearm Data frame}

\tabularnewline

\toprule\noalign{}
year & rate \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1983 & 4.31 \\
1984 & 4.42 \\
1985 & 4.52 \\
1986 & 4.35 \\
1987 & 4.39 \\
1988 & 4.21 \\

\end{longtable}

\textbf{Code book for Data Frame Firearm}

\textbf{Description} The data give the number of deaths caused by
firearms in Australia from 1983 to 1997, expressed as a rate per 100,000
of population.

Format

This data frame contains the following columns:

Year: Years from 1983 to 1997

Rate: Rate of deaths caused by firearms in Australia per 100,000
population

Source Deaths from firearms. (2013, September 26). Retrieved from
http://www.statsci.org/data/oz/firearms.html

References Australian Institute of Criminology, 1999.The data was
contributed by Rex Boggs, Glenmore State High School, Rockhampton,
Queensland, Australia.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  The economic crisis of 2008 affected many countries, though some more
  than others. Some people in Australia have claimed that Australia
  wasn't hurt that badly from the crisis. The bank assets (in billions
  of Australia dollars (AUD)) of the Reserve Bank of Australia (RBA) for
  the time period of September 1 1969, through March 1 2019, are
  contained in @bl-Australian (Reserve Bank of Australia, 2019). Create
  a time-series plot of the assets of the RBA and interpret any
  findings.
\end{enumerate}

\textbf{Code book for Data Frame Australian is below
Table~\ref{tbl-Australian}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  The consumer price index (CPI) is a measure used by the U.S.
  government to describe the cost of living. The cost of living for the
  U.S. from the years 1913 through 2019, with the year 1982 being used
  as the year that all others are compared (Consumer Price Index Data
  from 1913 to 2019, 2019) is given in Table~\ref{tbl-CPI}. Create a
  time-series plot of the Average Annual CPI and interpret.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{CPI}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/CPI\_US.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(CPI))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0500}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.1100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.1100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.1300}}@{}}

\caption{\label{tbl-CPI}Head of CPI Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Year
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Jan
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Feb
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Mar
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Apr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
May
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
June
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
July
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Aug
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Sep
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Oct
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Nov
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Dec
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Annual\_avg
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PerDec\_Dec
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Perc\_Avg\_Avg
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1913 & 9.8 & 9.8 & 9.8 & 9.8 & 9.7 & 9.8 & 9.9 & 9.9 & 10.0 & 10.0 &
10.1 & 10.0 & 9.9 & -- & -- \\
1914 & 10.0 & 9.9 & 9.9 & 9.8 & 9.9 & 9.9 & 10.0 & 10.2 & 10.2 & 10.1 &
10.2 & 10.1 & 10.0 & 1 & 1 \\
1915 & 10.1 & 10.0 & 9.9 & 10.0 & 10.1 & 10.1 & 10.1 & 10.1 & 10.1 &
10.2 & 10.3 & 10.3 & 10.1 & 2 & 1 \\
1916 & 10.4 & 10.4 & 10.5 & 10.6 & 10.7 & 10.8 & 10.8 & 10.9 & 11.1 &
11.3 & 11.5 & 11.6 & 10.9 & 12.6 & 7.9 \\
1917 & 11.7 & 12.0 & 12.0 & 12.6 & 12.8 & 13.0 & 12.8 & 13.0 & 13.3 &
13.5 & 13.5 & 13.7 & 12.8 & 18.1 & 17.4 \\
1918 & 14.0 & 14.1 & 14.0 & 14.2 & 14.5 & 14.7 & 15.1 & 15.4 & 15.7 &
16.0 & 16.3 & 16.5 & 15.1 & 20.4 & 18 \\

\end{longtable}

\textbf{Code book for Data frame CPI}

\textbf{Description} This table of Consumer Price Index (CPI) data is
based upon a 1982 base of 100.

Format

This data frame contains the following columns:

Year: Year from 1913 to 2019

Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec: CPI for a
particular month

Average\_Avg: The average CPI for a particular year

PerDec\_Dec: Percent change from December to December

Per\_Avg\_Avg: Percent change from Annual Average to Annual Average

Source Consumer Price Index Data from 1913 to 2019. (2019, June 12).
Retrieved July 10, 2019, from
https://www.usinflationcalculator.com/inflation/consumer-price-index-and-annual-percent-changes-from-1913-to-2008/

References US Inflation Calculator website, 2019.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  The mean and median incomes income in current dollars is given in
  Table~\ref{tbl-US_income}. Create a time-series plot and interpret.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{US\_income}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/US\_income.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(US\_income))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0595}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2262}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1905}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2381}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.2024}}@{}}

\caption{\label{tbl-US_income}Head of US\_income Data frame}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
year
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
number
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
med\_income\_current
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
med\_income\_2017
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean\_income\_current
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mean\_income\_2017
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2017 & 127586 & 61372 & 61372 & 86220 & 86220 \\
2016 & 126224 & 59039 & 60309 & 83143 & 84931 \\
2015 & 125819 & 56516 & 58476 & 79263 & 82012 \\
2014 & 124587 & 53657 & 55613 & 75738 & 78500 \\
2013 & 122952 & 51939 & 54744 & 72641 & 76565 \\
2012 & 122459 & 51017 & 54569 & 71274 & 76237 \\

\end{longtable}

\textbf{Code book for Data Frame US\_income}

\textbf{Description} This table is of US mean and median incomes in both
current dollars and in 2017 dollars.

Format

This data frame contains the following columns:

Year: Year from 1975 to 2017

number: Households as of March of the following year. (in thousands)

med\_income\_current: median income of a US household in current dollars

med\_income\_2017: median income of a US household in 2017 CPI-U-RS
adjusted dollars

mean\_income\_current: mean income of a US household in current dollars

mean\_income\_2017: mean income of a US household in 2017 CPI-U-RS
adjusted dollars

Source US Census Bureau. (2018, March 06). Data. Retrieved July 21,
2019, from https://www.census.gov/programs-surveys/cps/data-detail.html

References U.S. Census Bureau, Current Population Survey, Annual Social
and Economic Supplements.

\bookmarksetup{startatroot}

\chapter{Numerical Description of
Data}\label{numerical-description-of-data}

Chapter 1 discussed what a population, sample, parameter, and statistic
are, and how to take different types of samples. Chapter 2 discussed
ways to graphically display data. There was also a discussion of
important characteristics: center, variations, distribution, outliers,
and changing characteristics of the data over time. Distributions and
outliers can be answered using graphical means. Finding the center and
variation can be done using numerical methods that will be discussed in
this chapter. Both graphical and numerical methods are part of a branch
of statistics known as \textbf{descriptive statistics}. Later
descriptive statistics will be used to make decisions and/or estimate
population parameters using methods that are part of the branch called
\textbf{inferential statistics}.

\section{Measures of Center}\label{measures-of-center}

This section focuses on measures of central tendency. Many times you are
asking what to expect on average. Such as when you pick a major, you
would probably ask how much you expect to earn in that field. If you are
thinking of relocating to a new town, you might ask how much you can
expect to pay for housing. If you are planting vegetables in the spring,
you might want to know how long it will be until you can harvest. These
questions, and many more, can be answered by knowing the center of the
data set. There are three measures of the ``center'' of the data. They
are the mode, median, and mean. Any of the values can be referred to as
the ``average.''

The \textbf{mode} is the data value that occurs the most frequently in
the data. To find it, you count how often each data value occurs, and
then determine which data value occurs most often. The mode is not the
most useful measure of center. This is because, a data set can have more
than one mode. If there is a tie between two values for the most number
of times then both values are the mode and the data is called bimodal
(two modes). If every data point occurs the same number of times, there
is no mode. If there are more than two numbers that appear the most
times, then usually there is no mode.

The \textbf{median} is the data value in the middle of a sorted list of
data. To find it, you put the data in order, and then determine which
data value is in the middle of the data set.

The \textbf{mean} is the arithmetic average of the numbers. This is the
center that most people call the average, though all three -\/- mean,
median, and mode -\/- really are averages.

There are no symbols for the mode and the median, but the mean is used a
great deal, and statisticians gave it a symbol. There are actually two
symbols, one for the population parameter and one for the sample
statistic. In most cases you cannot find the population parameter, so
you use the sample statistic to estimate the population parameter.

\subsection{\texorpdfstring{\textbf{Population
Mean}}{Population Mean}}\label{population-mean}

\(\mu=\frac{\sum{x}}{N}\), pronounced mu

\textbf{N} is the size of the population.

\textbf{x} represents a data value.

\(\sum{x}\) means to add up all of the data values.

\subsection{\texorpdfstring{\textbf{Sample
Mean}:}{Sample Mean:}}\label{sample-mean}

\(\bar{x}=\frac{\sum{x}}{n}\), pronounced x bar.

\textbf{n} is the size of the sample.

\textbf{x} represents a data value.

\(\sum{x}\) means to add up all of the data values.

The value for \(\bar{x}\) is used to estimate \(\mu\) since \(\mu\)
can't be calculated in most situations.

\subsection{Example: Finding the Mean and Median using
r}\label{example-finding-the-mean-and-median-using-r}

Suppose a vet wants to find the average weight of cats. The weights (in
kg) of cats are in Table~\ref{tbl-Cats}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(cats))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-Cats}Head of Cats}

\tabularnewline

\toprule\noalign{}
Sex & Bwt & Hwt \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
F & 2.0 & 7.0 \\
F & 2.0 & 7.4 \\
F & 2.0 & 9.5 \\
F & 2.1 & 7.2 \\
F & 2.1 & 7.3 \\
F & 2.1 & 7.6 \\

\end{longtable}

The head command shows the variable names and the first few unit of
observation rows

Find the mean and median of the weight of a cat.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-18}

Before starting any mathematics problem, it is always a good idea to
define the unknown in the problem. In statistics, you want to define the
variable. The symbol for the variable is *x*.

The variable is \textbf{x} = weight of a cat

Mean: To find with r Studio, perform the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt, cats, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response     mean
1      Bwt 2.723611
\end{verbatim}

The mean weight is 2.72 kg

Median: To find with r Studio, perform the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt, cats, median)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response median
1      Bwt    2.7
\end{verbatim}

The median weight is 2.7 kg also. It appears the average weight is 2.7
kg of all cats.

\subsection{Example: Finding Mean and Median with
filtering}\label{example-finding-mean-and-median-with-filtering}

Looking at the data frame for cats weights Table~\ref{tbl-Cats} you see
that there are several variables You may want to know what the other
variables are. A Code Book describes the data set, explains what the
variables are including the units, and the source of the data frame. To
review the code book for a data frame, complete the following command.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{??cats}
\end{Highlighting}
\end{Shaded}

Then click on MASS::cats.

The output looks like:

\textbf{Image 3.1.1: Code book for cats data frame}

\begin{figure}[H]

{\centering \includegraphics{code_book_cats.jpg}

}

\caption{Code book for cats data frame}

\end{figure}%

Suppose you want to know if male cats weigh more than female cats.
Looking at the variables, you notice that there is a variable for the
sex of the cat. You can look at the weights of males and females
separately. This looks like:

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-19}

To find the mean and median, separated by sex, use this command in r
Studio:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(Bwt}\SpecialCharTok{\textasciitilde{}}\NormalTok{Sex, }\AttributeTok{data=}\NormalTok{cats, mean, median)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response Sex     mean median
1      Bwt   F 2.359574    2.3
2      Bwt   M 2.900000    2.9
\end{verbatim}

Notice that the female cats' mean weigh 2.4 kg and the male cats' mean
weigh 2.9 kg The median weight of female cats is 2.3 kg and for males is
is 2.9 kg So it does appear that males cats weight a bit more than the
female cats.

There are many different summary statistics that can be found. An
example is the minimum and maximum value. In this example, you will see
how to find the min and max values and then filter them out of a data
set to see what effect they have on the mean and median.

\subsection{Example: Affect of Extreme Values on Mean and
Median}\label{example-affect-of-extreme-values-on-mean-and-median}

Find the minimum and maximum values of cats weights.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-20}

The command in RStudio for finding the minimum and maximum is very
similar to how to find the mean and median, In fact all summary
statistics start with

df\_stats(\textasciitilde variable, data=Data Frame, desired statistics)

Here is the command in RStudio for the minimum and maximum of cat's body
weight.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt, }\AttributeTok{data=}\NormalTok{cats, min, max)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response min max
1      Bwt   2 3.9
\end{verbatim}

The minimum weight of a cat in this data frame is 2 kg and the maximum
weight of a cat is 3.9 kg.

Now create two new data sets. One data set will exclude the maximum
value. You can call it anything you want, but it would make sense to
call it something like nomax. The command to create the new data set is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nomax }\OtherTok{\textless{}{-}} \FunctionTok{filter}\NormalTok{(cats, Bwt}\SpecialCharTok{\textless{}}\FloatTok{3.9}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

Then create a data set that excludes the minimum value; call it nomin:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{nomin}\OtherTok{\textless{}{-}}\FunctionTok{filter}\NormalTok{(cats, Bwt}\SpecialCharTok{\textgreater{}}\DecValTok{2}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

The \textless- is the way to indicate to r what the data set nomin is
equivalent to what follows the symbol. Notice that it doesn't look like
anything happened, but new data sets were created in the background. Now
you can find the mean and median of each new data set:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt, }\AttributeTok{data=}\NormalTok{nomax, mean, median)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response     mean median
1      Bwt 2.707042    2.7
\end{verbatim}

The mean without the maximum value is 2.70 kg, and the median is 2.7 kg.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt, }\AttributeTok{data=}\NormalTok{nomin, mean, median)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response    mean median
1      Bwt 2.74964    2.7
\end{verbatim}

The mean without the minimum value is 2.75 kg, and the median is 2.7 kg.

From
\hyperref[example-affect-of-extreme-values-on-mean-and-median]{Example:
Affect of Extreme Values on Mean and Median}, the mean of the data set
with all the values is 2.72 kg where the median is 2.7 kg. Notice that
when the maximum value was excluded from the data set, the mean
decreased a little but the median didn't change, and when the minimum
value was excluded from the data set, the mean increased a little but
the median didn't change. The mean is much higher than the median. Why
is this? This is because the mean is affected by extreme values, while
the median is not. We say the median is a much more resistant measure of
center because it isn't affected by extreme values as much.

An outlier is a data value that is very different from the rest of the
data. It can be really high or really low. Extreme values may be an
outlier if the extreme value is far enough from the center. If there are
extreme values in the data, the median is a better measure of the center
than the mean. If there are no extreme values, the mean and the median
will be similar so most people use the mean. The mean is not a resistant
measure because it is affected by extreme values. The median is a
resistant measure because it not affected by extreme values.

As a consumer you need to be aware that people choose the measure of
center that best supports their claim. When you read an article in the
newspaper and it talks about the ``average'' it usually means the mean
but sometimes it refers to the median. Some articles will use the word
``median'' instead of ``average'' to be more specific. If you need to
make an important decision and the information says ``average'', it
would be wise to ask if the ``average'' is the mean or the median before
you decide.

As an example, suppose that a company wants to use the mean salary as
the average salary for the company. This is because the high salaries of
the administrators will pull the mean higher. The company can say that
the employees are paid well because the average is high. However, the
employees want to use the median since it discounts the extreme values
of the administration and will give a lower value of the average. This
will make the salaries seem lower and that a raise is in order.

Why use the mean instead of the median? The reason is because when
multiple samples are taken from the same population, the sample means
tend to be more consistent than other measures of the center.

To understand how the different measures of center related to skewed or
symmetric distributions, see Graph \textbackslash\#3.1.1. As you can see
sometimes the mean is smaller than the median, sometimes the mean is
larger than the median, and sometimes they are the same values.

\textbf{Graph \textbackslash\#3.1.1: Mean, Median, Mode as Related to a
Distribution}

\begin{figure}[H]

{\centering \includegraphics{centers_distribution.png}

}

\caption{Mean, median, mode as related to distribution}

\end{figure}%

One last type of average is a weighted average. \textbf{Weighted
averages} are used quite often in different situations. Some teachers
use them in calculating a student's grade in the course, or a grade on a
project. Some employers use them in employee evaluations. The idea is
that some activities are more important than others. As an example, a
full time teacher at a community college may be evaluated on their
service to the college, their service to the community, whether their
paperwork is turned in on time, and their teaching. However, teaching is
much more important than whether their paperwork is turned in on time.
When the evaluation is completed, more weight needs to be given to the
teaching and less to the paperwork. This is a weighted average.

\subsection{\texorpdfstring{\textbf{Weighted
Average}}{Weighted Average}}\label{weighted-average}

\(\text{weighted average}=\frac{\sum{x*w}}{\sum{w}}\)

where \textbf{w} is the weight of the data value, \textbf{x}.

\subsection{Example: Weighted Average}\label{example-weighted-average}

In your biology class, your final grade is based on several things: a
lab score, scores on two major tests, and your score on the final exam.
There are 100 points available for each score. The lab score is worth
15\% of the course, the two exams are worth 25\% of the course each, and
the final exam is worth 35\% of the course. Suppose you earned scores of
95 on the labs, 83 and 76 on the two exams, and 84 on the final exam.
Compute your weighted average for the course.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-21}

Variable: \textbf{x} = score

A weighted average can be found using technology. The commands for
finding the weighted mean using RStudio is as follows:

x\textless-c(type in the scores with commas in between)

w\textless-c(type in the weights as decimals with commas in between

weighted.mean(x,w)

The \textbf{x} and \textbf{w} represent the variables, \textless- means
make the variables equivalent to what follows, the c( means combine all
the values in the () as one combined variable.

For this example, the commands would be

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{95}\NormalTok{, }\DecValTok{83}\NormalTok{, }\DecValTok{76}\NormalTok{, }\DecValTok{84}\NormalTok{) }
\NormalTok{w}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(.}\DecValTok{15}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{25}\NormalTok{, .}\DecValTok{35}\NormalTok{) }
\FunctionTok{weighted.mean}\NormalTok{(x,w) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 83.4
\end{verbatim}

Your weighted mean in the biology class is 83.4\%. Using the traditional
grading scale, you have a B in the class.

\subsection{Example: Weighted Average}\label{example-weighted-average-1}

The faculty evaluation process at John Jingle University rates a faculty
member on the following activities: teaching, publishing, committee
service, community service, and submitting paperwork in a timely manner.
The process involves reviewing student evaluations, peer evaluations,
and supervisor evaluation for each teacher and awarding him/her a score
on a scale from 1 to 10 (with 10 being the best). The weights for each
activity are 20 for teaching, 18 for publishing, 6 for committee
service, 4 for community service, and 2 for paperwork.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  One faculty member had the following ratings: 8 for teaching, 9 for
  publishing, 2 for committee work, 1 for community service, and 8 for
  paperwork. Compute the weighted average of the evaluation.
\item
  Another faculty member had ratings of 6 for teaching, 8 for
  publishing, 9 for committee work, 10 for community service, and 10 for
  paperwork. Compute the weighted average of the evaluation.
\item
  Which faculty member had the higher average evaluation?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-22}

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\tightlist
\item
  One faculty member had the following ratings: 8 for teaching, 9 for
  publishing, 2 for committee work, 1 for community service, and 8 for
  paperwork. Compute the weighted average of the evaluation.
\end{enumerate}

Variable: \textbf{x} = rating, \textbf{w} = weight

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{2}\NormalTok{, }\DecValTok{1}\NormalTok{, }\DecValTok{8}\NormalTok{) }
\NormalTok{w}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{) }
\FunctionTok{weighted.mean}\NormalTok{(x,w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.08
\end{verbatim}

The weighted average is 7.08.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{1}
\tightlist
\item
  Another faculty member had ratings of 6 for teaching, 8 for
  publishing, 9 for committee work, 10 for community service, and 10 for
  paperwork. Compute the weighted average of the evaluation.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{6}\NormalTok{, }\DecValTok{8}\NormalTok{, }\DecValTok{9}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{10}\NormalTok{) }
\NormalTok{w}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{20}\NormalTok{, }\DecValTok{18}\NormalTok{, }\DecValTok{6}\NormalTok{, }\DecValTok{4}\NormalTok{, }\DecValTok{2}\NormalTok{) }
\FunctionTok{weighted.mean}\NormalTok{(x,w)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 7.56
\end{verbatim}

The weighted average for this employee is 7.56.

\begin{enumerate}
\def\labelenumi{\alph{enumi})}
\setcounter{enumi}{2}
\item
  Which faculty member had the higher average evaluation?

  The second faculty member has a higher average evaluation.
\end{enumerate}

The last thing to mention is which average is used on which type of
data.

Mode can be found on nominal, ordinal, interval, and ratio data, since
the mode is just the data value that occurs most often. You are just
counting the data values.

Median can be found on ordinal, interval, and ratio data, since you need
to put the data in order. As long as there is order to the data you can
find the median.

Mean can be found on interval and ratio data, since you must have
numbers to add together.

\subsection{Homework for Measures of Center
Section}\label{homework-for-measures-of-center-section}

\textbf{Use Technology on all problems. State the variable on all
problems.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Cholesterol levels were collected from patients certain days after
  they had a heart attack and are in Table~\ref{tbl-Cholesterol}. Find
  the mean and median for cholesterol levels 2 days after the heart
  attack.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cholesterol}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/cholesterol.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Cholesterol))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrrr@{}}

\caption{\label{tbl-Cholesterol}Head of Cholesterol Levels of Patients
After Heart Attack}

\tabularnewline

\toprule\noalign{}
patient & day2 & day4 & day14 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 270 & 218 & 156 \\
2 & 236 & 234 & NA \\
3 & 210 & 214 & 242 \\
4 & 142 & 116 & NA \\
5 & 280 & 200 & NA \\
6 & 272 & 276 & 256 \\

\end{longtable}

\textbf{Code book for Data Frame Cholesterol}

\textbf{Description} Cholesterol levels were collected from patients
certain days after they had a heart attack

This data frame contains the following columns:

Patient: Patient number

day2: Cholesterol level of patient 2 days after heart attack. (mg/dL)

day4: Cholesterol level of patient 4 days after heart attack. (mg/dL)

day14: Cholesterol level of patient 14 days after heart attack. (mg/dL)

Source Ryan, B. F., Joiner, B. L., \& Ryan, Jr, T. A. (1985).
Cholesterol levels after heart attack.Retrieved from
http://www.statsci.org/data/general/cholest.html

References Ryan, Joiner \& Ryan, Jr, 1985

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The lengths (in kilometers) of rivers on the South Island of New
  Zealand and what body of water they flow into are listed in
  Table~\ref{tbl-Length} (Lee, 1994). Find the mean and median length of
  rivers that flow into the Pacific Ocean and the mean and median length
  of rivers that flow into the Tasman Sea.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Length}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/length.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Length))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrl@{}}

\caption{\label{tbl-Length}Head of Length of New zealand rivers (km)}

\tabularnewline

\toprule\noalign{}
river & length & flowsto \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Clarence & 209 & Pacific \\
Conway & 48 & Pacific \\
Waiau & 169 & Pacific \\
Hurunui & 138 & Pacific \\
Waipara & 64 & Pacific \\
Ashley & 97 & Pacific \\

\end{longtable}

\textbf{Code book for data frame Length}

\textbf{Description} Rivers in New Zealand, the lengths of river and
what body of water the river flows into

This data frame contains the following columns:

River: Name of the river

length: how long the river is in kilometers

flowsto: what body of water the river flows into Pacific Ocean is
Pacific and the Tasman Sea is Tasman

Source Lee, A. (1994). Data analysis: An introduction based on r.
Auckland. Retrieved from http://www.statsci.org/data/oz/nzrivers.html

References Lee, A. (1994). Data analysis: An introduction based on r.
Auckland.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Print-O-Matic printing company's employees have salaries that are
  contained in Table~\ref{tbl-Pay}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pay}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/pay.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Pay))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lr@{}}

\caption{\label{tbl-Pay}Head of Salaries of Print-O-Matic Printing
Company Employees}

\tabularnewline

\toprule\noalign{}
employee & salary \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
CEO & 272500 \\
Driver & 58456 \\
CD74 & 100702 \\
CD65 & 57380 \\
Embellisher & 73877 \\
Folder & 65270 \\

\end{longtable}

\textbf{Code book for data frame Pay}

\textbf{Description} Salaries of Print-O-Matic printing company's
employees

This data frame contains the following columns:

employee:employees position in the company

salary: salary of that employee (Australian dollars (AUD))

Source John Matic provided the data from a company he worked with. The
company's name is fictitious, but the data is from an actual company.

References John Matic (2013)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Find the mean and median.
\item
  Find the mean and median with the CEO's salary removed.
\item
  What happened to the mean and median when the CEO's salary was
  removed? Why?
\item
  If you were the CEO, who is answering concerns from the union that
  employees are underpaid, which average (mean or median) using the
  complete data set of the complete data set would you prefer? Why?
\item
  If you were a platen worker, who believes that the employees need a
  raise, which average (mean or median) using the complete data set
  would you prefer? Why?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Print-O-Matic printing company spends specific amounts on fixed costs
  every month. The costs of those fixed costs are in a
  Table~\ref{tbl-Cost}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cost}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/cost.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Cost))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lr@{}}

\caption{\label{tbl-Cost}Fixed Costs for Print-O-Matic Printing Company}

\tabularnewline

\toprule\noalign{}
charges & cost \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Bank charges & 482 \\
Cleaning & 2208 \\
Computer expensive & 2471 \\
Lease payments & 2656 \\
Postage & 2117 \\
Uniforms & 2600 \\

\end{longtable}

\textbf{Code book for data frame Cost}

\textbf{Description} fixed monthly charges for Print-0-Matic printing
company

This data frame contains the following columns:

charges: Categories of monthly fixed charges

cost: fixed month costs (AUD)

Source John Matic provided the data from a company he worked with. The
company's name is fictitious, but the data is from an actual company.

References John Matic (2013)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Find the mean and median.
\item
  Find the mean and median with the bank charges removed.
\item
  What happened to the mean and median when the bank charges was
  removed? Why?
\item
  If it is your job to oversee the fixed costs, which average (mean or
  median) using the complete data set would you prefer to use when
  submitting a report to administration to show that costs are low? Why?
\item
  If it is your job to find places in the budget to reduce costs, which
  average (mean or median) using the complete data set would you prefer
  to use when submitting a report to administration to show that fixed
  costs need to be reduced? Why?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Looking at graph 3.1.2, state if the graph is skewed left, skewed
  right, or symmetric and then state which is larger, the mean or the
  median?
\end{enumerate}

\textbf{Graph 3.1.2: Skewed or Symmetric Graph}

\begin{figure}[H]

{\centering \includegraphics{graph_3_1_1.png}

}

\caption{Graph \#3.1.2}

\end{figure}%

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Looking at graph 3.1.3, state if the graph is skewed left, skewed
  right, or symmetric and then state which is larger, the mean or the
  median?
\end{enumerate}

\textbf{Graph 3.1.3: Skewed or Symmetric Graph}

\begin{figure}[H]

{\centering \includegraphics{graph_3_1_2.png}

}

\caption{Graph 3.1.3}

\end{figure}%

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  An employee at Coconino Community College (CCC) is evaluated based on
  goal setting and accomplishments toward the goals, job effectiveness,
  competencies, and CCC core values. Suppose for a specific employee,
  goal 1 has a weight of 30\%, goal 2 has a weight of 20\%, job
  effectiveness has a weight of 25\%, competency 1 has a weight of 4\%,
  competency 2 has a weight of 3\%, competency 3 has a weight of 3\%,
  competency 4 has a weight of 3\%, competency 5 has a weight of 2\%,
  and core values has a weight of 10\%. Suppose the employee has scores
  of 3.0 for goal 1, 3.0 for goal 2, 2.0 for job effectiveness, 3.0 for
  competency 1, 2.0 for competency 2, 2.0 for competency 3, 3.0 for
  competency 4, 4.0 for competency 5, and 3.0 for core values. Find the
  weighted average score for this employee. If an employee has a score
  less than 2.5, they must have a Performance Enhancement Plan written.
  Does this employee need a plan?
\item
  An employee at Coconino Community College (CCC) is evaluated based on
  goal setting and accomplishments toward goals, job effectiveness,
  competencies, CCC core values. Suppose for a specific employee, goal 1
  has a weight of 20\%, goal 2 has a weight of 20\%, goal 3 has a weight
  of 10\%, job effectiveness has a weight of 25\%, competency 1 has a
  weight of 4\%, competency 2 has a weight of 3\%, competency 3 has a
  weight of 3\%, competency 4 has a weight of 5\%, and core values has a
  weight of 10\%. Suppose the employee has scores of 2.0 for goal 1, 2.0
  for goal 2, 3.0 for goal 3, 2.0 for job effectiveness, 2.0 for
  competency 1, 3.0 for competency 2, 2.0 for competency 3, 3.0 for
  competency 4, and 4.0 for core values. Find the weighted average score
  for this employee. If an employee that has a score less than 2.5, they
  must have a Performance Enhancement Plan written. Does this employee
  need a plan?
\item
  A statistics class has the following activities and weights for
  determining a grade in the course: test 1 worth 15\% of the grade,
  test 2 worth 15\% of the grade, test 3 worth 15\% of the grade,
  homework worth 10\% of the grade, semester project worth 20\% of the
  grade, and the final exam worth 25\% of the grade. If a student
  receives an 85 on test 1, a 76 on test 2, an 83 on test 3, a 74 on the
  homework, a 65 on the project, and a 79 on the final, what grade did
  the student earn in the course?
\item
  A statistics class has the following activities and weights for
  determining a grade in the course: test 1 worth 15\% of the grade,
  test 2 worth 15\% of the grade, test 3 worth 15\% of the grade,
  homework worth 10\% of the grade, semester project worth 20\% of the
  grade, and the final exam worth 25\% of the grade. If a student
  receives a 92 on test 1, an 85 on test 2, a 95 on test 3, a 92 on the
  homework, a 55 on the project, and an 83 on the final, what grade did
  the student earn in the course?
\end{enumerate}

\section{Measures of Spread}\label{measures-of-spread}

Variability is an important idea in statistics. If you were to measure
the height of everyone in your classroom, every observation gives you a
different value. That means not every student has the same height. Thus
there is variability in people's heights. If you were to take a sample
of the income level of people in a town, every sample gives you
different information. There is variability between samples too.
Variability describes how the data are spread out. If the data are very
close to each other, then there is low variability. If the data are very
spread out, then there is high variability. How do you measure
variability? It would be good to have a number that measures it. This
section will describe some of the different measures of variability,
also known as variation.

In \hyperref[example-finding-the-mean-and-median-using-r]{Example:
Finding the Mean and Median using r}, the average weight of a cat was
calculated to be 2.72 kg. How much does this tell you about the weight
of all cats? Can you tell if most of the weights were close to 2.72 kg
or were the weights really spread out? The highest weight and the lowest
weight are known, but is there more that you can tell? All you know is
that the center of the weights is 2.72 kg.

You need more information.

The \textbf{range} of a set of data is the difference between the
highest and the lowest data values (or maximum and minimum values). The
\textbf{interval} is the lowest and highest values. The range is one
value while the interval is two.

\subsection{Example: Range}\label{example-range}

From
\hyperref[example-affect-of-extreme-values-on-mean-and-median]{Example:
Affect of Extreme Values on Mean and Median}, the maximum is 3.9 kg and
the minimum is 2 kg. So the range is \(3.9-2=1.9 kg\). But what does
that tell you? You don't know if the weights are really spread out, or
if they are close together.

Unfortunately, range doesn't really provide a very accurate picture of
the variability. A better way to describe how the data is spread out is
needed. Instead of looking at the distance the highest value is from the
lowest how about looking at the distance each value is from the mean.
This distance is called the \textbf{deviation}. You might want to find
the average of the deviation. Though the calculation for finding the
average deviation is not very straight forward, you end up with a value
called the \textbf{variance}. The symbol for the population variance is
\(\sigma^2\), and it is the average squared distance from the mean.
Statisticians like the variance, but many other people who work with
statistics use a descriptive statistics which is the square root of the
variance. This gives you the average distance from the mean. This is
called the standard deviation, and is denoted with the letter
\(\sigma\).

The standard deviation is the average (mean) distance from a data point
to the mean. It can be thought of as how much a typical data point
differs from the mean.

The \textbf{sample variance} formula:
\(s^2=\frac{\sum\left(x-\bar{x}\right)^2}{n-1}\), where \(\bar{x}\) is
the sample mean, \(n\) is the sample size, and \(\sum{}\) means to find
the sum of the values.The \(n-1\) on the bottom has to do with a concept
called degrees of freedom. Basically, it makes the sample variance a
better approximation of the population variance.

The \textbf{sample standard deviation} formula:
\(s=\sqrt{ \frac{\sum\left(x-\bar{x}\right)^2}{n-1}}\).

The \textbf{population variance} formula:
\(\sigma^2 = \frac{\sum\left(x-\mu \right)^2}{N}\), where \(\sigma\) is
the Greek letter sigma and \(\sigma^2\) represents the population
variance, \(\mu\) is the population mean, and \(N\) is the size of the
population.

The \textbf{population standard deviation} formula:
\(\sigma =\sqrt{ \frac{\sum\left(x-\mu \right)^2}{N}}\)

Both the sample variance and sample standard deviation can be found
using technology. If using rStudio, you would use

df\_stats(\textasciitilde variable, data=data\_frame, var, sd)

The next example will demonstrate this command.

\subsection{Example: Finding the Standard
Deviation}\label{example-finding-the-standard-deviation}

For the data frame Cats Table~\ref{tbl-Cats} find the variance and
standard derivation for weight of cats. Then find the variance and
standard deviation separated by sex of the cat.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-23}

The variance and standard deviation for all cats is found by performing
the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt, }\AttributeTok{data=}\NormalTok{cats, var, sd) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response       var        sd
1      Bwt 0.2355225 0.4853066
\end{verbatim}

The variance for all cats is 0.24 \(kg^2\) and the standard deviation is
0.49 kg.

To find out the mean, variance, and standard deviation for each sex of
the cats, use the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(Bwt}\SpecialCharTok{\textasciitilde{}}\NormalTok{Sex, }\AttributeTok{data=}\NormalTok{cats, mean, var, sd) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response Sex     mean        var        sd
1      Bwt   F 2.359574 0.07506938 0.2739879
2      Bwt   M 2.900000 0.21854167 0.4674844
\end{verbatim}

You can see that the mean weight of females cats is 2.36 kg, the
variance is 0.075 \(kg^2\), and the standard deviation is 0.27 kg. For
males cats, the mean is 2.9 kg, the variance is 0.22 \(kg^2\), and the
standard deviation is 0.47 kg. This means that female cats weigh less
than males and since the variance and standard deviations are much less
for female cats than males cats, female cats' weights are more
consistent than male cats.

In general a ``small'' variance and standard deviation means the data is
close together (more consistent) and a ``large'' variance and standard
deviation means the data is spread out (less consistent). Sometimes you
want consistent data and sometimes you don't. As an example if you are
making bolts, you want the lengths to be very consistent so you want a
small standard deviation. If you are administering a test to see who can
be a pilot, you want a large standard deviation so you can tell who are
the good pilots and who are the not so good pilots.

What do ``small'' and ``large'' standard deviation mean? To a bicyclist
whose average speed is 20 mph, \(s = 20 mph\) is huge. To an airplane
whose average speed is 500 mph, \(s = 20 mph\) is nothing. The ``size''
of the variation depends on the size of the numbers in the problem and
the mean. Another situation where you can determine whether a standard
deviation is small or large is when you are comparing two different
samples such as in
\hyperref[example-finding-the-standard-deviation]{Example: Finding the
Standard Deviation}. A sample with a smaller standard deviation is more
consistent than a sample with a larger standard deviation.

Many other books and authors stress that there is a computational
formula for calculating the standard deviation. However, this formula
doesn't give you an idea of what standard deviation is and what you are
doing. It is only good for doing the calculations quickly. It goes back
to the days when standard deviations were calculated by hand, and the
person needed a quick way to calculate the standard deviation. It is an
archaic formula that this author is trying to eradicate. It is not
necessary anymore, computers will do the calculations for you with as
much meaning as this formula gives. It is suggested that you never use
it. If you want to understand what the standard deviation is doing, then
you should use the definition formula. If you want an answer quickly,
use a computer.

\subsection{\texorpdfstring{\textbf{Use of Standard
Deviation}}{Use of Standard Deviation}}\label{use-of-standard-deviation}

One of the uses of the standard deviation is to describe how a
population is distributed. This describes where much of the data is for
most distributions. A general rule is that about 95\% of the data is
within 2 standard deviations of the mean. This is not perfect, but is
works for many distributions. There are rules like the empirical rule
and Chebyshev's theorem that give you more detailed percentages, but
95\% in 2 standard deviations is a very good approximation.

\subsection{Example: the general rule}\label{example-the-general-rule}

The U.S. Weather Service has provided the information in
Table~\ref{tbl-Tornado} about the total monthly/annual number of
reported tornadoes in Oklahoma for the years 1950 to 2018. (US
Department of Commerce \& Noaa, 2016)

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Tornado}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://krkozak.github.io/MAT160/Tornado\_OK.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Tornado))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0833}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0667}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1167}}@{}}

\caption{\label{tbl-Tornado}Monthly/Annual Number of tornadoes in
Oklahoma}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Year
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Jan
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Feb
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Mar
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Apr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
May
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Jun
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Jul
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Aug
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Sep
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Oct
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Nov
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Dec
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Annual
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1950 & 0 & 1 & 1 & 5 & 12 & 1 & 0 & 0 & 2 & 1 & 0 & 0 & 23 \\
1951 & 0 & 2 & 0 & 11 & 11 & 11 & 4 & 2 & 1 & 1 & 0 & 0 & 43 \\
1952 & 0 & 0 & 0 & 7 & 5 & 5 & 4 & 1 & 0 & 0 & 0 & 0 & 22 \\
1953 & 0 & 4 & 7 & 9 & 8 & 13 & 4 & 2 & 0 & 0 & 5 & 2 & 54 \\
1954 & 0 & 0 & 7 & 13 & 19 & 4 & 4 & 2 & 3 & 1 & 0 & 0 & 53 \\
1955 & 1 & 1 & 0 & 15 & 32 & 22 & 4 & 2 & 0 & 0 & 0 & 0 & 77 \\

\end{longtable}

\textbf{Code book for data frame Tornado}

\textbf{Description} The U.S. Weather Service has collected data on the
monthly and annual number of tornadoes in Oklahoma.

This data frame contains the following columns:

Year: Year from 1950-2018

Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec: Tornado
numbers in each moth of the year

Annual: Total number of tornadoes for each year

Source US Department of Commerce, \& Noaa. (2016, November 15). 1950
Oklahoma Tornadoes. Retrieved from
https://www.weather.gov/oun/tornadodata-ok-1950

References The data was supplied by The U.S. Weather Service

Find the general interval that contains about 95\% of the data.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-24}

Variable: \(x\) = number of annual tornadoes in Oklahoma

Find the mean and standard deviation:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Annual, }\AttributeTok{data=}\NormalTok{Tornado, mean, sd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response     mean       sd
1   Annual 56.02899 27.56061
\end{verbatim}

The mean is \(\mu=56\) tornadoes and the standard deviation is
\(\sigma=27.6\) tornadoes. The interval will be
\(\mu\pm2*\sigma=56\pm2*27.6=(0.8,111.2)\)

About 95\% of the years have between 0.8 or 1 and 111 tornadoes in
Oklahoma.

The general rule says that about 95\% of the data is within two standard
deviations of the mean. That percentage is fairly high. There isn't much
data outside two standard deviations. A rule that can be followed is
that if a data value is within two standard deviations, then that value
is a common data value. If the data value is outside two standard
deviations of the mean, either above or below, then the number is
uncommon. It could even be called unusual. An easy calculation that you
can do to figure it out is to find the difference between the data point
and the mean, and then divide that answer by the standard deviation. As
a formula this would be

\(z=\frac{x-\mu}{\sigma}\)

If you don't know the population mean, \(\mu\), and the population
standard deviation, \(\sigma\), then use the sample mean, \(\bar{x}\),
and the sample standard deviation, \(s\), to estimate the population
parameter values. Realize that using the sample standard deviation may
not actually be very accurate.

\subsection{Example: Determining If a Value Is
Unusual}\label{example-determining-if-a-value-is-unusual}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  In 1974, there were 45 tornadoes in Oklahoma. Is this value unusual?
  Why or why not?
\item
  In 1999, there were 145 tornadoes in the Oklahoma. Is this value
  unusual? Why or why not?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-25}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  In 1974, there were 45 tornadoes in Oklahoma. Is this value unusual?
  Why or why not?

  Variable: \(x\) = number of tornadoes in Oklahoma
\end{enumerate}

To answer this question, first find how many standard deviations 45 is
from the mean. From 3.2.4 example, we know \(\mu=56\) and
\(\sigma=27.6\). For \(x\)=45, \(z=\frac{45-56}{27.6}=-0.399\)

Since this value is between -2 and 2, then it is not unusual to have 45
tornadoes in a year in Oklahoma. The z value is negative, so that means
that 45 is less than the mean number of tornadoes.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\item
  In 1999, there were 145 tornadoes in the Oklahoma. Is this value
  unusual? Why or why not?

  Variable: \(x\) = number of tornadoes in Oklahoma
\end{enumerate}

For this question the \(x\) = 145, \(z=\frac{145-56}{27.6}=3.22\)

Since this value is more than 2, then it is unusual to have only 145
tornadoes in a year in Oklahoma.

\subsection{Homework for Measures of Spread
Section}\label{homework-for-measures-of-spread-section}

\textbf{Use Technology on all problems. State the variable on all
problems.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Cholesterol levels were collected from patients certain days after
  they had a heart attack and are in Table~\ref{tbl-Cholesterol}. Find
  the mean, median, range, variance, and standard deviation for
  cholesterol levels 2 days after the heart attack.
\end{enumerate}

\textbf{Code book for Data Frame Cholesterol} is below
Table~\ref{tbl-Cholesterol}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The lengths (in kilometers) of rivers on the South Island of New
  Zealand and what body of water they flow into are listed in
  Table~\ref{tbl-Length} (Lee, 1994). Find the mean, median, range,
  variance, and standard deviation of the length of rivers that flow
  into the Pacific Ocean and the mean, median, range, variance, and
  standard deviation of the length of rivers that flow into the Tasman
  Sea. Compare and contrast the length of rivers that flow to the
  Pacific Ocean versus the ones that flow into the Tasman Sea using both
  measures of spread and measures of variability.
\end{enumerate}

\textbf{Code book for data frame Length} is below
Table~\ref{tbl-Length}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Print-O-Matic printing company's employees have salaries that are
  contained in Table~\ref{tbl-Pay}. Find the mean, median, range,
  variance, and standard deviation for the salaries of all employees.
\end{enumerate}

\textbf{Code book for data frame Pay} below Table~\ref{tbl-Pay}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Print-O-Matic printing company spends specific amounts on fixed costs
  every month. The costs of those fixed costs are in
  Table~\ref{tbl-Cost}. Find the mean, median, range, variance, and
  standard deviation for the fixed costs.
\end{enumerate}

\textbf{Code book for Data frame Cost} is below Table~\ref{tbl-Cost}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The data frame Pulse Table~\ref{tbl-Pulse} contains various variables
  about a person including their pulse rates before the subject
  exercised and after the subject ran in place for one minute.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pulse}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://krkozak.github.io/MAT160/pulse.csv"}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Pulse))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0482}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0964}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1084}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0482}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1566}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1446}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0602}}@{}}

\caption{\label{tbl-Pulse}Head of Pulse Rates of people Before and After
Exercise}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
smokes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alcohol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
exercise
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ran
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_before
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_after
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
year
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
170 & 68 & 22 & male & yes & yes & moderate & sat & 70 & 71 & 93 \\
182 & 75 & 26 & male & yes & yes & moderate & sat & 80 & 76 & 93 \\
180 & 85 & 19 & male & yes & yes & moderate & ran & 68 & 125 & 95 \\
182 & 85 & 20 & male & yes & yes & low & sat & 70 & 68 & 95 \\
167 & 70 & 22 & male & yes & yes & low & sat & 92 & 84 & 96 \\
178 & 86 & 21 & male & yes & yes & low & sat & 76 & 80 & 98 \\

\end{longtable}

\textbf{Code book for data frame Pulse}

Description Students in an introductory statistics class (MS212 taught
by Professor John Eccleston and Dr Richard Wilson at The University of
Queensland) participated in a simple experiment. The students took their
own pulse rate. They were then asked to flip a coin. If the coin came up
heads, they were to run in place for one minute. Otherwise they sat for
one minute. Then everyone took their pulse again. The pulse rates and
other physiological and lifestyle data are given in the data.

Five class groups between 1993 and 1998 participated in the experiment.
The lecturer, Richard Wilson, was concerned that some students would
choose the less strenuous option of sitting rather than running even if
their coin came up heads, In the years 1995-1998 a different method of
random assignment was used. In these years, data forms were handed out
to the class before the experiment. The forms were pre-assigned to
either running or non-running and there were an equal number of each. In
1995 and 1998 not all of the forms were returned so the numbers running
and sitting was still not entirely controlled.

This data frame contains the following columns:

height: height of subject in cm

weight: weight of subject in kg

age: age of subject in years

gender: sex of subject, male, female

Smokes: whether a subject regularly smokes, yes means does smoke, no
means does not smoke

alcohol: whether a subject regularly drinks alcohol, yes means the
person does, no means the person does not

exercise: whether a subject exercises, low, moderate, high

ran: whether a subject ran one minute between pulse measurements (ran)
or sat between pulse measurement (sat)

pulse\_before: the pulse rate before a subject either ran or sat (bpm)

pulse\_after: the pulse rate after a subject either ran or sat (bpm)

year: what year the data was collected (93-98)

Source Pulse rates before and after exercise. (2013, September 25).
Retrieved from http://www.statsci.org/data/oz/ms212.html

References The data was supplied by Dr Richard J. Wilson, Department of
Mathematics, University of Queensland.

Create a data frame that contains only males, who drink alcohol, but do
not smoke. Then compare the pulse before and the pulse after using the
mean and standard deviation. Discuss whether pulse before or pulse after
has a higher mean and larger spread. The following command creates a new
data frame with just males, who drink alcohol, but do not smoke, use the
following command, where the new name is Males in Table~\ref{tbl-Males}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Males}\OtherTok{\textless{}{-}}\NormalTok{ Pulse }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(gender}\SpecialCharTok{==}\StringTok{"male"}\NormalTok{, smokes }\SpecialCharTok{==} \StringTok{"no"}\NormalTok{, alcohol }\SpecialCharTok{==} \StringTok{"yes"}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Males))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0482}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0964}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1084}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0482}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1566}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1446}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0602}}@{}}

\caption{\label{tbl-Males}Head of Pulse Rates of Nonsmoking Males Before
and After Exercise}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
smokes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alcohol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
exercise
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ran
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_before
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_after
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
year
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
195 & 84 & 18 & male & no & yes & high & sat & 71 & 73 & 93 \\
184 & 74 & 22 & male & no & yes & low & ran & 78 & 141 & 93 \\
168 & 60 & 23 & male & no & yes & moderate & ran & 88 & 150 & 93 \\
170 & 75 & 20 & male & no & yes & high & ran & 76 & 88 & 93 \\
187 & 59 & 18 & male & no & yes & high & sat & 78 & 82 & 93 \\
180 & 72 & 18 & male & no & yes & moderate & sat & 69 & 67 & 93 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  The data frame Pulse Table~\ref{tbl-Pulse} contains various variables
  about a person including their pulse rates before the subject
  exercised and after the subject ran in place for one minute. Create a
  data frame that contains females, who do not smoke but do drink
  alcohol. Compare the pulse rate before and after exercise using the
  mean and standard deviation. Discuss whether pulse before or pulse
  after has a higher mean and larger spread.
\item
  To determine if Reiki is an effective method for treating pain, a
  pilot study was carried out where a certified second-degree Reiki
  therapist provided treatment on volunteers. Pain was measured using a
  visual analogue scale (VAS) and a likert scale immediately before and
  after the Reiki treatment (Olson \& Hanson, 1997) and the data is in
  Table~\ref{tbl-Reiki}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Reiki}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/reki.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Reiki))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrrr@{}}

\caption{\label{tbl-Reiki}Head of Pain Measurements Before and After
Reiki Treatment}

\tabularnewline

\toprule\noalign{}
vas.before & vas.after & likert\_before & likert\_after \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
6 & 3 & 2 & 1 \\
2 & 1 & 2 & 1 \\
2 & 0 & 3 & 0 \\
9 & 1 & 3 & 1 \\
3 & 0 & 2 & 0 \\
3 & 2 & 2 & 2 \\

\end{longtable}

\textbf{Code book for data frame Reiki}

\textbf{Description} The purpose of this study was to explore the
usefulness of Reiki as an adjuvant to opioid therapy in the management
of pain. Since no studies in this area could be found, a pilot study was
carried out involving 20 volunteers experiencing pain at 55 sites for a
variety of reasons, including cancer. All Reiki treatments were provided
by a certified second-degree Reiki therapist. Pain was measured using
both a visual analogue scale (VAS) and a Likert scale immediately before
and after the Reiki treatment. Both instruments showed a highly
significant (p \textless{} 0.0001) reduction in pain following the Reiki
treatment.

This data frame contains the following columns:

vas.before: pain measured using a visual analogue scale (VAS) before
Reiki treatment

vas.after: pain measured using a visual analogue scale (VAS) after Reiki
treatment

likert\_before: pain measured using a likert before Reiki treatment

likert\_after: pain measured using a likert after Reiki treatment

Source Olson, K., \& Hanson, J. (1997). Using reiki to manage pain: a
preliminary report. Cancer Prev Control, 1(2), 108-13. Retrieved from
http://www.ncbi.nlm.nih.gov/pubmed/9765732

References** Using Reiki to manage pain: a preliminary report. Olson K1,
Hanson J., Cancer Prev Control 1997, Jun; 1(2): 108-13.

Since the data was collected both before and after the treatment for all
of the units of observations, you want to look at the effect size of the
treatment. You want to find the difference between before and after for
the pain scale. First you must create a new data frame that adds a
column for the difference in before and after. This data is known as
paired data. To create the new column in a new data frame called
Newreiki use the following commands, Table~\ref{tbl-Newreiki}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Newreiki}\OtherTok{\textless{}{-}}\NormalTok{Reiki }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{vas.diff=}\NormalTok{vas.before}\SpecialCharTok{{-}}\NormalTok{vas.after) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Newreiki))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrrrr@{}}

\caption{\label{tbl-Newreiki}Head of Pain Measurements Before and After
Reiki Treatment with Difference column}

\tabularnewline

\toprule\noalign{}
vas.before & vas.after & likert\_before & likert\_after & vas.diff \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
6 & 3 & 2 & 1 & 3 \\
2 & 1 & 2 & 1 & 1 \\
2 & 0 & 3 & 0 & 2 \\
9 & 1 & 3 & 1 & 8 \\
3 & 0 & 2 & 0 & 3 \\
3 & 2 & 2 & 2 & 1 \\

\end{longtable}

Now find the mean and standard deviation of the vas.diff variable in
Newreiki. Perform similar commands to create the likert.diff variable.
Then find the mean and standard deviation for likert.diff, and compare
and contrast the vas and likert methods for describing pain.

8.Yearly rainfall amounts (in millimeters) in Sydney, Australia, are in
Table~\ref{tbl-Rainfall} (Annual maximums of, 2013). a. Calculate the
mean and standard deviation. b. Suppose Sydney, Australia received 300
mm of rainfall in a year. Would this be unusual?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Rainfall}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{(}\StringTok{"https://krkozak.github.io/MAT160/rainfall.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Rainfall))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-Rainfall}Head of Yearly rainfall amounts in Sydney,
Australia}

\tabularnewline

\toprule\noalign{}
amount \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
146.8 \\
383.0 \\
90.9 \\
178.1 \\
267.5 \\
95.5 \\

\end{longtable}

\textbf{Code book for data frame Rainfall}

\textbf{Description} Daily rainfall (in millimeters) was recorded over a
47-year period in Turramurra, Sydney, Australia. For each year, the
wettest day was identified (that having the greatest rainfall). The data
show the rainfall recorded for the 47 annual maxima.

This data frame contains the following columns:

amount: daily rainfall (mm)

Source Annual maximums of daily rainfall in Sydney. (2013, September
25). Retrieved from http://www.statsci.org/data/oz/sydrain.html

References Rayner J.C.W. and Best D.J. (1989) Smooth tests of goodness
of fit. Oxford: Oxford University Press. Hand D.J., Daly F., Lunn A.D.,
McConway K.J., Ostrowski E. (1994). A Handbook of Small Data Sets.
London: Chapman \& Hall. Data set 157. Thanks to Jim Irish of the
University of Technology, Sydney, for assistance in identifying the
correct units for this data.

\section{Ranking}\label{ranking}

Along with the center and the variability, another useful numerical
measure is the ranking of a number. A \textbf{percentile} is a measure
of ranking. It represents a location measurement of a data value to the
rest of the values. Many standardized tests give the results as a
percentile. Doctors also use percentiles to track a child's growth.

The \(k^{th}\) \textbf{percentile} is the data value that has k\% of the
data at or below that value.

\subsection{Example: Interpreting
Percentile}\label{example-interpreting-percentile}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What does a score of the \(90^{th}\) percentile mean?
\item
  What does a score of the \(70^{th}\) percentile mean?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-26}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  What does a score of the \(90^{th}\) percentile mean?

  This means that 90\% of the scores were at or below this score. (A
  person did the same as or better than 90\% of the test takers.)
\item
  What does a score of the \(70^{th}\) percentile mean?

  This means that 70\% of the scores were at or below this score.
\end{enumerate}

\subsection{Example: Percentile Versus
Score}\label{example-percentile-versus-score}

If the test was out of 100 points and you scored at the \(80^{th}\)
percentile, what was your score on the test?

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-27}

You don't know! All you know is that you scored the same as or better
than 80\% of the people who took the test. If all the scores were really
low, you could have still failed the test. On the other hand, if many of
the scores were high you could have gotten a 95\% or more.

There are special percentiles called \textbf{quartiles}. Quartiles are
numbers that divide the data into fourths. One fourth (or a quarter) of
the data falls between consecutive quartiles.

\subsection{\texorpdfstring{\textbf{To find the
quartiles}:}{To find the quartiles:}}\label{to-find-the-quartiles}

The command in rStudio is

df\_stats(\textasciitilde variable, data=data\_frame, summary)

If you record the quartiles together with the maximum and minimum you
have five numbers. This is known as the five-number summary. The
five-number summary consists of the minimum, the first quartile
(\(Q1\)), the median, the third quartile (\(Q3\)), and the maximum (in
that order).

The interquartile range, \(IQR\), is the difference between the first
and third quartiles, \(Q1\) and \$Q3\$. Half of the data (50\%) falls in
the interquartile range. If the \(IQR\) is ``large'' the data is spread
out and if the \(IQR\) is ``small'' the data is closer together.

Interquartile Range (\(IQR\))

Determining probable outliers from \(IQR\): \textbf{fences}

A value that is less than \(Q1-1.5*IQR\) (this value is often referred
to as a \textbf{low fence}) is considered an outlier.

Similarly, a value that is more than \(Q3+1.5*IQR\) (the \textbf{high
fence}) is considered an outlier.

A boxplot (or box-and-whisker plot) is a graphical display of the
five-number summary. It can be drawn vertically or horizontally. The
basic format is a box from \(Q1\) to \(Q3\), a vertical line across the
box for the median and horizontal lines as whiskers extending out each
end to the minimum and maximum. The minimum and maximum can be
represented with dots. Don't forget to label the tick marks on the
number line and give the graph a title.

An alternate form of a Boxplot, known as a modified box plot, only
extends the left line to the smallest value greater than the \textbf{low
fence}, and extends the left line to the largest value less than the
\textbf{high fence}, and displays markers (dots, circles or asterisks)
for each outlier.

If the data are \textbf{symmetrical}, then the box plot will be visibly
symmetrical. If the data distribution has a left skew or a right skew,
the line on that side of the box plot will be visibly long. If the plot
is symmetrical, and the four quartiles are all about the same length,
then the data are likely a near \textbf{uniform} distribution. If a box
plot is symmetrical, and both outside lines are noticeably longer than
the \(Q1\) to median and median to \(Q3\) distance, the distribution is
then probably \textbf{bell-shaped}.

\subsection{Example: Five-number Summary and
Boxplot}\label{example-five-number-summary-and-boxplot}

Find the five-number summary, the interquartile range (*IQR*), and draw
a box-and-whiskers plot for the weight of cats Table~\ref{tbl-Cats}.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-28}

Variable: \(x\) = weight of cats To compute the five-number summary on
RStudio, use the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt, }\AttributeTok{data=}\NormalTok{cats, summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response Min. 1st Qu. Median     Mean 3rd Qu. Max.
1      Bwt    2     2.3    2.7 2.723611   3.025  3.9
\end{verbatim}

Note rStudio also calculates the mean as part of the summary command,
but the five-number summary is just the five numbers:

Minimum: 2 kg \(Q1\): 2.3 kg Median: 2.7 kg \(Q3\): 3.025 kg Maximum:
3.9 kg

To find the interquartile range, \(IQR\) find \$Q3-Q1\$, so
\(IQR=3.025-2.3=0.725 kg\)

To create a boxplot use the command

gf\_boxplot(\textasciitilde variable, data=data\_frame)

This is a modified boxplot which shows the outliers in the data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_boxplot}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt, }\AttributeTok{data=}\NormalTok{cats, }\AttributeTok{title=}\StringTok{"Weight of Cats"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Body Weight (kg)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Numerical-Description-of-Data_files/figure-pdf/fig-Cats-boxplot-1.pdf}

}

\caption{\label{fig-Cats-boxplot}Weight of Cats}

\end{figure}%

There are no outliers since there are no dots outside of the fences.

\subsection{Example: Separating based on a
factor}\label{example-separating-based-on-a-factor}

Find the five-number summary of the weights of cats separated by the sex
of the cat. Then create a box plot of the weights of cats for each sex
of the cat.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-29}

Variable: \(x_1\) = weight of female cat

Variable: \(x_2\) = weight of male cat

To find the five-number summary separated based on gender use the
following command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt}\SpecialCharTok{|}\NormalTok{Sex, }\AttributeTok{data=}\NormalTok{cats, summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response Sex Min. 1st Qu. Median     Mean 3rd Qu. Max.
1      Bwt   F    2    2.15    2.3 2.359574     2.5  3.0
2      Bwt   M    2    2.50    2.9 2.900000     3.2  3.9
\end{verbatim}

The five-number summary for female cats is (in kg)

Minimum: 2 \(Q1\): 2.15 Median: 2.3 \(Q3\): 2.5 Maximum: 3.0

The five-number summary for male cats is (in kg)

Minimum: 2 \(Q1\): 2.50 Median: 2.9 \(Q3\): 3.2 Maximum: 3.9

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_boxplot}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Bwt}\SpecialCharTok{|}\NormalTok{Sex, }\AttributeTok{data=}\NormalTok{cats, }\AttributeTok{title=}\StringTok{"Weights of Cats"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Body Weight in (kg)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Numerical-Description-of-Data_files/figure-pdf/fig-Cats-boxplot-sex-1.pdf}

}

\caption{\label{fig-Cats-boxplot-sex}Weight of Cats Faceted by Sex}

\end{figure}%

Notice that the weights of female cats has a median less than male cats,
and in fact it can be seen that the \(Q1\) to \(Q3\) of the female cats
is less than the \(Q1\) to \(Q3\) of the male cats.

\subsection{Example: Putting it all
together}\label{example-putting-it-all-together}

The time (in 1/50 seconds) between successive pulses along a nerve fiber
(``Time between nerve,'' 2013) are given in Table~\ref{tbl-Nerve}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Nerve}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Nerve\_pulse.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Nerve))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-Nerve}Head of Successive pulses along a nerve fiber}

\tabularnewline

\toprule\noalign{}
time \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
10.5 \\
1.5 \\
2.5 \\
5.5 \\
29.5 \\
3.0 \\

\end{longtable}

\textbf{Code book for data frame Nerve}

\textbf{Description} The data gives the time between 800 successive
pulses along a nerve fiber. There are 799 observations rounded to the
nearest half in units of 1/50 second.

This data frame contains the following columns:

time: time between successive Pulses along a nerve fiber, 1/50 second.

Source Time between nerve pulses. (2019, July 3). Retrieved from
\textless http://www.statsci.org/data/general/nerve.html

References Fatt, P., and Katz, B. (1952). Spontaneous subthreshold
activity at motor nerve endings. Journal of Physiology 117, 109-128.

Cox, D. R., and Lewis, P. A. W. (1966). The Statistical Analysis of
Series of Events. Methuen, London.

Jorgensen, B. (1982). The Generalized Inverse-Gaussian Distribution.
Springer-Verlag.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-30}

First, it might be useful to look at a visualization of the data, so
create a density plot

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{time, }\AttributeTok{data=}\NormalTok{Nerve, }\AttributeTok{title=}\StringTok{"Time between Successive Nerve Pulses"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Time (1/50 second)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Numerical-Description-of-Data_files/figure-pdf/fig-Pulse-density-1-1.pdf}

}

\caption{\label{fig-Pulse-density-1}Weight of Cats Faceted by Sex}

\end{figure}%

From the graph Figure~\ref{fig-Pulse-density-1} the data appears to be
skewed right. Most of the time between successive nerve pulses appear to
be around 5 or 10 1/50 second, but there are some times that are 60 1/50
second.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{time, }\AttributeTok{data=}\NormalTok{Nerve, mean, median, sd, summary)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response     mean median       sd Min. 1st Qu. Median     Mean 3rd Qu. Max.
1     time 10.95119    7.5 10.45956  0.5     3.5    7.5 10.95119      15   69
\end{verbatim}

Numerical descriptions might also be useful. Using technology, the mean
is 11 1/50 second,the median is 7.5 1/50 second, the standard deviation
is 10.5 1/50 second, and the five-number summary is minimum = 3.5, Q1 =
3.5, median = 7.5, Q3 = 15, and maximum = 69 1/50 second.

To visualize the five-number summary, create a box plot.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_boxplot}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{time, }\AttributeTok{data=}\NormalTok{Nerve, }\AttributeTok{title=}\StringTok{"Nerve Pulses"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Time (1/50 second)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Numerical-Description-of-Data_files/figure-pdf/fig-Nerve-box-1.pdf}

}

\caption{\label{fig-Nerve-box}Boxplot of Nerve Pulses}

\end{figure}%

Since there are many dots outside the upper fence the data has many
outliers. From all of this information, one could say that nerve pulses
between successive pulses is around 11 1/50 second, with a spread of
19.5 1/50 second. Most of the values are round 11 1/50 second, but they
are not very consistent. The density plot and boxplot show that there is
a great deal of spread of the data and it is skewed to the right. This
means mostly the speed is around 11 1/50 second, but there is a great
deal of variability in the values.

\subsection{Homework for Ranking
Section}\label{homework-for-ranking-section}

\textbf{Use Technology on all problems. State the variable on all
problems.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Suppose you take a standardized test and you are in the \(10^{th}\)
  percentile. What does this percentile mean? Can you say that you
  failed the test? Explain.
\item
  Suppose your child takes a standardized test in mathematics and scores
  in the \(96^{th}\) percentile. What does this percentile mean? Can you
  say your child passed the test? Explain.
\item
  Suppose your child is in the \(83^{rd}\) percentile in height and
  \(24^{th}\) percentile in weight. Describe what this tells you about
  your child's stature.
\item
  Suppose your work evaluates the employees and places them on a
  percentile ranking. If your evaluation is in the \(65^{th}\)
  percentile, do you think you are working hard enough? Explain.
\item
  Cholesterol levels were collected from patients certain days after
  they had a heart attack and are in table Table~\ref{tbl-Cholesterol}.
\end{enumerate}

\textbf{Code book for Data Frame Cholesterol} below
Table~\ref{tbl-Cholesterol}.

Find the five-number summary and interquartile range (IQR) for the
cholesterol level on day 2, and draw a boxplot

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  The lengths (in kilometers) of rivers on the South Island of New
  Zealand and what body of water they flow into are listed in table
  Table~\ref{tbl-Length} (Lee, 1994).
\end{enumerate}

\textbf{Code book for data frame Length} below Table~\ref{tbl-Length}.

Find the five-number summary and interquartile range (IQR) for the
lengths of rivers that go to the Pacific Ocean and ones that go to the
Tasman Sea, and draw a boxplot of both.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Print-O-Matic printing company's employees have salaries that are
  contained in Table~\ref{tbl-Pay} Find the five number summary and draw
  a boxplot for the salaries of all employees.
\end{enumerate}

\textbf{Code book for data frame Pay} below Table~\ref{tbl-Pay}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  The data frame Pulse Table~\ref{tbl-Pulse} contains various variables
  about a person including their pulse rates before the subject
  exercised and after after the subject ran in place for one minute.
\end{enumerate}

\textbf{Code book for data frame Pulse} below Table~\ref{tbl-Pulse}.

Create a data frame that contains only people who drink alcohol, but do
not smoke. Then find the five number summary and draw a boxplot for both
males and females separately.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  To determine if Reiki is an effective method for treating pain, a
  pilot study was carried out where a certified second-degree Reiki
  therapist provided treatment on volunteers. Pain was measured using a
  visual analogue scale (VAS) and a likert scale immediately before and
  after the Reiki treatment (Olson \& Hanson, 1997) and the data is in
  Table~\ref{tbl-Reiki}.
\end{enumerate}

\textbf{Code book for data frame Reiki} below Table~\ref{tbl-Reiki}.

Find the five number summary for both the before and after VAS scores
and draw boxplots of before and after VAS scores. To draw two boxplots
at the same time, after the command to create the first box plot type
the piping symbol \textbar\textgreater{} (base r) or \%\textgreater\%
(magrittr package) before pressing enter. (Note: \textbar\textgreater{}
and \%\textgreater\% are piping symbols that can be thought of as ``and
then.'') Then type the command for the second boxplot after the + symbol
or on the next line in the r chunk if using an rmd or qmd file. Then
press enter. You may want to graph each boxplot as a different color. To
do this, the command would be

gf\_boxplot(\textasciitilde variable, data=data\_frame, color=``red'',
xlab=``type a label'')

You can pick any color you want. Just replace the word red with the
color you want to use. Now compare and contrast the before and after VAS
scores.

\bookmarksetup{startatroot}

\chapter{Probability}\label{probability}

Understanding probabilities are important in life. Examples of mundane
questions that probability can answer for you are if you need to carry
an umbrella or wear a heavy coat on a given day. More important
questions that probability can help with are your chances that the car
you are buying will need more maintenance, your chances of passing a
class, your chances of winning the lottery, your chances of being in a
car accident, and the chances that the U.S. will be attacked by
terrorists. Most people do not have a very good understanding of
probability, so they worry about being attacked by a terrorist but not
about being in a car accident. The probability of being in a terrorist
attack is much smaller than the probability of being in a car accident,
thus it actually would make more sense to worry about driving. Also, the
chance of you winning the lottery is very small, yet many people will
spend their money on lottery tickets. Yet, if instead they saved the
money that they spend on the lottery, they would have more money. In
general, events that have a low probability (under 5\%) are unlikely to
occur. Whereas if an event has a high probability of happening (over
80\%), then there is a good chance that the event will happen. This
chapter will present some of the theory that you need to help make a
determination of whether an event is likely to happen or not.

One story about how probability theory was developed is that a gambler
wanted to know when to bet more and when to bet less. He talked to a
couple of friends of his that happened to be mathematicians. Their names
were Pierre de Fermat and Blaise Pascal. Since then many other
mathematicians have worked to develop probability theory. There are
actually two types of probability, \textbf{Empirical Probability} and
\textbf{Theoretical Probability} that have been developed since the
start of probability.

\section{Empirical Probability}\label{empirical-probability}

Empirical probabilities are found by actually conducting an experiment
many times and counting the number of times the event happens. To
understand how this is performed, first some definitions are needed.

\textbf{Outcomes}: the results of an experiment

\textbf{Event}: a set of certain outcomes of an experiment that you want
to have happen

\textbf{Sample Space}: collection of all possible outcomes of the
experiment. Usually denoted as \(SS\).

\textbf{Event space}: the set of outcomes that make up an event. The
symbol is usually a capital letter.

\textbf{Frequency}: how often an event happens

\textbf{Relative Frequency}: the frequency divided by the number of
times the experiment is repeated

Start with an experiment. Suppose that the experiment is rolling a die.
The sample space is \{1, 2, 3, 4, 5, 6\}. The event that you want is to
get a 6, and the event space is \{6\}. To do this, roll a die 10 times.
When you do that, you get a 6 two times. Based on this experiment, the
probability of getting a 6 is 2 out of 10 or 1/5. To get more accuracy,
repeat the experiment more times. It is easiest to put this in a table,
where *n* represents the number of times the experiment is repeated.
When you put the number of 6s found over the number of times you repeat
the experiment, this is the relative frequency.

\begin{longtable}[]{@{}rrr@{}}

\caption{\label{tbl-Die}trials for Die Experiment}

\tabularnewline

\toprule\noalign{}
n & number\_of\_6s & relative\_frequency \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
10 & 2 & 0.200 \\
50 & 6 & 0.120 \\
100 & 18 & 0.180 \\
500 & 81 & 0.162 \\
1000 & 163 & 0.163 \\

\end{longtable}

Notice that as \(n\) increased, the relative frequency seems to approach
a number. It looks like it is approaching 0.163. You can say that the
probability of getting a 6 is approximately 0.163. If you want more
accuracy, then increase \(n\) even more.

These probabilities are called \textbf{experimental probabilities} since
they are found by actually doing the experiment. They come about from
the relative frequencies and give an approximation of the true
probability. The approximate probability of an event \(A\), \(P(A)\), is

\subsection{\texorpdfstring{\textbf{Experimental
Probabilities}}{Experimental Probabilities}}\label{experimental-probabilities}

\(P(A)=\frac{\text{number of times} }{ \text{number of times experiment is conducted}}\)

For the event of getting a 6, the probability would be

\(P(6)=\frac{163}{1000}=0.163\).

You must do experimental probabilities whenever it is not possible to
calculate probabilities using other means. An example is if you want to
find the probability that a family has 5 children, you would have to
actually look at many families, and count how many have 5 children. Then
you could calculate the probability. Another example is if you want to
figure out if a die is fair. You would have to roll the die many times
and count how often each side comes up. Make sure you repeat an
experiment many times, because otherwise you will not be able to
estimate the true probability. This is due to the law of large numbers.

\textbf{Law of large numbers}: as \(n\) increases, the relative
frequency tends towards the actual probability value.

Note: probability, relative frequency, percentage, and proportion are
all different words for the same concept. Also, probabilities can be
given as percentages, decimals, or fractions.

To find probabilities from data, you can take a data frame and count the
number of values for each outcome.

\subsection{Example: Statistics class
survey}\label{example-statistics-class-survey}

Data was collected for two semesters in a statistics class. The data
frame in is in Table~\ref{tbl-Class}.

Find the probability (proportion) of people who like Cookie Dough ice
cream.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-31}

To count the number of people who like cookie dough ice cream, use the
following command in r Studio:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ice\_cream, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{margins=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ice_cream
          Butter Pecan              Chocolate    Chocolate Brownie.  
                     2                      2                      1 
                coffee           Cookie Dough      Cookies and Cream 
                     1                      6                      1 
               Mint CC           Moose Tracks                   none 
                     6                      1                      1 
            Rocky Road                Sherbet Strawberry and banana  
                     2                      2                      1 
               Vanilla                  Total 
                     1                     27 
\end{verbatim}

From this tally, it can be seen that 6 people like cookie dough ice
cream. The probability that someone likes cookie dough is thus 6 divided
by the number of people in the data frame, the Total. Instead of
dividing, the following command will find the proportions for you.
Proportions are just probabilities.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{ice\_cream, }\AttributeTok{data=}\NormalTok{Class, }\AttributeTok{format=}\StringTok{"proportion"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
ice_cream
          Butter Pecan              Chocolate    Chocolate Brownie.  
            0.07407407             0.07407407             0.03703704 
                coffee           Cookie Dough      Cookies and Cream 
            0.03703704             0.22222222             0.03703704 
               Mint CC           Moose Tracks                   none 
            0.22222222             0.03703704             0.03703704 
            Rocky Road                Sherbet Strawberry and banana  
            0.07407407             0.07407407             0.03703704 
               Vanilla 
            0.03703704 
\end{verbatim}

So the probability that a person in the class likes cookie dough ice
cream is 0.22.

\subsection{Homework for Empirical Probability
Section}\label{homework-for-empirical-probability-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The number of M\&M's of each color that were found in a packet is in
  Table~\ref{tbl-MaM} (M\&M's Color Distribution Analysis, 2019).
\end{enumerate}

\begin{longtable}[]{@{}llr@{}}

\caption{\label{tbl-MaM}M\&M Distribution}

\tabularnewline

\toprule\noalign{}
color & type & pack \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
orange & plain & 1 \\
red & plain & 1 \\
green & plain & 1 \\
red & plain & 1 \\
yellow & plain & 1 \\
blue & plain & 1 \\

\end{longtable}

\textbf{Code book for Data Frame MaM}

\textbf{Description} An analysis of the colors in a case of M\&M's to
see if they match the published percentages

Usage MaM

Format

This data frame contains the following columns:

color: color of M\&Ms

type: The type of M\&M such as plain, peanut, peanut butter

pack: which pack the M\&Ms came from.

Source M\&M's Color Distribution Analysis. (n.d.). Retrieved July 11,
2019, from
https://joshmadison.com/2007/12/02/mms-color-distribution-analysis/

References Josh Madison, 2019

Find the probability of choosing each color based on this data frame.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Eyeglassomatic manufactures eyeglasses for different retailers. They
  test to see how many defective lenses they made the time period of
  January 1 to March 31. The defect and the number of defects is in
  Table~\ref{tbl-Defects}.
\end{enumerate}

\textbf{Code book for Data Frame Defects} below Table~\ref{tbl-Defects}.

Find the probability of each defect type based on this data.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  In Australia in 1995, of the 2907 indigenous people in prison 17 of
  them died. In that same year, of the 14501 non-indigenous people in
  prison 42 of them died (\textbackslash{}``Aboriginal deaths
  in,\textbackslash{}'' 2013). Find the probability that an indigenous
  person dies in prison and the probability that a non-indigenous person
  dies in prison. Compare these numbers and discuss what the numbers may
  mean.
\item
  A project conducted by the Australian Federal Office of Road Safety
  asked people many questions about their cars. One question was the
  reason that a person chooses a given car, and that data is in
  Table~\ref{tbl-Car_pref} (Car Preferences, 2019).
\end{enumerate}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0154}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0154}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0269}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0231}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0269}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0269}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0231}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0231}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0385}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0577}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0308}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0692}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0577}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0577}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0577}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0577}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0692}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0577}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0577}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0692}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0692}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 42\tabcolsep) * \real{0.0692}}@{}}

\caption{\label{tbl-Car_pref}Reason for Choosing a Car}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LicYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
LicMth
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ActCar
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kids5
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Kids6
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PreferCar
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Carsmall\_new5K
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reason
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Cost
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Reliable
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Perform
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Fuel
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Safety
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AC.PS
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Park
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Room
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Doors
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Prestige
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Colour
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
110 & 18 & male & 0 & 2 & large & no & no & medium & large\_used &
safety & important & very\_important & important & important & important
& important & important & important & important & important &
little\_importance \\
111 & 25 & female & 8 & 0 & small & no & no & small & small\_new &
safety & very\_important & very\_important & very\_important &
very\_important & very\_important & important & very\_important &
very\_important & little\_importance & important & important \\
112 & 63 & male & 46 & 0 & large & no & no & large & large\_used &
comfort & important & important & important & important & important &
important & important & important & important & important & important \\
113 & 51 & female & 35 & 0 & large & no & no & medium & large\_used &
safety & little\_importance & important & important & important &
very\_important & important & important & important & important &
not\_important & important \\
114 & 19 & female & 2 & 0 & medium & no & no & medium & small\_new &
looks & important & important & very\_important & important & important
& little\_importance & important & important & little\_importance &
important & important \\
115 & 51 & female & 30 & 0 & medium & yes & yes & medium & large\_used &
comfort & important & very\_important & very\_important & important &
very\_important & very\_important & important & important & important &
little\_importance & important \\

\end{longtable}

\textbf{Code book for Data Frame Car\_pref}

\textbf{Description} These data were collected as part of a project for
the Federal Office for Road Safety conducted by the Research Institute
of Gender and Health at the University of Newcastle. There is evidence
that women drivers who are involved in motor vehicle accidents are more
likely than men to be injured. A possible reason is that women often
drive smaller cars that provide less protection in a collision. One of
the aims of the project was to examine preferences for cars among men
and women and investigate the extent to which safety was a factor in
determining preferences. The survey was conducted by research assistants
who asked people in car parks to participate and administered a
structured questionnaire. They were instructed to obtain data from men
and women with small, medium and large cars, with 50 people per group
for a total of 300 respondents. (The sample size was based on power
requirements for another part of the survey that involved anthropometric
measurements.) The research assistants approached people in car parks of
the University of Newcastle and nearby shopping centers during December
1997 and January 1998.

Usage Car\_pref

Format

This data frame contains the following columns:

ID: Identification number of respondent

Age: Age of respondent (years)

Sex: female, male

LicYr: Time they have held a full driving licence, in years and months
(years)

LicMth: Time they have held a full driving licence, in years and months
(months)

ActCar: Make, model and year of car most often driven, coded to size of
car small, medium, large

Kids5: Children under five, yes, no

Kids6: Children 6 to 16, yes, no

PrefCar: Preferred car, coded to size of car small, medium, large

Car15k: Preferred type of car if cost \textbackslash\$15000, small new
car; large second-hand car

Reason: safety, reliability, cost, performance, comfort, looks

Cost: How important is cost when buying a car? not important, little
importance, important, very important

Reliable: How important is reliability \ldots?

Perform: How important is performance \ldots?

Fuel: How important is fuel consumption \ldots?

Safety: How important is safety \ldots?

AC/PS: How important is air conditioning/power steering \ldots?

Park: How important is ease of parking \ldots?

Room: How important is space/roominess \ldots?

Doors: How important is the number of doors \ldots?

Prestige: How important is prestige/style \ldots?

Colour: How important is colour \ldots?

Source

Car Preferences. (n.d.). Retrieved July 11, 2019, from
http://www.statsci.org/data/oz/carprefs.html

References

The data was contributed to OzDASL by Professor Annette Dobson,
University of Queensland. Information on the data set was originally
provided by Jenny Powers.

Find the probability a person chooses a car for each of the given
reasons.

\section{Theoretical Probability}\label{theoretical-probability}

It is not always feasible to conduct an experiment over and over again,
so it would be better to be able to find the probabilities without
conducting the experiment. These probabilities are called
\textbf{Theoretical Probabilities}.

To be able to do theoretical probabilities, there is an assumption that
you need to consider. It is that all of the outcomes in the sample space
need to be **equally likely outcomes**. This means that every outcome of
the experiment needs to have the same chance of happening.

\subsection{Example: Equally Likely
Outcomes}\label{example-equally-likely-outcomes}

Which of the following experiments have equally likely outcomes?

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Rolling a fair die.
\item
  Flip a coin that is weighted so one side comes up more often than the
  other.
\item
  Pull a ball out of a can containing 6 red balls and 8 green balls. All
  balls are the same size.
\item
  Picking a card from a deck.
\item
  Rolling a die to see if it is fair.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-32}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Rolling a fair die.

  Since the die is fair, every side of the die has the same chance of
  coming up. The outcomes are the different sides, so each outcome is
  equally likely
\item
  Flip a coin that is weighted so one side comes up more often than the
  other.

  Since the coin is weighted, one side is more likely to come up than
  the other side. The outcomes are the different sides, so each outcome
  is not equally likely
\item
  Pull a ball out of a can containing 6 red balls and 8 green balls. All
  balls are the same size.

  Since each ball is the same size, then each ball has the same chance
  of being chosen. The outcomes of this experiment are the individual
  balls, so each outcome is equally likely. Don't assume that because
  the chances of pulling a red ball are less than pulling a green ball
  that the outcomes are not equally likely. The outcomes are the
  individual balls and they are equally likely.
\item
  Picking a card from a deck.

  If you assume that the deck is fair, then each card has the same
  chance of being chosen. Thus the outcomes are equally likely outcomes.
  You do have to make this assumption. For many of the experiments you
  will do, you do have to make this kind of assumption.
\item
  Rolling a die to see if it is fair.

  In this case you are not sure the die is fair. The only way to
  determine if it is fair is to actually conduct the experiment, since
  you don't know if the outcomes are equally likely. If the experimental
  probabilities are fairly close to the theoretical probabilities, then
  the die is fair.
\end{enumerate}

If the outcomes are not equally likely, then you must do experimental
probabilities. If the outcomes are equally likely, then you can do
theoretical probabilities.

\textbf{Theoretical Probabilities}: If the outcomes of an experiment are
equally likely, then the probability of event A happening is

\(P(A)=\frac{\text{number of outcomes in event space}}{\text{number of outcomes in sample space}}\)

\subsection{Example: Calculating Theoretical
Probabilities}\label{example-calculating-theoretical-probabilities}

Suppose you conduct an experiment where you flip a fair coin twice

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What is the sample space?
\item
  What is the probability of getting exactly one head?
\item
  What is the probability of getting at least one head?
\item
  What is the probability of getting a head and a tail?
\item
  What is the probability of getting a head or a tail?
\item
  What is the probability of getting a foot?
\item
  What is the probability of each outcome? What is the sum of these
  probabilities?
\end{enumerate}

\subsubsection{Solution}\label{solution-33}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  What is the sample space?

  There are several different sample spaces you can do. One is SS=\{0,
  1, 2\} where you are counting the number of heads. However, the
  outcomes are not equally likely since you can get one head by getting
  a head on the first flip and a tail on the second or a tail on the
  first flip and a head on the second. There are 2 ways to get that
  outcome and only one way to get the other outcomes. Instead it might
  be better to give the sample space as listing what can happen on each
  flip. Let H = head and T = tail, and list which can happen on each
  flip.

  \(SS\)=\{HH, HT, TH, TT\}
\item
  What is the probability of getting exactly one head?

  Let \(A\) = getting exactly one head. The event space is \(A\) = \{HT,
  TH\}. So \(P(A)=\frac{2}{4}\)

  It may not be advantageous to reduce the fractions to lowest terms,
  since it is easier to compare fractions if they have the same
  denominator.
\item
  What is the probability of getting at least one head?

  Let \(B\) = getting at least one head. At least one head means get one
  or more. The event space is \(B\) = \{HT, TH, HH\} and
  \(P(B)=\frac{3}{4}\) Since \(P(B)\) is greater than the \(P(A)\), then
  event \(B\) is more likely to happen than event \(A\).
\item
  What is the probability of getting a head and a tail?

  Let \(C\) = getting a head and a tail = \{HT, TH\} and
  \(P(C)=\frac{2}{4}\) This is the same event space as event \(A\), but
  it is a different event. Sometimes two different events can give the
  same event space.
\item
  What is the probability of getting a head or a tail?

  Let \(D\) = getting a head or a tail. Since or means one or the other
  or both and it doesn't specify the number of heads or tails, then
  \(D\) = \{HH, HT, TH, TT\} and \(P(D)=\frac{3}{4}\)
\item
  What is the probability of getting a foot?

  Let \(E\) = getting a foot. Since you can't get a foot, \(E\) = \{\}
  or the empty set and \(P(E)=\frac{0}{4}=0\)
\item
  What is the probability of each outcome? What is the sum of these
  probabilities?

  \(P(HH)=P(HT)=P(TH)=P(TT)=\frac{1}{4}\). If you add all of these
  probabilities together you get \(1\).
\end{enumerate}

This example had some results in it that are important concepts. They
are summarized below:

\subsection{\texorpdfstring{\textbf{Probability
Properties}}{Probability Properties}}\label{probability-properties}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(0 \le P(\text{event}) \le 1\)
\item
  If the \(P(\text{event}) = 1\), then it will happen and is called the
  certain event
\item
  If the \(P(\text{event}) = 0\), then it cannot happen and is called
  the impossible event
\item
  \(\sum{P(\text{all outcomes})}=1\)
\end{enumerate}

\subsection{Example: Calculating Theoretical Probabilities
2}\label{example-calculating-theoretical-probabilities-2}

Suppose you conduct an experiment where you pull a card from a standard
deck.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What is the sample space?
\item
  What is the probability of getting a Spade?
\item
  What is the probability of getting a Jack?
\item
  What is the probability of getting an Ace?
\item
  What is the probability of not getting an Ace?
\item
  What is the probability of not getting an Ace?
\item
  What is the probability of getting a Spade or an Ace?
\item
  What is the probability of getting a Jack and an Ace?
\item
  What is the probability of getting a Jack and an Ace?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-34}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  What is the sample space?

  \(SS\) = \{2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS, KS, AS, 2C,
  3C, 4C, 5C, 6C, 7C, 8C, 9C, 10C, JC, QC, KC, AC, 2D, 3D, 4D, 5D, 6D,
  7D, 8D, 9D, 10D, JD, QD, KD, AD, 2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H,
  JH, QH, KH, AH\}
\item
  What is the probability of getting a Spade?

  Getting a spade = \{2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS, KS,
  AS\} so \(P(spade)=\frac{13}{52}\)
\item
  What is the probability of getting a Jack?

  Getting a Jack = \{JS, JC, JH, JD\} so \(P(jack)=\frac{4}{52}\)
\item
  What is the probability of getting an Ace?

  Getting an Ace = \{AS, AC, AH, AD\} so \(P(ace)=\frac{4}{52}\)
\item
  What is the probability of not getting an Ace?

  Not getting an Ace = \{2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS,
  KS, 2C, 3C, 4C, 5C, 6C, 7C, 8C, 9C, 10C, JC, QC, KC, 2D, 3D, 4D, 5D,
  6D, 7D, 8D, 9D, 10D, JD, QD, KD, 2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H,
  JH, QH, KH\} so \(P(\text{not ace})=\frac{48}{52}\)
\end{enumerate}

Notice, \(P(ace)+P(\text{not ace})=1\), so you could have found the
probability of not ace by doing \(1\) minus the probability of ace.
\(P(\text{not ace})=1-P(ace) = 1-\frac{4}{52} = \frac{48}{52}\)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{5}
\item
  What is the probability of getting a Spade and an Ace?

  Getting a Spade and an Ace = \{AS\} so \(P(AS)=\frac{1}{52}\)
\item
  What is the probability of getting a Spade or an Ace?

  Getting a Spade and an Ace =\{2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS,
  QS, KS, AS, AC, AD, AH\} so \(P(\text{spade and ace})=\frac{16}{52}\)
\item
  What is the probability of getting a Jack and an Ace?

  Getting a Jack and an Ace = \{ \} since you can't do that when picking
  one card. So \(P(\text{Jack and Ace})=\frac{0}{52}=0\)
\item
  What is the probability of getting a Jack or an Ace?

  Getting a Jack or an Ace = \{JS, JC, JD, JH, AS, AC, AD, AH\} so
  \(P(\text{Jack or Ace})=\frac{8}{52}\)
\end{enumerate}

\subsection{Example: Calculating Theoretical Probabilities
3}\label{example-calculating-theoretical-probabilities-3}

Suppose you have an iPhone and playing iTunes with the following songs
on it: 5 Rolling Stones songs, 7 Beatles songs, 9 Bob Dylan songs, 4
Faith Hill songs, 2 Taylor Swift songs, 7 U2 songs, 4 Mariah Carey
songs, 7 Bob Marley songs, 6 Bunny Wailer songs, 7 Elton John songs, 5
Led Zeppelin songs, and 4 Dave Mathews Band songs. The different genre
that you have are rock from the 60s which includes Rolling Stones,
Beatles, and Bob Dylan; country includes Faith Hill and Taylor Swift;
rock of the 90s includes U2 and Mariah Carey; Reggae includes Bob Marley
and Bunny Wailer; rock of the 70s includes Elton John and Led Zeppelin;
and bluegrass-rock includes Dave Mathews Band.

Suppose the iTunes is set to shuffle the songs, so it randomly picks the
next song so you have no idea what the next song will be. Now you would
like to calculate the probability that you will hear the type of music
or the artist that you are interested in. The sample set is too
difficult to write out, but you can figure it from looking at the number
in each set and the total number. The total number of songs you have is
67.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What is the probability that you will hear a Faith Hill song?
\item
  What is the probability that you will hear a Bunny Wailer song?
\item
  What is the probability that you will hear a song from the 60s?
\item
  What is the probability that you will hear a Reggae song?
\item
  What is the probability that you will hear a song from the 90s or a
  bluegrass-rock song?
\item
  What is the probability that you will hear an Elton John or a Taylor
  Swift song?
\item
  What is the probability that you will hear a country song or a U2
  song?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-35}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  What is the probability that you will hear a Faith Hill song?

  There are 4 Faith Hill songs out of the 67 songs, so
  \(P(\text{Faith Hill})=\frac{4}{67}\)
\item
  What is the probability that you will hear a Bunny Wailer song?

  There are 6 Bunny Wailer songs, so
  \(P(\text{Bunny Wailer})=\frac{6}{67}\)
\item
  What is the probability that you will hear a song from the 60s?

  There are 5, 7, and 9 songs that are classified as rock from the 60s,
  which is 21 total, so \(P(\text{song from 60s})=\frac{21}{67}\)
\item
  What is the probability that you will hear a Reggae song?

  There are 6 and 7 songs that are classified as Reggae, which is 13
  total, so \(P(\text{Reggae})=\frac{13}{67}\)
\item
  What is the probability that you will hear a song from the 90s or a
  bluegrass-rock song?

  There are 7 and 4 songs that are songs from the 90s and 4 songs that
  are bluegrass-rock, for a total of 15, so
  \(P(\text{song 90s or bluegrass-rock})=\frac{15}{67}\)
\item
  What is the probability that you will hear an Elton John or a Taylor
  Swift song?

  There are 7 Elton John songs and 2 Taylor Swift songs, for a total of
  9, so \(P(\text{Elton John or Taylor Swift})=\frac{9}{67}\)
\item
  What is the probability that you will hear a country song or a U2
  song?

  There are 6 country songs and 7 U2 songs, for a total of 13, so
  \(P(\text{country or U2})=\frac{13}{67}\)
\end{enumerate}

Of course you can do any other combinations you would like.

Notice in
\hyperref[example-calculating-theoretical-probabilities]{Example:
Calculating Theoretical Probabilities} part e, it was mentioned that the
probability of getting an ace plus the probability of not getting an ace
was 1. This is because these two events have no outcomes in common, and
together they make up the entire sample space. Events that have this
property are called \textbf{complementary events}.

If two events are \textbf{complementary events} then to find the
probability of one just subtract the probability of the other from one.
Notation used for complement of \(A\) is \(\text{not }A\) or \(A^{c}\).

\(P(A)+P(\text{not }A)=1\)

\subsection{Example: Complementary
Events}\label{example-complementary-events}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Suppose you know that the probability of it raining today is 0.45.
  What is the probability of it not raining?
\item
  Suppose you know the probability of not getting the flu is 0.24. What
  is the probability of getting the flu?
\item
  In an experiment of picking a card from a deck, what is the
  probability of not getting a card that is a Queen?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-36}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Suppose you know that the probability of it raining today is 0.45.
  What is the probability of it not raining?

  Since not raining is the complement of raining, then
  \(P(\text{not raining})=1-P(\text{raining}) = 1-0.45=0.55\)
\item
  Suppose you know the probability of not getting the flu is 0.24. What
  is the probability of getting the flu?

  Since getting the flu is the complement of not getting the flu, then
  \(P(\text{getting flu})=1-P(flu)=1-0.24=0.76\)
\item
  In an experiment of picking a card from a deck, what is the
  probability of not getting a card that is a Queen?

  You could do this problem by listing all the ways to not get a queen,
  but that set is fairly large. One advantage of the complement is that
  it reduces the workload. You use the complement in many situations to
  make the work shorter and easier. In this case it is easier to list
  all the ways to get a Queen, find the probability of the Queen, and
  then subtract from one.

  Queen = \{QS, QC, QD, QH\} so \(P(Queen)=\frac{4}{52}\) and
  \(P(\text{not Queen})=1-P(Queen)=1-\frac{4}{52}=\frac{48}{52}\)
\end{enumerate}

The complement is useful when you are trying to find the probability of
an event that involves the words at least or an event that involves the
words at most. As an example of an at least event is suppose you want to
find the probability of making at least \textbackslash\$50,000 when you
graduate from college. That means you want the probability of your
salary being greater than or equal to \textbackslash\$50,000. An example
of an at most event is suppose you want to find the probability of
rolling a die and getting at most a 4. That means that you want to get
less than or equal to a 4 on the die. The reason to use the complement
is that sometimes it is easier to find the probability of the complement
and then subtract from 1.

\subsection{Example: Using the Complement to Find
Probabilities}\label{example-using-the-complement-to-find-probabilities}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  In an experiment of rolling a fair die one time, find the probability
  of rolling at most a 4 on the die.
\item
  In an experiment of pulling a card from a fair deck, find the
  probability of pulling at least a 5 (ace is a high card in this
  example).
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-37}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  In an experiment of rolling a fair die one time, find the probability
  of rolling at most a 4 on the die.

  The sample space for this experiment is \{1, 2, 3, 4, 5, 6\}. You want
  the event of getting at most a 4, which is the same as thinking of
  getting 4 or less. The event space is \{1, 2, 3, 4\}. The probability
  is \(P(\text{at most a 4})=\frac{4}{6}\)

  Or you could have used the complement. The complement of rolling at
  most a 4 would be rolling number bigger than 4. The event space for
  the complement is \{5, 6\}. The probability of the complement is
  \(P(\text{more than 4})=\frac{2}{6}\). The probability of at most 4
  would be
  \(P(\text{at most 4})=1-P(\text{more than 4})=1-\frac{2}{6}=\frac{4}{6}\)

  Notice you have the same answer, but the event space was easier to
  write out. For this example the complement probability wasn't that
  useful, but in the future there will be events where it is much easier
  to use the complement.
\item
  In an experiment of pulling a card from a fair deck, find the
  probability of pulling at least a 5 (ace is a high card in this
  example).

  The sample space for this experiment is \(SS\) = \{2S, 3S, 4S, 5S, 6S,
  7S, 8S, 9S, 10S, JS, QS, KS, AS, 2C, 3C, 4C, 5C, 6C, 7C, 8C, 9C, 10C,
  JC, QC, KC, AC, 2D, 3D, 4D, 5D, 6D, 7D, 8D, 9D, 10D, JD, QD, KD, AD,
  2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH, AH\}
\end{enumerate}

Pulling a card that is at least a 5 would involve listing all of the
cards that are a 5 or more. It would be much easier to list the outcomes
that make up the complement. The complement of at least a 5 is less than
a 5. That would be the event of 4 or less. The event space for the
complement would be \{2S, 3S, 4S, 2C, 3C, 4C, 2D, 3D, 4D, 2H, 3H, 4H\}.
The probability of the complement would be \(\frac{12}{52}\). The
probability of at least a 5 would be
\(P(\text{at least 5})=1-P(\text{at most 4})=1-\frac{12}{52}=\frac{40}{52}\)

Another concept was shown in
\hyperref[example-calculating-theoretical-probabilities-2]{Example:
Calculating Theoretical Probabilities 2} parts g and i. The problems
were looking for the probability of one event or another. In part g, it
was looking for the probability of getting a Spade or an Ace. That was
equal to \(\frac{16}{52}\). In part i, it was looking for the
probability of getting a Jack or an Ace. That was equal to
\(\frac{8}{52}\). If you look back at the parts b, c, and d, you might
notice the following result: \(P(\text{Jack or Ace})=P(Jack)+P(Ace)\)
but \(P(\text{Spade or Ace})\ne P(Spade)+P(Ace)\).

Why does adding two individual probabilities together work in one
situation to give the probability of one or another event and not give
the correct probability in the other?

The reason this is true in the case of the Jack and the Ace is that
these two events cannot happen together. There is no overlap between the
two events, and in fact the \(P(\text{Jack and Ace})=0\). However, in
the case of the Spade and Ace, they can happen together. There is
overlap, mainly the ace of spades. The \(P(\text{Spade and Ace})\ne0\).

When two events cannot happen at the same time, they are called
\textbf{mutually exclusive}. In the above situation, the events Jack and
Ace are mutually exclusive, while the events Spade and Ace are not
mutually exclusive.

\subsection{\texorpdfstring{\textbf{Addition
Rules:}}{Addition Rules:}}\label{addition-rules}

If two events A and B are mutually exclusive, then
\(P(\text{A and B})=0\) and \(P(\text{A or B})=P(A)+P(B)\)

If two events A and B are not mutually exclusive, then
\(P(\text{A or B})=P(A)+P(B)-P(\text{A and B})\)

\subsection{Example: Using Addition
Rules}\label{example-using-addition-rules}

Suppose your experiment is to roll two fair dice.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What is the sample space?
\item
  What is the probability of getting a sum of 5?
\item
  What is the probability of getting the first die a 2?
\item
  What is the probability of getting a sum of 7?
\item
  What is the probability of getting a sum of 5 and the first die a 2?
\item
  What is the probability of getting a sum of 5 or the first die a 2?
\item
  What is the probability of getting a sum of 5 and sum of 7?
\item
  What is the probability of getting a sum of 5 or sum of 7?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-38}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  What is the sample space?

  As with the other examples you need to come up with a sample space
  that has equally likely outcomes. One sample space is to list the sums
  possible on each roll. That sample space would look like: \(SS\) =
  \{2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12\}. However, there are more ways
  to get a sum of 7 then there are to get a sum of 2, so these outcomes
  are not equally likely. Another thought is to list the possibilities
  on each roll. As an example you could roll the dice and on the first
  die you could get a \(1\). The other die could be any number between
  \(1\) and \(6\), but say it is a \(1\) also. Then this outcome would
  look like (1,1). Similarly, you could get (1, 2), (1, 3), (1,4), (1,
  5), or (1, 6). Also, you could get a 2, 3, 4, 5, or 6 on the first die
  instead. Putting this all together, you get the sample space:

  \(SS\) = \{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6),

  (2,1), (2,2), (2,3), (2,4), (2,5), (2,6),

  (3,1), (3,2), (3,3), (3,4), (3,5), (3,6),

  (4,1), (4,2), (4,3), (4,4), (4,5), (4,6),

  (5,1), (5,2), (5,3), (5,4), (5,5), (5,6),

  (6,1), (6,2), (6,3), (6,4), (6,5), (6,6)\}

  Notice that a (2,3) is different from a (3,2), since the order that
  you roll the die is important and you can tell the difference between
  these two outcomes. You don't need any of the doubles twice, since
  these are not distinguishable from each other in either order.
\end{enumerate}

\textbf{This will always be the sample space for rolling two dice.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\item
  What is the probability of getting a sum of 5?

  Getting a sum of 5 = \{(4,1), (3,2), (2,3), (1,4)\} so
  \(P(\text{sum of 5})=\frac{4}{36}\)
\item
  What is the probability of getting the first die a 2?

  Getting first die a 2 = \{(2,1), (2,2), (2,3), (2,4), (2,5), (2,6)\}
  so \(P(\text{1st die 2})=\frac{6}{36}\)
\item
  What is the probability of getting a sum of 7?

  Getting a sum of 7 = \{(6,1), (5,2), (4,3), (3,4), (2,5), (1,6)\} so
  \(P(\text{sum of 7})=\frac{6}{36}\)
\item
  What is the probability of getting a sum of 5 and the first die a 2?

  This is events A and B which contains the outcome \{(2,3)\} so
  \(P(\text{sum of 5 and 1st die a 2})=\frac{1}{36}\)
\item
  What is the probability of getting a sum of 5 or the first die a 2?

  Notice from part e, that these two events are not mutually exclusive,
  so
\end{enumerate}

\(P(\text{sum of 5 or 1st die a 2})\)

\(=P(\text{sum of 5})+P(\text{1st die 2})-P(\text{sum of 5 and 1st die a 2})\)

\(= \frac{4}{36}+\frac{6}{36}-\frac{1}{36}=\frac{9}{36}\)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{6}
\item
  What is the probability of getting a sum of 5 and sum of 7?

  These are the events parts a and c, which have no outcomes in common.
  Thus sum of 5 and sum of 7 = \{ \} so
  \(P(\text{sum of 5 and sum of 7})=0\)
\item
  What is the probability of getting a sum of 5 or sum of 7?

  From part g, these two events are mutually exclusive, so
  \(P(\text{sum of 5 or sum of 7})=P(\text{sum of 5})+P(\text{sum of 7})\)

  \(=\frac{4}{36}+\frac{6}{36}=\frac{10}{36}\)
\end{enumerate}

\subsection{Odds}\label{odds}

Many people like to talk about the odds of something happening or not
happening. Mathematicians, statisticians, and scientists prefer to deal
with probabilities since odds are difficult to work with, but gamblers
prefer to work in odds for figuring out how much they are paid if they
win.

The \textbf{actual odds against} event \(A\) occurring are the ratio
\(\frac{P(\text{not }A)}{P(A)}\), usually expressed in the form \(a:b\)
or \(a\) to \(b\), where \(a\) and \(b\) are integers with no common
factors.

The \textbf{actual odds in favor} event \(A\) occurring are the ratio
\(\frac{P(A)}{P(\text{not }A)}\), which is the reciprocal of the odds
against. If the odds against event \(A\) are \(a:b\), then the odds in
favor event \(A\) are \(b:a\).

The \textbf{payoff odds} against event \(A\) occurring are the ratio of
the net profit (if you win) to the amount bet.

payoff odds against event \(A\) = (net profit) : (amount bet)

\subsection{Example: Odds Against and Payoff
Odds}\label{example-odds-against-and-payoff-odds}

In the game of Craps, if a shooter has a come-out roll of a 7 or an 11,
it is called a natural and the pass line wins. The payoff odds are given
by a casino as \(1:1\).

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the probability of a natural.
\item
  Find the actual odds for a natural.
\item
  Find the actual odds against a natural.
\item
  If the casino pays 1:1, how much profit does the casino make on a
  \textbackslash\$10 bet?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-39}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Find the probability of a natural.

  A natural is a 7 or 11. The sample space is

  SS = \{(1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (2,1), (2,2), (2,3),
  (2,4), (2,5), (2,6), (3,1), (3,2), (3,3), (3,4), (3,5), (3,6), (4,1),
  (4,2), (4,3), (4,4), (4,5), (4,6), (5,1), (5,2), (5,3), (5,4), (5,5),
  (5,6), (6,1), (6,2), (6,3), (6,4), (6,5), (6,6)\}

  The event space is \{(1,6), (2,5), (3,4), (4,3), (5,2), (6,1), (5,6),
  (6,5)\}

  So \(P(\text{7 or 11})=\frac{8}{36}\)
\item
  Find the actual odds for a natural.

  \(\text{odds of natural}=\frac{P(\text{7 or 11})}{P(\text{not 7 or 11})} =\frac{\frac{8}{36}}{1-\frac{8}{36}} =\frac{\frac{8}{36}}{\frac{28}{36}} =\frac{8}{28}=\frac{2}{7}\)
\item
  Find the actual odds against a natural.

  \(\text{odds of against a natural}=\frac{P(\text{not 7 or 11})}{P(\text{7 or 11})} =\frac{1-\frac{8}{36}}{\frac{8}{36}} =\frac{\frac{28}{36}}{\frac{8}{36}} =\frac{28}{8}=\frac{3.5}{1}\)
\item
  If the casino pays 1:1, how much profit does the casino make on a
  \textbackslash\$10 bet?

  The actual odds are 3.5 to 1 while the payoff odds are 1 to 1. The
  casino pays you \textbackslash\$10 for your \textbackslash\$10 bet. If
  the casino paid you the actual odds, they would pay
  \textbackslash\$3.50 on every \textbackslash\$1 bet, and on
  \textbackslash\$10, they pay \$3.5*10\$ =\textbackslash\$35. Their
  profit is \(35-10\)= \textbackslash\$25.
\end{enumerate}

\subsection{Homework for Theoretical Probability
Section}\label{homework-for-theoretical-probability-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  In \hyperref[homework-for-empirical-probability-section]{Homework for
  Empirical Probability Section}, the probabilities of each color of
  M\&Ms in a packet were found. Use that information to answer the
  following questions.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the probability of choosing a green or red M\&M.
\item
  Find the probability of choosing a blue, red, or yellow M\&M.
\item
  Find the probability of not choosing a brown M\&M.
\item
  Find the probability of not choosing a green M\&M.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  In \hyperref[homework-for-empirical-probability-section]{Homework for
  Empirical Probability Section}, the probabilities for defects in
  eyeglasses manufactured by Eyeglassomatic were calculated. Use that
  information to find the following probabilities.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the probability of picking a lens that is scratched or flaked.
\item
  Find the probability of picking a lens that is the wrong PD or was
  lost in lab.
\item
  Find the probability of picking a lens that is not scratched.
\item
  Find the probability of picking a lens that is not the wrong shape.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  An experiment is to flip a fair coin three times.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the sample space.
\item
  Find the probability of getting exactly two heads. Make sure you state
  the event space.
\item
  Find the probability of getting at least two heads. Make sure you
  state the event space.
\item
  Find the probability of getting an odd number of heads. Make sure you
  state the event space.
\item
  Find the probability of getting all heads or all tails. Make sure you
  state the event space.
\item
  Find the probability of getting exactly two heads or exactly two
  tails.
\item
  Find the probability of not getting an odd number of heads.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  An experiment is rolling a fair die and then flipping a fair coin.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the sample space.
\item
  Find the probability of getting a head. Make sure you state the event
  space.
\item
  Find the probability of getting a 6. Make sure you state the event
  space.
\item
  Find the probability of getting a 6 or a head.
\item
  Find the probability of getting a 3 and a tail.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  An experiment is rolling two fair dice.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the sample space.
\item
  Find the probability of getting a sum of 3. Make sure you state the
  event space.
\item
  Find the probability of getting the first die is a 4. Make sure you
  state the event space.
\item
  Find the probability of getting a sum of 8. Make sure you state the
  event space.
\item
  Find the probability of getting a sum of 3 or sum of 8.
\item
  Find the probability of getting a sum of 3 or the first die is a 4.
\item
  Find the probability of getting a sum of 8 or the first die is a 4.
\item
  Find the probability of not getting a sum of 8.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  An experiment is pulling one card from a fair deck.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the sample space.
\item
  Find the probability of getting a Ten. Make sure you state the event
  space.
\item
  Find the probability of getting a Diamond. Make sure you state the
  event space.
\item
  Find the probability of getting a Club. Make sure you state the event
  space.
\item
  Find the probability of getting a Diamond or a Club.
\item
  Find the probability of getting a Ten or a Diamond.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  An experiment is pulling a ball from an urn that contains 3 blue balls
  and 5 red balls.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the probability of getting a red ball.
\item
  Find the probability of getting a blue ball.
\item
  Find the odds for getting a red ball.
\item
  Find the odds for getting a blue ball.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  In the game of roulette, there is a wheel with spaces marked 0 through
  36 and a space marked 00.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the probability of winning if you pick the number 7 and it comes
  up on the wheel.
\item
  Find the odds against winning if you pick the number 7.
\item
  The casino will pay you \textbackslash\$20 for every dollar you bet if
  your number comes up. How much profit is the casino making on the bet?
\end{enumerate}

\section{Conditional Probability}\label{conditional-probability}

Suppose you want to figure out if you should buy a new car. When you
first go and look, you find two cars that you like the most. In your
mind they are equal, and so each has a 50\% chance that you will pick
it. Then you start to look at the reviews of the cars and realize that
the first car has had 40\% of them needing to be repaired in the first
year, while the second car only has 10\% of the cars needing to be
repaired in the first year. You could use this information to help you
decide which car you want to actually purchase. Both cars no longer have
a 50\% chance of being the car you choose. You could actually calculate
the probability you will buy each car, which is a conditional
probability. You probably wouldn't do this, but it gives you an example
of what a conditional probability is.

\textbf{Conditional probabilities} are probabilities calculated after
information is given. This is where you want to find the probability of
event \(A\) happening after you know that event \(B\) has happened. If
you know that \(B\) has happened, then you don't need to consider the
rest of the sample space. You only need the outcomes that make up event
\(B\). Event \(B\) becomes the new sample space, which is called the
\textbf{restricted sample space,} \(R\). If you always write a
restricted sample space when doing conditional probabilities and use
this as your sample space, you will have no trouble with conditional
probabilities. The notation for conditional probabilities is
\(P(\text{A, given B})=P(A|B)\). The event following the vertical line
is always the restricted sample space.

\subsection{Example: Conditional
Probabilities}\label{example-conditional-probabilities}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Suppose you roll two dice. What is the probability of getting a sum of
  5, given that the first die is a 2?
\item
  Suppose you roll two dice. What is the probability of getting a sum of
  7, given the first die is a 4?
\item
  Suppose you roll two dice. What is the probability of getting the
  second die a 2, given the sum is a 9?
\item
  Suppose you pick a card from a deck. What is the probability of
  getting a Spade, given that the card is a Jack?
\item
  Suppose you pick a card from a deck. What is the probability of
  getting an Ace, given the card is a Queen?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-40}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Suppose you roll two dice. What is the probability of getting a sum of
  5, given that the first die is a 2?

  Since you know that the first die is a 2, then this is your restricted
  sample space, \(R\) = \{(2,1), (2,2), (2,3), (2,4), (2,5), (2,6)\} Out
  of this restricted sample space, the way to get a sum of 5 is
  \{(2,3)\}. Thus

  \(P(\text{sum of 5, given first die a 2})=P(\text{sum of 5}|\text{first die 2})=\frac{1}{6}\)
\item
  Suppose you roll two dice. What is the probability of getting a sum of
  7, given the first die is a 4?

  Since you know that the first die is a 4, this is your restricted
  sample space, \(R\) = \{(4,1), (4,2), (4,3), (4,4), (4,5), (4,6)\} Out
  of this restricted sample space, the way to get a sum of 7 is
  \{(4,3)\}. Thus

  \(P(\text{sum of 7, given first die a 4})=P(\text{sum of 7}|\text{first die 4})=\frac{1}{6}\)
\item
  Suppose you roll two dice. What is the probability of getting the
  second die a 2, given the sum is a 9?

  Since you know the sum is a 9, this is your restricted sample space,
  \(R\) = \{(3,6), (4,5), (5,4), (6,3)\}. Out of this restricted sample
  space there is no way to get the second die a 2. Thus

  \(P(\text{first die a 2, given sum is 9})=P(\text{1st die a 2}|\text{sum of 9})=\frac{0}{4}\)
\item
  Suppose you pick a card from a deck. What is the probability of
  getting a Spade, given that the card is a Jack?

  Since you know that the card is a Jack, this is your restricted sample
  space, \(R\) = \{JS, JC, JD, JH\}. Out of this restricted sample
  space, the way to get a Spade is \{JS\}. Thus

  \(P(\text{spade, given card a Jack})=P(\text{spade}|\text{Jack})=\frac{1}{4}\)
\item
  Suppose you pick a card from a deck. What is the probability of
  getting an Ace, given the card is a Queen?

  Since you know that the card is a Queen, then this is your restricted
  sample space, \(R\) = \{QS, QC, QD, QH\} Out of this restricted sample
  space, there is no way to get an Ace, thus

  \(P(\text{Ace, given Queen})=P(\text{Ace}|\text{Queen})=\frac{0}{4}\)
\end{enumerate}

If you look at the results of
\hyperref[example-calculating-theoretical-probabilities-2]{Example:
Calculating Theoretical Probabilities 2} part d and
\hyperref[example-calculating-theoretical-probabilities]{Example:
Calculating Theoretical Probabilities} part b, you will notice that you
get the same answer. This means that knowing that the first die is a 4
did not change the probability that the sum is a 7. This added knowledge
did not help you in any way. It is as if that information was not given
at all. However, if you compare example
\hyperref[example-calculating-theoretical-probabilities-2]{Example:
Calculating Theoretical Probabilities 2} part b and
\hyperref[example-calculating-theoretical-probabilities]{Example:
Calculating Theoretical Probabilities} part a, you will notice that they
are not the same answer. In this case, knowing that the first die is a 2
did change the probability of getting a sum of 5. In the first case, the
events sum of 7 and first die is a 4 are called \textbf{independent
events}. In the second case, the events sum of 5 and first die is a 2
are called \textbf{dependent events}.

Events \(A\) and \(B\) are considered \textbf{independent events} if the
fact that one event happens does not change the probability of the other
event happening. In other words, events \(A\) and \(B\) are independent
if the fact that \(B\) has happened does not affect the probability of
event \(A\) happening and the fact that \(A\) has happened does not
affect the probability of event \(B\) happening. Otherwise, the two
events are dependent.

In symbols, \(A\) and \(B\) are independent if \(P(A|B)=P(A)\) or
\(P(B|A)=P(B)\)

\subsection{Example: Independent
Events}\label{example-independent-events}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Suppose you roll two dice. Are the events ``sum of 7'' and ``first die
  is a 3'' independent?
\item
  Suppose you roll two dice. Are the events ``sum of 6'' and ``first die
  is a 4'' independent?
\item
  Suppose you pick a card from a deck. Are the events ``Jack'' and
  ``Spade'' independent?
\item
  Suppose you pick a card from a deck. Are the events ``Heart'' and
  ``Red'' card independent?
\item
  Suppose you have two children via separate births. Are the events
  ``the first is a boy'' and ``the second is a girl'' independent?
\item
  Suppose you flip a coin 50 times and get a head every time, what is
  the probability of getting a head on the next flip?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-41}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Suppose you roll two dice. Are the events ``sum of 7'' and ``first die
  is a 3'' independent?

  To determine if they are independent, you need to see if
  \(P(\text{sum of 7| first die a 3}=P(\text{sum of 7})\) or the other
  way around. It doesn't matter which order these are calculated in, so
  pick whichever is easier. sum of 7 = \{(1,6), (2,5), (3,4), (4,3),
  (5,2), (6,1)\} first die is a 3 = \{(3,1), (3,2), (3,3), (3,4), (3,5),
  (3,6)\} \(P(\text{sum of 7| first die a 3})\) means that you assume
  that first die is a 3 has happened. The restricted sample space is the
  first die is a 3, \(R\) = \{(3,1), (3,2), (3,3), (3,4), (3,5), (3,6)\}
  In this restricted sample space, the way for a sum of 7 to happen is
  \{(3,4)\}, so \(P(\text{sum of 7| first die a 3})=\frac{1}{6}\) The
  \(P(\text{sum of 7})=\frac{6}{36}=\frac{1}{6}\). Since
  \(P(\text{sum of 7| first die a 3})=P(\text{sum of 7})\), the ``sum of
  7'' and ``first die is a 3'' are independent events.
\item
  Suppose you roll two dice. Are the events ``sum of 6'' and ``first die
  is a 4'' independent?

  To determine if they are independent, you need to see if

  \(P(\text{sum of 6| first die a 4}) = P(\text{sum of 6})\).

  Again it doesn't matter what order you do this in. Do which is easier.
  sum of 6 = \{(1,5), (2,4), (3,3), (4,2), (5,1)\} and first die is a 4
  = \{(4,1), (4,2), (4,3), (4,4), (4,5), (4,6)\}, if want
  \(P(\text{sum of 6| first die a 4})\), the restricted sample space is
  1st die is a 4, \(R\) = \{(4,1), (4,2), (4,3), (4,4), (4,5), (4,6)\}
  In this restricted sample space, the way to get a sum of 6 is
  \{(4,2)\}, so \(P(\text{sum of 6| first die a 4})=\frac{1}{6}\). The
  \(P(\text{sum of 6})=\frac{5}{36}\) Notice
  \(P(\text{sum of 6| first die a 4})\ne P(\text{sum of 6})\), Thus
  ``sum of 6'' and ``first die is a 4'' are dependent.
\item
  Suppose you pick a card from a deck. Are the events ``Jack'' and
  ``Spade'' independent?

  To determine if they are independent, you need to see if

  \(P(Jack| space)=P(Jack)\).

  Remember, you can do this the other order if you wish. Jack = \{JS,
  JC, JD, JH\} and \(R\) = Spade \{2S, 3S, 4S, 5S, 6S,7S, 8S, 9S, 10S,
  JS, QS, KS, AS\} For \(P(\text{Jack| Spade})\), the restricted sample
  space is Spade, \(R\) = \{2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS,
  KS, AS\}. In this restricted sample space, the way to get a Jack is
  \{JS\}, so \(P(\text{Jack| Spade})=\frac{1}{13}\). The
  \(P(Jack)=\frac{4}{52}=\frac{1}{13}\) Since
  \(P(\text{Jack| Spade})=P(\text{Jack})\), ``Jack'' and ``Spade'' are
  independent.
\item
  Suppose you pick a card from a deck. Are the events ``Heart'' and
  ``Red'' card independent?

  To determine if they are independent, you need to see if

  \(P(\text{Heart| Red})=P(\text{Heart})\).

  Heart = \{2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH, AH\} and
  Red card = \{2D, 3D, 4D, 5D, 6D, 7D, 8D, 9D, 10D, JD, QD,KD, AD, 2H,
  3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH, AH\}. The restricted
  sample space is, red card, \$R\$ = \{2D, 3D, 4D, 5D, 6D, 7D, 8D, 9D,
  10D, JD, QD, KD, AD, 2H, 3H,4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH,
  AH\} In this restricted sample space, the way to get a heart is 13,
  and

  \(P(\text{Heart| Red})=\frac{13}{26}\).

  \(P(\text{Heart})=\frac{13}{52}\)

  Note \(P(\text{Heart| Red})\ne P(\text{Heart})\), so, ``Heart'' and
  ``Red'' card are dependent.
\item
  Suppose you have two children via separate births. Are the events
  ``the first is a boy'' and ``the second is a girl'' independent?

  In this case, you actually don't need to do any calculations. The sex
  of one child does not affect the sex of the second child. The events
  are independent.
\item
  Suppose you flip a coin 50 times and get a head every time, what is
  the probability of getting a head on the next flip?

  Since one flip of the coin does not affect the next flip (the coin
  does not remember what it did the time before), the probability of
  getting a head on the next flip is still one-half.
\end{enumerate}

\subsection{\texorpdfstring{\textbf{Multiplication
Rule}:}{Multiplication Rule:}}\label{multiplication-rule}

Two more useful formulas:

If two events are dependent, then \(P(\text{A and B})=P(A)*P(B|A)\)

If two events are independent, then \(P(\text{A and B})=P(A)*P(B)\)

These two formulas are useful if the sample space is too large to write
out, but it the sample space isn't too large, it is better to find
probabilities of and statements using the sample space techniques.

If you solve the first equation for \(P(B|A)\), you obtain
\(P(B|A)=\frac{P(\text{A and B})}{P(A)}\), which is a formula to
calculate a conditional probability. However, it is easier to find a
conditional probability by using the restricted sample space and
counting unless the sample space is large.

\subsection{Example: Multiplication
Rule}\label{example-multiplication-rule}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Suppose you pick three cards from a deck, what is the probability that
  they are all Queens if the cards are not replaced after they are
  picked?
\item
  Suppose you pick three cards from a deck, what is the probability that
  they are all Queens if the cards are replaced after they are picked
  and before the next card is picked?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-42}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Suppose you pick three cards from a deck, what is the probability that
  they are all Queens if the cards are not replaced after they are
  picked?

  This sample space is too large to write out, so using the
  multiplication rule makes sense. Since the cards are not replaced,
  then the probability will change for the second and third cards. They
  are dependent events. This means that on the second draw there is one
  less Queen and one less card, and on the third draw there are two less
  Queens and 2 less cards.

  \(P(\text{3 Queens})\)\(=P(\text{Queen on 1st})*P(\text{Queen on 2nd, given Queen on first})*P(\text{Queen on third, given Queens on fist 2 draws})\)

  \(=\frac{4}{52}*\frac{3}{51}*\frac{2}{50}\)
\item
  Suppose you pick three cards from a deck, what is the probability that
  they are all Queens if the cards are replaced after they are picked
  and before the next card is picked?

  Again, the sample space is too large to write out, so using the
  multiplication rule makes sense. Since the cards are put back, one
  draw has no affect on the next draw and they are all independent.
  \(P(\text{3 Queens})=P(\text{Queen on 1st})*P(\text{Queen on 2nd})*P(\text{Queen on 3rd})\)
  \(=\frac{4}{52}*\frac{4}{52}*\frac{4}{52}\)
\end{enumerate}

\subsection{Example: Application
Problem}\label{example-application-problem}

A project conducted by the Australian Federal Office of Road Safety
asked people many questions about their cars. One question was the
reason that a person chooses a given car, and that data is in
Table~\ref{tbl-Car_pref} (Car Preferences, 2019).

\textbf{Code book for Data Frame Car\_pref} is below
Table~\ref{tbl-Car_pref}.

A contingency table, is a cross tabulation of the data into different
categories. As an example, a contingency table of if the person has
children under 5 and their current car is below, the following command
in R Studio can be used to create this table.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Kids5}\SpecialCharTok{+}\NormalTok{ActCar, }\AttributeTok{data=}\NormalTok{Car\_pref, }\AttributeTok{margins=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       ActCar
Kids5   large medium small Total
  no       87     90    94   271
  yes      13     10     6    29
  Total   100    100   100   300
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the probability that a person questioned has kids under 5.
\item
  Find the probability that a person questioned actually has a large
  car.
\item
  Find the probability that a person questioned actually has a large car
  and has children under 5.
\item
  Find the probability that a person questioned has a large car given
  that they have children under 5.
\item
  Find the probability that a person has a large car or has children
  under 5.
\item
  Find the probability that a person questioned has children under 5
  given that they have a large car.
\item
  Are the events that a person questioned has a ``large car'' and ``kids
  under 5'' independent events? Why or why not?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-43}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Find the probability that a person questioned has kids under 5.

  First, you need to find the number of people questioned. Add the first
  row, there are 150 people who did not have kids under 5. Adding the
  second row, there are 29 people who do have kids under 5. Also, you
  can add the columns. There are 100 people who have large cars, 100
  people who have medium cars, and 100 who have small cars. Adding
  either the row totals or the columns totals, gives you 300 people in
  the study. Out of the 300 people, 29 people had kids under 5. So the

  \(P(\text{kids under 5})=\frac{29}{300}\).

  So 9.7\% of the people questioned had children under 5.
\item
  Find the probability that a person questioned actually has a large
  car.

  There are 100 people with large cars out of 300 people. So,

  \(P(\text{large car})= \frac{100}{300}\). There are 33\% of the people
  who have a large car.
\item
  Find the probability that a person questioned actually has a large car
  and has children under 5.

  There are 13 people who have a large car and have children under 5, so
  the \(P(\text{large car and children under 5})=\frac{13}{300}=0.043\).
  4.3\% of all people surveyed have a large car and children under 5.
\item
  Find the probability that a person questioned has a large car given
  that they have children under 5.

  In this case you know that the person had children under 5. You don't
  need to consider the people who don't. You only need to look at the
  row with people who have have children under 5. In that row, look to
  see how many people have a large car. There are 13 people with a large
  car out of the 29 people with kids under 5.
  So,\(P(\text{large car|kids under 5})=\frac{13}{29}=0.45\)

  There is 45\% chance that a person with a large car have children
  under 5.
\item
  Find the probability that a person has a large car or has children
  under 5.

  This problem can be done two ways. One is to use the addition formula,
  but a better way is to realize that there are 29 people who have kids
  under 5, and there are 100 people who have a large car. That is 34
  people. But the 13 people who have large cars and kids under 5 were
  just counted twice. So subtract the 13 people from the 34. That give
  21 people who have either kids under 5 or a large car. So

  \(P(\text{large car or kids under 5})=\frac{21}{300}=0.07\).

  That means 70\% of the people questioned has a large car or has
  children under 5.
\item
  Find the probability that a person questioned has children under 5
  given that they have a large car.

  In this case you know that the person has a large car. You don't need
  to include the people who have medium or small cars. You only need to
  consider the column headed by large. In that column, there are 100
  people who have large cars and out of those 100, 13 have children
  under 5. So, \(P(\text{kids under 5|large})=\frac{13}{100}=0.13\).
  Thus 13\% of people have children under 5 given that they have a large
  car.
\item
  Are the events that a person questioned has a ``large car'' and ``kids
  under 5'' independent events? Why or why not?

  In order for these events to be independent, either
  \(P(\text{kids under 5|large car})=P(\text{kids under 5})\) or
  \(P(\text{large car|kids under 5})=P(\text{large car})\) have to be
  true. Part (d) showed \(P(\text{kids under 5|large car})=0.44\) and
  part (b) showed \(P(\text{kids under 5})==0.33\). Since these are not
  equal, then these two events are dependent.
\end{enumerate}

A big deal has been made about the difference between dependent and
independent events while calculating the probability of *and* compound
events. You must multiply the probability of the first event with the
conditional probability of the second event.

Why do you care? Calculating probabilities when performing sampling is
important, as this will be seen later. But here is a simplification that
can make the calculations a lot easier: when the sample size is very
small compared to the population size, you can assume that the
conditional probabilities just don't change very much over the sample.

For example, consider acceptance sampling. Suppose there is a big
population of parts delivered to you factory, say 12,000 parts. Suppose
there are 85 defective parts in the population. You decide to randomly
select ten parts, and see if you should reject the shipment. What is the
probability of rejecting the shipment?

There are many different ways you could reject the shipment. For
example, maybe the first three parts are good, one is bad, and the rest
are good. Or all ten parts could be bad, or maybe the first five. So
many ways to reject! But there is only \textbf{one} way that you'd
accept the shipment: if \textbf{all} \textbf{ten} parts are good. That
would happen if the first part is good, \textbf{and} the second part is
good, \textbf{and} the third part is good, and so on. Since the
probability of the second part being good is (slightly) dependent on
whether the first part was good, technically you should take this into
consideration when you calculate the probability that all ten are good.

The probability of getting the first sampled part good is
\$\textbackslash frac\{1200-85\}\{1200\}=\textbackslash frac\{11915\}\{1200\}\$.
So the probability that all ten being good is

\(\frac{11915}{1200}*\frac{11914}{1200}*\frac{11913}{1200}*\cdots*\frac{11906}{1200}=0.931357\).

If instead you assume that the probability doesn't change much, you get
\((\frac{11915}{12000})^{10}=0.931382\). So as you can see, there is not
much difference. So here is the rule: if the sample is very small
compared to the size of the population, then you can assume that the
probabilities are independent, even though they aren't technically. By
the way, the probability of rejecting the shipment is
\(1-0.9314=0.0686\).

\subsection{Homework for Conditional Probability
Section}\label{homework-for-conditional-probability-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Are owning a refrigerator and owning a car independent events? Why or
  why not?
\item
  Are owning a computer, tablet, or smart phone and paying for Internet
  service independent events? Why or why not?
\item
  Are passing your statistics class and passing your biology class
  independent events? Why or why not?
\item
  Are owning a bike and owning a car independent events? Why or why not?
\item
  An experiment is picking a card from a fair deck.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What is the probability of picking a Jack given that the card is a
  face card?
\item
  What is the probability of picking a heart given that the card is a
  three?
\item
  What is the probability of picking a red card given that the card is
  an ace?
\item
  Are the events Jack and face card independent events? Why or why not?
\item
  Are the events red card and ace independent events? Why or why not?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  An experiment is rolling two dice.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What is the probability that the sum is 6 given that the first die is
  a 5?
\item
  What is the probability that the first die is a 3 given that the sum
  is 11?
\item
  What is the probability that the sum is 7 given that the fist die is a
  2?
\item
  Are the two events sum of 6 and first die is a 5 independent events?
  Why or why not?
\item
  Are the two events sum of 7 and first die is a 2 independent events?
  Why or why not?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\item
  You flip a coin four times. What is the probability that all four of
  them are heads?
\item
  You flip a coin six times. What is the probability that all six of
  them are heads?
\item
  You pick three cards from a deck with replacing the card each time
  before picking the next card. What is the probability that all three
  cards are kings?
\item
  You pick three cards from a deck without replacing a card before
  picking the next card. What is the probability that all three cards
  are kings?
\item
  A project conducted by the Australian Federal Office of Road Safety
  asked people many questions about their cars. One question was the
  reason that a person chooses a given car, and that data is in
  Table~\ref{tbl-Car_pref} (Car Preferences, 2019).
\end{enumerate}

\textbf{Code book for Data Frame Car\_pref} is below
Table~\ref{tbl-Car_pref}.

The contingency table for the sex of a person and the size car the
person prefers is in table below

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{PreferCar}\SpecialCharTok{+}\NormalTok{Sex, }\AttributeTok{data=}\NormalTok{Car\_pref, }\AttributeTok{margins=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         Sex
PreferCar female male Total
   4           6   17    23
   large      26   47    73
   medium     75   61   136
   small      43   25    68
   Total     150  150   300
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What is the probability that a person questioned was female?
\item
  What is the probability that a person questioned prefers a medium car?
\item
  What is the probability that a person questioned prefers a medium car
  given that the person was female?
\item
  What is the probability that a person questioned was a female and
  prefers a medium car?
\item
  What is the probability that a person questioned was a female or
  prefers a medium car?
\item
  Are the events person questioned is a female and person questioned
  prefers a medium car mutually exclusive? Why or why not?
\item
  Are the events person questioned is a female and person questioned
  prefers a medium car independent? Why or why not?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  Researchers watched groups of dolphins off the coast of Ireland in
  1998 to determine what activities the dolphins partake in at certain
  times of the day (Activities of Dolphin Groups, 2019). The numbers in
  table \textbackslash\#4.3.5 represent the number of groups of dolphins
  that were partaking in an activity at certain times of days.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dolphin}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/dolphins.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Dolphin))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}ll@{}}

\caption{\label{tbl-Dolphin}Dolphin Activity}

\tabularnewline

\toprule\noalign{}
activity & period \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Travel & Morning \\
Travel & Morning \\
Travel & Morning \\
Travel & Morning \\
Travel & Morning \\
Travel & Morning \\

\end{longtable}

\textbf{Code book for Data Frame Dolphin}

\textbf{Description} Groups of dolphins were observed off the coast of
Iceland near Keflavik in 1998. The data here give the time of the day
and the main activity of the group, whether travelling quickly, feeding
or socializing. The dolphin groups varied in size - usually feeding or
socializing groups were larger than travelling groups.

Usage Dolphin

Format

This data frame contains the following columns:

Activity: Main activity of group: travelling (Travel), feeding (Feed) or
socializing (Social)

Period: Time of the day: Morning, Noon, Afternoon or Evening

Source Activities of Dolphin Groups. (n.d.). Retrieved July 12, 2019,
from http://www.statsci.org/data/general/dolpacti.html

References Marianne Rasmussen, Department of Biology, University of
Southern Denmark, Odense, Denmark.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  What is the probability that a dolphin group is partaking in travel?
\item
  What is the probability that a dolphin group is around in the morning?
\item
  What is the probability that a dolphin group is partaking in travel
  given that it is morning?
\item
  What is the probability that a dolphin group is around in the morning
  given that it is partaking in socializing?
\item
  What is the probability that a dolphin group is around in the
  afternoon given that it is partaking in feeding?
\item
  What is the probability that a dolphin group is around in the
  afternoon and is partaking in feeding?
\item
  What is the probability that a dolphin group is around in the
  afternoon or is partaking in feeding?
\item
  Are the events dolphin group around in the afternoon and dolphin group
  feeding mutually exclusive events? Why or why not?
\item
  Are the events dolphin group around in the morning and dolphin group
  partaking in travel independent events? Why or why not?
\end{enumerate}

\section{Counting Techniques}\label{counting-techniques}

There are times when the sample space or event space are very large,
that it isn't feasible to write it out. In that case, it helps to have
mathematical tools for counting the size of the sample space and event
space. These tools are known as counting techniques.

\subsection{\texorpdfstring{\textbf{Multiplication Rule in Counting
Techniques}}{Multiplication Rule in Counting Techniques}}\label{multiplication-rule-in-counting-techniques}

If task 1 can be done ways, task 2 can be done ways, and so forth to
task \(n\) being done ways. Then the number of ways to do task 1,
2,\ldots, \(n\) together would be \(m_{1}*m_{2}*\cdots *m_{n}\).

\subsection{Example: Multiplication Rule in
Counting}\label{example-multiplication-rule-in-counting}

A menu offers a choice of 3 salads, 8 main dishes, and 5 desserts. How
many different meals consisting of one salad, one main dish, and one
dessert are possible?

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-44}

There are three tasks, picking a salad, a main dish, and a dessert. The
salad task can be done 3 ways, the main dish task can be done 8 ways,
and the dessert task can be done 5 ways. The ways to pick a salad, main
dish, and dessert are \(3*8*5=120\).

\subsection{Example: Multiplication Rule in
Counting}\label{example-multiplication-rule-in-counting-1}

How many three letter ``words'' can be made from the letters a, b, and c
with no letters repeating? A ``word'' is just an ordered group of
letters. It doesn't have to be a real word in a dictionary.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-45}

There are three tasks that must be done in this case. The tasks are to
pick the first letter, then the second letter, and then the third
letter. The first task can be done 3 ways since there are 3 letters. The
second task can be done 2 ways, since the first task took one of the
letters. The third task can be done 1 way, since the first and second
task took two of the letters. There are \(3*2*1=6\)

In \hyperref[example-multiplication-rule]{Example: Multiplication Rule},
the solution was found by find \(3*2*1\). Many counting problems involve
multiplying a list of decreasing numbers. This is called a
\textbf{factorial}. There is a special symbol for this.

\subsection{\texorpdfstring{\textbf{Factorial}}{Factorial}}\label{factorial}

\(n!=n(n-1)(n-2)*\cdots*2*1\)

As an example: \(5!=5*4*3*2*1=120\) \(8!=8*7*6*5*4*3*2*1=40320\)

\textbf{0 factorial} is defined to be \(0!=1\) and \textbf{1 factorial}
is defined to be \(1!=1\). In rStudio, the command for factorial is
factorial(number). As an example \(7!\) using r Studio would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{factorial}\NormalTok{(}\DecValTok{7}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 5040
\end{verbatim}

Sometimes you are trying to select \(r\) objects from \(n\) total
objects. The number of ways to do this depends on if the order you
choose the \(r\) objects matters or if it doesn't. As an example if you
are trying to call a person on the phone, you have to have their number
in the right order. Otherwise, you call someone you didn't mean to. In
this case, the order of the numbers matters. If however you were picking
random numbers for the lottery, it doesn't matter which number you pick
first. As long as you have the same numbers that the lottery people
pick, you win. In this case the order doesn't matter. A
\textbf{permutation} is an arrangement of items with a specific order.
You use permutations to count items when the order matters. When the
order doesn't matter you use combinations. A \textbf{combination} is an
arrangement of items when order is not important. When you do a counting
problem, the first thing you should ask yourself is ``does order
matter?''

\subsection{\texorpdfstring{\textbf{Permutation
Formula}}{Permutation Formula}}\label{permutation-formula}

Picking \(r\) objects from \(n\) total objects when order matters

\(_{n}P_{r}=\frac{n!}{(n-r)!}\).

\subsection{\texorpdfstring{\textbf{Combination
Formula}}{Combination Formula}}\label{combination-formula}

Picking \(r\) objects from \(n\) total objects when order doesn't matter

\(_{n}C_{r}=\frac{n!}{r!(n-r)!}\)

Most calculators have a factorial button on them, and many have the
combination and permutation functions also.

\subsection{Homework for Counting Techniques
Sections}\label{homework-for-counting-techniques-sections}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  You are going to a benefit dinner, and need to decide before the
  dinner what you want for salad, main dish, and dessert. You have 2
  different salads to choose from, 3 main dishes, and 5 desserts. How
  many different meals are available?
\item
  How many different phone numbers are possible in the area code 928?
\item
  You are opening a T-shirt store. You can have long sleeves or short
  sleeves, three different colors, five different designs, and four
  different sizes. How many different shirts can you make?
\item
  The California license plate has one number followed by three letters
  followed by three numbers. How many different license plates are
  there?
\item
  Find \(_9P_4\)
\item
  Find \(_{10}P_6\)
\item
  Find \(_{10}C_5\)
\item
  Find \(_{20}P_4\)
\item
  You have a group of twelve people. You need to pick a president,
  treasurer, and secretary from the twelve. How many different ways can
  you do this?
\item
  A baseball team has a 25-person roster. A batting order has nine
  people. How many different batting orders are there?
\item
  An urn contains five red balls, seven yellow balls, and eight white
  balls. How many different ways can you pick two red balls?
\item
  How many ways can you choose seven people from a group of twenty?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Discrete Probability
Distribution}\label{discrete-probability-distribution}

When computing probabilities, the sample space, which contains all the
outcomes of the experiment, is listed. If the probabilities for all of
the outcomes are also listed then these two together are called a
probability distribution. With a probability distribution, the shape can
be determined, the mean and standard deviation can be calculated, and
the probability of events can be found. How to find all of these
concepts depends on what type of quantitative variables are being
considered. Remember there are different types of quantitative
variables, called discrete or continuous. What is the difference between
discrete and continuous data? \textbf{Discrete} data can only take on
particular values in a range. \textbf{Continuous} data can take on any
value in a range. Discrete data usually arises from counting while
continuous data usually arises from measuring.

If you have a variable, and can find a probability associated with that
variable, it is called a \textbf{random variable}. In many cases the
random variable is what you are measuring, but when it comes to discrete
random variables, it is usually what you are counting. So for the
example of how tall is a plant given a new fertilizer, the random
variable is the height of the plant given a new fertilizer. For the
example of how many fleas are on prairie dogs in a colony, the random
variable is the number of fleas on a prairie dog in a colony.

\subsection{\texorpdfstring{\textbf{Examples of
each:}}{Examples of each:}}\label{examples-of-each}

How tall is a plant given a new fertilizer? Continuous. This is
something you measure.

How many fleas are on prairie dogs in a colony? Discrete. This is
something you count.

Now suppose you put all the values of the random variable together with
the probability that the random variable would occur. You could then
have a distribution like before, but now it is called a probability
distribution since it involves probabilities. A \textbf{probability
distribution} is an assignment of probabilities to the values of the
random variable.

With the idea of a probability distribution, the next thing is to look
at the basics of a probability distribution.

\section{Basics of Probability
Distributions}\label{basics-of-probability-distributions}

As a reminder, a variable or what will be called the random variable
from now on, is represented by the letter \(x\) and it represents a
quantitative (numerical) variable that is measured or observed in an
experiment.

As with probabilities, probability distributions, have the properties,
\(0 \le P(outcome)\le1\) and \(\sum{P(outcomes)}=1\)

\subsection{Example: Probability
Distribution}\label{example-probability-distribution}

The 2010 U.S. Census found the chance of a household being a certain
size. The data is in Table~\ref{tbl-Household}
(\textbackslash{}``Households by age,\textbackslash{}'' 2013). Note, the
category 7 is really 7 or more people in the household. Draw the
probability distribution and find the mean, variance, and standard
deviation.

\begin{longtable}[]{@{}rr@{}}

\caption{\label{tbl-Household}Household Size from U.S. Census of 2010}

\tabularnewline

\toprule\noalign{}
size & prob \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0.267 \\
2 & 0.336 \\
3 & 0.158 \\
4 & 0.137 \\
5 & 0.063 \\
6 & 0.024 \\
7 & 0.015 \\

\end{longtable}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-46}

In this case, the random variable is \(x\) = size of household. This is
a discrete random variable, since you are counting the number of people
in a household.

It is a probability distribution since you have the \(x\) value and the
probabilities that go with it, all of the probabilities are between zero
and one, and the sum of all of the probabilities is one.

You can give a probability distribution in table form (as in
Table~\ref{tbl-Household}) or as a graph. The graph looks like a
histogram. To graph the histogram, use the following commands and
process in rStudio.

First you need to load a few packages using the following commands.
These packages are ``arm'' and ``Weighted.Desc.Stat''. If these packages
have not been installed, they need to be installed before you can load
them using library. Once you have installed them, they will always be
available in /r Studio to be loaded. To load a package, use the command

library(``name of package'')

In this case the packages you need are arm and Weighted.Desc.Stat.

\begin{verbatim}
Loading required package: MASS
\end{verbatim}

\begin{verbatim}
Loading required package: Matrix
\end{verbatim}

\begin{verbatim}
Loading required package: lme4
\end{verbatim}

\begin{verbatim}

arm (Version 1.14-4, built: 2024-4-1)
\end{verbatim}

\begin{verbatim}
Working directory is /Users/mori/CSU Fullerton Dropbox/Mortaza Jamshidian/Statistics Text Using Rguroo Positron/Statistics-Using-Technology-book
\end{verbatim}

To draw the probability distribution, use the following command. First
you need to create variables for \(x\), size, and the probability,
\(prob\), in r Studio. Then you can draw the distribution.

(ref:discrete-histogram-cap) Histogram of Size of Family

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{discrete.histogram}\NormalTok{(Household}\SpecialCharTok{$}\NormalTok{size,Household}\SpecialCharTok{$}\NormalTok{prob, }\AttributeTok{bar.width =} \DecValTok{1}\NormalTok{, }\AttributeTok{main=}\StringTok{"Size of family"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Size"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Discrete-Probability-Distribution_files/figure-pdf/fig-discrete-histogram-1.pdf}

}

\caption{\label{fig-discrete-histogram}Histogram of Household Size from
U.S. Census of 2010}

\end{figure}%

This command is different than the commands used in the past, but is
needed for discrete probability distributions. So putting a title on the
graph uses the command main=``title you want'' instead of title= as
before.

Notice this graph Figure~\ref{fig-discrete-histogram} is skewed right,
which means that most families have around 2 people in them and larger
families become more and more rare.

To find the mean, variance, and standard deviation using r Studio, make
sure that the package Weighted.Desc.Stat is loaded, then use the
following commands.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{w.mean}\NormalTok{(Household}\SpecialCharTok{$}\NormalTok{size, Household}\SpecialCharTok{$}\NormalTok{prob) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.525
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{w.var}\NormalTok{(Household}\SpecialCharTok{$}\NormalTok{size, Household}\SpecialCharTok{$}\NormalTok{prob) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.023375
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{w.sd}\NormalTok{(Household}\SpecialCharTok{$}\NormalTok{size, Household}\SpecialCharTok{$}\NormalTok{prob)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.422454
\end{verbatim}

The mean is 2.525 people, the variance is 2.02 \(people^2\), and the
standard deviation is 1.42 people.

When calculating the mean and standard deviation of a probability
distribution, you can consider the population distribution the
population even though it was most likely created from a large sample.
Since a probability distribution is basically a population, the mean and
standard deviation that are calculated are actually the population
parameters and not the sample statistics. The notation used is the same
as the notation for population mean, \(\mu\), and population standard
deviation, \$\textbackslash sigma\$, that was used in chapter 3. Note:
the mean can also be thought of as the expected value. It is the value
you expect to get if the trials were repeated infinite number of times.
The mean or expected value does not need to be a whole number, even if
the possible values of \(x\) are whole numbers. This means one can find
what value they can expect to get in the long run for gambling or
insurance including extended warranties using the mean of a probability
distribution. First one needs to figure out the probability
distribution, and then follow the process in example 5.1.1.

\subsection{Example: Calculating the Expected
Value}\label{example-calculating-the-expected-value}

In the Arizona lottery game called Pick 3, a player pays
\textbackslash\$1 and then picks a three-digit number. If those three
numbers are picked in that specific order the person wins
\textbackslash\$500. What is the expected value in this game?

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-47}

To find the expected value, you need to first create the probability
distribution. In this case, the random variable \(x\) = winnings. If you
pick the right numbers in the right order, then you win
\textbackslash\$500, but you paid \textbackslash\$1 to play, so you
actually win \textbackslash\$499. If you didn't pick the right numbers,
you lose the \textbackslash\$1, the \(x\) value is \$-1\$. You also need
the probability of winning and losing. Since you are picking a
three-digit number, and for each digit there are 10 numbers you can pick
with each independent of the others, you can use the multiplication
rule. To win, you have to pick the right numbers in the right order. The
first digit, you pick 1 number out of 10, the second digit you pick 1
number out of 10, and the third digit you pick 1 number out of 10. The
probability of picking the right number in the right order is
\$\textbackslash frac\{1\}\{1000\}\$. The probability of losing (not
winning) would be

\(1-\frac{1}{1000}=\frac{999}{1000}\).

Putting this information into a table will help to organize the
information and find the expected value.

\begin{longtable}[]{@{}lll@{}}
\caption{Probability Distribution of Lottery}\tabularnewline
\toprule\noalign{}
outcome & amount & probability \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
outcome & amount & probability \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
win & 499 & 0.001 \\
lose & -1 & 0.999 \\
\end{longtable}

Now type the values into r using the following command:

Now to find the expected value, it is the same as finding the mean,
though the command is a little different since you don't have a data
frame for this data.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{weighted.mean}\NormalTok{(amount, probability)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.5
\end{verbatim}

The expected value (or mean) is -0.5. That is -\textbackslash\$0.50.
Since it is negative, that means you lose \textbackslash\$0.50 every
time you play the Pick 3. It seems you would be better off putting the
\textbackslash\$1 every week into a savings account then playing the
Pick 3 lottery.

The reason probability is studied in statistics is to help in making
decisions in inferential statistics. To understand how that is done the
concept of a rare event is needed.

\subsection{\texorpdfstring{\textbf{Rare Event Rule for Inferential
Statistics}}{Rare Event Rule for Inferential Statistics}}\label{rare-event-rule-for-inferential-statistics}

If, under a given assumption, the probability of a particular observed
event is extremely small, then you can conclude that the assumption is
probably not correct.

An example of this is suppose you roll an assumed fair die 1000 times
and get a six 600 times, when you should have only rolled a six around
160 times, then you should believe that your assumption about it being a
fair die is untrue.

\subsection{\texorpdfstring{\textbf{Determining if an event is
unusual}}{Determining if an event is unusual}}\label{determining-if-an-event-is-unusual}

If you are looking at a value of \(x\) for a discrete variable, and the
P(the variable has a value of \(x\) or more) is less than 0.05, then you
can consider the \(x\) an unusually high value. Another way to think of
this is if the probability of getting such a high value is less than
0.05, then the event of getting the value x is unusual.

Similarly, if the P(the variable has a value of \(x\) or less) is less
than 0.05, then you can consider this an unusually low value. Another
way to think of this is if the probability of getting a value as small
as \(x\) is less than 0.05, then the event \(x\) is considered unusual.

Why is it ``\(x\) or more'' or ``\(x\) or less'' instead of just
``\(x\)'' when you are determining if an event is unusual? Consider this
example: you and your friend go out to lunch every day. Instead of Going
Dutch (each paying for their own lunch), you decide to flip a coin, and
the loser pays for both. Your friend seems to be winning more often than
you'd expect, so you want to determine if this is unusual before you
decide to change how you pay for lunch (or accuse your friend of
cheating). The process for how to calculate these probabilities will be
presented in the next section on the binomial distribution. If your
friend won 6 out of 10 lunches, the probability of that happening turns
out to be about 20.5\%, not unusual. The probability of winning 6 or
more is about 37.7\%. But what happens if your friend won 501 out of
1,000 lunches? That doesn't seem so unlikely! The probability of winning
501 or more lunches is about 47.8\%, and that is consistent with your
hunch that this isn't so unusual. But the probability of winning exactly
501 lunches is much less, only about 2.5\%. That is why the probability
of getting exactly that value is not the right question to ask: you
should ask the probability of getting that value or more (or that value
or less on the other side).

The value 0.05 will be explained later, and it is not the only value you
can use for unusual events.

\subsection{Example: Is the Event
Unusual}\label{example-is-the-event-unusual}

The 2010 U.S. Census found the chance of a household being a certain
size. The data is in the table (\textbackslash{}``Households by
age,\textbackslash{}'' 2013).

The 2010 U.S. Census found the chance of a household being a certain
size. The data is in Table~\ref{tbl-Household}
(\textbackslash{}``Households by age,\textbackslash{}'' 2013). Note, the
category 7 is really 7 or more people in the household.

State random variable:

\textbf{Solution}

State random variable

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Is it unusual for a household to have six people in the family?
\end{enumerate}

size = number of people in a household

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Is it unusual for a household to have six people in the family?
\item
  If you did come upon many families that had six people in the family,
  what would you think?
\item
  Is it unusual for a household to have four people in the family?
\item
  If you did come upon a family that has four people in it, what would
  you think?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-48}

To determine this, you need to look at probabilities. However, you
cannot just look at the probability of six people. You need to look at
the probability of \(x\) being six or less people or the probability of
\(x\) being six or more people. The

\(P(x \le 6)=P(1)+P(2)+P(3)+P(4)+P(5)+P(6)\)
\(=0.267+0.336+0.158+0.137+0.063+0.024=0.985\)

Since this probability is more than 5\%, then six is not an unusually
low value.

The \(P(x \ge 6)=P(6)+P(7)=0.024+0.015=0.039\)

Since this probability is less than 5\%, then six is an unusually high
value. It is unusual for a household to have six people in the family.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  If you did come upon many families that had six people in the family,
  what would you think?
\end{enumerate}

Since it is unusual for a family to have six people in it, then you may
think that either the size of families is increasing from what it was or
that you are in a location where families are larger than in other
locations.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Is it unusual for a household to have four people in the family?
\end{enumerate}

To determine this, you need to look at probabilities. Again, look at the
probability of \(x\) being four or less or the probability of \(x\)
being four or more. The

\(P(x \le 4)=P(0)+P(1)+P(2)+P(3)+P(4)\)
\(=0.267+0.336+0.158+0.137=0.898\)

Since this probability is more than 5\%, four is not an unusually low
value.

The

\(P(\ge4)=P(4)+P(5)+P(5)+P(7)\) \(=0.137+0.063+0.024+0.015=0.239\)

Since this probability is more than 5\%, four is not an unusually low
value. Thus, four is not an unusual size of a family.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  If you did come upon a family that has four people in it, what would
  you think?
\end{enumerate}

Since it is not unusual for a family to have four members, then you
would not think anything is amiss.

\subsection{Homework for Basics of Probability Distributions
Section}\label{homework-for-basics-of-probability-distributions-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Eyeglassomatic manufactures eyeglasses for different retailers. The
  number of days it takes to fix defects in an eyeglass and the
  probability that it will take that number of days are in
  Table~\ref{tbl-Days}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Days}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{(}
  \StringTok{"https://krkozak.github.io/MAT160/table\_5\_1\_3.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Days)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}

\caption{\label{tbl-Days}Nuumber of Days to fix Eyeglasses}

\tabularnewline

\toprule\noalign{}
days & prob \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 0.249 \\
2 & 0.108 \\
3 & 0.091 \\
4 & 0.123 \\
5 & 0.133 \\
6 & 0.114 \\
7 & 0.070 \\
8 & 0.046 \\
9 & 0.019 \\
10 & 0.013 \\
11 & 0.010 \\
12 & 0.008 \\
13 & 0.006 \\
14 & 0.004 \\
15 & 0.002 \\
16 & 0.002 \\
17 & 0.001 \\
18 & 0.001 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Draw a histogram of the number of days to fix defects
\item
  Find the mean number of days to fix defects.
\item
  Find the variance for the number of days to fix defects.
\item
  Find the standard deviation for the number of days to fix defects.
\item
  Find probability that a lens will take at least 16 days to make a fix
  the defect.
\item
  Is it unusual for a lens to take 16 days to fix a defect?
\item
  If it does take 16 days for eyeglasses to be repaired, what would you
  think?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Suppose you have an experiment where you flip a coin three times. You
  then count the number of heads.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Write the probability distribution for the number of heads.
\item
  Draw a histogram for the number of heads.
\item
  Find the mean number of heads.
\item
  Find the variance for the number of heads.
\item
  Find the standard deviation for the number of heads.
\item
  Find the probability of having two or more number of heads.
\item
  Is it unusual for to flip two heads?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  The Ohio lottery has a game called Pick 4 where a player pays
  \textbackslash\$1 and picks a four-digit number. If the four numbers
  come up in the order you picked, then you win \textbackslash\$2,500.
  What is your expected value?
\item
  An LG Dishwasher, which costs \textbackslash\$800, has a 20\% chance
  of needing to be replaced in the first 2 years of purchase. A two-year
  extended warranty costs \textbackslash\$112.10 on a dishwasher. What
  is the expected value of the extended warranty assuming it is replaced
  in the first 2 years?
\end{enumerate}

\section{Binomial Probability
Distribution}\label{binomial-probability-distribution}

Section 5.1 introduced the concept of a probability distribution. The
focus of the section was on discrete probability distributions. To find
the probability distribution for a situation, you usually needed to
actually conduct the experiment and collect data. Then you can calculate
the experimental probabilities. Normally you cannot calculate the
theoretical probabilities. However, there are certain types of
experiment that allow you to calculate the theoretical probability. One
of those types is called a \textbf{Binomial Experiment}.

Properties of a \textbf{binomial experiment} (or Bernoulli trial):

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Fixed number of trials, \(n\), which means that the experiment is
  repeated a specific number of times.
\item
  The \(n\) trials are independent, which means that what happens on one
  trial does not influence the outcomes of other trials.
\item
  There are only two outcomes, which are called a success and a failure.
\item
  The probability of a success doesn't change from trial to trial, where
  \(p\) = probability of success and \(q = 1-p\) = probability of
  failure.
\end{enumerate}

If you know you have a binomial experiment, then you can calculate
binomial probabilities. This is important because binomial probabilities
come up often in real life. Examples of binomial experiments are:

Toss a fair coin ten times, and find the probability of getting two
heads.

Question twenty people in class, and look for the probability of more
than half being women?

Shoot five arrows at a target, and find the probability of hitting it
five times?

\subsection{\texorpdfstring{\textbf{Formula for the probabilities for a
Binomial
experiment}}{Formula for the probabilities for a Binomial experiment}}\label{formula-for-the-probabilities-for-a-binomial-experiment}

First, the random variable in a binomial experiment is \(x\) = number of
successes.

Be careful, a success is not always a good thing. Sometimes a success is
something that is bad, like finding a defect. A success just means you
observed the outcome you wanted to see happen.

Binomial Formula for the probability of \(r\) successes in \(n\) trials
is \(P(X=r)=_nC_r*p^r*q^{n-r}\)

where \(_nC_r\) is the number of combinations of \(n\) things taking
\(r\) at a time. It is read ``\(n\) choose \(r\)''.

When solving problems, make sure you define your random variable and
state what \(n, p\), and \(r\) are. Without doing this, the problems are
a great deal harder.

The command to find a binomial probability in r Studio is

P\((X=r)=\)

dbinom(r, n, p)

\(P(x \le r)=\)

pbinom(r, n, p, lower.tail=TRUE)

\(P(x \ge r)=\)

pbinom(r-1, n, p, lower.tail = FALSE)

\subsection{Example: Calculating Binomial
Probabilities}\label{example-calculating-binomial-probabilities}

When looking at a person's eye color, it turns out that 1\% of people in
the world has green eyes (``What percentage of,'' 2013). Consider a
group of 20 people.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Argue that this is a binomial experiment
\item
  Find the probability that none of the 20 people have green eyes.
\item
  Find the probability that nine have green eyes.
\item
  Find the probability that at most three have green eyes.
\item
  Find the probability that at most two have green eyes.
\item
  Find the probability that at least four have green eyes.
\item
  In Europe, four people out of twenty have green eyes. Is this unusual?
  What does that tell you?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-49}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\end{enumerate}

\(x\) = number of people with green eyes

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Argue that this is a binomial experiment.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  There are 20 people, and each person is a trial, so there are a fixed
  number of trials. In this case, \(n\) = 20.
\item
  If you assume that each person in the group is chosen at random the
  eye color of one person doesn't affect the eye color of the next
  person, thus the trials are independent.
\item
  Either a person has green eyes or they do not have green eyes, so
  there are only two outcomes. In this case, the success is a person has
  green eyes.
\item
  The probability of a person having green eyes is 0.01. This is the
  same for every trial since each person has the same chance of having
  green eyes.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the probability that none of the 20 people have green eyes.
\end{enumerate}

If none have green eyes, then \(r=0\).

Probability that none have green eyes is \(P(X=0)=0.818\), using the
command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{20}\NormalTok{,}\FloatTok{0.01}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8179069
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the probability that nine have green eyes.
\end{enumerate}

If nine have green eyes, then \(r=9\).

Probability that 9 have green eyes is

\(P(X=9)=1.50X10^{-13}\). Notice that r gives the answer as 1.50391e-13.
This is the way many computer programs write a number in scientific
notation. It isn't possible for a computer to write it as
\(1.50381X10^{-13}\), but it is possible for humans to write it
correctly. So make sure the answer is written in the correct scientific
notation.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\DecValTok{9}\NormalTok{,}\DecValTok{20}\NormalTok{,}\FloatTok{0.01}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1.50381e-13
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Find the probability that at most three have green eyes.
\end{enumerate}

At most three means that three is the highest value you will have. Find
the probability of \(x\) is less than or equal to three.

Since this is less than, then the lower tail of the probability
distribution is being used, so \(P(X \le 3)=0.99996\) using the command
in r Studio of

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbinom}\NormalTok{(}\DecValTok{3}\NormalTok{,}\DecValTok{20}\NormalTok{,}\FloatTok{0.01}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9999574
\end{verbatim}

The reason the answer is written to more decimal places is because when
it is rounded to three decimal places the rounding makes the answer 1.
But 1 means that the event will happen, when in reality there is a
slight chance that it won't happen. It is best to write the answer to
more decimal places or it can be written as \(>0.999\) to represent that
the number is very close to 1, but isn't 1.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Find the probability that at most two have green eyes.
\end{enumerate}

At most 2 means 2 or less. So find the probability that there are less
than or equal to 2. \(P(X \le 2)=0.999\), and again, this is the lower
tail of the probability distribution, so use lower.tail=TRUE in the r
command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbinom}\NormalTok{(}\DecValTok{2}\NormalTok{,}\DecValTok{20}\NormalTok{,}\FloatTok{0.01}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9989964
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Find the probability that at least four have green eyes.
\end{enumerate}

At least four means four or more. Find the probability of \(x\) being
greater than or equal to four. Since it is greater than or equal to,
this is the right tail of the probability distribution. However, if you
just use lower.tail=FALSE, then the 4 is not included in r calculations.
You want all numbers from 4 on up, so you need to use

\(r=4-1=3\) in the r command. This will include 4 in the calculation.
\(P(X \ge 4)=4.26X10^{-5}\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbinom}\NormalTok{(}\DecValTok{4{-}1}\NormalTok{,}\DecValTok{20}\NormalTok{,}\FloatTok{0.01}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.262093e-05
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  In Europe, four people out of twenty have green eyes. Is this unusual?
  What does that tell you?
\end{enumerate}

Since the probability of finding four or more people with green eyes is
much less than 0.05, it is unusual to find four people out of twenty
with green eyes. That should make you wonder if the proportion of people
in Europe with green eyes is more than the 1\% for the general
population. If this is true, then you may want to ask why Europeans have
a higher proportion of green-eyed people. That of course could lead to
more questions.

\subsection{Example: Calculating Binomial
Probabilities}\label{example-calculating-binomial-probabilities-1}

According to the Center for Disease Control (CDC), about 1 in 88
children in the U.S. have been diagnosed with autism (``CDC-data and
statistics,'' 2013). Suppose you consider a group of 10 children.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Argue that this is a binomial experiment
\item
  Find the probability that none have autism.
\item
  Find the probability that seven have autism.
\item
  Find the probability that at least five have autism.
\item
  Find the probability that at most two have autism.
\item
  Suppose five children out of ten have autism. Is this unusual? What
  does that tell you?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-50}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\end{enumerate}

\(x\) = number of children with autism.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Argue that this is a binomial experiment
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  There are 10 children, and each child is a trial, so there are a fixed
  number of trials. In this case, \(n\) = 10.
\item
  If you assume that each child in the group is chosen at random, then
  whether a child has autism does not affect the chance that the next
  child has autism. Thus the trials are independent.
\item
  Either a child has autism or they do not have autism, so there are two
  outcomes. In this case, the success is a child has autism.
\item
  The probability of a child having autism is \(\frac{1}{88}\). This is
  the same for every trial since each child has the same chance of
  having autism.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the probability that none have autism.
\end{enumerate}

\(P(X=0)=0.892\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{10}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{88}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.892002
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the probability that seven have autism.
\end{enumerate}

\(P(X=7)=2.84X10^{-12}\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{dbinom}\NormalTok{(}\DecValTok{7}\NormalTok{,}\DecValTok{10}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{88}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 2.837346e-12
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Find the probability that at least five have autism.
\end{enumerate}

\(P(X \ge 5)=4.553X10^{-8}\). Again, this is the upper tail of the
probability distribution, so use lower=tail=FALSE and

\(r=5-1=4\) to make sure that r calculates for 5 and on up.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbinom}\NormalTok{(}\DecValTok{5{-}1}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{88}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 4.553416e-08
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Find the probability that at most two have autism.
\end{enumerate}

\(P(X \le 2)=0.9998\). This is using the lower tail of the probability
distribution.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pbinom}\NormalTok{(}\DecValTok{2}\NormalTok{, }\DecValTok{10}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{88}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9998341
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Suppose five children out of ten have autism. Is this unusual? What
  does that tell you?
\end{enumerate}

Since the probability of five or more children in a group of ten having
autism is much less than 5\%, it is unusual to happen. If this does
happen, then one may think that the proportion of children diagnosed
with autism is actually more than \(\frac{1}{88}\).

\subsection{Homework for Binomial Probability Distribution
Section}\label{homework-for-binomial-probability-distribution-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Approximately 10\% of all people are left-handed (\textbackslash{}``11
  little-known facts,\textbackslash{}'' 2013). Consider a grouping of
  fifteen people.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Argue that this is a binomial experiment
\item
  Find the probability that none are left-handed.
\item
  Find the probability that seven are left-handed.
\item
  Find the probability that at least two are left-handed.
\item
  Find the probability that at most three are left-handed.
\item
  Find the probability that at least seven are left-handed.
\item
  Seven of the last 15 U.S. Presidents were left-handed. Is this
  unusual? What does that tell you?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  According to an article in the American Heart Association's
  publication *Circulation*, 24\% of patients who had been hospitalized
  for an acute myocardial infarction did not fill their cardiac
  medication by the seventh day of being discharged (Ho, Bryson \&
  Rumsfeld, 2009). Suppose there are twelve people who have been
  hospitalized for an acute myocardial infarction.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Argue that this is a binomial experiment
\item
  Find the probability that all filled their cardiac medication.
\item
  Find the probability that seven did not fill their cardiac medication.
\item
  Find the probability that none filled their cardiac medication.
\item
  Find the probability that at most two did not fill their cardiac
  medication.
\item
  Find the probability that at least three did not fill their cardiac
  medication.
\item
  Find the probability that at least ten did not fill their cardiac
  medication.
\item
  Suppose of the next twelve patients discharged, ten did not fill their
  cardiac medication, would this be unusual? What does this tell you?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Eyeglassomatic manufactures eyeglasses for different retailers. In
  March 2010, they tested to see how many defective lenses they made,
  and there were 16.9\% defective lenses due to scratches. Suppose
  Eyeglassomatic examined twenty eyeglasses.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Argue that this is a binomial experiment
\item
  Find the probability that none are scratched.
\item
  Find the probability that all are scratched.
\item
  Find the probability that at least three are scratched.
\item
  Find the probability that at most five are scratched.
\item
  Find the probability that at least ten are scratched.
\item
  Is it unusual for ten lenses to be scratched? If it turns out that ten
  lenses out of twenty are scratched, what might that tell you about the
  manufacturing process?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The proportion of brown M\&M's in a milk chocolate packet is
  approximately 14\% (Madison, 2013). Suppose a package of M\&M's
  typically contains 52 M\&M's.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Argue that this is a binomial experiment
\item
  Find the probability that six M\&M's are brown.
\item
  Find the probability that twenty-five M\&M's are brown.
\item
  Find the probability that all of the M\&M's are brown.
\item
  Would it be unusual for a package to have only brown M\&M's? If this
  were to happen, what would you think is the reason?
\end{enumerate}

\section{Mean and Standard Deviation of Binomial
Distribution}\label{mean-and-standard-deviation-of-binomial-distribution}

If you list all possible values of \(x\) in a Binomial distribution, you
get the \textbf{Binomial Probability Distribution}. You can draw a
histogram of the probability distribution and find the mean (expected
value), variance, and standard deviation of it. To have r Studio
calculate the binomial values and save them to a variable, use the
command

x\textless-c(0:n) p\textless-dbinom(0:n, n, p)

\subsection{Example: Finding the Probability Distribution, Mean,
Variance and Standard Deviation of a Binomial
Distribution}\label{example-finding-the-probability-distribution-mean-variance-and-standard-deviation-of-a-binomial-distribution}

When looking at a person's eye color, it turns out that 1\% of people in
the world has green eyes (``What percentage of,'' 2013). Consider a
group of 20 people.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Write the probability distribution.
\item
  Draw a histogram.
\item
  Find the mean, variance, and standard deviation.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-51}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\end{enumerate}

\(x\) = number of people who have green eyes

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Write the probability distribution.
\end{enumerate}

In this case you need to write each value of \(x\) and its corresponding
probability. It is easiest to do this by using the r Command:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{green}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{20}\NormalTok{) }
\NormalTok{probability\_green}\OtherTok{\textless{}{-}}\FunctionTok{dbinom}\NormalTok{(}\DecValTok{0}\SpecialCharTok{:}\DecValTok{20}\NormalTok{,}\DecValTok{20}\NormalTok{, }\FloatTok{0.01}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

It looks like nothing happened, but r save the values as variables. To
see what is in each of those values, type

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{green}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{probability\_green}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] 8.179069e-01 1.652337e-01 1.585576e-02 9.609552e-04 4.125313e-05
 [6] 1.333434e-06 3.367259e-08 6.802543e-10 1.116579e-11 1.503810e-13
[11] 1.670900e-15 1.534344e-17 1.162381e-19 7.225371e-22 3.649177e-24
[16] 1.474415e-26 4.654088e-29 1.106141e-31 1.862190e-34 1.980000e-37
[21] 1.000000e-40
\end{verbatim}

These can now be typed into a table if desired.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Draw a histogram.
\end{enumerate}

On r, this is like what was done in Section 5.1. Makes sure that the
packages ``arm'' and ``Weighted.Desc.Stat'' are loaded. Then perform the
command to get:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{discrete.histogram}\NormalTok{(green, probability\_green, }\AttributeTok{bar.width =} \DecValTok{1}\NormalTok{, }\AttributeTok{main=}\StringTok{"Number of People with Green Eyes"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Numbr of People with Green Eyes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Discrete-Probability-Distribution_files/figure-pdf/fig-Green-1.pdf}

}

\caption{\label{fig-Green}Histogram of Number of People with Green Eyes}

\end{figure}%

Notice this graph Figure~\ref{fig-Green} is skewed right.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the mean, variance, and standard deviation
\end{enumerate}

Using r Studio command such as those in Section 5.1:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{w.mean}\NormalTok{(green, probability\_green) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{w.var}\NormalTok{(green, probability\_green) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.198
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{w.sd}\NormalTok{(green, probability\_green)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.4449719
\end{verbatim}

You expect on average that out of 20 people, less than 1 person would
have green eyes, with are variance of 0.198 \(people^2\) and a standard
deviation of 0.44 people.

\subsection{Homework for Mean and Standard Deviation of Binomial
Distribution
Section}\label{homework-for-mean-and-standard-deviation-of-binomial-distribution-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Suppose a random variable, \(x\), arises from a binomial experiment.
  Suppose \(n = 6\), and \(p = 0.13\).
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Write the probability distribution.
\item
  Draw a histogram.
\item
  Describe the shape of the histogram.
\item
  Find the mean.
\item
  Find the variance.
\item
  Find the standard deviation.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Suppose a random variable, \(x\), arises from a binomial experiment.
  Suppose \(n = 10\), and \(p = 0.81\).
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Write the probability distribution.
\item
  Draw a histogram.
\item
  Describe the shape of the histogram.
\item
  Find the mean.
\item
  Find the variance.
\item
  Find the standard deviation.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Suppose a random variable, \(x\), arises from a binomial experiment.
  Suppose \(n = 7\), and \(p = 0.50\).
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Write the probability distribution.
\item
  Draw a histogram.
\item
  Describe the shape of the histogram.
\item
  Find the mean.
\item
  Find the variance.
\item
  Find the standard deviation.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Approximately 10\% of all people are left-handed. Consider a grouping
  of fifteen people.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Write the probability distribution.
\item
  Draw a histogram.
\item
  Describe the shape of the histogram.
\item
  Find the mean.
\item
  Find the variance.
\item
  Find the standard deviation.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  According to an article in the American Heart Association's
  publication *Circulation*, 24\% of patients who had been hospitalized
  for an acute myocardial infarction did not fill their cardiac
  medication by the seventh day of being discharged (Ho, Bryson \&
  Rumsfeld, 2009). Suppose there are twelve people who have been
  hospitalized for an acute myocardial infarction.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Write the probability distribution.
\item
  Draw a histogram.
\item
  Describe the shape of the histogram.
\item
  Find the mean.
\item
  Find the variance.
\item
  Find the standard deviation.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Eyeglassomatic manufactures eyeglasses for different retailers. In
  March 2010, they tested to see how many defective lenses they made,
  and there were 16.9\% defective lenses due to scratches. Suppose
  Eyeglassomatic examined twenty eyeglasses.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Write the probability distribution.
\item
  Draw a histogram.
\item
  Describe the shape of the histogram.
\item
  Find the mean.
\item
  Find the variance.
\item
  Find the standard deviation.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  The proportion of brown M\&M's in a milk chocolate packet is
  approximately 14\% (Madison, 2013). Suppose a package of M\&M's
  typically contains 52 M\&M's.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State the random variable.
\item
  Find the mean.
\item
  Find the variance.
\item
  Find the standard deviation.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Continuous Probability
Distribution}\label{continuous-probability-distribution}

Chapter 5 dealt with probability distributions arising from discrete
random variables. Mostly that chapter focused on the binomial
experiment. There are many other experiments from discrete random
variables that exist but are not covered in this book.

Chapter 6 deals with probability distributions that arise from
continuous random variables. The focus of this chapter is a distribution
known as the normal distribution, though realize that there are many
other distributions that exist. A few others are examined in future
chapters.

Looking at the density plot of a quantitative variable, one can guess
what the distribution of that variable is. As an example, consider the
NHANES data frame. One variable to consider is Weight. The density plot
of Weight is

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-density-cap-1.pdf}

}

\caption{\label{fig-density-cap}Density plot of the Weight of a Person}

\end{figure}%

Figure~\ref{fig-density-cap} looks somewhat symmetric, and maybe bell
shaped.

Consider, the variable head circumference (HeadCirc) in the NHANES data
frame. The density plot for this variable is
Figure~\ref{fig-HeadCirc-density}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-HeadCirc-density-1.pdf}

}

\caption{\label{fig-HeadCirc-density}Density plot of head circumference
of a person}

\end{figure}%

Figure~\ref{fig-HeadCirc-density} looks somewhat skewed left.

Now consider the variable BMI from the NHANES data frame. The density
plot is

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-BMI-density-1.pdf}

}

\caption{\label{fig-BMI-density}Density Plot of BMI of a Person}

\end{figure}%

This density plot Figure~\ref{fig-BMI-density} appears to be skewed
right

Now consider the variable SmokeAge. Its density plot is
Figure~\ref{fig-Smoking-density}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-Smoking-density-1.pdf}

}

\caption{\label{fig-Smoking-density}Density Plot of Age when Person
Started Smoking}

\end{figure}%

This distribution appears to be bimodal.

lastly, consider the variable Pulse. The density plot is
Figure~\ref{fig-Pulse-density}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-Pulse-density-1.pdf}

}

\caption{\label{fig-Pulse-density}Density Plot of Pulse Rate of a
person}

\end{figure}%

This density plot appears to be symmetric and could almost be considered
bell shaped.

The reason that one considers the density plots to understand the
distribution of the population, is that in some cases the distribution
can be approximated with a known distribution that has certain
properties. There are many known distribution. Some examples are the
Uniform distribution, the Chi-Squared distribution, the Student's T
distribution, and the normal distribution. The normal distribution is
one of the more common distributions to use as a model, and it will be
explored in this chapter. But do realize that there are many other
distributions that one can use.

\section{Normal Distribution}\label{normal-distribution}

Many populations have a distribution that is a symmetric, unimodal, and
bell-shaped. For example: height, blood pressure, and cholesterol level.
However, not every bell shaped curve is a normal curve. In a normal
curve, there is a specific relationship between its ``height'' and its
``width.'' Normal curves can be tall and skinny or they can be short and
fat. They are all symmetric, unimodal, and centered at \(\mu\), the
population mean.

Figure~\ref{fig-normal-curvea} and Figure~\ref{fig-normal-curveb} show
two different normal curves drawn on the same scale. Both have \(\mu=2\)
but the one in Figure~\ref{fig-normal-curvea} has a standard deviation
of 1 and the one in Figure~\ref{fig-normal-curveb} has a standard
deviation of 4. Notice that the larger standard deviation makes the
graph wider (more spread out) and shorter.

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curvea-1.pdf}

}

\caption{\label{fig-normal-curvea}Normal curve with mean 2 and standard
deviation 1}

\end{figure}%

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curveb-1.pdf}

}

\caption{\label{fig-normal-curveb}Normal curve with mean 2 and standard
deviation 4}

\end{figure}%

Every normal curve has common features.

\begin{itemize}
\tightlist
\item
  The center, or the highest point, is at the population mean, \(\mu\).
\item
  The transition points are the places where the curve changes from a
  ``hill'' to a ``valley''. The distance from the mean to the transition
  point is one standard deviation.
\item
  The area under the whole curve is exactly 1. Therefore, the area under
  the half below or above the mean is 0.5.
\end{itemize}

Just as in a discrete probability distribution, the object is to find
the probability of an event occurring. However, unlike in a discrete
probability distribution where the event can be a single value, in a
continuous probability distribution the event must be a range. You are
interested in finding the probability of \(x\) occurring in the range
between \(a\) and \(b\), or \(P(a \le x \le b) = P(a<x<b)\). Calculus
tells us this probability is the area under the curve in the interval
from \(a\) to \(b\).

Before looking at the process for finding the probabilities under the
normal curve, it is somewhat useful to look at the \textbf{Empirical
Rule} that gives approximate values for these areas. The Empirical Rule
is just an approximation and it will only be used in this section to
give you an idea of what the size of the probabilities is for different
shadings. A more precise method for finding probabilities for the normal
curve will be demonstrated in the next section. Please do not use the
empirical rule except for real rough estimates.

\textbf{The Empirical Rule} for any normal distribution: Approximately
68\% of the data is within one standard deviation of the mean.
Approximately 95\% of the data is within two standard deviations of the
mean. Approximately 99.7\% of the data is within three standard
deviations of the mean.

\begin{figure}[H]

{\centering \includegraphics{Empirical_rule.png}

}

\caption{Empirical Rule Graph}

\end{figure}%

Be careful, there is still some area left over in each end. Remember,
the maximum a probability can be is 100\%, so if you calculate you will
see that for both ends together there is 0.3\% of the curve. Because of
symmetry, you can divide this equally between both ends and find that
there is 0.15\% in each tail beyond the 3rd standard deviations.

\section{Finding Probabilities for the Normal
Distribution}\label{finding-probabilities-for-the-normal-distribution}

The Empirical Rule is just an approximation and only works for certain
values. What if you want to find the probability for \(x\) values that
are not integer multiples of the standard deviation? The probability is
the area under the curve. To find areas under the curve, you need
calculus. Before technology, you needed to convert every \(x\) value to
a standardized number, called the \(z\)-score or \(z\)-value or simply
just \(z\). The \(z\)-score is a measure of how many standard deviations
an \(x\) value is from the mean. To convert from a normally distributed
\(x\) value to a \(z\)-score, you use the following formula.

\(z-score=\frac{x-\mu}{\sigma}\)

where \(\mu\) = mean of the population of the \(x\) value and \(\sigma\)
= standard deviation for the population of the \(x\) value

The \(z\)-score is normally distributed, with a mean of 0 and a standard
deviation of 1. It is known as the standard normal curve. The
\(z\)-score is a measure of how many standard deviations a data value is
from its mean. If the \(z\) - score is positive, the data value is above
the mean. If the \(z\)-score is negative, the data value is below the
mean. The farther the \(z\)-value is from 0, the farther the data value
is from the mean.

These days technology can find probabilities without converting to the
\(z\)-score and looking the probabilities up in a table. There are many
programs available that will calculate the probability for a normal
curve. The command on r to find the area to the left \(P(x<value)\) is

pnorm(value, mean, standard\_deviation, lower.tail=TRUE)

The command on r to find the area to the right, \(P(x>value)\) is

pnorm(value, mean, standard\_deviation, lower.tail=FALSE)

\subsection{Example: General Normal
Distribution}\label{example-general-normal-distribution}

The length of a human pregnancy is normally distributed with a mean of
272 days with a standard deviation of 9 days (Bhat \& Kushtagi, 2006).

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable
\item
  Find the probability of a pregnancy lasting more than 280 days.
\item
  Find the probability of a pregnancy lasting less than 250 days.
\item
  Find the probability that a pregnancy lasts between 265 and 280 days.
\item
  Find the length of pregnancy that 10\% of all pregnancies last less
  than.
\item
  Suppose you meet a woman who says that she was pregnant for less than
  250 days. Would this be unusual and what might you think?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-52}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\end{enumerate}

\(x\) = length of a human pregnancy

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Find the probability of a pregnancy lasting more than 280 days.
\end{enumerate}

First translate the statement into a mathematical statement.
\(P(x>280)\)

Now, draw a picture Figure~\ref{fig-normal-curve61}.

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curve61-1.pdf}

}

\caption{\label{fig-normal-curve61}Normally distributed with mean 272
and standard deviation 9, and P(x\textgreater280)}

\end{figure}%

The probability of a pregnancy lasting longer than 280 days is
\(P(x>280)=0.187\). The command in rStudio is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{272}\NormalTok{, }\DecValTok{9}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1870314
\end{verbatim}

Thus 18.7\% of all pregnancies last more than 280 days. This is not
unusual since the probability is greater than 5\%.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the probability of a pregnancy lasting less than 250 days.
\end{enumerate}

First translate the statement into a mathematical statement.
\(P(x<250)\)

Now, draw a picture Figure~\ref{fig-normal-curve62}.

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curve62-1.pdf}

}

\caption{\label{fig-normal-curve62}Density plot of pregnancy length.
Normally distributed with mean 272 and standard deviation 9, and
P(x\textless250)}

\end{figure}%

The probability of a pregnancy lasting longer than 250 days is
\(P(x<250)=0.0073\). The command in r Studio is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{250}\NormalTok{, }\DecValTok{272}\NormalTok{, }\DecValTok{9}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.007253771
\end{verbatim}

Thus 0.73\% of all pregnancies last less than 250 days. This is unusual
since the probability is less than 5\%.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the probability that a pregnancy lasts between 265 and 280 days.
\end{enumerate}

First translate the statement into a mathematical statement.
\(P(265<x<280)\)

Now draw a picture Figure~\ref{fig-normal-curve63}.

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curve63-1.pdf}

}

\caption{\label{fig-normal-curve63}Density plot of pregnancy length.
Normally distributed with mean 272 and standard deviation 9, and
P(265\textless x\textless280)}

\end{figure}%

The probability of a pregnancy lasting between 265 days and 280 days is
\(P(265<x<280)=0.187\). To find the area between two values on the
normal distribution, first, find the area to the left of the lower
value, Graphically, this looks like Figure~\ref{fig-normal-curve-preg}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curve-preg-1.pdf}

}

\caption{\label{fig-normal-curve-preg}Density plot of pregnancy length.
Normally distributed with mean 272 and standard deviation 9, and
P(x\textless265)}

\end{figure}%

Now find the area less than 280. Graphically this looks like
Figure~\ref{fig-normal-curve-preg1}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curve-preg1-1.pdf}

}

\caption{\label{fig-normal-curve-preg1}Density plot of pregnancy length.
Normally distributed with mean 272 and standard deviation 9, and
P(x\textless280)}

\end{figure}%

Looking at the three figures, if you take the area in
Figure~\ref{fig-normal-curve-preg1} and subtract the area in
Figure~\ref{fig-normal-curve-preg} you get the area in
Figure~\ref{fig-normal-curve63}. In rStudio, the way to find the
probability the probability of a pregnancy lasting between 265 days and
280 days, \(P(265<x<280)=0.595\) use the following command

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{280}\NormalTok{, }\DecValTok{272}\NormalTok{, }\DecValTok{9}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{{-}}\FunctionTok{pnorm}\NormalTok{(}\DecValTok{265}\NormalTok{, }\DecValTok{272}\NormalTok{, }\DecValTok{9}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5946186
\end{verbatim}

Thus 59.5\% of all pregnancies last between 265 and 280 days.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Find the length of pregnancy that 10\% of all pregnancies last less
  than.
\end{enumerate}

This problem is asking you to find an \(x\) value from a probability.
You want to find the \(x\) value that has 10\% of the length of
pregnancies to the left of it. In this case, you are given the
probability. In r, the command is

qnorm(area, mean, standard\_deviation, lower.tail=TRUE or FALSE)

For this example since you know the area in the lower tail, then use
lower.tail=TRUE. So the command is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.1}\NormalTok{, }\DecValTok{272}\NormalTok{, }\DecValTok{9}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 260.466
\end{verbatim}

Thus 10\% of all pregnancies last less than approximately 260 days.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Suppose you meet a woman who says that she was pregnant for less than
  250 days. Would this be unusual and what might you think?
\end{enumerate}

From part (c) you found the probability that a pregnancy lasts less than
250 days is 0.73\%. Since this is less than 5\%, it is very unusual. You
would think that either the woman had a premature baby, or that she may
be wrong about when she actually became pregnant.

\subsection{Example: General Normal
Distribution}\label{example-general-normal-distribution-1}

The mean mathematics SAT score in 2012 was 514 with a standard deviation
of 117 (``Total group profile,'' 2012). Assume the mathematics SAT score
is normally distributed.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that a person has a mathematics SAT score over
  700.
\item
  Find the probability that a person has a mathematics SAT score of less
  than 400.
\item
  Find the probability that a person has a mathematics SAT score between
  a 500 and a 650.
\item
  Find the mathematics SAT score that represents the top 1\% of all
  scores.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-53}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\end{enumerate}

\(x\) = mathematics SAT score

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Find the probability that a person has a mathematics SAT score over
  700.
\end{enumerate}

First translate the statement into a mathematical statement.
\(P(x>700)\)

Now, draw a picture Figure~\ref{fig-normal-curve-SAT}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curve-SAT-1.pdf}

}

\caption{\label{fig-normal-curve-SAT}Density plot of SAT mathematics
score. Normally distributed with mean 514 and standard deviation 117 and
P(x\textgreater700)}

\end{figure}%

To find \(P(x>700)=0.0559\), the command in r would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{700}\NormalTok{, }\DecValTok{514}\NormalTok{,}\DecValTok{117}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.05594631
\end{verbatim}

There is a 5.6\% chance that a person scored above a 700 on the
mathematics SAT test. This is not unusual.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the probability that a person has a mathematics SAT score of less
  than 400.
\end{enumerate}

First translate the statement into a mathematical statement.
\(P(x<400)\)

Now, draw a picture Figure~\ref{fig-normal-curve-SAT1}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curve-SAT1-1.pdf}

}

\caption{\label{fig-normal-curve-SAT1}Density plot of SAT mathematics
score. Normally distributed with mean 514 and standard deviation 117 and
P(x\textless400)}

\end{figure}%

To find \(P(x<400)=0.165\), the command in r would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{400}\NormalTok{, }\DecValTok{514}\NormalTok{, }\DecValTok{117}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1649392
\end{verbatim}

So, there is a 16.5\% chance that a person scores less than a 400 on the
mathematics part of the SAT.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the probability that a person has a mathematics SAT score between
  a 500 and a 650.
\end{enumerate}

First translate the statement into a mathematical statement
\(P(500<x<650)\)

Now, draw a picture Figure~\ref{fig-normal-curve-SAT2}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-normal-curve-SAT2-1.pdf}

}

\caption{\label{fig-normal-curve-SAT2}Density plot of SAT mathematics
score. Normally distributed with mean 514 and standard deviation 117 and
P(514\textless x\textless650)}

\end{figure}%

To find \(P(500<x<650)=0.425\), the command in r would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{650}\NormalTok{, }\DecValTok{514}\NormalTok{, }\DecValTok{117}\NormalTok{, }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{{-}}\FunctionTok{pnorm}\NormalTok{(}\DecValTok{500}\NormalTok{, }\DecValTok{514}\NormalTok{, }\DecValTok{117}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.4250851
\end{verbatim}

So, there is a 42.5\% chance that a person has a mathematical SAT score
between 500 and 650.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Find the mathematics SAT score that represents the top 1\% of all
  scores.
\end{enumerate}

This problem is asking you to find an \(x\) value from a probability.
You want to find the \(x\) value that has 1\% of the mathematics SAT
scores to the right of it. In this case you are using the upper tail of
the curve. To find this \(x\) value on rStudio, use the command

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{qnorm}\NormalTok{(}\FloatTok{0.01}\NormalTok{, }\DecValTok{514}\NormalTok{, }\DecValTok{117}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 786.1827
\end{verbatim}

So, 1\% of all people who took the SAT scored over about 786 points on
the mathematics SAT.

\subsection{Homework for Normal Distribution
Section}\label{homework-for-normal-distribution-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find each of the probabilities, where \(z\) is a \(z\)-score from the
  standard normal distribution with mean of \(\mu=0\) and standard
  deviation \(\sigma=1\). It helps to draw a picture for each problem.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \(P(z<2.36)\)
\item
  \(P(z>0.67)\)
\item
  \(P(0<x<2.11)\)
\item
  \(P(-2.78<z<1.97)\)
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Find the \emph{z}-score corresponding to the given area. Remember,
  \emph{z} is distributed as the standard normal distribution with mean
  of \(\mu=0\) and standard deviation \(\sigma=1\).
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  The area to the left of \(z\) is 15\%.
\item
  The area to the right of \(z\) is 65\%.
\item
  The area to the left of \(z\) is 10\%.
\item
  The area to the right of \(z\) is 5\%.
\item
  The area between \(-z\) and \(z\) is 95\%. (Hint draw a picture and
  figure out the area to the left of \(-z\).)
\item
  The area between \(-z\) and \(z\) is 99\%.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  If a random variable that is normally distributed has a mean of 25 and
  a standard deviation of 3, convert the given value to a \(z\)-score.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \(x\) = 23
\item
  \(x\) = 33
\item
  \(x\) = 19
\item
  \(x\) = 45
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  According to the WHO MONICA Project the mean blood pressure for people
  in China is 128 mmHg with a standard deviation of 23 mmHg (Kuulasmaa,
  Hense \& Tolonen, 1998). Assume that blood pressure is normally
  distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that a person in China has blood pressure of 135
  mmHg or more.
\item
  Find the probability that a person in China has blood pressure of 141
  mmHg or less.
\item
  Find the probability that a person in China has blood pressure between
  120 and 125 mmHg.
\item
  Is it unusual for a person in China to have a blood pressure of 135
  mmHg? Why or why not?
\item
  What blood pressure do 90\% of all people in China have less than?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The size of fish is very important to commercial fishing. A study
  conducted in 2012 found the length of Atlantic cod caught in nets in
  Karlskrona to have a mean of 49.9 cm and a standard deviation of 3.74
  cm (Ovegard, Berndt \& Lunneryd, 2012). Assume the length of fish is
  normally distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that an Atlantic cod has a length less than 52
  cm.
\item
  Find the probability that an Atlantic cod has a length of more than 74
  cm.
\item
  Find the probability that an Atlantic cod has a length between 40.5
  and 57.5 cm.
\item
  If you found an Atlantic cod to have a length of more than 74 cm, what
  could you conclude?
\item
  What length are 15\% of all Atlantic cod longer than?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  The mean cholesterol levels of women age 45-59 in Ghana, Nigeria, and
  Seychelles is 5.1 mmol/l and the standard deviation is 1.0 mmol/l
  (Lawes, Hoorn, Law \& Rodgers, 2004). Assume that cholesterol levels
  are normally distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that a woman age 45-59 in Ghana, Nigeria, or
  Seychelles has a cholesterol level above 6.2 mmol/l (considered a high
  level).
\item
  Find the probability that a woman age 45-59 in Ghana, Nigeria, or
  Seychelles has a cholesterol level below 5.2 mmol/l (considered a
  normal level).
\item
  Find the probability that a woman age 45-59 in Ghana, Nigeria, or
  Seychelles has a cholesterol level between 5.2 and 6.2 mmol/l
  (considered borderline high).
\item
  If you found a woman age 45-59 in Ghana, Nigeria, or Seychelles having
  a cholesterol level above 6.2 mmol/l, what could you conclude?
\item
  What value do 5\% of all woman ages 45-59 in Ghana, Nigeria, or
  Seychelles have a cholesterol level less than?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  In the United States, males between the ages of 40 and 49 eat on
  average 103.1 g of fat every day with a standard deviation of 4.32 g
  (``What we eat,'' 2012). Assume that the amount of fat a person eats
  is normally distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that a man age 40-49 in the U.S. eats more than
  110 g of fat every day.
\item
  Find the probability that a man age 40-49 in the U.S. eats less than
  93 g of fat every day.
\item
  Find the probability that a man age 40-49 in the U.S. eats less than
  65 g of fat every day.
\item
  If you found a man age 40-49 in the U.S. who says he eats less than 65
  g of fat every day, would you believe him? Why or why not?
\item
  What daily fat level do 5\% of all men age 40-49 in the U.S. eat more
  than?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  A dishwasher has a mean life of 12 years with an estimated standard
  deviation of 1.25 years (``Appliance life expectancy,'' 2013). Assume
  the life of a dishwasher is normally distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that a dishwasher will last more than 15 years.
\item
  Find the probability that a dishwasher will last less than 6 years.
\item
  Find the probability that a dishwasher will last between 8 and 10
  years.
\item
  If you found a dishwasher that lasted less than 6 years, would you
  think that you have a problem with the manufacturing process? Why or
  why not?
\item
  A manufacturer of dishwashers only wants to replace free of charge 5\%
  of all dishwashers. How long should the manufacturer make the warranty
  period?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  The mean starting salary for nurses is \textbackslash\$67,694
  nationally (``Staff nurse -,'' 2013). The standard deviation is
  approximately \$10,333. Assume that the starting salary is normally
  distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that a starting nurse will make more than
  \textbackslash\$80,000.
\item
  Find the probability that a starting nurse will make less than
  \textbackslash\$60,000.
\item
  Find the probability that a starting nurse will make between
  \textbackslash\$55,000 and \textbackslash\$72,000.
\item
  If a nurse made less than \textbackslash\$50,000, would you think the
  nurse was under paid? Why or why not?
\item
  What salary do 30\% of all nurses make more than?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  The mean yearly rainfall in Sydney, Australia, is about 137 mm and the
  standard deviation is about 69 mm (``Annual maximums of,''2013).
  Assume rainfall is normally distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that the yearly rainfall is less than 100 mm.
\item
  Find the probability that the yearly rainfall is more than 240 mm.
\item
  Find the probability that the yearly rainfall is between 140 and 250
  mm.
\item
  If a year has a rainfall less than 100mm, does that mean it is an
  unusually dry year? Why or why not?
\item
  What rainfall amount are 90\% of all yearly rainfalls more than?
\end{enumerate}

\section{Assessing Normality}\label{assessing-normality}

The distributions you have seen up to this point have been assumed to be
normally distributed, but how do you determine if it is normally
distributed. One way is to take a sample and look at the sample to
determine if it appears normal. If the sample looks normal, then most
likely the population is also. Here are some guidelines that are use to
help make that determination.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Density Plot}: Make a density plot. For a normal distribution,
  the density plot should be roughly bell-shaped. For small samples,
  this is not very accurate, and another method is needed. A
  distribution may not look normally distributed from the density plot,
  but it still may be normally distributed.
\item
  \textbf{Normal quantile plot (or normal probability plot)}: This plot
  is provided through statistical software on a computer. If the points
  lie close to a line, the data comes from a distribution that is
  approximately normally distributed. If the points do not lie close to
  a line or they show a pattern that is not a line, the data are likely
  to come from a distribution that is not normally distributed.
\end{enumerate}

\subsection{\texorpdfstring{\textbf{To create a density plot on
rStudio:}}{To create a density plot on rStudio:}}\label{to-create-a-density-plot-on-rstudio}

Read the Data Frame into r Studio. The command for density is

gf\_density(\textasciitilde variable, data=Data\_Frame)

See chapter 2 for more examples of this.

\subsection{\texorpdfstring{\textbf{To create a normal quantile plot on
rStudio}}{To create a normal quantile plot on rStudio}}\label{to-create-a-normal-quantile-plot-on-rstudio}

Read the Data Frame into rStudio. The command for normal quantile plot
is

gf\_qqnorm(\textasciitilde variable, data=Data\_Frame)

Realize that your random variable may be normally distributed, even if
the sample fails the two tests. However, if the density plot definitely
doesn't look symmetric and bell shaped, and the normal probability plot
doesn't look linear, then you can be fairly confident that the data set
does not come from a population that is normally distributed.

\subsection{Example: Is It Normal?}\label{example-is-it-normal}

In Kiama, NSW, Australia, there is a blowhole. The data in
Table~\ref{tbl-Eruption} are times in seconds between eruptions (``Kiama
blowhole eruptions,'' 2013). Do the data come from a population that is
normally distributed?

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Eruption}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Blowhole\_eruptions.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Eruption))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-Eruption}Time (in Seconds) Between Kiama Blowhole
Eruptions}

\tabularnewline

\toprule\noalign{}
Interval \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
83 \\
51 \\
87 \\
60 \\
28 \\
95 \\

\end{longtable}

\textbf{Code book for Data Frame Eruption}

\textbf{Description} The ocean swell produces spectacular eruptions of
water through a hole in the cliff at Kiama, about 120km south of Sydney,
known as the Blowhole. The times at which 65 successive eruptions
occurred from 1340 hours on 12 July 1998 were observed using a digital
watch.

Format This data frame contains the following columns:

Interval: Waiting time between eruptions (seconds)

Source Kiama Blowhole Eruptions. (n.d.). Retrieved from
http://www.statsci.org/data/oz/kiama.html

References The data was collected and contributed by Jim Irish, Faculty
of Engineering, University of Technology, Sydney.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable
\item
  Draw a Density plot
\item
  Draw the normal quantile plot.
\item
  Do the data come from a population that is normally distributed?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-54}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable
\end{enumerate}

\(x\) = time in seconds between eruptions of Kiama Blowhole

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Draw a Density plot
\end{enumerate}

The density plot produced is in Figure~\ref{fig-blowhole-density}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Interval, }\AttributeTok{data=}\NormalTok{Eruption, }\AttributeTok{title=}\StringTok{"Eruption times for Kiama Blowhole"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Time (seconds)"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-blowhole-density-1.pdf}

}

\caption{\label{fig-blowhole-density}Density Plot of Eruption Times for
Kiama Blowhole}

\end{figure}%

This looks skewed right and not symmetric.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Draw the normal quantile plot.
\end{enumerate}

The normal quantile plot is in Figure~\ref{fig-blowhole-qq}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Interval, }\AttributeTok{data=}\NormalTok{Eruption, }\AttributeTok{title=}\StringTok{"Eruption times for Kiama Blowhole"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-blowhole-qq-1.pdf}

}

\caption{\label{fig-blowhole-qq}Normal Quantile Plot of Eruption Times
for Kiama blowhole.}

\end{figure}%

Figure~\ref{fig-blowhole-qq} looks more like an exponential growth than
linear.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Do the data come from a population that is normally distributed?
\end{enumerate}

Considering the density plot is skewed right, and the normal probability
plot does not look linear, then the conclusion is that this sample is
not from a population that is normally distributed.

\subsection{Example: Is It Normal?}\label{example-is-it-normal-1}

The US National Center for Health Statistics (NCHS) conducted a series
of health and nutrition surveys called NHANES. One of the many variables
in NHANES is pulse. Determine if pulse is a normally distributed
variable. The NHANES data frame is Table~\ref{tbl-NHANES}.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable
\item
  Draw a density plot
\item
  Draw the normal quantile plot.
\item
  Do the data come from a population that is normally distributed?
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-55}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable
\end{enumerate}

\(x\) = pulse

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Draw a density plot
\end{enumerate}

The density plot is in Figure~\ref{fig-pulse-density}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Pulse, }\AttributeTok{data=}\NormalTok{NHANES, }\AttributeTok{title=}\StringTok{"Pulse Rate"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Pulse Rate (bpm)"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-pulse-density-1.pdf}

}

\caption{\label{fig-pulse-density}Density plot of Pulse Rate (bpm)}

\end{figure}%

This looks somewhat symmetric and bell shaped.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Draw the normal quantile plot.
\end{enumerate}

The normal quantile plot is in Figure~\ref{fig-pulse-qq}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Pulse, }\AttributeTok{data=}\NormalTok{NHANES, }\AttributeTok{title=}\StringTok{"Pulse Rate"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-pulse-qq-1.pdf}

}

\caption{\label{fig-pulse-qq}Normal Quantile Plot of Pulse Rate (bmp)}

\end{figure}%

Figure~\ref{fig-pulse-qq} looks fairly linear.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Do the data come from a population that is normally distributed?
\end{enumerate}

Considering the density plot is bell shaped and the normal probability
plot looks linear. The conclusion is that this sample is from a
population that is normally distributed.

\subsection{Homework for Assessing Normality
Section}\label{homework-for-assessing-normality-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Cholesterol data was collected on patients four days after having a
  heart attack. The data is in Table~\ref{tbl-Cholesterol}. Assess if
  the data is from a population that is normally distributed.
\end{enumerate}

\textbf{Code Book for Cholesterol} See is below
Table~\ref{tbl-Cholesterol}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The size of fish is very important to commercial fishing. A study
  conducted in 2012 collected the lengths of Atlantic cod caught in nets
  in Karlskrona (Ovegard, Berndt \& Lunneryd, 2012). Data based on
  information from the study is in Table~\ref{tbl-Cod}. Determine if the
  data is from a population that is normally distributed.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cod}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/cod.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Cod))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-Cod}Atlantic Cod Lengths}

\tabularnewline

\toprule\noalign{}
length \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
48 \\
50 \\
50 \\
55 \\
53 \\
50 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The WHO MONICA Project collected blood pressure data for people in
  China (Kuulasmaa, Hense \& Tolonen, 1998). Data based on information
  from the study is in Table~\ref{tbl-BP}. Determine if the data is from
  a population that is normally distributed.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{BP}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/bp.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(BP))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-BP}Blood Pressure Values for People in China}

\tabularnewline

\toprule\noalign{}
pressure \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
114 \\
141 \\
154 \\
137 \\
131 \\
132 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Annual rainfalls for Sydney, Australia are given in
  Table~\ref{tbl-Annual} (``Annual maximums of,'' 2013). Can you assume
  rainfall is normally distributed?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Annual}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/annual.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Annual))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-Annual}Annual Rainfall in Sydney, Australia}

\tabularnewline

\toprule\noalign{}
amount \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
146.8 \\
383.0 \\
90.9 \\
178.1 \\
267.5 \\
95.5 \\

\end{longtable}

\section{Sampling Distribution and the Central Limit
Theorem}\label{sampling-distribution-and-the-central-limit-theorem}

You now have most of the skills to start statistical inference, but you
need one more concept.

First, it would be helpful to state what statistical inference is in
more accurate terms.

\textbf{Statistical Inference}: to make accurate decisions about
parameters from statistics

When it says ``accurate decision,'' you want to be able to measure how
accurate. You measure how accurate using probability. In both binomial
and normal distributions, you needed to know that the random variable
followed either distribution. You need to know how the statistic is
distributed and then you can find probabilities. In other words, you
need to know the shape of the sample mean or whatever statistic you want
to make a decision about.

How is the statistic distributed? This is answered with a sampling
distribution.

\textbf{Sampling Distribution}: how a sample statistic is distributed
when repeated trials of size \(n\) are taken.

\subsection{Example: Sampling
Distribution}\label{example-sampling-distribution}

The NHANES data frame has the pulse rates for approximately 50,000
individuals. The random variable is \(x\) = pulse rate. The probability
distribution of this random variable is presented in
Figure~\ref{fig-NHANES6h-density}. Although pulse rates from 50,000
individuals isn't the entire population, the sample is most likely a
good representation of the population. Thus, it is safe to assume the
population is normally distributed. An estimate for the population mean
is 73.6 pbm, and the population standard deviation estimate is 12.2 bpm.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Pulse, }\AttributeTok{data=}\NormalTok{NHANES, }\AttributeTok{title =} \StringTok{"Pulse Rate"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Pulse (bpm)"}\NormalTok{) }
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Pulse, }\AttributeTok{data=}\NormalTok{NHANES, mean, sd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response     mean       sd
1    Pulse 73.55973 12.15542
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-NHANES6h-density-1.pdf}

}

\caption{\label{fig-NHANES6h-density}Distribution of Pulse Rate}

\end{figure}%

Suppose you take a random sample of 10 pulse rates from those 50,000
individuals. A random sample of data from 10 individuals is:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NHANES}\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size=}\DecValTok{10}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 10 x 76
      ID SurveyYr Gender   Age AgeDecade AgeMonths Race1    Race3    Education  
   <int> <fct>    <fct>  <int> <fct>         <int> <fct>    <fct>    <fct>      
 1 60810 2009_10  female    38 " 30-39"        463 White    <NA>     College Gr~
 2 55085 2009_10  female    19 " 10-19"        239 White    <NA>     <NA>       
 3 67231 2011_12  male      18 " 10-19"         NA Black    Black    <NA>       
 4 56434 2009_10  male      26 " 20-29"        313 White    <NA>     Some Colle~
 5 68306 2011_12  male      17 " 10-19"         NA White    White    <NA>       
 6 53676 2009_10  male      51 " 50-59"        619 White    <NA>     Some Colle~
 7 53190 2009_10  female    47 " 40-49"        573 Other    <NA>     8th Grade  
 8 70758 2011_12  male      21 " 20-29"         NA Hispanic Hispanic High School
 9 57543 2009_10  female    28 " 20-29"        342 Mexican  <NA>     9 - 11th G~
10 53012 2009_10  female    65 " 60-69"        785 White    <NA>     College Gr~
# i 67 more variables: MaritalStatus <fct>, HHIncome <fct>, HHIncomeMid <int>,
#   Poverty <dbl>, HomeRooms <int>, HomeOwn <fct>, Work <fct>, Weight <dbl>,
#   Length <dbl>, HeadCirc <dbl>, Height <dbl>, BMI <dbl>,
#   BMICatUnder20yrs <fct>, BMI_WHO <fct>, Pulse <int>, BPSysAve <int>,
#   BPDiaAve <int>, BPSys1 <int>, BPDia1 <int>, BPSys2 <int>, BPDia2 <int>,
#   BPSys3 <int>, BPDia3 <int>, Testosterone <dbl>, DirectChol <dbl>,
#   TotChol <dbl>, UrineVol1 <int>, UrineFlow1 <dbl>, UrineVol2 <int>, ...
\end{verbatim}

It might be useful to find the mean pulse rate from a random sample of
size 10.

\begin{verbatim}
  response mean
1    Pulse 63.5
\end{verbatim}

Now suppose you took another random sample of size 10 and found the mean
pulse rate for that sample. Repeat this process 100 times. At this point
you would basically have a new sample of 100 mean pulse rates. You could
assess how this sample is distributed by creating a density plot
Figure~\ref{fig-NHANES6e-density}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Trials }\OtherTok{\textless{}{-}} \FunctionTok{do}\NormalTok{(}\DecValTok{100}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ \{ NHANES }\SpecialCharTok{|\textgreater{}} 
    \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{df\_stats}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{Pulse, }\AttributeTok{means =}\NormalTok{ mean)\}}
\FunctionTok{gf\_density}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{means, }\AttributeTok{data =}\NormalTok{ Trials, }\AttributeTok{title =} \StringTok{"Density plot of sample mean when n=10"}\NormalTok{)}

\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{means, }\AttributeTok{data=}\NormalTok{Trials, mean, sd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response     mean       sd
1    means 73.84606 4.160248
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-NHANES6e-density-1.pdf}

}

\caption{\label{fig-NHANES6e-density}Density Plot of Sample Means When n
= 10}

\end{figure}%

This distribution is a sampling distribution. That is all a sampling
distribution is. It is a distribution created from statistics.

Notice the distribution does look a great deal like the distribution of
the original random variable. Notice the mean of the sample means
\(\mu_{\bar{x}} = 73.8\) bpm which is almost the same of as the mean of
the population. The standard deviation of the sample means,
\(\sigma_{\bar{x}}=4.35\) pbm is about \(\frac{1}{3}\) of the population
standard deviation.

What does this distribution look like if instead of repeating the
experiment 10 times you repeat it 50 times instead?

This density plot of the sampling distribution is displayed in
Figure~\ref{fig-NHANES6f-density}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Trials }\OtherTok{\textless{}{-}} \FunctionTok{do}\NormalTok{(}\DecValTok{100}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ \{ NHANES }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{50}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
    \FunctionTok{df\_stats}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{Pulse, }\AttributeTok{means =}\NormalTok{ mean) \}}
\FunctionTok{gf\_density}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{means, }\AttributeTok{data =}\NormalTok{ Trials, }\AttributeTok{title=}\StringTok{"Sample means when n=50"}\NormalTok{)}\SpecialCharTok{|\textgreater{}}
  \FunctionTok{gf\_lims}\NormalTok{(}\AttributeTok{x=}\FunctionTok{c}\NormalTok{(}\DecValTok{68}\NormalTok{,}\DecValTok{79}\NormalTok{))}
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{means, }\AttributeTok{data=}\NormalTok{Trials, mean, sd) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response     mean       sd
1    means 73.49777 1.796123
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-NHANES6f-density-1.pdf}

}

\caption{\label{fig-NHANES6f-density}Density Plot of Sample Means When n
= 50}

\end{figure}%

Notice this density plot of the sample mean looks approximately
symmetrical and could almost be called normal. Notice, the mean of the
sample means is 73.6 bpm which is approximately what the population mean
is. The standard deviation of the sample means is 1.77 bpm which is
around \(\frac{1}{7}\) of the population standard deviation. What if you
keep increasing \(n\)? What will the sampling distribution of the sample
mean look like? In other words, what does the sampling distribution of
\(\bar{x}\) look like as \(n\) gets even larger?

This depends on how the original distribution is distributed. In
\hyperref[example-sampling-distribution]{Example: Sampling
Distribution}, the random variable was approximately normally
distributed. When \(n\) was 10, the distribution of the mean looked
approximately normal. What if the original distribution wasn't normal?
How big would \(n\) have to be? Consider a different variable in the
NHANES data frame that isn't normally distributed such as age when a
participant started to smoke cigarettes (SmokeAge). The density plot for
the large sample is in Figure~\ref{fig-NHANES6g-density}. The mean for
the large sample is 17.8 years and the standard deviation is 5.3 years,
so \(\mu=17.8\) dollars and \(\sigma=5.3\) dollars approximately.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{SmokeAge, }\AttributeTok{data=}\NormalTok{NHANES, }\AttributeTok{title =} \StringTok{"Density Plot of Age when Person Started Smoking"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Age"}\NormalTok{) }
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{SmokeAge, }\AttributeTok{data=}\NormalTok{NHANES, mean, sd) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response     mean      sd
1 SmokeAge 17.82662 5.32666
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-NHANES6g-density-1.pdf}

}

\caption{\label{fig-NHANES6g-density}Density Plot of Age When Person
Started Smoking.}

\end{figure}%

Now take 100 samples of size 50 individuals from the NHANES Data Frame.
Then graph a density plot of SmokeAge, the age when someone started to
smoke. Notice the the sampling distribution of the sample means looks
fairly normally distributed even though the original random variable was
not normally distributed. The mean of the sample mean.
\(\mu_{ \bar{x}}=\) 17.8 years and the standard deviation of the sample
mean, \(\sigma_{\bar{x}}=\) 1.56 years. The mean of the sample mean is
the same as the mean of the population, but the standard deviation of
the sample mean is much less than the standard deviation of the original
data.

\begin{verbatim}
  response     mean       sd
1    means 17.87489 1.381834
\end{verbatim}

\begin{figure}

\centering{

\includegraphics{Continuous-Probability-Distribution_files/figure-pdf/fig-NHANES6-random-density-1.pdf}

}

\caption{\label{fig-NHANES6-random-density}Density Plot of Age When
Person Started Smoking when sample size is 50.}

\end{figure}%

One question is, why is the mean of the sample means the same as the
mean of the population? Suppose you have a random variable that has a
population mean, \(\mu\), and a population standard deviation,
\(\sigma\). If a sample of size \(n\) is taken, then the sample mean,
has a mean \(\mu_{ \bar{x}}=\mu\) and standard deviation of
\(\sigma_{ \bar{x}}=\frac{\sigma}{\sqrt{n}}\) . The standard deviation
of the sample mean is lower because by taking the mean you are averaging
out the extreme values, which makes the distribution of the sample mean
less spread out.

You now know the center and the variability of \(\bar{x}\). You also
want to know the shape of the distribution of \(\bar{x}\). You hope it
is normal, since you know how to find probabilities using the normal
curve. The following theorem tells you the requirement to have
\(\bar{x}\) be normally distributed.

\subsection{\texorpdfstring{\textbf{Central Limit
Theorem}}{Central Limit Theorem}}\label{central-limit-theorem}

Suppose a random variable is from any distribution. If a sample of size
\(n\) is taken, the the sample mean, \(\bar{x}\), becomes normally
distributed as \(n\) increases.

What this says is that no matter what \(x\) looks like, \(\bar{x}\)
would look normal if \(n\) is large enough. Now, what size of \(n\) is
large enough? That depends on how \(x\) is distributed in the first
place. If the original random variable is normally distributed, then
\(n\) just needs to be 2 or more data points. If the original random
variable is somewhat mound shaped and symmetrical, then \(n\) needs to
be greater than or equal to 30. Sometimes the sample size can be
smaller, but this is a good general rule to use. The sample size may
have to be much larger if the original random variable is really skewed
one way or another.

Now that you know when the sample mean will look like a normal
distribution, then you can find the probability related to the sample
mean. Remember that the mean of the sample mean is just the mean of the
original data (\(\mu_{\bar{x}}=\mu\)), but the standard deviation of the
sample mean, \(\sigma_{\bar{x}}\), also known as the standard error of
the mean, is actually \(\sigma_{\bar{x}}=\frac{\sigma}{\sqrt{n}}\). Make
sure you use this in all calculations. If you are using the \(z\)-score,
the formula when working with \(\bar{x}\) is
\(z=\frac{x-\mu_{\bar{x}}}{\sigma_{\bar{x}}}=\frac{x-\mu}{\frac{\sigma}{\sqrt{n}}}\).
To use rStudio to calculate probabilities use
\(P(\bar{x}<a)= pnorm(a, \mu_{\bar{x}}, \sigma_{\bar{x}}, lower.tail=TRUE)\)
\(P(\bar{x}>a)= pnorm(a, \mu_{\bar{x}}, \sigma_{\bar{x}}, lower.tail=FALSE)\).

\subsection{Example: Finding Probabilities for Sample
Means}\label{example-finding-probabilities-for-sample-means}

The birth weight of boy babies of European descent who were delivered at
40 weeks is normally distributed with a mean of 3687.6 g with a standard
deviation of 410.5 g (Janssen, Thiessen, Klein, Whitfield, MacNab \&
Cullis-Kuhl, 2007). Suppose there were nine European descent boy babies
born on a given day and the mean birth weight is calculated.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  What is the mean of the sample mean?
\item
  What is the standard deviation of the sample mean?
\item
  What distribution is the sample mean distributed as?
\item
  Find the probability that the mean weight of the nine boy babies born
  was less than 3500.4 g.
\item
  Find the probability that the mean weight of the nine babies born was
  less than 3452.5 g.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-56}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\end{enumerate}

\(x\) = birth weight of boy babies (Note: the random variable is
something you measure, and it is not the mean birth weight. Mean weight
is calculated.)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  What is the mean of the sample mean?
\end{enumerate}

\(\mu_{\bar{x}}=\mu=3687.4g\)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  What is the standard deviation of the sample mean?
\end{enumerate}

\(\sigma_{\bar{x}}=\frac{\sigma}{\sqrt{n}}=\frac{410.5g}{\sqrt{9}}=136.8g\)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  What distribution is the sample mean distributed as?
\end{enumerate}

Since the original random variable is distributed normally, then the
sample mean is distributed normally.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Find the probability that the mean weight of the nine boy babies born
  was less than 3500.4 g.
\end{enumerate}

To find \(P(\bar{x}<3500.4)=0.086\). use the rStudio command

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\FloatTok{3500.4}\NormalTok{,}\FloatTok{3687.6}\NormalTok{, }\FloatTok{410.5}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{9}\NormalTok{), }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.08564231
\end{verbatim}

There is an 8.6\% chance that the mean birth weight of the nine boy
babies born would be less than 3500.4 g. Since this is more than 5\%,
this is not unusual.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Find the probability that the mean weight of the nine babies born was
  less than 3452.5 g.
\end{enumerate}

You are looking for the \(P(\bar{x}<3452.5)\).

To find in rStudio, \(P(\bar{x}<3452.5)=0.043\) use the command

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\FloatTok{3452.5}\NormalTok{, }\FloatTok{3687.4}\NormalTok{, }\FloatTok{410.5}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{9}\NormalTok{), }\AttributeTok{lower.tail =} \ConstantTok{TRUE}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.04301819
\end{verbatim}

There is a 4.3\% chance that the mean birth weight of the nine boy
babies born would be less than 3452.5 g. Since this is less than 5\%
this would be an unusual event. If it actually happened, then you may
think there is something unusual about this sample. Maybe some of the
nine babies were born as multiples, which brings the mean weight down,
or some or all of the babies were not of European descent (in fact the
mean weight of South Asian boy babies is 3452.5 g), or some were born
before 40 weeks, or the babies were born at high altitudes.

\subsection{Example: Finding Probabilities for Sample
Means}\label{example-finding-probabilities-for-sample-means-1}

For Americans that smoke, the average age that they started smoking is
17.8 years, with a standard deviation of approximately 1.56 years from
the NHANES data. This random variable is not normally distributed,
though it is somewhat mound shaped.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Suppose a sample of 35 smoking American's is taken. Find the
  probability that the mean age that these 35 smoking Americans started
  to smoke is more than 21 years.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-57}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\end{enumerate}

\(x\) = age that smoking Americans started to smoke

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Suppose a sample of 35 smoking American's is taken. Find the
  probability that the mean age that these 35 smoking Americans started
  to smoke is more than 21 years.
\end{enumerate}

Even though the original random variable is not normally distributed,
the sample size is over 30, by the central limit theorem the sample mean
will be normally distributed. The mean of the sample mean is
\(\mu_{\bar{x}}=17.8\). The standard deviation of the sample mean is
\(\sigma_{\bar{x}}=\frac{\sigma}{\sqrt{n}}=\frac{1.56}{\sqrt{35}}\). You
have all the information you need to use the normal command using
rStudio. Without the central limit theorem, you couldn't use the normal
command, and you would not be able to answer this question.

The probability that the mean age that 35 smoking Americans start to
smoke is more than 21 years, is the mathematical statement
\(P(\bar{x}>21)\)

To find \(P(\bar{x}>21)= 3.42X10^{-34}\) using r studio, use the
command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\DecValTok{21}\NormalTok{, }\FloatTok{17.8}\NormalTok{, }\FloatTok{1.56}\SpecialCharTok{/}\FunctionTok{sqrt}\NormalTok{(}\DecValTok{35}\NormalTok{), }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3.422499e-34
\end{verbatim}

The probability of a sample mean of 35 smoking Americans being more than
21 years when they smoked for the first time is very small. This is
extremely unlikely to happen. If it does, it may make you wonder about
the sample. Could the population mean have increased from the 17.8 years
as was stated? Could the sample not have been random, and instead have
been a group of smoking Americans who had started to smoke much later?
These questions, and more, are ones that you would want to ask as a
researcher

\subsection{Homework for Sampling Distribution and the Central Limit
Theorem
Section}\label{homework-for-sampling-distribution-and-the-central-limit-theorem-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  A random variable is not normally distributed, but it is mound shaped.
  It has a mean of 14 and a standard deviation of 3.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  If you take a sample of size 10, can you say what the shape of the
  sampling distribution for the sample mean is? Why?
\item
  For a sample of size 10, state the mean of the sample mean and the
  standard deviation of the sample mean.
\item
  If you take a sample of size 35, can you say what the shape of the
  distribution of the sample mean is? Why?
\item
  For a sample of size 35, state the mean of the sample mean and the
  standard deviation of the sample mean.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  A random variable is normally distributed. It has a mean of 245 and a
  standard deviation of 21.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  If you take a sample of size 10, can you say what the shape of the
  distribution for the sample mean is? Why?
\item
  For a sample of size 10, state the mean of the sample mean and the
  standard deviation of the sample mean.
\item
  For a sample of size 10, find the probability that the sample mean is
  more than 241.
\item
  If you take a sample of size 35, can you say what the shape of the
  distribution of the sample mean is? Why?
\item
  For a sample of size 35, state the mean of the sample mean and the
  standard deviation of the sample mean.
\item
  For a sample of size 35, find the probability that the sample mean is
  more than 241.
\item
  Compare your answers in part c and f.~Why is one smaller than the
  other?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The mean starting salary for nurses is \textbackslash\$67,694
  nationally (``Staff nurse -,'' 2013). The standard deviation is
  approximately \$10,333. The starting salary is not normally
  distributed but it is mound shaped. A sample of 42 starting salaries
  for nurses is taken.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  What is the mean of the sample mean?
\item
  What is the standard deviation of the sample mean?
\item
  What is the shape of the sampling distribution of the sample mean?
  Why?
\item
  Find the probability that the sample mean is more than
  \textbackslash\$75,000.
\item
  Find the probability that the sample mean is less than
  \textbackslash\$60,000.
\item
  If you did find a sample mean of more than \textbackslash\$75,000
  would you find that unusual? What could you conclude?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  According to the WHO MONICA Project the mean blood pressure for people
  in China is 128 mmHg with a standard deviation of 23 mmHg (Kuulasmaa,
  Hense \& Tolonen, 1998). Blood pressure is normally distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Suppose a sample of size 15 is taken. State the shape of the
  distribution of the sample mean.
\item
  Suppose a sample of size 15 is taken. State the mean of the sample
  mean.
\item
  Suppose a sample of size 15 is taken. State the standard deviation of
  the sample mean.
\item
  Suppose a sample of size 15 is taken. Find the probability that the
  sample mean blood pressure is more than 135 mmHg.
\item
  Would it be unusual to find a sample mean of 15 people in China of
  more than 135 mmHg? Why or why not?
\item
  If you did find a sample mean for 15 people in China to be more than
  135 mmHg, what might you conclude?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The size of fish is very important to commercial fishing. A study
  conducted in 2012 found the length of Atlantic cod caught in nets in
  Karlskrona to have a mean of 49.9 cm and a standard deviation of 3.74
  cm (Ovegard, Berndt \& Lunneryd, 2012). The length of fish is normally
  distributed. A sample of 15 fish is taken.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the mean of the sample mean.
\item
  Find the standard deviation of the sample mean
\item
  What is the shape of the distribution of the sample mean? Why?
\item
  Find the probability that the sample mean length of the Atlantic cod
  is less than 52 cm.
\item
  Find the probability that the sample mean length of the Atlantic cod
  is more than 74 cm.
\item
  If you found sample mean length for Atlantic cod to be more than 74
  cm, what could you conclude?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  The mean cholesterol levels of women age 45-59 in Ghana, Nigeria, and
  Seychelles is 5.1 mmol/l and the standard deviation is 1.0 mmol/l
  (Lawes, Hoorn, Law \& Rodgers, 2004). Assume that cholesterol levels
  are normally distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that a woman age 45-59 in Ghana has a cholesterol
  level above 6.2 mmol/l (considered a high level).
\item
  Suppose doctors decide to test the woman's cholesterol level again and
  average the two values. Find the probability that this woman's mean
  cholesterol level for the two tests is above 6.2 mmol/l.
\item
  Suppose doctors being very conservative decide to test the woman's
  cholesterol level a third time and average the three values. Find the
  probability that this woman's mean cholesterol level for the three
  tests is above 6.2 mmol/l.
\item
  If the sample mean cholesterol level for this woman after three tests
  is above 6.2 mmol/l, what could you conclude?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  In the United States, males between the ages of 40 and 49 eat on
  average 103.1 g of fat every day with a standard deviation of 4.32 g
  (``What we eat,'' 2012). The amount of fat a person eats is not
  normally distributed but it is relatively mound shaped.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the probability that a sample mean amount of daily fat intake for
  35 men age 40-59 in the U.S. is more than 100 g.
\item
  Find the probability that a sample mean amount of daily fat intake for
  35 men age 40-59 in the U.S. is less than 93 g.
\item
  If you found a sample mean amount of daily fat intake for 35 men age
  40-59 in the U.S. less than 93 g, what would you conclude?
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  A dishwasher has a mean life of 12 years with an estimated standard
  deviation of 1.25 years (``Appliance life expectancy,'' 2013). The
  life of a dishwasher is normally distributed. Suppose you are a
  manufacturer and you take a sample of 10 dishwashers that you made.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State the random variable.
\item
  Find the mean of the sample mean.
\item
  Find the standard deviation of the sample mean.
\item
  What is the shape of the sampling distribution of the sample mean?
  Why?
\item
  Find the probability that the sample mean of the dishwashers is less
  than 6 years.
\item
  If you found the sample mean life of the 10 dishwashers to be less
  than 6 years, would you think that you have a problem with the
  manufacturing process? Why or why not?
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{One Sample Inference}\label{one-sample-inference}

Now that you have all this information about descriptive statistics and
probabilities, it is time to start inferential statistics. There are two
branches of inferential statistics: hypothesis testing and confidence
intervals.

\textbf{Hypothesis Testing}: making a decision about a parameter(s)
based on a statistic(s).

\textbf{Confidence Interval}: estimating a parameter(s) based on a
statistic(s).

This chapter will describe hypothesis testing, but as was stated in
Chapter 1, the American Statistical Association (ASA) is suggesting not
discussing statistical significance and p-values. So this chapter is
mostly for background to understand previously published studies.

\section{Basics of Hypothesis
Testing}\label{basics-of-hypothesis-testing}

To understand the process of a hypothesis tests, you need to first have
an understanding of what a hypothesis is, which is an educated guess
about a parameter. Once you have the hypothesis, you collect data and
use the data to make a determination to see if there is enough evidence
to show that the hypothesis is true. However, in hypothesis testing you
actually assume something else is true, and then you look at your data
to see how likely it is to get an event that your data demonstrates with
that assumption. If the event is very unusual, then you might think that
your assumption is actually false. If you are able to say this
assumption is false, then your hypothesis must be true. This is known as
a proof by contradiction. You assume the opposite of your hypothesis is
true and show that it can't be true. If this happens, then your
hypothesis must be true. All hypothesis tests go through the same
process. Once you have the process down, then the concept is much
easier. It is easier to see the process by looking at an example.
Concepts that are needed will be detailed in this example.

\subsection{Example: Basics of Hypothesis
Testing}\label{example-basics-of-hypothesis-testing}

Suppose a manufacturer of the XJ35 battery claims the mean life of the
battery is 500 days with a standard deviation of 25 days. You are the
buyer of this battery and you think this claim is incorrect. You would
like to test your belief because without a good reason you can't get out
of your contract.

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-58}

What do you do?

Well first, you should know what you are trying to measure. Define the
random variable.

Let \(x\) = life of a XJ35 battery

Now you are not just trying to find different \$x\$ values. You are
trying to find what the true mean is. Since you are trying to find it,
it must be unknown. You don't think it is 500 days. If you did, you
wouldn't be doing any testing. The true mean, \(\mu\), is unknown. That
means you should define that too.

Let \(\mu\) = mean life of a XJ35 battery

Now what?

You may want to collect a sample. What kind of sample?

You could ask the manufacturers to give you batteries, but there is a
chance that there could be some bias in the batteries they pick. To
reduce the chance of bias, it is best to take a random sample.

How big should the sample be?

A sample of size 30 or more means that you can use the central limit
theorem. Pick a sample of size 50.

Table~\ref{tbl-Battery} contains the data for the sample you collected:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Battery}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/battery.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Battery))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-Battery}Data on Battery Life}

\tabularnewline

\toprule\noalign{}
life \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
491 \\
485 \\
503 \\
492 \\
482 \\
490 \\

\end{longtable}

Now what should you do? Looking at the data set, you see some of the
times are above 500 and some are below. But looking at all of the
numbers is too difficult. It might be helpful to calculate the mean for
this sample.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{life, }\AttributeTok{data=}\NormalTok{Battery, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response mean
1     life  490
\end{verbatim}

The sample mean is 491.42 days. Looking at the sample mean, one might
think that you are right. However, the standard deviation and the sample
size also plays a role, so maybe you are wrong.

Before going any farther, it is time to formalize a few definitions.

You have a guess that the mean life of a battery is not 500 days. This
is opposed to what the manufacturer claims. There really are two
hypotheses, which are just guesses here --- the one that the
manufacturer claims and the one that you believe. It is helpful to have
names for them.

\textbf{Null Hypothesis}: historical value, claim, or product
specification. The symbol used is \(H_o\).

\textbf{Alternate Hypothesis}: what you want to prove. This is what you
want to accept as true when you reject the null hypothesis. There are
two symbols that are commonly used for the alternative hypothesis:
\(H_a\) or \(H_1\). The symbol \(H_a\) will be used in this book.

In general, the hypotheses look something like this:

\(H_0:\mu=\mu_o\)

\(H_a:\mu\ne \mu_o\)

where \(\mu_o\) just represents the value that the claim says the
population mean is actually equal to.

Also, \(H_a\) can be less than, greater than, or not equal to, though
not equal to is more common these days.

For this problem:

\(H_o:\mu=500\text{ days}\), since the manufacturer says the mean life
of a battery is 500 days.

\(H_a:\mu\ne 500\text{ days}\), since you believe that the mean life of
the battery is not 500 days.

Now back to the mean. You have a sample mean of 491.42 days. Is this
different enough to believe that you are right and the manufacturer is
wrong? How different does it have to be?

If you calculated a sample mean of 235 or 690, you would definitely
believe the population mean is not 500. But even if you had a sample
mean of 435 or 575 you would probably believe that the true mean was not
500. What about 475? or 535? Or 483? or 514? There is some point where
you would stop being so sure that the population mean is not 500. That
point separates the values of where you are sure or pretty sure that the
mean is not 500 from the area where you are not so sure. How do you find
that point?

Well it depends on how much error you want to make. Of course you don't
want to make any errors, but unfortunately that is unavoidable in
statistics. You need to figure out how much error you made with your
sample. Take the sample mean, and find the probability of getting
another sample mean less than it, assuming for the moment that the
manufacturer is right. The idea behind this is that you want to know
what is the chance that you could have come up with your sample mean
even if the population mean really is 500 days.

Chances are probabilities. So you want to find the probability that the
sample mean of 491.42 is unusual given that the population mean is
really 500 days. To compute this probability, you need to know how the
sample mean is distributed. Since the sample size is at least 30, then
you know the sample mean is approximately normally distributed. Now, you
want to find the \(z\)-value. The \(z\)-value is
\(z=\frac{491.42-500}{\frac{25}{\sqrt{50}}}=-2.43\).

This is more than 2 standard deviations below the mean, so that seems
that the sample mean is usual. It might be helpful to find the
probability though. Since you are saying that the sample mean is
different from 500 days, then you are asking if it is greater than or
less than. This means that you are in the tails of the normal curve. So
the probability you want to find is the probability being more than 2.43
or less than \(-2.43\). This is \(P(-2.43<z)+P(z>2.43)=0.015\)

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{pnorm}\NormalTok{(}\SpecialCharTok{{-}}\FloatTok{2.43}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{TRUE}\NormalTok{)}\SpecialCharTok{+}\FunctionTok{pnorm}\NormalTok{(}\FloatTok{2.43}\NormalTok{, }\DecValTok{0}\NormalTok{, }\DecValTok{1}\NormalTok{, }\AttributeTok{lower.tail=}\ConstantTok{FALSE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.01509882
\end{verbatim}

So the probability of being in the tails is 0.015. This probability is
known as a p-value for probability-value. This is unusual, so it is
unlikely to get a sample mean of 491.42 if the population mean is 500
days.

So it appears the assumption that the population mean is 500 days is
wrong, and you can reject the manufacturer's claim.

But how do you quantify really small? Is 5\% or 10\% or 15\% really
small? How do you decide?

Before you answer that question, a couple more definitions are needed.

\textbf{Test statistic}:
\(z=\frac{\bar{x}-\mu_o}{\frac{\sigma}{\sqrt{n}}}\) since it is
calculated as part of the testing of the hypothesis

\textbf{p - value}: probability that the test statistic will take on
more extreme values than the observed test statistic, given that the
null hypothesis is true. It is the probability that was calculated
above.

Now, how small is small enough? To answer that, you really want to know
the types of errors you can make.

There are actually only two errors that can be made. The first error is
if you say that is false, when in fact it is true. This means you reject
when was true. The second error is if you say that is true, when in fact
it is false. This means you fail to reject when is false. The following
table organizes this for you:

\subsection{Type of errors:}\label{type-of-errors}

\begin{longtable}[]{@{}lll@{}}
\caption{Type of errors}\tabularnewline
\toprule\noalign{}
& Ho true & Ho false \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
& Ho true & Ho false \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Reject Ho & Type I error & no error \\
Fail to reject Ho & no error & Type II error \\
\end{longtable}

Thus

\textbf{Type I Error} is rejecting \(H_o\) when \(H_o\) is true, and

\textbf{Type II Error} is failing to reject \(H_o\) when is \(H_o\)
false.

Since these are the errors, then one can define the probabilities
attached to each error.

\(\alpha\)= P(type I error) = P(rejecting\$H\_o\$ given it is true)

\(\beta\)= P(type II error) = P(failing to reject\$H\_o\$ given it is
false)

\(\alpha\) is also called the \textbf{level of significance}.

Another common concept that is used is Power = \(1-\beta\)

Now there is a relationship between \(\alpha\) and \(\beta\). They are
not complements of each other. How are they related?

If \(\alpha\) increases that means the chances of making a type I error
will increase. It is more likely that a type I error will occur. It
makes sense that you are less likely to make type II errors, only
because you will be rejecting more often. You will be failing to reject
less, and therefore, the chance of making a type II error will decrease.
Thus, as \(\alpha\) increases, \(\beta\) will decrease, and vice versa.
That makes them seem like complements, but they aren't complements. What
gives? Consider one more factor -\/- sample size.

Consider if you have a larger sample that is representative of the
population, then it makes sense that you have more accuracy then with a
smaller sample. Think of it this way, which would you trust more, a
sample mean of 490 if you had a sample size of 35 or sample size of 350
(assuming a representative sample)? Of course the 350 because there are
more data points and so more accuracy. If you are more accurate, then
there is less chance that you will make any error. By increasing the
sample size of a representative sample, you decrease both \(\alpha\) and
\(\beta\).

Summary of all of this:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  For a certain sample size, \(\alpha\) increases, \(\beta\) decreases.
\item
  For a certain level of significance, \(\alpha\), if \(n\) increases,
  \(\beta\) decreases.
\end{enumerate}

Now how do you find \(\alpha\) and \(\beta\)? Well \(\alpha\) is
actually chosen. There are only two values that are usually picked for
\(\alpha\): 0.01 and 0.05. is very difficult to find \(\beta\), so
usually it isn't found. If you want to make sure it is small you take as
large of a sample as you can afford provided it is a representative
sample. This is one use of the Power. You want to be small and the Power
of the test is large. The Power word sounds good.

Which pick of \(\alpha\) do you pick? Well that depends on what you are
working on. Remember in this example you are the buyer who is trying to
get out of a contract to buy these batteries. If you create a type I
error, you said that the batteries are bad when they aren't, most likely
the manufacturer will sue you. You want to avoid this. You might pick
\(\alpha\) to be 0.01. This way you have a small chance of making a type
I error. Of course this means you have more of a chance of making a type
II error. No big deal right? What if the batteries are used in
pacemakers and you tell the person that their pacemaker's batteries are
good for 500 days when they actually last less, that might be bad. If
you make a type II error, you say that the batteries do last 500 days
when they last less, then you have the possibility of killing someone.
You certainly do not want to do this. In this case you might want to
pick \(\alpha\) as 0.05. If both errors are equally bad, then pick
\(\alpha\) as 0.05.

The above discussion is why the choice of depends on what you are
researching. As the researcher, you are the one that needs to decide
what level to use based on your analysis of the consequences of making
each error is.

If a type I error is really bad, then pick \(\alpha\)= 0.01.

If a type II error is really bad, then pick \(\alpha\)= 0.05

If neither error is bad, or both are equally bad, then pick \(\alpha\) =
0.05

Usually \(\alpha\) is picked to be 0.05 in most cases.

The main thing is to always pick the \(\alpha\) before you collect the
data and start the test.

The above discussion was long, but it is really important information.
If you don't know what the errors of the test are about, then there
really is no point in making conclusions with the tests. Make sure you
understand what the two errors are and what the probabilities are for
them.

Now it is time to go back to the example and put this all together. This
is the basic structure of testing a hypothesis, usually called a
hypothesis test. Since this one has a test statistic involving \(z\), it
is also called a \(z\)-test. And since there is only one sample, it is
usually called a one-sample \(z\)-test.

\subsection{Example: Battery Example
Revisited.}\label{example-battery-example-revisited.}

Steps of a hypothesis test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words
\item
  State the null and alternative hypothesis and the level of
  significance
\item
  State and check the conditions for a hypothesis test
\item
  Find the sample statistic, test statistic, and p-value
\item
  Conclusion:
\item
  Interpretation:
\end{enumerate}

\subsubsection{Solution}\label{solution-59}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words
\end{enumerate}

\(x\) = life of battery

\(\mu\) = mean life of a XJ35 battery

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypothesis and the level of
  significance
\end{enumerate}

\(H_o:\mu=500\)

\(H_a:\mu\ne500\)

\(\alpha\) = 0.05 (from above discussion about consequences)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

Every hypothesis has some conditions that be met to make sure that the
results of the hypothesis are valid. The conditions are different for
each test. This test has the following conditions.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  A random sample of size \(n\) is taken.
\end{enumerate}

This occurred in this example, since it was stated that a random sample
of 50 battery lives were taken.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The population standard deviation is known.
\end{enumerate}

This is true, since it was given in the problem.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The sample size is at least 30 or the population of the random
  variable is normally distributed.
\end{enumerate}

The sample size was 30, so this condition is met.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

The test statistic depends on how many samples there are, what parameter
you are testing, and conditions that need to be checked. In this case,
there is one sample and you are testing the mean. The conditions were
checked above.

Sample statistic:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{life, }\AttributeTok{data=}\NormalTok{Battery, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response mean
1     life  490
\end{verbatim}

Test statistic: The z-value is
\(z=\frac{491.42-400}{\frac{25}{\sqrt{n}}}=-2.43\).

p-value: \(P(-2.43<z)+P(z>2.43)=0.015\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion:
\end{enumerate}

Now what? Well, this p-value is 0.015. This is a lot smaller than the
amount of error you would accept in the problem \(\alpha\) = 0.05. That
means that finding a sample mean less than 490 days is unusual to happen
if is true. This should make you think that is not true. You should
reject \(H_o\).

In fact, in general:

Reject \(H_o\) if the p-value \(<\alpha\)

Fail to reject \(H_o\) if the p-value \(\ge\alpha\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation:
\end{enumerate}

Since you rejected \(H_o\), what does this mean in the real world? That
it what goes in the interpretation. Since you rejected the claim by the
manufacturer that the mean life of the batteries is 500 days, then you
now can believe that your hypothesis was correct. In other words, there
is enough evidence to support that the mean life of the battery is less
than 500 days.

Now that you know that the batteries last less than 500 days, should you
cancel the contract? Statistically, there is evidence that the batteries
do not last as long as the manufacturer says they should. However, based
on this sample there are only ten days less on average that the
batteries last. There may not be practical significance in this case.
Ten days do not seem like a large difference. In reality, if the
batteries are used in pacemakers, then you would probably tell the
patient to have the batteries replaced every year. You have a large
buffer whether the batteries last 490 days or 500 days. It seems that it
might not be worth it to break the contract over ten days. What if the
10 days was practically significant? Are there any other things you
should consider? You might look at the business relationship with the
manufacturer. You might also look at how much it would cost to find a
new manufacturer. These are also questions to consider before making any
changes. What this discussion should show you is that just because a
hypothesis has statistical significance does not mean it has practical
significance. The hypothesis test is just one part of a research
process. There are other pieces that you need to consider.

That's it. That is what a hypothesis test looks like. All hypothesis
tests are done with the same six steps. Those general six steps are
outlined below.

\subsection{Steps for hypothesis test}\label{steps-for-hypothesis-test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words. This is where
  you are defining what the unknowns are in this problem.
\end{enumerate}

\(x\) = random variable

\(\mu\) = mean of random variable, if the parameter of interest is the
mean. There are other parameters you can test, and you would use the
appropriate symbol for that parameter.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\mu=\mu_o\), where \(\mu_o\) is the known mean

\(H_a:\mu\ne\mu_o\), You can replace \(\ne\) with \(<\) or \(>\) but
usually you use \(\ne\)

Also, state your level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

Each hypothesis test has its own conditions. They will be stated when
the different hypothesis tests are discussed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

This depends on what parameter you are working with, how many samples,
and the conditions of the test. Technology will be used to find the
sample statistic, test statistic, and p-value.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject \(H_o\) or fail to reject \(H_o\). The
rule is: if the p-value \(<\alpha\), then reject \(H_o\). If the p-value
\(\ge\alpha\), then fail to reject \(H_o\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

Sorry, one more concept about the conclusion and interpretation. First,
the conclusion is that you reject or you fail to reject \$H\_o\$. Why
was it said like this? It is because you never accept the null
hypothesis. If you wanted to accept the null hypothesis, then why do the
test in the first place? In the interpretation, you either have enough
evidence to support \(H_a\), or you do not have enough evidence to
support \(H_a\). You wouldn't want to go to all this work and then find
out you wanted to accept the claim. Why go through the trouble? You
always want to have enough evidence to support the alternative
hypothesis. Sometimes you can do that and sometimes you can't. If you
don't have enough evidence to support \(H_a\), it doesn't mean you
support the null hypothesis; it just means you can't support the
alternative hypothesis. Here is an example to demonstrate this.

\subsection{Example: Conclusions in Hypothesis
Tests}\label{example-conclusions-in-hypothesis-tests}

In the U.S. court system a jury trial could be set up as a hypothesis
test. To really help you see how this works, let's use OJ Simpson as an
example. In the court system, a person is presumed innocent until he/she
is proven guilty, and this is your null hypothesis. OJ Simpson was a
football player in the 1970s. In 1994 his ex-wife and her friend were
killed. OJ Simpson was accused of the crime, and in 1995 the case was
tried. The prosecutors wanted to prove OJ was guilty of killing his wife
and her friend, and that is the alternative hypothesis. In this case, a
verdict of not guilty was given. That does not mean that he is innocent
of this crime. It means there was not enough evidence to prove he was
guilty. Many people believe that OJ was guilty of this crime, but the
jury did not feel that the evidence presented was enough to show there
was guilt. The verdict in a jury trial is always guilty or not guilty!

The same is true in a hypothesis test. There is either enough or not
enough evidence to support the alternative hypothesis. It is not that
you proved the null hypothesis true.

When identifying hypothesis, it is important to state your random
variable and the appropriate parameter you want to make a decision
about. If you count something, then the random variable is the number of
whatever you counted. The parameter is the proportion of what you
counted. If the random variable is something you measured, then the
parameter is the mean of what you measured. (Note: there are other
parameters you can calculate, and some analysis of those will be
presented in later chapters.)

\subsection{Example: Stating
Hypotheses}\label{example-stating-hypotheses}

Identify the hypotheses necessary to test the following statements:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  The average salary of a teacher is different from
  \textbackslash\$30,000.
\item
  The proportion of students who like math is not 10\%.
\item
  The average age of students in this class differs from 21.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-60}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  The average salary of a teacher is different from
  \textbackslash\$30,000.
\end{enumerate}

\(x\) = salary of teacher

\(\mu=\) mean salary of teacher

The guess is that \(\mu\ne30000\) and that is the alternative
hypothesis.

The null hypothesis has the same parameter and number with an equal
sign.

\(H_o:\mu=30000\) \(H_a:\mu\ne30000\)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The proportion of students who like math is not 10\%.
\end{enumerate}

\(x\) = number of students who like math

\(p\) = proportion of students who like math

The guess is that \(p\) is not 0.10 and that is the alternative
hypothesis. \(H_a:p\ne0.10\) and the null hypothesis would be
\(H_o:p=0.10\)

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The average age of students in this class differs from 21.
\end{enumerate}

\(x\) = age of students in this class

\(\mu\)=mean age of students in this class

The guess is that \(\mu\ne21\) and that is the alternative hypothesis.
\(H_a:\mu\ne21\) and the null hypothesis would be \(H_o: \mu=21\)

\subsection{Example: Stating Type I and II Errors and Picking Level of
Significance}\label{example-stating-type-i-and-ii-errors-and-picking-level-of-significance}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  The plant-breeding department at a major university developed a new
  hybrid raspberry plant called YumYum Berry. Based on research data,
  the claim is made that from the time shoots are planted 90 days on
  average are required to obtain the first berry with a standard
  deviation of 9.2 days. A corporation that is interested in marketing
  the product tests 60 shoots by planting them and recording the number
  of days before each plant produces its first berry. The sample mean is
  92.3 days. The corporation wants to know if the mean number of days is
  more than the 90 days claimed. State the type I and type II errors in
  terms of this problem, consequences of each error, and state which
  level of significance to use.
\item
  A concern was raised in Australia that the percentage of deaths of
  Aboriginal prisoners was higher than the percent of deaths of
  non-indigenous prisoners, which is 0.27\%. State the type I and type
  II errors in terms of this problem, consequences of each error, and
  state which level of significance to use.
\end{enumerate}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-61}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  The plant-breeding department at a major university developed a new
  hybrid raspberry plant called YumYum Berry. Based on research data,
  the claim is made that from the time shoots are planted 90 days on
  average are required to obtain the first berry with a standard
  deviation of 9.2 days. A corporation that is interested in marketing
  the product tests 60 shoots by planting them and recording the number
  of days before each plant produces its first berry. The sample mean is
  92.3 days. The corporation wants to know if the mean number of days is
  more than the 90 days claimed. State the type I and type II errors in
  terms of this problem, consequences of each error, and state which
  level of significance to use.
\end{enumerate}

\(x\) = time to first berry for YumYum Berry plant

\(\mu\)= mean time to first berry for YumYum Berry plant

Type I Error: If the corporation does a type I error, then they will say
that the plants take longer to produce than 90 days when they don't.
They probably will not want to market the plants if they think they will
take longer. They will not market them even though in reality the plants
do produce in 90 days. They may have loss of future earnings, but that
is all.

Type II error: The corporation do not say that the plants take longer
then 90 days to produce when they do take longer. Most likely they will
market the plants. The plants will take longer, and so customers might
get upset and then the company would get a bad reputation. This would be
really bad for the company.

Level of significance: It appears that the corporation would not want to
make a type II error. Pick a 5\% level of significance, \(\alpha=0.05\).

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  A concern was raised in Australia that the percentage of deaths of
  Aboriginal prisoners was higher than the percent of deaths of
  non-indigenous prisoners, which is 0.27\%. State the type I and type
  II errors in terms of this problem, consequences of each error, and
  state which level of significance to use.
\end{enumerate}

\(x\) = number of Aboriginal prisoners who have died

\(p\) = proportion of Aboriginal prisoners who have died

Type I error: Rejecting that the proportion of Aboriginal prisoners who
died was 0.27\%, when in fact it was 0.27\%. This would mean you would
say there is a problem when there isn't one. You could anger the
Aboriginal community, and spend time and energy researching something
that isn't a problem.

Type II error: Failing to reject that the proportion of Aboriginal
prisoners who died was 0.27\%, when in fact it is higher than 0.27\%.
This would mean that you wouldn't think there was a problem with
Aboriginal prisoners dying when there really is a problem. You risk
causing deaths when there could be a way to avoid them.

Level of significance: It appears that both errors may be issues in this
case. You wouldn't want to anger the Aboriginal community when there
isn't an issue, and you wouldn't want people to die when there may be a
way to stop it. It may be best to pick a 5\% level of significance,
\(\alpha=0.05\).

Hint -\/- hypothesis testing is really easy if you follow the same
recipe every time. The only differences in the various problems are the
conditions of the test and the test statistic you calculate so you can
find the p-value. Do the same steps, in the same order, with the same
words, every time and these problems become very easy.

\subsection{Homework for Basics of Hypothesis Testing
Section}\label{homework-for-basics-of-hypothesis-testing-section}

\textbf{For the problems in this section, a question is being asked.
This is to help you understand what the hypotheses are. You are not to
run any hypothesis tests nor come up with any conclusions in this
section.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The Arizona Republic/Morrison/Cronkite News poll published on Monday,
  October 20, 2016, found 390 of the registered voters surveyed favor
  Proposition 205, which would legalize marijuana for adults. The
  statewide telephone poll surveyed 779 registered voters between
  Oct.~10 and Oct.~15. (Sanchez, 2016) Fifty-five percent of Colorado
  residents supported the legalization of marijuana. Does the data
  provide evidence that the percentage of Arizona residents who support
  legalization of marijuana is different from the proportion of Colorado
  residents who support it? State the random variable, population
  parameter, and hypotheses.
\item
  According to the February 2008 Federal Trade Commission report on
  consumer fraud and identity theft, 23\% of all complaints in 2007 were
  for identity theft. In that year, Alaska had 321 complaints of
  identity theft out of 1,432 consumer complaints
  (\textbackslash{}``Consumer fraud and,\textbackslash{}'' 2008). Does
  this data provide enough evidence to show that Alaska had a different
  proportion of identity theft than 23\%? State the random variable,
  population parameter, and hypotheses.
\item
  The Kyoto Protocol was signed in 1997, and required countries to start
  reducing their carbon emissions. The protocol became enforceable in
  February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per
  capita. Is there enough evidence to show that the mean CO2 emission is
  different in 2010 than in 2004? State the random variable, population
  parameter, and hypotheses.
\item
  The FDA regulates that fish that is consumed is allowed to contain 1.0
  mg/kg of mercury. In Florida, bass fish were collected in 53 different
  lakes to measure the amount of mercury in the fish. Do the data
  provide enough evidence to show that the fish in Florida lakes has a
  different amount of mercury than the allowable amount? State the
  random variable, population parameter, and hypotheses.
\item
  The Arizona Republic/Morrison/Cronkite News poll published on Monday,
  October 20, 2016, found 390 of the registered voters surveyed favor
  Proposition 205, which would legalize marijuana for adults. The
  statewide telephone poll surveyed 779 registered voters between
  Oct.~10 and Oct.~15. (Sanchez, 2016) Fifty-five percent of Colorado
  residents supported the legalization of marijuana. Does the data
  provide evidence that the percentage of Arizona residents who support
  legalization of marijuana is different from the proportion of Colorado
  residents who support it. State the type I and type II errors in this
  case, consequences of each error type for this situation from the
  perspective of the manufacturer, and the appropriate alpha level to
  use. State why you picked this alpha level.
\item
  According to the February 2008 Federal Trade Commission report on
  consumer fraud and identity theft, 23\% of all complaints in 2007 were
  for identity theft. In that year, Alaska had 321 complaints of
  identity theft out of 1,432 consumer complaints
  (\textbackslash{}``Consumer fraud and,\textbackslash{}'' 2008). Does
  this data provide enough evidence to show that Alaska had a different
  proportion of identity theft than 23\%? State the type I and type II
  errors in this case, consequences of each error type for this
  situation from the perspective of the state of Alaska, and the
  appropriate alpha level to use. State why you picked this alpha level.
\item
  The Kyoto Protocol was signed in 1997, and required countries to start
  reducing their carbon emissions. The protocol became enforceable in
  February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per
  capita. Is there enough evidence to show that the mean CO2 emission is
  lower in 2010 than in 2004? State the type I and type II errors in
  this case, consequences of each error type for this situation from the
  perspective of the agency overseeing the protocol, and the appropriate
  alpha level to use. State why you picked this alpha level.
\item
  The FDA regulates that fish that is consumed is allowed to contain 1.0
  mg/kg of mercury. In Florida, bass fish were collected in 53 different
  lakes to measure the amount of mercury in the fish. Do the data
  provide enough evidence to show that the fish in Florida lakes has
  different amount of mercury than the allowable amount? State the type
  I and type II errors in this case, consequences of each error type for
  this situation from the perspective of the FDA, and the appropriate
  alpha level to use. State why you picked this alpha level.
\end{enumerate}

\section{One-Sample Proportion Test}\label{one-sample-proportion-test}

There are many different parameters that you can test. There is a test
for the mean, such as was introduced with the \$z\$-test. There is also
a test for the population proportion, \(p\). This is where you might be
curious if the proportion of students who smoke at your school is lower
than the proportion in your area. Or you could question if the
proportion of accidents caused by teenage drivers who do not have a
drivers' education class is more than the national proportion.

To test a population proportion, there are a few things that need to be
defined first. Usually, Greek letters are used for parameters and Latin
letters for statistics. When talking about proportions, it makes sense
to use \(p\) for proportion. The Greek letter for \(p\) is \(\pi\), but
that is too confusing to use. Instead, it is best to use \(p\) for the
population proportion. That means that a different symbol is needed for
the sample proportion. The convention is to use, \(\hat{p}\), known as
p-hat. This way you know that \(p\) is the population proportion, and
that \(\hat{p}\) is the sample proportion related to it.

Now proportion tests are about looking for the percentage of individuals
who have a particular attribute. You are really looking for the number
of successes that happen. Thus, a proportion test involves a binomial
distribution.

\subsection{\texorpdfstring{\textbf{Hypothesis Test for One Population
Proportion (1-Prop
Test)}}{Hypothesis Test for One Population Proportion (1-Prop Test)}}\label{hypothesis-test-for-one-population-proportion-1-prop-test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = number of successes

\(p\) = proportion of successes

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:p=p_o\), where \(p_o\) is the known proportion

\(H_a:p\ne p_o\), you can also use \textless{} or \textgreater, but
\(\ne\) is the more common one to use.

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A simple random sample of size \(n\) is taken. Check: describe
  how the sample was collected
\item
  State: The conditions for the binomial experiment are satisfied.
  Check: Show all four properties are true.
\item
  State: The sampling distribution of \(\hat{p}\) is normally
  distributed. Check: you need to show that \(p*n\ge5\) and \(q*n\ge5\),
  where \(q=1-p\). If this requirement is true, then the sampling
  distribution of \(\hat{p}\) is well approximated by a normal curve.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

This will be computed on r Studio using the command

prop.test(r, n, p=what\_Ho\_says)

where \(r\)=observed number of successes and \(n\) = number of trials.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject or fail to reject \(H_o\). The rule is:
if the p-value \(<\alpha\), then reject \(H_0\). If the p-value
\(\ge\alpha\), then fail to reject \(H_o\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

\subsection{Example: Hypothesis Test for One
Proportion}\label{example-hypothesis-test-for-one-proportion}

A concern was raised in Australia that the percentage of deaths of
Aboriginal prisoners was different than the percent of deaths of
non-Aboriginal prisoners, which is 0.27\%. A sample of six years
(1990-1995) of data was collected, and it was found that out of 14,495
Aboriginal prisoners, 51 died (\textbackslash{}``Indigenous deaths
in,\textbackslash{}'' 1996). Do the data provide enough evidence to show
that the proportion of deaths of Aboriginal prisoners is different from
0.27\%?

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-62}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = number of Aboriginal prisoners who die

\(p\) = proportion of Aboriginal prisoners who die

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:p=0.0027\)

\(H_a:p\ne0.0027\)

From
\hyperref[example-stating-type-i-and-ii-errors-and-picking-level-of-significance]{Example:
Stating Type I and II Errors and Picking Level of Significance} part b,
the argument was made to pick 5\% for the level of significance. So
\(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  A simple random sample of 14,495 Aboriginal prisoners was taken.
  Check: The sample was not a random sample, since it was data from six
  years. It is the numbers for all prisoners in these six years, but the
  six years were not picked at random. Unless there was something
  special about the six years that were chosen, the sample is probably a
  representative sample. This condition is probably met.
\item
  The properties of a binomial experiment are met. There are 14,495
  prisoners in this case. Check: The prisoners are all Aboriginals, so
  you are not mixing Aboriginal with non-Aboriginal prisoners. There are
  only two outcomes, either the prisoner dies or doesn't. The chance
  that one prisoner dies over another may not be constant, but if you
  consider all prisoners the same, then it may be close to the same
  probability. Thus the conditions for the binomial distribution are
  satisfied
\item
  The sampling distribution of \(\hat{p}\) can be approximated with a
  normal distributed. Check: In this case \(p = 0.0027\) and
  \(n = 14,495\). \(n*p=39.1365\ge5\) and \(n*q=14455.86\ge5\). So, the
  sampling distribution for \(\hat{p}\) is normally distributed.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

Use the following command in rStudio:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\DecValTok{51}\NormalTok{, }\DecValTok{14495}\NormalTok{, }\AttributeTok{p=}\FloatTok{0.0027}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    1-sample proportions test with continuity correction

data:  51 out of 14495
X-squared = 3.3084, df = 1, p-value = 0.06893
alternative hypothesis: true p is not equal to 0.0027
95 percent confidence interval:
 0.002647440 0.004661881
sample estimates:
          p 
0.003518455 
\end{verbatim}

Sample Proportion: \(\hat{p}=0.0035\)

Test Statistic: \(\chi^2=3.3085\)

p-value: \(p-value=0.06893\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Since the \(p-value\ge0.05\), then fail to reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is not enough evidence to support that the proportion of deaths of
Aboriginal prisoners is different from non-Aboriginal prisoners.

\subsection{Example: Hypothesis Test for One
Proportion}\label{example-hypothesis-test-for-one-proportion-1}

A researcher who is studying the effects of income levels on
breastfeeding of infants hypothesizes that countries with a low income
level have a different rate of infant breastfeeding than higher income
countries. It is known that in Germany, considered a high-income country
by the World Bank, 22\% of all babies are breastfeed. In Tajikistan,
considered a low-income country by the World Bank, researchers found
that in a random sample of 500 new mothers that 125 were breastfeeding
their infant. At the 5\% level of significance, does this show that
low-income countries have a different incident of breastfeeding?

\subsubsection{Solution}\label{solution-63}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State you random variable and the parameter in words.
\end{enumerate}

\(x\) = number of woman who breastfeed in a low-income country

\(p\) = proportion of woman who breastfeed in a low-income country

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:p=0.22\)

\(H_a:p\ne0.22\)

\(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  A simple random sample of 500 breastfeeding habits of woman in a
  low-income country was taken. Check: This was stated in the problem.
\item
  The properties of a Binomial Experiment have been met. Check: There
  were 500 women in the study. The women are considered identical,
  though they probably have some differences. There are only two
  outcomes, either the woman breastfeeds or she doesn't. The probability
  of a woman breastfeeding is probably not the same for each woman, but
  it is probably not very different for each woman. The conditions for
  the binomial distribution are satisfied
\item
  The sampling distribution of \(\hat{p}\) can be approximated with a
  normal distributed. Check: In this case, \(n = 500\) and \(p = 0.22\).
  \(n*p= 110\ge5\) and \(n*q=390\ge5\), so the sampling distribution of
  \(\hat{p}\) is well approximated by a normal curve.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

On r studio, use the following command

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop\_test}\NormalTok{(}\DecValTok{125}\NormalTok{, }\DecValTok{500}\NormalTok{, }\AttributeTok{p=}\FloatTok{0.22}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    1-sample proportions test with continuity correction

data:  125 out of 500
X-squared = 2.4505, df = 1, p-value = 0.1175
alternative hypothesis: true p is not equal to 0.22
95 percent confidence interval:
 0.2131062 0.2908059
sample estimates:
   p 
0.25 
\end{verbatim}

Sample Statistic: \(\hat{p}=0.25\)

test Statistic: \(\chi^2=2.4505\)

p-value: \(p-value=0.1175\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Since the p-value is more than 0.05, you fail to reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is not enough evidence to support that the proportion of women who
breastfeed in low-income countries is different from the proportion of
women in high-income countries who breastfeed.

Notice, the conclusion is that there wasn't enough evidence to support
\(H_a\). The conclusion was not that you support \(H_o\). There are many
reasons why you can't say that \(H_o\) is true. It could be that the
countries you chose were not very representative of what truly happens.
If you instead looked at all high-income countries and compared them to
low-income countries, you might have different results. It could also be
that the sample you collected in the low-income country was not
representative. It could also be that income level is not an indication
of breastfeeding habits. It could be that the sample that was taken
didn't show evidence but another sample would show evidence. There could
be other factors involved. This is why you can't say that you support
\(H_o\). There are too many other factors that could be the reason that
you failed to reject \(H_0\).

\subsection{Homework for One-Sample Proportion Test
Section}\label{homework-for-one-sample-proportion-test-section}

\textbf{In each problem show all steps of the hypothesis test. If some
of the conditions are not met, note that the results of the test may not
be correct and then continue the process of the hypothesis test.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The Arizona Republic/Morrison/Cronkite News poll published on Monday,
  October 20, 2016, found 390 of the registered voters surveyed favor
  Proposition 205, which would legalize marijuana for adults. The
  statewide telephone poll surveyed 779 registered voters between
  Oct.~10 and Oct.~15. (Sanchez, 2016) Fifty-five percent of Colorado
  residents supported the legalization of marijuana. Does the data
  provide evidence that the percentage of Arizona residents who support
  legalization of marijuana is different from the proportion of Colorado
  residents who support it. Test at the 1\% level.
\item
  In July of 1997, Australians were asked if they thought unemployment
  would increase, and 47\% thought that it would increase. In November
  of 1997, they were asked again. At that time 284 out of 631 said that
  they thought unemployment would increase (\textbackslash{}``Morgan
  Gallup poll,\textbackslash{}'' 2013). At the 5\% level, is there
  enough evidence to show that the proportion of Australians in November
  1997 who believe unemployment would increase is different from the
  proportion who felt it would increase in July 1997?
\item
  According to the February 2008 Federal Trade Commission report on
  consumer fraud and identity theft, 23\% of all complaints in 2007 were
  for identity theft. In that year, Arkansas had 1,601 complaints of
  identity theft out of 3,482 consumer complaints
  (\textbackslash{}``Consumer fraud and,\textbackslash{}'' 2008). Does
  this data provide enough evidence to show that Arkansas had a
  different percentage of identity theft than 23\%? Test at the 5\%
  level.
\item
  According to the February 2008 Federal Trade Commission report on
  consumer fraud and identity theft, 23\% of all complaints in 2007 were
  for identity theft. In that year, Alaska had 321 complaints of
  identity theft out of 1,432 consumer complaints
  (\textbackslash{}``Consumer fraud and,\textbackslash{}'' 2008). Does
  this data provide enough evidence to show that Alaska had a different
  proportion of identity theft than 23\%? Test at the 5\% level.
\item
  In 2001, the Gallup poll found that 81\% of American adults believed
  that there was a conspiracy in the death of President Kennedy. In
  2013, the Gallup poll asked 1,039 American adults if they believe
  there was a conspiracy in the assassination, and found that 634
  believe there was a conspiracy (\textbackslash{}``Gallup news
  service,\textbackslash{}'' 2013). Do the data show that the proportion
  of Americans who believe in this conspiracy has changed? Test at the
  1\% level.
\item
  In 2008, there were 507 children in Arizona out of 32,601 who were
  diagnosed with Autism Spectrum Disorder (ASD)
  (\textbackslash{}``Autism and developmental,\textbackslash{}'' 2008).
  Nationally 1 in 88 children are diagnosed with ASD
  (\textbackslash{}``CDC features -,\textbackslash{}'' 2013). Is there
  sufficient data to show that the incident of ASD is different in
  Arizona than nationally? Test at the 1\% level.
\end{enumerate}

\section{One-Sample Test for the
Mean}\label{one-sample-test-for-the-mean}

It is time to go back to look at the test for the mean that was
introduced in section 7.1 called the \(z\)-test. In the example, you
knew what the population standard deviation, \(\sigma\), was. What if
you don't know \(\sigma\)?

If you don't know \(\sigma\), then you don't know the sampling
distribution of the mean. Can it be found another way? The answer is of
course, yes. One way is to use a method called resampling. The following
example explains how resampling is performed.

\subsection{Example: Resampling}\label{example-resampling}

A random sample of 10 body mass index (BMI) were taken from the NHANES
Data frame The mean BMI of Australians is 27.2 \(kg/m^2\). Is there
evidence that Americans have a different BMI from people in Australia.
Test at the 5\% level.

\subsubsection{Solution}\label{solution-64}

The standard deviation of BMI is not known for Australians. To answer
this questions, first look at the sample from NHANES
Table~\ref{tbl-NHANES-10-sample}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_NHANES\_10}\OtherTok{\textless{}{-}} 
\NormalTok{  NHANES }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{slice\_sample}\NormalTok{(}\AttributeTok{n=}\DecValTok{10}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(sample\_NHANES\_10))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0051}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0177}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0215}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0177}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0177}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0177}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}@{}}

\caption{\label{tbl-NHANES-10-sample}Sample of size 10 from NHANES}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SurveyYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AgeDecade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeMonths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MaritalStatus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HHIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HHIncomeMid
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Poverty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HomeRooms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HomeOwn
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Work
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HeadCirc
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BMI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMICatUnder20yrs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMI\_WHO
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Pulse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSysAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDiaAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Testosterone
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DirectChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TotChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diabetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DiabetesAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HealthGen
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysPhysHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysMentHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LittleInterest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Depressed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nPregnancies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nBabies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age1stBaby
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SleepHrsNight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SleepTrouble
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PhysActive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PhysActiveDays
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TVHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CompHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TVHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
CompHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alcohol12PlusYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SmokeNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SmokeAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marijuana
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeFirstMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RegularMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeRegMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HardDrugs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexEver
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartnLife
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SameSex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexOrientation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PregnantNow
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
52197 & 2009\_10 & male & 39 & 30-39 & 470 & White & NA & Some College &
Married & 75000-99999 & 87500 & 3.49 & 6 & Own & Working & 130.7 & NA &
NA & 179.3 & 40.66 & NA & 30.0\_plus & 58 & 140 & 65 & 138 & 56 & 140 &
76 & 140 & 54 & NA & 0.88 & 3.85 & 302 & 0.944 & NA & NA & No & NA &
Fair & 5 & 0 & None & None & NA & NA & NA & 6 & No & Yes & 2 & NA & NA &
NA & NA & Yes & 3 & 12 & No & Yes & Smoker & 17 & Yes & 17 & Yes & 18 &
No & Yes & 19 & 4 & 1 & No & Heterosexual & NA \\
59175 & 2009\_10 & female & 57 & 50-59 & 688 & Black & NA & High School
& Married & 75000-99999 & 87500 & 5.00 & 5 & Own & Working & 96.2 & NA &
NA & 162.0 & 36.66 & NA & 30.0\_plus & 74 & 135 & 83 & 138 & 80 & 138 &
84 & 132 & 82 & NA & NA & NA & 121 & 0.508 & NA & NA & No & NA & Vgood &
0 & 0 & None & None & NA & NA & NA & 5 & No & Yes & 2 & NA & NA & NA &
NA & Yes & NA & 0 & NA & No & Non-Smoker & NA & No & NA & No & NA & No &
Yes & 18 & 3 & 1 & No & Heterosexual & NA \\
59001 & 2009\_10 & female & 19 & 10-19 & 233 & White & NA & NA & NA &
75000-99999 & 87500 & 3.85 & 5 & Own & Working & 62.0 & NA & NA & 163.0
& 23.34 & NA & 18.5\_to\_24.9 & 84 & 107 & 46 & 106 & 44 & 106 & 48 &
108 & 44 & NA & 1.55 & 5.20 & 30 & 0.229 & 17 & 0.447 & No & NA & Fair &
4 & 0 & None & None & NA & NA & NA & 10 & No & No & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & Yes & 17 & Yes & 17 & Yes & Yes
& 16 & 4 & 3 & No & Heterosexual & NA \\
67552 & 2011\_12 & female & 16 & 10-19 & NA & White & White & NA & NA &
more 99999 & 100000 & 4.07 & 7 & Own & Looking & 49.2 & NA & NA & 161.0
& 19.00 & NormWeight & 18.5\_to\_24.9 & 102 & 106 & 69 & 106 & 62 & 106
& 70 & 106 & 68 & 12.01 & 2.09 & 4.06 & 112 & 2.240 & NA & NA & No & NA
& Good & 1 & 10 & NA & NA & NA & NA & NA & 7 & Yes & Yes & NA & 1\_hr &
3\_hr & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA \\
53206 & 2009\_10 & female & 68 & 60-69 & 817 & White & NA & High School
& Married & 35000-44999 & 40000 & 2.40 & 7 & Own & NotWorking & 77.5 &
NA & NA & 158.0 & 31.04 & NA & 30.0\_plus & 74 & 82 & 45 & 92 & 54 & 80
& 44 & 84 & 46 & NA & 0.85 & 3.47 & 27 & 0.643 & 71 & 0.490 & No & NA &
Vgood & 0 & 0 & None & None & 5 & 5 & 19 & 7 & No & No & NA & NA & NA &
NA & NA & No & NA & 0 & Yes & Yes & Smoker & 18 & NA & NA & NA & NA & No
& Yes & 17 & 1 & NA & No & NA & NA \\
56640 & 2009\_10 & male & 40 & 40-49 & 480 & White & NA & College Grad &
Married & more 99999 & 100000 & 5.00 & 8 & Own & Working & 117.4 & NA &
NA & 175.0 & 38.33 & NA & 30.0\_plus & 70 & 113 & 72 & 112 & 66 & 112 &
72 & 114 & 72 & NA & 0.88 & 5.69 & 128 & 1.707 & NA & NA & No & NA &
Vgood & 0 & 0 & None & None & NA & NA & NA & 7 & No & No & NA & NA & NA
& NA & NA & Yes & 4 & 3 & No & Yes & Smoker & 13 & Yes & 15 & No & NA &
No & Yes & 25 & 1 & 1 & No & Heterosexual & NA \\

\end{longtable}

The mean BMI from this sample is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{BMI, }\AttributeTok{data=}\NormalTok{sample\_NHANES\_10, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response   mean
1      BMI 30.748
\end{verbatim}

The sample mean for Americans is different from the mean BMI for
Australians, but could it just be by chance. Suppose you take another
sample of size 10, but you only have these 10 BMIs to work with. So how
could you do this. One way is to assume that the sample you took is
representative of the entire population, and so you create a population
by copying this sample over and over again. So you could have over 1000
copies of this sample of 10 BMIs. Then take a sample of size 10 from
this created population. When doing this, you could conceivably choose
the same number several times that was in the original sample and not
choose some of the numbers that were in the original sample. Instead of
physically creating this new population, you could just take samples
from your original sample but with replacement. This means that you
randomly pick the first number, record it, and then put it back that
value back before collecting the next number. This kind a sampling is
called \textbf{randomization sampling}. A sample using randomization
could be Table~\ref{tbl-NHANES-10-resample}.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{resample}\NormalTok{(sample\_NHANES\_10))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0050}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0150}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0150}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0212}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0075}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0162}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0175}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0200}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0137}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0112}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0175}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0162}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0125}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0100}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0087}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0200}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0100}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0187}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0150}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 152\tabcolsep) * \real{0.0100}}@{}}

\caption{\label{tbl-NHANES-10-resample}Resample from NHANES Sample}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SurveyYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AgeDecade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeMonths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MaritalStatus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HHIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HHIncomeMid
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Poverty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HomeRooms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HomeOwn
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Work
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HeadCirc
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BMI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMICatUnder20yrs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMI\_WHO
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Pulse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSysAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDiaAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Testosterone
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DirectChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TotChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diabetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DiabetesAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HealthGen
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysPhysHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysMentHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LittleInterest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Depressed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nPregnancies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nBabies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age1stBaby
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SleepHrsNight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SleepTrouble
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PhysActive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PhysActiveDays
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TVHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CompHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TVHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
CompHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alcohol12PlusYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SmokeNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SmokeAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marijuana
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeFirstMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RegularMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeRegMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HardDrugs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexEver
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartnLife
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SameSex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexOrientation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PregnantNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
orig.id
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
52371 & 2009\_10 & female & 31 & 30-39 & 380 & White & NA & Some College
& Married & more 99999 & 100000 & 4.67 & 6 & Own & Working & 115.3 & NA
& NA & 167.8 & 40.95 & NA & 30.0\_plus & 70 & 100 & 63 & 106 & 62 & 106
& 62 & 94 & 64 & NA & 1.06 & 5.17 & 328 & 2.563 & NA & NA & No & NA &
Good & 0 & 4 & None & None & 2 & 2 & 26 & 7 & Yes & No & NA & NA & NA &
NA & NA & Yes & 2 & 3 & NA & No & Non-Smoker & NA & Yes & 17 & No & NA &
No & Yes & 16 & 2 & 1 & No & Heterosexual & No & 7 \\
56640 & 2009\_10 & male & 40 & 40-49 & 480 & White & NA & College Grad &
Married & more 99999 & 100000 & 5.00 & 8 & Own & Working & 117.4 & NA &
NA & 175.0 & 38.33 & NA & 30.0\_plus & 70 & 113 & 72 & 112 & 66 & 112 &
72 & 114 & 72 & NA & 0.88 & 5.69 & 128 & 1.707 & NA & NA & No & NA &
Vgood & 0 & 0 & None & None & NA & NA & NA & 7 & No & No & NA & NA & NA
& NA & NA & Yes & 4 & 3 & No & Yes & Smoker & 13 & Yes & 15 & No & NA &
No & Yes & 25 & 1 & 1 & No & Heterosexual & NA & 6 \\
68223 & 2011\_12 & female & 9 & 0-9 & NA & White & White & NA & NA &
10000-14999 & 12500 & 0.30 & 5 & Own & NA & 57.4 & NA & NA & 138.5 &
29.90 & Obese & 25.0\_to\_29.9 & 70 & 115 & 59 & 106 & 42 & 114 & 48 &
116 & 70 & NA & NA & NA & 77 & 0.464 & NA & NA & No & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & 4 & 3\_hr & 0\_to\_1\_hr & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & 10 \\
53206 & 2009\_10 & female & 68 & 60-69 & 817 & White & NA & High School
& Married & 35000-44999 & 40000 & 2.40 & 7 & Own & NotWorking & 77.5 &
NA & NA & 158.0 & 31.04 & NA & 30.0\_plus & 74 & 82 & 45 & 92 & 54 & 80
& 44 & 84 & 46 & NA & 0.85 & 3.47 & 27 & 0.643 & 71 & 0.490 & No & NA &
Vgood & 0 & 0 & None & None & 5 & 5 & 19 & 7 & No & No & NA & NA & NA &
NA & NA & No & NA & 0 & Yes & Yes & Smoker & 18 & NA & NA & NA & NA & No
& Yes & 17 & 1 & NA & No & NA & NA & 5 \\
52371 & 2009\_10 & female & 31 & 30-39 & 380 & White & NA & Some College
& Married & more 99999 & 100000 & 4.67 & 6 & Own & Working & 115.3 & NA
& NA & 167.8 & 40.95 & NA & 30.0\_plus & 70 & 100 & 63 & 106 & 62 & 106
& 62 & 94 & 64 & NA & 1.06 & 5.17 & 328 & 2.563 & NA & NA & No & NA &
Good & 0 & 4 & None & None & 2 & 2 & 26 & 7 & Yes & No & NA & NA & NA &
NA & NA & Yes & 2 & 3 & NA & No & Non-Smoker & NA & Yes & 17 & No & NA &
No & Yes & 16 & 2 & 1 & No & Heterosexual & No & 7 \\
71316 & 2011\_12 & male & 57 & 50-59 & NA & Other & Other & 9 - 11th
Grade & Married & 65000-74999 & 70000 & 3.13 & 5 & Own & Working & 70.8
& NA & NA & 169.3 & 24.70 & NA & 18.5\_to\_24.9 & 48 & 113 & 65 & 114 &
66 & 116 & 66 & 110 & 64 & 599 & 1.14 & 3.88 & 97 & 0.890 & NA & NA & No
& NA & Vgood & 4 & 0 & None & None & NA & NA & NA & 7 & No & No & 1 &
2\_hr & 0\_to\_1\_hr & NA & NA & Yes & 12 & 24 & Yes & Yes & Smoker & 25
& Yes & 25 & Yes & 25 & No & Yes & 16 & 3 & 1 & No & Heterosexual & NA &
9 \\
67537 & 2011\_12 & male & 33 & 30-39 & NA & Other & Other & College Grad
& Married & more 99999 & 100000 & 5.00 & 3 & Rent & Working & 73.8 & NA
& NA & 179.5 & 22.90 & NA & 18.5\_to\_24.9 & 62 & 119 & 67 & 112 & 68 &
122 & 66 & 116 & 68 & NA & 1.32 & 5.74 & 38 & 1.000 & NA & NA & No & NA
& Vgood & 2 & 1 & None & None & NA & NA & NA & 7 & No & Yes & 3 & 1\_hr
& 3\_hr & NA & NA & Yes & 1 & 104 & NA & No & Non-Smoker & NA & No & NA
& No & NA & No & Yes & 27 & 4 & 1 & No & Heterosexual & NA & 8 \\
56640 & 2009\_10 & male & 40 & 40-49 & 480 & White & NA & College Grad &
Married & more 99999 & 100000 & 5.00 & 8 & Own & Working & 117.4 & NA &
NA & 175.0 & 38.33 & NA & 30.0\_plus & 70 & 113 & 72 & 112 & 66 & 112 &
72 & 114 & 72 & NA & 0.88 & 5.69 & 128 & 1.707 & NA & NA & No & NA &
Vgood & 0 & 0 & None & None & NA & NA & NA & 7 & No & No & NA & NA & NA
& NA & NA & Yes & 4 & 3 & No & Yes & Smoker & 13 & Yes & 15 & No & NA &
No & Yes & 25 & 1 & 1 & No & Heterosexual & NA & 6 \\
52197 & 2009\_10 & male & 39 & 30-39 & 470 & White & NA & Some College &
Married & 75000-99999 & 87500 & 3.49 & 6 & Own & Working & 130.7 & NA &
NA & 179.3 & 40.66 & NA & 30.0\_plus & 58 & 140 & 65 & 138 & 56 & 140 &
76 & 140 & 54 & NA & 0.88 & 3.85 & 302 & 0.944 & NA & NA & No & NA &
Fair & 5 & 0 & None & None & NA & NA & NA & 6 & No & Yes & 2 & NA & NA &
NA & NA & Yes & 3 & 12 & No & Yes & Smoker & 17 & Yes & 17 & Yes & 18 &
No & Yes & 19 & 4 & 1 & No & Heterosexual & NA & 1 \\
59001 & 2009\_10 & female & 19 & 10-19 & 233 & White & NA & NA & NA &
75000-99999 & 87500 & 3.85 & 5 & Own & Working & 62.0 & NA & NA & 163.0
& 23.34 & NA & 18.5\_to\_24.9 & 84 & 107 & 46 & 106 & 44 & 106 & 48 &
108 & 44 & NA & 1.55 & 5.20 & 30 & 0.229 & 17 & 0.447 & No & NA & Fair &
4 & 0 & None & None & NA & NA & NA & 10 & No & No & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & Yes & 17 & Yes & 17 & Yes & Yes
& 16 & 4 & 3 & No & Heterosexual & NA & 3 \\

\end{longtable}

Notice that some of the unit of observations are repeated. That is what
happens when you resample. Now one resampling isn't enough. So you want
to resample many times so you can create a resampling distribution
Figure~\ref{fig-NHANES-10-resample-histogram}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# mutate NHANES to subtract 27.2 (Australia\textquotesingle{}s BMI) from US BMI measurements }
\NormalTok{mutate\_NHANES }\OtherTok{\textless{}{-}}\NormalTok{ NHANES }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{NewBMI=}\NormalTok{BMI}\FloatTok{{-}27.2}\NormalTok{)}
\CommentTok{\# Generate the single sample}
\NormalTok{Single\_sample}\OtherTok{\textless{}{-}}\NormalTok{ mutate\_NHANES }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{sample\_n}\NormalTok{(}\AttributeTok{size =} \DecValTok{10}\NormalTok{)}
\CommentTok{\#Calculate the mean age of the single sample }
\NormalTok{Single\_sample\_mean }\OtherTok{\textless{}{-}} 
\NormalTok{  Single\_sample }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{df\_stats}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ NewBMI, }\AttributeTok{means =}\NormalTok{ mean)}
\CommentTok{\#Take 200 resamples from the single sample}
\NormalTok{Trials\_resample }\OtherTok{\textless{}{-}} 
  \FunctionTok{do}\NormalTok{(}\DecValTok{200}\NormalTok{) }\SpecialCharTok{*}\NormalTok{ \{ Single\_sample }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{resample}\NormalTok{() }\SpecialCharTok{|\textgreater{}}
      \FunctionTok{df\_stats}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ NewBMI, }\AttributeTok{means =}\NormalTok{ mean) \}}
\CommentTok{\# Plot the resample distribution of means}
\FunctionTok{gf\_density}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ means, }\AttributeTok{data =}\NormalTok{ Trials\_resample, }\AttributeTok{bins =} \DecValTok{10}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{gf\_lims}\NormalTok{(}\AttributeTok{x =} \FunctionTok{c}\NormalTok{(}\SpecialCharTok{{-}}\DecValTok{5}\NormalTok{, }\DecValTok{10}\NormalTok{)) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{gf\_labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Resampling Distribution"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{gf\_vline}\NormalTok{(}\AttributeTok{data =}\NormalTok{ Single\_sample\_mean, }\AttributeTok{xintercept =} \SpecialCharTok{\textasciitilde{}}\NormalTok{ means, }\AttributeTok{color=}\StringTok{"red"}\NormalTok{)}
\FunctionTok{df\_stats}\NormalTok{( }\SpecialCharTok{\textasciitilde{}}\NormalTok{ means, }\AttributeTok{data =}\NormalTok{ Trials\_resample, mean, sd)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response    mean       sd
1    means -1.2846 2.107796
\end{verbatim}

\begin{figure}[H]

\centering{

\includegraphics{One-Sample-Inference_files/figure-pdf/fig-NHANES-10-resample-histogram-1.pdf}

}

\caption{\label{fig-NHANES-10-resample-histogram}Resampling distribution
of mean BMI with sample size 10}

\end{figure}%

Notice the sample mean from the resampling is very close to 0, so that
means that the US BMI are not that different from the Australian BMI.
There doesn't seem to be enough evidence to show that the US BMI is
different from the Australian BMI. One note, the sample size used here
was 10 so you could see the sample, but really the sample size should be
more than 100 for this method to be valid.

So this is one way to answer the question about if there is evidence to
show a population mean is different from a value. This is actually the
method that Ronald Fisher developed when he create all the foundation
work that he did in statistics in the early 1900s. However, at the time,
computers didn't exist, so taking 100 resampling samples was not
possible at that time. So other methods had to be developed that could
be computed during that time. One method was developed by William (W.S)
Gossett, a Chemist who worked for Guinness as their head brewer. Gossett
developed a distribution called the Student's T-distribution. His
process was to use the sample standard deviation, \(s\), as an
approximation of \(\sigma\). This means the test statistic is now
\(t=\frac{x-\mu}{\frac{s}{\sqrt{n}}}\). This new test statistic is
actually distributed as a Student's t-distribution, developed by W.S.
Gossett. There are some conditions that must be made for this formula to
be a Student's t-distribution. These are outlined in the following
theorem. Note: the t-distribution is called the Student's t-distribution
because that is the name he published under because he couldn't publish
under his own name due to his employer not wanting him to publish under
his own name. His employer by the way was Guinness and they didn't want
competitors knowing they had a chemist/statistician working for them. It
is not called the Student's t-distribution because it is only used by
students.

Theorem: If the following conditions are met

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  A random sample of size \(n\) is taken.
\item
  The distribution of the random variable is normal.
\end{enumerate}

Then the distribution of is a Student's t-distribution with \(n-1\)
degrees of freedom.

\textbf{Explanation of degrees of freedom}: Recall the formula for
sample standard deviation is \(\sqrt{{\frac{\sum{x-\bar{x}}}{n-1}}}\).
Notice the denominator is \(n-1\). This is the same as the degrees of
freedom. This is no accident. The reason the denominator and the degrees
of freedom are both comes from how the standard deviation is calculated.
First you take each data value and subtract \(\bar{x}\). If you add up
all of these new values, you will get 0. This must happen. Since it must
happen, the first \(n-1\) data values you have ``freedom of choice'',
but the nth data value, you have no freedom to choose. Hence, you have
\(n-1\) degrees of freedom. Another way to think about it is that if you
five people and five chairs, the first four people have a choice of
where they are sitting, but the last person does not. They have no
freedom of where to sit. Only \(n-1\) people have freedom of choice.

The Student's t-distribution is bell-shape that is more spread out than
the normal distribution. There are many \(t\)-distributions, one for
each different degree of freedom.

Figure~\ref{fig-tdistribution-graph} is of the normal distribution and
the Student's t-distribution for df = 1, df = 3, df=8, df=30.

\begin{figure}

\centering{

\includegraphics{One-Sample-Inference_files/figure-pdf/fig-tdistribution-graph-1.pdf}

}

\caption{\label{fig-tdistribution-graph}Typical Student t-Distributions}

\end{figure}%

As the degrees of freedom increases, the student's t-distribution looks
more like the normal distribution.

To find probabilities for the t-distribution, again technology can do
this for you. There are many technologies out there that you can use.

\subsection{\texorpdfstring{\textbf{Hypothesis Test for One Population
Mean
(t-Test)}}{Hypothesis Test for One Population Mean (t-Test)}}\label{hypothesis-test-for-one-population-mean-t-test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = random variable

\(\mu\) = mean of random variable

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\mu=\mu_o\) , where \(\mu_o\) is the known mean

\(H_a:\mu\ne\mu_o\), you can also use \textless{} or \textgreater, but
\(\ne\) is the more modern one to use.

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of size \(n\) is taken. Check: Describe the
  process taken to collect the sample.
\item
  State: The population of the random variable is normally distributed.
  Check: examine density graph and normal quantile plot. Note: The
  t-test is fairly robust to the condition if the sample size is large.
  This means that if this condition isn't met, but your sample size is
  quite large, then the results of the t-test are valid.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

On rStudio, the command is

t.test(\textasciitilde variable, data=data\_frame, mu=what\_Ho\_says)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject or fail to reject \(H_o\). The rule is:
if the p-value \(<\alpha\), then reject \(H_o\). If the p-value
\(\ge \alpha\), then fail to reject \(H_o\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

Note: if the conditions behind this test are not valid, then the
conclusions you make from the test are not valid. If you do not have a
random sample, that is your fault. Make sure the sample you take is as
random as you can make it following sampling techniques from chapter 1.
If the population of the random variable is not normal, then take a
larger sample. If you cannot afford to do that, or if it is not
logistically possible, then you do different tests called non-parametric
tests or you can try resampling. The advantage fo resampling is that you
don't need to know the under laying distribution of the random variable.

\subsection{Example: Test of the Mean Using One Sample
T-test}\label{example-test-of-the-mean-using-one-sample-t-test}

A random sample of 50 body mass index (BMI) were taken from the NHANES
Data frame Table~\ref{tbl-sample_NHANES_50}. The mean BMI of Australians
is 27.2 \(kg/m^2\). Is there evidence that Americans have a different
BMI from people in Australia. Test at the 5\% level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_NHANES\_50}\OtherTok{\textless{}{-}} \FunctionTok{sample\_n}\NormalTok{(NHANES, }\AttributeTok{size=}\DecValTok{50}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(sample\_NHANES\_50))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0051}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0177}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0215}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0076}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0177}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0177}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0139}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0114}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0177}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0202}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0152}}@{}}

\caption{\label{tbl-sample_NHANES_50}BMI of Americans}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SurveyYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AgeDecade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeMonths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MaritalStatus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HHIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HHIncomeMid
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Poverty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HomeRooms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HomeOwn
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Work
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HeadCirc
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BMI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMICatUnder20yrs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMI\_WHO
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Pulse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSysAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDiaAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Testosterone
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DirectChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TotChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diabetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DiabetesAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HealthGen
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysPhysHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysMentHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LittleInterest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Depressed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nPregnancies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nBabies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age1stBaby
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SleepHrsNight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SleepTrouble
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PhysActive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PhysActiveDays
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TVHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CompHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TVHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
CompHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alcohol12PlusYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SmokeNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SmokeAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marijuana
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeFirstMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RegularMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeRegMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HardDrugs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexEver
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartnLife
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SameSex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexOrientation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PregnantNow
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
52231 & 2009\_10 & female & 56 & 50-59 & 674 & White & NA & Some College
& Widowed & 35000-44999 & 40000 & 3.88 & 9 & Own & Working & 112.0 & NA
& NA & 163.7 & 41.79 & NA & 30.0\_plus & 64 & 114 & 70 & 118 & 66 & 112
& 70 & 116 & 70 & NA & 1.14 & 3.54 & 150 & 2.206 & NA & NA & No & NA &
Vgood & 0 & 10 & None & None & 2 & 2 & 20 & 7 & Yes & Yes & 5 & NA & NA
& NA & NA & No & NA & 0 & NA & No & Non-Smoker & NA & No & NA & No & NA
& No & Yes & 18 & 3 & 0 & No & Heterosexual & NA \\
70754 & 2011\_12 & female & 15 & 10-19 & NA & White & White & NA & NA &
55000-64999 & 60000 & 2.88 & 11 & Own & NA & 67.6 & NA & NA & 159.7 &
26.50 & OverWeight & 25.0\_to\_29.9 & 72 & 102 & 64 & 106 & 68 & 102 &
68 & 102 & 60 & 26.87 & 1.50 & 4.47 & 80 & 0.266 & NA & NA & No & NA &
Good & 30 & 15 & NA & NA & NA & NA & NA & NA & NA & No & NA & 2\_hr &
0\_to\_1\_hr & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA \\
52842 & 2009\_10 & female & 77 & 70+ & 934 & White & NA & College Grad &
NeverMarried & 25000-34999 & 30000 & 3.00 & 6 & Own & NotWorking & 68.6
& NA & NA & 143.3 & 33.41 & NA & 30.0\_plus & 72 & 159 & 0 & 160 & 0 &
160 & 0 & 158 & 0 & NA & NA & NA & 227 & 1.188 & NA & NA & No & NA &
Good & 0 & 0 & Several & None & NA & NA & NA & 8 & No & No & NA & NA &
NA & NA & NA & No & 1 & 6 & NA & No & Non-Smoker & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA \\
53773 & 2009\_10 & male & 51 & 50-59 & 613 & White & NA & High School &
Married & 10000-14999 & 12500 & 0.75 & 6 & Own & NotWorking & 137.1 & NA
& NA & 172.8 & 45.91 & NA & 30.0\_plus & 66 & 107 & 80 & 104 & 72 & 110
& 80 & 104 & 80 & NA & 1.32 & 5.43 & 65 & 0.542 & NA & NA & Yes & 45 &
NA & NA & NA & NA & NA & NA & NA & NA & 6 & Yes & Yes & 7 & NA & NA & NA
& NA & NA & NA & NA & Yes & Yes & Smoker & 9 & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA \\
61420 & 2009\_10 & male & 27 & 20-29 & 328 & White & NA & Some College &
Married & 25000-34999 & 30000 & 1.45 & 7 & Own & Working & 108.4 & NA &
NA & 189.4 & 30.22 & NA & 30.0\_plus & 66 & 121 & 66 & 122 & 70 & 124 &
64 & 118 & 68 & NA & 1.40 & 6.34 & 99 & 1.088 & NA & NA & No & NA &
Vgood & 2 & 7 & Several & Several & NA & NA & NA & 6 & No & No & NA & NA
& NA & NA & NA & Yes & 3 & 20 & No & Yes & Smoker & 14 & Yes & 14 & Yes
& 14 & No & Yes & 16 & 5 & 1 & No & Heterosexual & NA \\
62804 & 2011\_12 & male & 69 & 60-69 & NA & White & White & College Grad
& Married & 75000-99999 & 87500 & 5.00 & 9 & Own & NotWorking & 91.0 &
NA & NA & 172.0 & 30.80 & NA & 30.0\_plus & 54 & 122 & 72 & 128 & 76 &
118 & 72 & 126 & 72 & 425.46 & 1.29 & 3.52 & 50 & 0.676 & NA & NA & No &
NA & Good & 0 & 0 & None & None & NA & NA & NA & 8 & No & Yes & 1 &
3\_hr & 1\_hr & NA & NA & Yes & 1 & 260 & NA & No & Non-Smoker & NA & NA
& NA & NA & NA & No & Yes & 18 & 1 & NA & No & NA & NA \\

\end{longtable}

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-65}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = BMI of an American

\(\mu\) = mean BMI of Americans

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\mu=27.2\)

\(H_a:\mu\ne 27.2\)

level of significance \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  A random sample of 50 BMI levels was taken. Check: A random sample was
  taken from the NHANES data frame using r Studio
\item
  The population of BMI levels is normally distributed. Check:
\end{enumerate}

(ref:sample-NHANES-50-density-cap) Density Plot of BMI from NHANES
sample

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{BMI, }\AttributeTok{data=}\NormalTok{sample\_NHANES\_50, }\AttributeTok{title=}\StringTok{"Body Mass Index"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Body Mass Index"}\NormalTok{)}
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{BMI, }\AttributeTok{data=}\NormalTok{sample\_NHANES\_50, }\AttributeTok{title=}\StringTok{"Body Mass Index"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Body Mass Index"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{One-Sample-Inference_files/figure-pdf/fig-sample-NHANES-50-density-1.pdf}

}

\caption{\label{fig-sample-NHANES-50-density-1}Density Plot of BMI from
NHANES sample}

\end{figure}%

\begin{figure}[H]

\centering{

\includegraphics{One-Sample-Inference_files/figure-pdf/fig-sample-NHANES-50-density-2.pdf}

}

\caption{\label{fig-sample-NHANES-50-density-2}Density Plot of BMI from
NHANES sample}

\end{figure}%

The density plot looks somewhat skewed right and the normal quantile
plot looks somewhat linear. However, there doesn't seem to be strong
evidence that the sample comes from a population that is normally
distributed. However, since the sample is moderate to large, the
\(t\)-test is robust to this condition not being met. So the results of
the test are probably valid.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and \(p\)-value
\end{enumerate}

On rStudio, the command would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{BMI, }\AttributeTok{data=}\NormalTok{ sample\_NHANES\_50, }\AttributeTok{mu=}\FloatTok{27.2}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  BMI
t = -0.3069, df = 46, p-value = 0.7603
alternative hypothesis: true mean is not equal to 27.2
95 percent confidence interval:
 24.62355 29.09474
sample estimates:
mean of x 
 26.85915 
\end{verbatim}

The test statistic is the \(t\) in the output, the sample statistic is
the mean of \(x\) in the output, and the \(p\)-value is the \(p\)-value
is the output.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Since the \(p\)-value is not less than 5\%, then fail to reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is not enough evidence to support that Americans have a different
BMI from Australians.

Note: this is the same conclusion that was found when using resampling.
So the two method could give similar conclusions.

\subsection{Example: Test of the Mean Using One Sample
T-test}\label{example-test-of-the-mean-using-one-sample-t-test-1}

In 2011, the average life expectancy for a woman in Europe was 79.8
years. The data in Table~\ref{tbl-Expectancy} are the life expectancies
for all people in European countries (\textbackslash{}``WHO life
expectancy,'' 2013). The Table~\ref{tbl-Expectancy-male} filtered the
data frame for just males and just year 2000. The year 2000 was randomly
chosen as the year to use. Do the data indicate that men's life
expectancy is different from women's? Test at the 1\% level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Expectancy}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Life\_expectancy\_Europe.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Expectancy))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rlllr@{}}

\caption{\label{tbl-Expectancy}Life Expectancies for European Countries}

\tabularnewline

\toprule\noalign{}
year & WHO\_region & country & sex & expect \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1990 & Europe & Albania & Male & 67 \\
1990 & Europe & Albania & Female & 71 \\
1990 & Europe & Albania & Both sexes & 69 \\
2000 & Europe & Albania & Male & 68 \\
2000 & Europe & Albania & Female & 73 \\
2000 & Europe & Albania & Both sexes & 71 \\

\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Expectancy\_male}\OtherTok{\textless{}{-}} 
\NormalTok{  Expectancy }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{filter}\NormalTok{(sex}\SpecialCharTok{==}\StringTok{"Male"}\NormalTok{, year}\SpecialCharTok{==}\StringTok{"2000"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Expectancy\_male))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rlllr@{}}

\caption{\label{tbl-Expectancy-male}Life Expectancies of males in
European Countries in 2000}

\tabularnewline

\toprule\noalign{}
year & WHO\_region & country & sex & expect \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2000 & Europe & Albania & Male & 68 \\
2000 & Europe & Andorra & Male & 76 \\
2000 & Europe & Armenia & Male & 68 \\
2000 & Europe & Austria & Male & 75 \\
2000 & Europe & Azerbaijan & Male & 64 \\
2000 & Europe & Belarus & Male & 63 \\

\end{longtable}

\textbf{Code book for data frame Expectancy}

\textbf{Description} This data extract has been generated by the Global
Health Observatory of the World Health Organization. The data was
extracted on 2013-09-19 13:10:20.0.

This data frame contains the following columns:

year: year for life expectancies

WHO\_region: World Health Organizations designation for the location of
the country

country: country where the epectancies are from

sex: sex of the group that expectancies are calculated for

expect: average life expectancies of the different groups of the
different countries.

Source
http://apps.who.int/gho/athena/data/download.xsl?format=xml\&target=GHO/WHOSIS\_000001\&profile=excel\&filter=COUNTRY:*;SEX:*;REGION:EUR

References World Health Organization (WHO).

\subsubsection{\texorpdfstring{\textbf{Solution}}{Solution}}\label{solution-66}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = life expectancy for a European man

\(\mu\) = mean life expectancy for European men

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\mu=79.8\)

\(H_a:\mu\ne79.8\)

\(\alpha=0.01\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of 53 life expectancies of European men in 2000
  was taken.

  Check: The data is actually all of the life expectancies for every
  country that is considered part of Europe by the World Health
  Organization in the year 2000. Since the year 2000 was picked at
  random, then the sample is a random sample.
\item
  State: The distribution of life expectancies of European men in 2000
  is normally distributed.

  Check:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{expect, }\AttributeTok{data=}\NormalTok{Expectancy\_male, }\AttributeTok{title=}\StringTok{"Life Expectancies of Males in Europe in 2000"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Life expectancy"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{One-Sample-Inference_files/figure-pdf/fig-expanctancy-male-density-1.pdf}

}

\caption{\label{fig-expanctancy-male-density}Density Plot of Life
Expectancy of Males in Europe in 2000}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{expect, }\AttributeTok{data=}\NormalTok{Expectancy\_male, }\AttributeTok{title=}\StringTok{"Life Expectancies of Males in Europe in 2000"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{One-Sample-Inference_files/figure-pdf/fig-expanctancy-male-quantile-1.pdf}

}

\caption{\label{fig-expanctancy-male-quantile}Quantile Plot of Life
Expectancy of Males in Europe in 2000}

\end{figure}%

This sample does not appear to come from a population that is normally
distributed. This sample is moderate to large, so it is good that the
t-test is robust.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and \(p\)-value
\end{enumerate}

On rStudio, the command is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{expect, }\AttributeTok{data=}\NormalTok{Expectancy\_male, }\AttributeTok{mu=}\FloatTok{79.8}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  expect
t = -11.733, df = 52, p-value = 3.145e-16
alternative hypothesis: true mean is not equal to 79.8
95 percent confidence interval:
 69.11930 72.23919
sample estimates:
mean of x 
 70.67925 
\end{verbatim}

Sample statistic is 70.68 years, test statistic is \(t = -11.733\), and
\(p-value =3.14X10^{-16}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Since the p-value is less than 1\%, then reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is enough evidence to support that the mean life expectancy for
European men is different than the mean life expectancy for European
women of 79.8 years.

Note: if you want to conduct a hypothesis test with \(H_a:\mu>\mu_o\),
then the rStudio command would be

t.test(\textasciitilde variable, data=Data\_Frame, mu=number \(H_0\)
equals, alternative=``greater'')

If you want to conduct a hypothesis test with \(H_a:\mu<\mu_o\), then
the r Studio command would be

t.test(\textasciitilde variable, data=Data\_Frame, mu=number \(H_0\)
equals, alternative=``less'')

\subsection{Homework for One-Sample Test for the Mean
Section}\label{homework-for-one-sample-test-for-the-mean-section}

\textbf{In each problem show all steps of the hypothesis test. If some
of the conditions are not met, note that the results of the test may not
be correct and then continue the process of the hypothesis test.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The Kyoto Protocol was signed in 1997, and required countries to start
  reducing their carbon emissions. The protocol became enforceable in
  February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per
  capita. The Table~\ref{tbl-Emission} contains a random sample of CO2
  emissions in 2010 (CO2 emissions (metric tons per capita), 2018). Is
  there enough evidence to show that the mean CO2 emission is different
  in 2010 than in 2004? Test at the 1\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Emission }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/CO2\_emission.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Emission))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0197}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0181}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0148}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0099}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0099}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0099}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 118\tabcolsep) * \real{0.0099}}@{}}

\caption{\label{tbl-Emission}CO2 Emissions (in metric tons per capita)
in 2010}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
country
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1960
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1961
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1962
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1963
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1964
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1965
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1966
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1967
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1968
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1969
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1970
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1971
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1972
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1973
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1974
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1975
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1976
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1977
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1978
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1979
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1980
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1981
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1982
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1983
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1984
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1985
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1986
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1987
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1988
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1989
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1990
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1991
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1992
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1993
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1994
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1995
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1996
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1997
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1998
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1999
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2001
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2002
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2003
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2004
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2005
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2006
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2007
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2008
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2009
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2010
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2012
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2013
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2014
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2015
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2016
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2017
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2018
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Aruba & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
2.8683194 & 7.2351980 & 10.0261792 & 10.6347326 & 26.3745032 &
26.0461298 & 21.4425588 & 22.0007862 & 21.0362451 & 20.7719362 &
20.3183534 & 20.4268177 & 20.5876692 & 20.3115668 & 26.1948752 &
25.9340244 & 25.6711618 & 26.4204521 & 26.517293 & 27.2007078 &
26.9477260 & 27.8950228 & 26.2295527 & 25.9153221 & 24.6705289 &
24.5075162 & 13.1577223 & 8.353561 & 8.4100642 & NA & NA & NA & NA \\
Afghanistan & 0.0460567 & 0.0535888 & 0.0737208 & 0.0741607 & 0.0861736
& 0.1012849 & 0.1073989 & 0.1234095 & 0.1151425 & 0.0865099 & 0.1496515
& 0.1652083 & 0.1299956 & 0.1353666 & 0.1545032 & 0.1676124 & 0.1535579
& 0.1815222 & 0.1618942 & 0.1670664 & 0.1317829 & 0.1506147 & 0.1631039
& 0.2012243 & 0.2319613 & 0.2939569 & 0.2677719 & 0.2692296 & 0.2468233
& 0.2338822 & 0.2106434 & 0.1833636 & 0.0961966 & 0.0850871 & 0.0758065
& 0.0686399 & 0.0624346 & 0.0566423 & 0.0527632 & 0.0407225 & 0.0372348
& 0.0378461 & 0.0473773 & 0.0504813 & 0.038410 & 0.0517440 & 0.0624275 &
0.0838928 & 0.1517209 & 0.2383985 & 0.2899876 & 0.4064242 & 0.3451488 &
0.310341 & 0.2939464 & NA & NA & NA & NA \\
Angola & 0.1008353 & 0.0822038 & 0.2105315 & 0.2027373 & 0.2135603 &
0.2058909 & 0.2689414 & 0.1721017 & 0.2897181 & 0.4802340 & 0.6082236 &
0.5645482 & 0.7212460 & 0.7512399 & 0.7207764 & 0.6285689 & 0.4513535 &
0.4692212 & 0.6947369 & 0.6830629 & 0.6409664 & 0.6111351 & 0.5193546 &
0.5513486 & 0.5209829 & 0.4719028 & 0.4516189 & 0.5440851 & 0.4635083 &
0.4372955 & 0.4317436 & 0.4155308 & 0.4105229 & 0.4417211 & 0.2881191 &
0.7870325 & 0.7262335 & 0.4963612 & 0.4758152 & 0.5770829 & 0.5819615 &
0.5743161 & 0.7229589 & 0.5002254 & 1.001878 & 0.9857364 & 1.1050190 &
1.2031340 & 1.1850005 & 1.2344251 & 1.2440915 & 1.2526808 & 1.3302186 &
1.253776 & 1.2903068 & NA & NA & NA & NA \\
Albania & 1.2581949 & 1.3741860 & 1.4399560 & 1.1816811 & 1.1117420 &
1.1660990 & 1.3330555 & 1.3637463 & 1.5195513 & 1.5589676 & 1.7532399 &
1.9894979 & 2.5159144 & 2.3038974 & 1.8490067 & 1.9106336 & 2.0135846 &
2.2758764 & 2.5306250 & 2.8982085 & 1.9350583 & 2.6930239 & 2.6248568 &
2.6832399 & 2.6942914 & 2.6580154 & 2.6653562 & 2.4140608 & 2.3315985 &
2.7832431 & 1.6781067 & 1.3122126 & 0.7747249 & 0.7237903 & 0.6002037 &
0.6545371 & 0.6366253 & 0.4903651 & 0.5602714 & 0.9601644 & 0.9781747 &
1.0533042 & 1.2295407 & 1.4126972 & 1.376213 & 1.4124982 & 1.3025764 &
1.3223349 & 1.4843111 & 1.4956002 & 1.5785736 & 1.8037147 & 1.6929083 &
1.749211 & 1.9787633 & NA & NA & NA & NA \\
Andorra & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & 7.4673357 & 7.1824566 & 6.9120534 & 6.7360548 & 6.4942004
& 6.6620517 & 7.0650715 & 7.2397127 & 7.6607839 & 7.9754544 & 8.0192843
& 7.7869500 & 7.5906151 & 7.3157607 & 7.358625 & 7.2998719 & 6.7460521 &
6.5193871 & 6.4278100 & 6.1215799 & 6.1225947 & 5.8674102 & 5.9168840 &
5.901775 & 5.8329062 & NA & NA & NA & NA \\
Arab World & 0.6457359 & 0.6874654 & 0.7635736 & 0.8782377 & 1.0030533 &
1.1705403 & 1.2781736 & 1.3374436 & 1.5522420 & 1.7986689 & 1.8103078 &
2.0037220 & 2.1208746 & 2.4095329 & 2.2858907 & 2.1967827 & 2.5843424 &
2.6487624 & 2.7623331 & 2.8636143 & 3.0928915 & 2.9302350 & 2.7231544 &
2.8165670 & 2.9813539 & 3.0618504 & 3.2844996 & 3.1978064 & 3.2950428 &
3.2566742 & 3.0169588 & 3.2366449 & 3.4154849 & 3.6694456 & 3.6743582 &
3.4240095 & 3.3283037 & 3.1455322 & 3.3499672 & 3.3283411 & 3.7038571 &
3.6079561 & 3.6046128 & 3.7964674 & 4.068562 & 4.1856773 & 4.2857192 &
4.1171475 & 4.4089483 & 4.5620151 & 4.6368134 & 4.5594617 & 4.8377796 &
4.674925 & 4.8869875 & NA & NA & NA & NA \\

\end{longtable}

\textbf{Code book for data frame Emission}

\textbf{Description} Carbon dioxide emissions are those stemming from
the burning of fossil fuels and the manufacture of cement. They include
carbon dioxide produced during consumption of solid, liquid, and gas
fuels and gas flaring.

This data frame contains the following columns:

country: country around the world

y1960-y2018: weighted averages of CO2 emission for the years 1960
through 2018 in metric tons per capita

Source CO2 emissions (metric tons per capita). (n.d.). Retrieved July
18, 2019, from https://data.worldbank.org/indicator/EN.ATM.CO2E.PC

References Carbon Dioxide Information Analysis Center, Environmental
Sciences Division, Oak Ridge National Laboratory, Tennessee, United
States.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The amount of sugar in a Krispy Kream glazed donut is 10 g. Many
  people feel that cereal is a healthier alternative for children over
  glazed donuts. The Table~\ref{tbl-Sugar} contains the amount of sugar
  in a sample of cereal (breakfast cereal, 2019). Is there enough
  evidence to show that the mean amount of sugar in children's cereal is
  different than in a glazed donut? Test at the 5\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Sugar }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/cereal.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Sugar))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.1970}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.1136}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0455}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0379}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0682}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0606}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0303}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0530}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0455}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0379}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0455}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0455}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0758}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0303}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0530}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0606}}@{}}

\caption{\label{tbl-Sugar}Nutrition Amounts in Cereal}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
manf
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
calories
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
protein
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sodium
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fiber
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
carb
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sugar
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
shelf
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
potassium
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
vit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
serving
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
100\%\_Bran & Nabisco & adult & cold & 70 & 4 & 1 & 130 & 10.0 & 5.0 & 6
& 3 & 280 & 25 & 1 & 0.33 \\
100\%\_Natural\_Bran & Quaker\_Oats & adult & cold & 120 & 3 & 5 & 15 &
2.0 & 8.0 & 8 & 3 & 135 & 0 & 1 & -1.00 \\
All-Bran & Kelloggs & adult & cold & 70 & 4 & 1 & 260 & 9.0 & 7.0 & 5 &
3 & 320 & 25 & 1 & 0.33 \\
All-Bran\_with\_Extra\_Fiber & Kelloggs & adult & cold & 50 & 4 & 0 &
140 & 14.0 & 8.0 & 0 & 3 & 330 & 25 & 1 & 0.50 \\
Almond\_Delight & Ralston\_Purina & adult & cold & 110 & 2 & 2 & 200 &
1.0 & 14.0 & 8 & 3 & -1 & 25 & 1 & 0.75 \\
Apple\_Cinnamon\_Cheerios & General\_Mills & child & cold & 110 & 2 & 2
& 180 & 1.5 & 10.5 & 10 & 1 & 70 & 25 & 1 & 0.75 \\

\end{longtable}

\textbf{Code book for data frame Sugar}

\textbf{Description} Nutritional information about cereals.

This data frame contains the following columns:

name: the cereal brand

manf: manufacturer

age: whether the cereal is geared towards children or adults

type: whether the cereal is considered a hot or cold cereal

calories: the number of calories in the cereal (number)

protein: the amount of protein in a serving of the cereal (g)

fat: the amount of fat a serving of the cereal (g)

sodium: the amount of sodium in a serving of the cereal (mg)

fiber: the amount of fiber in a serving of the cereal (g)

carb: the amount of complex carbohydrates in a serving of the cereal (g)

sugars: the amount of sugar in a serving of the cereal (g)

display shelf: what shelf the cereal is on counting from the floor

potassium: the amount of potassium in a serving of the cereal (mg)

vit: the amount of vitamins and minerals in a serving of the cereal (0,
25, or 100)

weight: weight in ounces of one serving

serving: cups per serving

Source (n.d.). Retrieved July 18, 2019, from
https://www.idvbook.com/teaching-aid/data-sets/the-breakfast-cereal-data-set/
The Best Kids' Cereal. (n.d.). Retrieved July 18, 2019, from
https://www.ranker.com/list/best-kids-cereal/ranker-food

References Interactive Data Visualization Foundations, Techniques,
Applications (Matthew Ward \textbar{} Georges Grinstein \textbar{}
Daniel Keim)

A new data frame Table~\ref{tbl-Sugar_children} will need to be created
of just cereal for children. To create that use the following command in
rStudio

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Sugar\_children}\OtherTok{\textless{}{-}} 
\NormalTok{  Sugar}\SpecialCharTok{\%\textgreater{}\%} 
  \FunctionTok{filter}\NormalTok{(age}\SpecialCharTok{==}\StringTok{"child"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Sugar\_children))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.1846}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.1154}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0462}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0385}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0692}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0615}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0308}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0538}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0462}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0385}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0462}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0462}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0769}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0308}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0538}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 30\tabcolsep) * \real{0.0615}}@{}}

\caption{\label{tbl-Sugar_children}Nutrition Amounts in Children's
Cereal}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
manf
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
type
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
calories
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
protein
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fat
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sodium
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
fiber
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
carb
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
sugar
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
shelf
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
potassium
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
vit
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
serving
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Apple\_Cinnamon\_Cheerios & General\_Mills & child & cold & 110 & 2 & 2
& 180 & 1.5 & 10.5 & 10 & 1 & 70 & 25 & 1 & 0.75 \\
Apple\_Jacks & Kelloggs & child & cold & 110 & 2 & 0 & 125 & 1.0 & 11.0
& 14 & 2 & 30 & 25 & 1 & 1.00 \\
Bran\_Chex & Ralston\_Purina & child & cold & 90 & 2 & 1 & 200 & 4.0 &
15.0 & 6 & 1 & 125 & 25 & 1 & 0.67 \\
Cap'n'Crunch & Quaker\_Oats & child & cold & 120 & 1 & 2 & 220 & 0.0 &
12.0 & 12 & 2 & 35 & 25 & 1 & 0.75 \\
Cheerios & General\_Mills & child & cold & 110 & 6 & 2 & 290 & 2.0 &
17.0 & 1 & 1 & 105 & 25 & 1 & 1.25 \\
Cinnamon\_Toast\_Crunch & General\_Mills & child & cold & 120 & 1 & 3 &
210 & 0.0 & 13.0 & 9 & 2 & 45 & 25 & 1 & 0.75 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The FDA regulates that fish that is consumed is allowed to contain 1.0
  mg/kg of mercury. In Florida, bass fish were collected in 53 different
  lakes to measure the health of the lakes. The data frame of
  measurements from Florida lakes is in Table~\ref{tbl-Mercury} (NISER
  081107 ID Data, 2019). Do the data provide enough evidence to show
  that the fish in Florida lakes has different amounts of mercury than
  the allowable amount? Test at the 10\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Mercury}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/mercury.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Mercury))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0278}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1204}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1019}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0370}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0741}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1111}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0741}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1019}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0463}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0463}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1759}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0833}}@{}}

\caption{\label{tbl-Mercury}Health of Florida lake Fish}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
lake
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
alkalinity
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ph
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
calcium
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
chlorophyll
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
mercury
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
no.samples
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
min
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
max
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
X3\_yr\_standmercury
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age\_data
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Alligator & 5.9 & 6.1 & 3.0 & 0.7 & 1.23 & 5 & 0.85 & 1.43 & 1.53 &
1 \\
2 & Annie & 3.5 & 5.1 & 1.9 & 3.2 & 1.33 & 7 & 0.92 & 1.90 & 1.33 & 0 \\
3 & Apopka & 116.0 & 9.1 & 44.1 & 128.3 & 0.04 & 6 & 0.04 & 0.06 & 0.04
& 0 \\
4 & Blue\_Cypress & 39.4 & 6.9 & 16.4 & 3.5 & 0.44 & 12 & 0.13 & 0.84 &
0.44 & 0 \\
5 & Brick & 2.5 & 4.6 & 2.9 & 1.8 & 1.20 & 12 & 0.69 & 1.50 & 1.33 &
1 \\
6 & Bryant & 19.6 & 7.3 & 4.5 & 44.1 & 0.27 & 14 & 0.04 & 0.48 & 0.25 &
1 \\

\end{longtable}

\textbf{Code book for data frame Mercury}

\textbf{Description} Largemouth bass were studied in 53 different
Florida lakes to examine the factors that influence the level of mercury
contamination. Water samples were collected from the surface of the
middle of each lake in August 1990 and then again in March 1991. The pH
level, the amount of chlorophyll, calcium, and alkalinity were measured
in each sample. The average of the August and March values were used in
the analysis. Next, a sample of fish was taken from each lake with
sample sizes ranging from 4 to 44 fish. The age of each fish and mercury
concentration in the muscle tissue was measured. (Note: Since fish
absorb mercury over time, older fish will tend to have higher
concentrations). Thus, to make a fair comparison of the fish in
different lakes, the investigators used a regression estimate of the
expected mercury concentration in a three year old fish as the
standardized value for each lake. Finally, in 10 of the 53 lakes, the
age of the individual fish could not be determined and the average
mercury concentration of the sampled fish was used instead of the
standardized value. ( Reference: Lange, Royals, \& Connor. (1993))

This data frame contains the following columns:

ID: ID number

Lake: Name of lake

alkalinity: Alkalinity (mg/L as Calcium Carbonate)

pH: pH

calcium: calcium (mg/l)

chlorophyll: chlorophyll (mg/l)

mercury: Average mercury concentration (parts per million) in the muscle
tissue of the fish sampled from that lake

no.samples: How many fish were sampled from the lake

min: Minimum mercury concentration among the sampled fish

max: Maximum mercury concentration among the sampled fish

X3\_yr\_Standard\_mercury: Regression estimate of the mercury
concentration in a 3 year old fish from the lake (or = Avg Mercury when
age data was not available)

age\_data: Indicator of the availability of age data on fish sampled

Source Lange TL, Royals HE, Connor LL (1993) Influence of water
chemistry on mercury concentration in largemouth bass from Florida
lakes. Trans Am Fish Soc 122:74-84. Michael K. Saiki, Darell G. Slotton,
Thomas W. May, Shaun M. Ayers, and Charles N. Alpers (2000) Summary of
Total Mercury Concentrations in Fillets of Selected Sport Fishes
Collected during 2000--2003 from Lake Natoma, Sacramento County,
California (Raw data is included in appendix), U.S. Geological Survey
Data Series 103, 1-21. NISER 081107 ID Data. (n.d.). Retrieved July 18,
2019, from
http://wiki.stat.ucla.edu/socr/index.php/NISER\_081107\_ID\_Data

References NISER 081107 ID Data

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The data frame Table~\ref{tbl-Pulse} contains various variables about
  a person including their pulse rates before the subject exercised and
  after the subject ran in place for one minute. The mean pulse rate
  after running for 1 minute of females who do not drink is 97 beats per
  minute. Do the data show that the mean pulse rate of females who do
  drink alcohol is higher than the mean pulse rate of females who do not
  drink? Test at the 5\% level.
\end{enumerate}

\textbf{Code book for data frame Pulse} is below Table~\ref{tbl-Pulse}.

Create a data frame Table~\ref{tbl-Females} that contains only females
who drink alcohol. Then test the pulse after for woman who do drink
alcohol to the known value for females who do not drink alcohol. To
create a new data frame with just females who drink alcohol use the
following command, where the new name is Females:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Females}\OtherTok{\textless{}{-}}\NormalTok{ Pulse}\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{filter}\NormalTok{(gender}\SpecialCharTok{==}\StringTok{"female"}\NormalTok{, alcohol}\SpecialCharTok{==}\StringTok{"yes"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Females))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0482}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0843}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0964}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1084}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0482}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1566}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.1446}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 20\tabcolsep) * \real{0.0602}}@{}}

\caption{\label{tbl-Females}Pulse Rates Before and After Exercise of
Females who do drink Alcohol}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
smokes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alcohol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
exercise
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ran
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_before
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_after
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
year
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
165 & 60 & 19 & female & yes & yes & low & ran & 88 & 120 & 98 \\
163 & 47 & 23 & female & yes & yes & low & ran & 71 & 125 & 98 \\
173 & 57 & 18 & female & no & yes & moderate & sat & 86 & 88 & 93 \\
179 & 58 & 19 & female & no & yes & moderate & ran & 82 & 150 & 93 \\
167 & 62 & 18 & female & no & yes & high & ran & 96 & 176 & 93 \\
173 & 64 & 18 & female & no & yes & low & sat & 90 & 88 & 93 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The economic dynamism is an index of productive growth in dollars.
  Economic data for many countries are in Table~\ref{tbl-Economics}
  (SOCR Data 2008 World CountriesRankings, 2019). Countries that are
  considered high-income have a mean economic dynamism of 60.29.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Economics }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Economics\_country.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Economics))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0283}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0849}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0377}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1038}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0849}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1509}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0472}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0755}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0755}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0755}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0755}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0755}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0283}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0566}}@{}}

\caption{\label{tbl-Economics}Economic Data for Countries}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
incGroup
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
key
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
popGroup
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
key2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ED
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Edu
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
QOL
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
OA
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Relig
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
0 & Low & al & Albania & Small & Southern\_Europe & popS & 34.0862 &
81.0164 & 71.0244 & 67.9240 & 58.6742 & 57 & 39 \\
1 & Middle & dz & Algeria & Medium & North\_Africa & popM & 25.8057 &
74.8027 & 66.1951 & 60.9347 & 32.6054 & 85 & 95 \\
2 & Middle & ar & Argentina & Medium & South\_America & popM & 37.4511 &
69.8825 & 78.2683 & 68.1559 & 68.6647 & 46 & 66 \\
3 & High & au & Australia & Medium & Australia & popM & 71.4888 &
91.4802 & 95.1707 & 90.5729 & 90.9629 & 4 & 65 \\
4 & High & at & Austria & Small & Central\_Europe & popS & 53.9431 &
90.4578 & 90.3415 & 87.5630 & 91.2073 & 18 & 20 \\
5 & Low & az & Azerbaijan & Small & central\_Asia & popS & 53.6457 &
68.9880 & 58.9512 & 68.9572 & 40.0390 & 69 & 50 \\

\end{longtable}

\textbf{Code book for data frame Economics}

\textbf{Description} These data represent commonly accepted measures for
raking Countries on variety of factors which affect the country's
internal and external international perception of the country's rank
relative the to rest of the World.

This data frame contains the following columns:

id: Unique country identifier

incGroup: Income group: Low: GNI per capita \textless{}
\textbackslash\$3,946, Middle: \textbackslash\$3,946 \textless{} GNI per
capita \textless{} \textbackslash\$12,195, High: GNI per capita
\textgreater{} \textbackslash\$12,196

key: unique 2-letter country code

name: Country Name

popGroup: Population Group: Small: Population \textless{} 20 million,
Medium: 20 million \textless{} Population \textless{} 50 million, Large:
Population \textgreater{} 50 million

region: Relative geographic position of the Country

key2: Country Group Classification Label: world: All countries, g7: G7,
g20: G20, latin: Latin America \& Caribbean, eu: European Union,
centasia: Europe \& Central Asia, pacasia: East Asia \& Pacific, asean:
Asean, sasia: South Asia, mideast: Middle East \& North Africa, africa:
Sub-Saharan Africa, bric: Brazil, Russia, India and China (BRIC)

ED: Economic Dynamism: Index of Productive growth in dollars (GDP/capita
at PPP, Avg of GDP/capita growth rate over last ten years, GDP/capita
growth rate over next ten years, Economic Dynamism: Manufacturing
percent of GDP, Services percent of GDP percent (100=best, 0=worst).

Edu: Education/Literacy Rate (percent of population able to read and
write at a specified age)

HI: Health Index: The average number of years a person lives in full
health, taking into account years lived in less than full health

QOL: Quality of Life: Population percent living on \textless{}
\textbackslash\$2/day

PE: Political Environment: Freedom house rating of political
participation (qualitative assessment of voter participation/turn-out
for national elections, citizens engagement with politics)

OA: Overall country ranking taking all measures into account.

Relig: Religiosity of the Country as a percent (\%) of the population.

Source SOCR Data 2008 World CountriesRankings. (n.d.). Retrieved July
19, 2019, from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_2008\_World\_CountriesRankings\#SOCR\_Data\_-\_Ranking\_of\_the\_top\_100\_Countries\_based\_on\_Political.2C\_Economic.2C\_Health.2C\_and\_Quality-of-Life\_Factors

References SOCR Data 2008 World CountriesRankings, Amazon Web-Services
World's Best Countries.

Create a data frame that contains only middle income countries. Do the
data show that the mean economic dynamism of middle-income countries is
less than the mean for high-income countries? Test at the 5\% level. To
create a new data frame Table~\ref{tbl-Middle_economics} with just
middle income countries use the following command, where the new name is
Middle\_economics:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Middle\_economics}\OtherTok{\textless{}{-}} 
\NormalTok{  Economics }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(incGroup}\SpecialCharTok{==}\StringTok{"Middle"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Middle\_economics))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0286}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0381}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0952}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0857}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.1524}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0476}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0762}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0762}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0762}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0762}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0762}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0286}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 26\tabcolsep) * \real{0.0571}}@{}}

\caption{\label{tbl-Middle_economics}Economic Data for Middle income
Countries}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
Id
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
incGroup
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
key
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
popGroup
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
key2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
ED
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Edu
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
QOL
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PE
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
OA
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Relig
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & Middle & dz & Algeria & Medium & North\_Africa & popM & 25.8057 &
74.8027 & 66.1951 & 60.9347 & 32.6054 & 85 & 95 \\
2 & Middle & ar & Argentina & Medium & South\_America & popM & 37.4511 &
69.8825 & 78.2683 & 68.1559 & 68.6647 & 46 & 66 \\
7 & Middle & by & Belarus & Small & central\_Asia & popS & 51.9150 &
86.6155 & 66.1951 & 74.1467 & 34.0501 & 56 & 34 \\
10 & Middle & bw & Botswana & Small & Africa & popS & 43.6952 & 73.4608
& 34.8049 & 50.0875 & 72.6833 & 80 & 80 \\
11 & Middle & br & Brazil & Large & South\_America & popL & 47.8506 &
71.3735 & 71.0244 & 62.4238 & 67.4131 & 48 & 87 \\
12 & Middle & bg & Bulgaria & Small & Southern\_Europe & popS & 43.7178
& 82.2277 & 75.8537 & 73.1197 & 73.1686 & 38 & 50 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  In 1999, the average percentage of women who received prenatal care
  per country is 80.1\%. Table~\ref{tbl-Fert_prenatal} contains the
  percentage of woman receiving prenatal care in a sample of countries
  over several years. (births per woman), 2019). Do the data show that
  the average percentage of women receiving prenatal care in 2009
  (p2009) is different than in 1999? Test at the 5\% level.
\end{enumerate}

\textbf{Code book for Data frame Fert\_prenatal} is below
Table~\ref{tbl-Fert_prenatal}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Maintaining your balance may get harder as you grow older. A study was
  conducted to see how steady the elderly is on their feet. They had the
  subjects stand on a force platform and have them react to a noise. The
  force platform then measured how much they swayed forward and
  backward, and the data is in Table~\ref{tbl-Sway} (Maintaining Balance
  while Concentrating, 2019). Do the data show that the elderly sway
  more than the mean forward sway of younger people, which is 18.125 mm?
  Test at the 5\% level. Follow the filtering methods in other homework
  problems to create a data frame for only Elderly.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Sway }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/sway.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Sway))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-Sway}Sway (in mm) of Elderly Subjects}

\tabularnewline

\toprule\noalign{}
age & fbsway & sidesway \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Elderly & 19 & 14 \\
Elderly & 30 & 41 \\
Elderly & 20 & 18 \\
Elderly & 19 & 11 \\
Elderly & 29 & 16 \\
Elderly & 25 & 24 \\

\end{longtable}

\textbf{Code book for data frame Sway}

\textbf{Description} How difficult is it to maintain your balance while
concentrating? It is more difficult when you are older? Nine elderly (6
men and 3 women) and eight young men were subjects in an experiment.
Each subject stood barefoot on a ``force platform'' and was asked to
maintain a stable upright position and to react as quickly as possible
to an unpredictable noise by pressing a hand held button. The noise came
randomly and the subject concentrated on reacting as quickly as
possible. The platform automatically measured how much each subject
swayed in millimeters in both the forward/backward and the side-to-side
directions.

This data frame contains the following columns:

Age: Elderly or Young

FBSway: Sway in forward/backward direction

SideSwayy: Sway in side to side direction

Source Maintaining Balance while Concentrating. (n.d.). Retrieved July
19, 2019, from http://www.statsci.org/data/general/balaconc.html

References Teasdale, N., Bard, C., La Rue, J., and Fleury, M. (1993). On
the cognitive penetrability of posture control. Experimental Aging
Research 19, 1-13. The data was obtained from the DASL Data and Story
Line online database.

\bookmarksetup{startatroot}

\chapter{Estimation}\label{estimation}

In hypothesis tests, the purpose was to make a decision about a
parameter, in terms of it being greater than, less than, or not equal to
a value. But what if you want to actually know what the parameter is.
You need to do estimation. There are two types of estimation -\/- point
estimator and confidence interval. The American Statistical Association
(ASA) is recommending that confidence intervals are the process that
should be followed when analyzing data.

\section{Basics of Confidence
Intervals}\label{basics-of-confidence-intervals}

A point estimator is just the statistic that you have calculated
previously. As an example, when you wanted to estimate the population
mean, \(\mu\), the point estimator is the sample mean, \(\bar{x}\). To
estimate the population proportion, \(p\), you use the sample
proportion, \(\hat{p}\). In general, if you want to estimate any
population parameter, we will call it \(\theta\), you use the sample
statistic, \(\hat{\theta}\).

Point estimators are really easy to find, but they have some drawbacks.
First, if you have a large sample size, then the estimate is better. But
with a point estimator, you don't know what the sample size is. Also,
you don't know how accurate the estimate is. Both of these problems are
solved with a confidence interval.

\textbf{Confidence interval}: This is where you have an interval
surrounding your parameter, and the interval has a chance of being a
true statement. In general, a confidence interval looks like:
\(\hat{\theta}\pm E\), where \(\hat{\theta}\) is the point estimator and
\(E\) is the margin of error term that is added and subtracted from the
point estimator. Thus making an interval.

\subsection{Interpreting a confidence
interval:}\label{interpreting-a-confidence-interval}

The statistical interpretation is that the confidence interval has a
probability \(C=(1-\alpha)\) (where \(\alpha\) is the complement of the
confidence level) of containing the population parameter. As an example,
if you have a 95\% confidence interval of \$0.65 \textless{} p
\textless{} 0.73\$, then you would say, ``you are 95\% confident that
the interval 0.65 to 0.73 contains the true population proportion.''
This means that if you have 100 intervals, 95 of them will contain the
true proportion, and 5 will not. The wrong interpretation is that there
is a 95\% confidence that the true value of \(p\) will fall between 0.65
and 0.73. The reason that this interpretation is wrong is that the true
value is fixed out there somewhere. You are trying to capture it with
this interval. So this is the chance that your interval captures it, and
not that the true value falls in the interval.

There is also a real world interpretation that depends on the situation.
It is where you are telling people what numbers you found the parameter
to lie between. So your real world is where you tell what values your
parameter is between. There is no probability attached to this
statement. That probability is in the statistical interpretation.

The common probabilities used for confidence intervals are 90\%, 95\%,
and 99\%. These are known as the confidence level. The confidence level
and the alpha level are related. If you are conducting a hypothesis test
with \(H_a:\mu\ne \mu_o\), then the confidence level is \(C=1-\alpha\).
This is because the \(\alpha\) is both tails and the confidence level is
area between the two tails. As an example, for a hypothesis test
\(H_a:\mu\ne \mu_o\) with \(\alpha\) equal to 0.05, the confidence level
would be 0.95 or 95\%. If you have a hypothesis test with
\(H_a:\mu<\mu_o\), then your \(\alpha\) is only one tail of the curve.
Because of symmetry the other tail is also \(\alpha\). You have
\(2\alpha\) with both tails. So the confidence level, which is the area
between the two tails, is \(C-2\alpha\).

\subsection{Example: Stating the Statistical and Real World
Interpretations for a Confidence
Interval}\label{example-stating-the-statistical-and-real-world-interpretations-for-a-confidence-interval}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Suppose you have a 95\% confidence interval for the mean age a woman
  gets married in 2013 is \$26\textless\textbackslash mu\textless28\$.
  State the statistical and real world interpretations of this
  statement.
\item
  Suppose a 99\% confidence interval for the proportion of Americans who
  have tried marijuana as of 2013 is \$0.35\textless p\textless0.41\$.
  State the statistical and real world interpretations of this
  statement.
\end{enumerate}

\subsubsection{Solution}\label{solution-67}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Suppose you have a 95\% confidence interval for the mean age a woman
  gets married in 2013 is \$26\textless\textbackslash mu\textless28\$.
  State the statistical and real world interpretations of this
  statement.
\end{enumerate}

Statistical Interpretation: You are 95\% confident that the interval
contains the mean age in 2013 that a woman gets married.

Real World Interpretation: The mean age that a woman married in 2013 is
between 26 and 28 years of age.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Suppose a 99\% confidence interval for the proportion of Americans who
  have tried marijuana as of 2013 is \$0.35\textless p\textless0.41\$.
  State the statistical and real world interpretations of this
  statement.
\end{enumerate}

Statistical Interpretation: You are 99\% confident that the interval
contains the proportion of Americans who have tried marijuana as of
2013.

Real World Interpretation: The proportion of Americans who have tried
marijuana as of 2013 is between 0.35 and 0.41.

One last thing to know about confidence is how the sample size and
confidence level affect how wide the interval is. The following
discussion demonstrates what happens to the width of the interval as you
get more confident.

Think about shooting an arrow into the target. Suppose you are really
good at that and that you have a 90\% chance of hitting the bull's eye.
Now the bull's eye is very small. Since you hit the bull's eye
approximately 90\% of the time, then you probably hit inside the next
ring out 95\% of the time. You have a better chance of doing this, but
the circle is bigger. You probably have a 99\% chance of hitting the
target, but that is a much bigger circle to hit. You can see, as your
confidence in hitting the target increases, the circle you hit gets
bigger. The same is true for confidence intervals. This is demonstrated
in Image \textbackslash\#8.1.1.

\begin{figure}[H]

{\centering \includegraphics{effect_confidence_level.png}

}

\caption{Image \#8.1.1 Confidence Level Effect}

\end{figure}%

The higher level of confidence makes a wider interval. There's a trade
off between width and confidence level. You can be really confident
about your answer but your answer will not be very precise. Or you can
have a precise answer (small margin of error) but not be very confident
about your answer.

Now look at how the sample size affects the size of the interval.
Suppose Image \textbackslash\#8.1.2 represents confidence intervals
calculated on a 95\% interval. A larger sample size from a
representative sample makes the width of the interval narrower. This
makes sense. Large samples are closer to the true population so the
point estimate is pretty close to the true value.

\begin{figure}[H]

{\centering \includegraphics{effect_sample_size.png}

}

\caption{Image \#8.1.2 Effect of Sample size}

\end{figure}%

Now you know everything you need to know about confidence intervals
except for the actual formula. The formula depends on which parameter
you are trying to estimate. With different situations you will be given
the confidence interval for that parameter.

\subsection{Homework for Basics of Confidence Intervals
Section}\label{homework-for-basics-of-confidence-intervals-section}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Suppose you compute a confidence interval with a sample size of 25.
  What will happen to the confidence interval if the sample size
  increases to 50?
\item
  Suppose you compute a 95\% confidence interval. What will happen to
  the confidence interval if you increase the confidence level to 99\%?
\item
  Suppose you compute a 95\% confidence interval. What will happen to
  the confidence interval if you decrease the confidence level to 90\%?
\item
  Suppose you compute a confidence interval with a sample size of 100.
  What will happen to the confidence interval if the sample size
  decreases to 80?
\item
  A 95\% confidence interval is \(6353km< \mu<6384km\), where \(\mu\) is
  the mean diameter of the Earth. State the statistical interpretation.
\item
  A 95\% confidence interval is \(6353 km < \mu < 6384 km\), where
  \(\mu\) is the mean diameter of the Earth. State the real world
  interpretation.
\item
  In 2013, Gallup conducted a poll and found a 95\% confidence interval
  of \$0.52 \textless{} p \textless{} 0.60\$, where \emph{p} is the
  proportion of Americans who believe it is the government's
  responsibility for health care. Give the real world interpretation.
\item
  In 2013, Gallup conducted a poll and found a 95\% confidence interval
  of \$0.52 \textless{} p \textless{} 0.60\$, where \emph{p} is the
  proportion of Americans who believe it is the government's
  responsibility for health care. Give the statistical interpretation.
\end{enumerate}

\section{One-Sample Interval for the
Proportion}\label{one-sample-interval-for-the-proportion}

Suppose you want to estimate the population proportion, \emph{p}. As an
example you may be curious what proportion of students at your school
smoke. Or you could wonder what is the proportion of accidents caused by
teenage drivers who do not have a drivers' education class.

\subsection{Confidence Interval for One Population Proportion (1-Prop
Interval)}\label{confidence-interval-for-one-population-proportion-1-prop-interval}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = number of successes

\(p\) = proportion of successes

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the confidence interval
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A simple random sample of size \(n\) is taken. Check: describe
  how sample was taken.
\item
  State: The condition for the binomial distribution are satisfied.
  Check: argue that each condition has been met.
\item
  State: The sampling distribution of \(\hat{p}\) can be approximated by
  a normal distributed. check: To determine the sampling distribution of
  \(\hat{p}\) is normally distributed, you need to show that
  \(n*\hat{p}\ge5\) and , \(n*\hat{q}\ge5\) where \(\hat{q}=1-\hat{p}\).
  If this requirement is true, then the sampling distribution of
  \(\hat{p}\) is well approximated by a normal curve. (In reality this
  is not really true, since the correct condition deals with \(p\).
  However, in a confidence interval you do not know \(p\), so you must
  use \(\hat{p}\).)
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the sample statistic and the confidence interval
\end{enumerate}

This will be conducted using rStudio. The command is

prop.test(r, n, conf.level=C) \#type C as a decimal

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: In general this looks like, ``you are C\%
  confident that \(\hat{p}\pm E\) contains the true proportion.''
\item
  Real World Interpretation: This is where you state what interval
  contains the true proportion.
\end{enumerate}

\subsection{Example: Confidence Interval for the Population
Proportion}\label{example-confidence-interval-for-the-population-proportion}

A concern was raised in Australia that the percentage of deaths of
Aboriginal prisoners was higher than the percent of deaths of
non-Aboriginal prisoners, which is 0.27\%. A sample of six years
(1990-1995) of data was collected, and it was found that out of 14,495
Aboriginal prisoners, 51 died (\textbackslash{}``Indigenous deaths
in,\textbackslash{}'' 1996). Find a 95\% confidence interval for the
proportion of Aboriginal prisoners who died.

\subsubsection{Solution}\label{solution-68}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = number of Aboriginal prisoners who die

\(p\) = proportion of Aboriginal prisoners who die

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the confidence interval
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A simple random sample of 14,495 Aboriginal prisoners was
  taken. Check: The sample was not a random sample, since it was data
  from six years. It is the numbers for all prisoners in these six
  years, but the six years were not picked at random. Unless there was
  something special about the six years that were chosen, the sample is
  probably a representative sample. This condition is probably met.
\item
  State: The properties of the binomial experiment have been met. Check:
  There are 14,495 prisoners in this case. The prisoners are all
  Aboriginals, so you are not mixing Aboriginal with non-Aboriginal
  prisoners. There are only two outcomes, either the prisoner dies or
  doesn't. The chance that one prisoner dies over another may not be
  constant, but if you consider all prisoners the same, then it may be
  close to the same probability. Thus the properties of the binomial
  experiment are satisfied
\item
  State: The sampling distribution of \(\hat{p}\) can be approximated
  with a normal distribution. Check:
  \(\hat{p}*n=\frac{51}{14495}*14495=51\ge5\) and
  \(\hat{q}*n=\frac{14495-51}{14495}*14495=14444\ge5\). The sampling
  distribution of \(\hat{p}\) can be approximated with a normal
  distribution.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the sample statistic and the confidence interval
\end{enumerate}

The command in r Studio for a confidence interval for a proportion is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\DecValTok{51}\NormalTok{,}\DecValTok{14495}\NormalTok{, }\AttributeTok{conf.level =} \FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    1-sample proportions test with continuity correction

data:  51 out of 14495
X-squared = 14290, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5
95 percent confidence interval:
 0.002647440 0.004661881
sample estimates:
          p 
0.003518455 
\end{verbatim}

the 95\% confidence level is \(0.002647440<p<0.004661881\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: You are 95\% confident that the interval
  \(0.0026<p<0.0047\) contains the proportion of Aboriginal prisoners
  who have died in prison.
\item
  Real World Interpretation: The proportion of Aboriginal prisoners who
  died in prison is between 0.26\% and 0.47\%.
\end{enumerate}

\subsection{Example: Confidence Interval for the Population
Proportion}\label{example-confidence-interval-for-the-population-proportion-1}

A researcher who is studying the effects of income levels on
breastfeeding of infants hypothesizes that countries with a low income
level have a different rate of infant breastfeeding than higher income
countries. It is known that in Germany, considered a high-income country
by the World Bank, 22\% of all babies are breastfeed. In Tajikistan,
considered a low-income country by the World Bank, researchers found
that in a random sample of 500 new mothers that 125 were breastfeeding
their infant. Find a 90\% confidence interval of the proportion of
mothers in low-income countries who breastfeed their infants?

\subsubsection{Solution}\label{solution-69}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State you random variable and the parameter in words.
\end{enumerate}

\(x\) = number of woman who breastfeed in a low-income country

\(p\) = proportion of woman who breastfeed in a low-income country

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the confidence interval
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A simple random sample of 500 breastfeeding habits of woman in
  a low-income country was taken. Check: This was stated in the problem.
\item
  State: The properties of a Binomial Experiment have been met. Check:
  There were 500 women in the study. The women are considered identical,
  though they probably have some differences. There are only two
  outcomes, either the woman breastfeeds or she doesn't. The probability
  of a woman breastfeeding is probably not the same for each woman, but
  it is probably not very different for each woman. The conditions for
  the binomial distribution are satisfied
\item
  State: The sampling distribution of \(\hat{p}\) can be approximated
  with a normal distributed.
  Check:\(n*\hat{p}= 500*\frac{125}{500}=125\ge5\) and
  \(n*\hat{q}=500*\frac{500-125}{500}=375\ge5\), so the sampling
  distribution of \(\hat{p}\) is well approximated by a normal
  distribution.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic and confidence interval
\end{enumerate}

On rstudio, use the following command

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\DecValTok{125}\NormalTok{, }\DecValTok{500}\NormalTok{, }\AttributeTok{conf.level =}\NormalTok{ .}\DecValTok{90}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    1-sample proportions test with continuity correction

data:  125 out of 500
X-squared = 124, df = 1, p-value < 2.2e-16
alternative hypothesis: true p is not equal to 0.5
90 percent confidence interval:
 0.2185980 0.2841772
sample estimates:
   p 
0.25 
\end{verbatim}

90\% confidence interval for \(p\) is \(0.2185980<p<0.2841772\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: You are 90\% confident that
  \(0.2185980<p<0.2841772\) contains the proportion of women in
  low-income countries who breastfeed their infants.
\item
  Real World Interpretation: The proportion of women in low-income
  countries who breastfeed their infants is between 0.219 and 0.284.
\end{enumerate}

\subsection{Homework for One-Sample Interval for the Proportion
Section}\label{homework-for-one-sample-interval-for-the-proportion-section}

\textbf{In each problem show all steps of the confidence interval. If
some of the conditions are not met, note that the results of the
interval may not be correct and then continue the process of the
confidence interval.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The Arizona Republic/Morrison/Cronkite News poll published on Monday,
  October 20, 2016, found 390 of the registered voters surveyed favor
  Proposition 205, which would legalize marijuana for adults. The
  statewide telephone poll surveyed 779 registered voters between
  Oct.~10 and Oct.~15. (Sanchez, 2016) Find a 99\% confidence interval
  for the proportion of Arizona's who supported legalizing marijuana for
  adults.
\item
  In November of 1997, Australians were asked if they thought
  unemployment would increase. At that time 284 out of 631 said that
  they thought unemployment would increase (\textbackslash{}``Morgan
  gallup poll,\textbackslash{}'' 2013). Estimate the proportion of
  Australians in November 1997 who believed unemployment would increase
  using a 95\% confidence interval?
\item
  According to the February 2008 Federal Trade Commission report on
  consumer fraud and identity theft, Arkansas had 1,601 complaints of
  identity theft out of 3,482 consumer complaints
  (\textbackslash{}``Consumer fraud and,\textbackslash{}'' 2008).
  Calculate a 90\% confidence interval for the proportion of identity
  theft in Arkansas.
\item
  According to the February 2008 Federal Trade Commission report on
  consumer fraud and identity theft, Alaska had 321 complaints of
  identity theft out of 1,432 consumer complaints
  (\textbackslash{}``Consumer fraud and,\textbackslash{}'' 2008).
  Calculate a 90\% confidence interval for the proportion of identity
  theft in Alaska.
\item
  In 2013, the Gallup poll asked 1,039 American adults if they believe
  there was a conspiracy in the assassination of President Kennedy, and
  found that 634 believe there was a conspiracy
  (\textbackslash{}``Gallup news service,\textbackslash{}'' 2013).
  Estimate the proportion of American's who believe in this conspiracy
  using a 98\% confidence interval.
\item
  In 2008, there were 507 children in Arizona out of 32,601 who were
  diagnosed with Autism Spectrum Disorder (ASD)
  (\textbackslash{}``Autism and developmental,\textbackslash{}'' 2008).
  Find the proportion of ASD in Arizona with a confidence level of 99\%.
\end{enumerate}

\section{One-Sample Interval for the
Mean}\label{one-sample-interval-for-the-mean}

Suppose you want to estimate the mean height of Americans, or you want
to estimate the mean salary of college graduates. A confidence interval
for the mean would be the way to estimate these means.

\subsection{Confidence Interval for One Population Mean
(t-Interval)}\label{confidence-interval-for-one-population-mean-t-interval}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = random variable

\(\mu\) = mean of random variable

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the confidence interval
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of size \(n\) is taken. Check: describe how the
  sample was collected.
\item
  State: The population of the random variable is normally distributed.
  Check: look at density plot and normal quantile plot. Note: though the
  t-test is fairly robust to the condition if the sample size is large.
  This means that if this condition isn't met, but your sample size is
  quite large, then the results of the t-test are valid.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the sample statistic and confidence interval
\end{enumerate}

Use rStudio to find the confidence interval. The command is

t.test(\textasciitilde variable, data= Data\_Frame, conf.level=C) \#type
C as a decimal

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: In general this looks like, ``You are C\%
  confident that the interval contains the true mean.''
\item
  Real World Interpretation: This is where you state what interval
  contains the true mean.
\end{enumerate}

\subsection{Example: Confidence Interval for the Population
Mean}\label{example-confidence-interval-for-the-population-mean}

A random sample of 50 body mass index (BMI) were taken from the NHANES
Data frame Table~\ref{tbl-NHANES_50}. Estimate the mean BMI of Americans
at the 95\% level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sample\_NHANES\_50}\OtherTok{\textless{}{-}} \FunctionTok{sample\_n}\NormalTok{(NHANES, }\AttributeTok{size=}\DecValTok{50}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(sample\_NHANES\_50))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0113}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0050}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0176}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0151}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0151}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0113}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0214}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0075}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0113}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0113}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0113}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0151}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0201}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0201}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0176}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0176}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0201}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0201}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0151}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0113}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0113}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0138}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0113}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0176}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0164}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0151}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0126}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0088}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0201}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0101}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0189}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 150\tabcolsep) * \real{0.0151}}@{}}

\caption{\label{tbl-NHANES_50}BMI of Americans}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
ID
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SurveyYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
AgeDecade
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeMonths
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Race3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Education
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
MaritalStatus
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HHIncome
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HHIncomeMid
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Poverty
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HomeRooms
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HomeOwn
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Work
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
HeadCirc
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BMI
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMICatUnder20yrs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
BMI\_WHO
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Pulse
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSysAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDiaAve
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPSys3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
BPDia3
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Testosterone
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DirectChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TotChol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow1
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineVol2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
UrineFlow2
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Diabetes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DiabetesAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HealthGen
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysPhysHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
DaysMentHlthBad
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
LittleInterest
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Depressed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nPregnancies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
nBabies
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
Age1stBaby
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SleepHrsNight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SleepTrouble
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PhysActive
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
PhysActiveDays
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
TVHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
CompHrsDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
TVHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
CompHrsDayChild
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Alcohol12PlusYr
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholDay
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AlcoholYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SmokeNow
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Smoke100n
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SmokeAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Marijuana
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeFirstMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
RegularMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
AgeRegMarij
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
HardDrugs
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexEver
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexAge
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartnLife
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
SexNumPartYear
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SameSex
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
SexOrientation
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
PregnantNow
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
66944 & 2011\_12 & male & 72 & 70+ & NA & White & White & College Grad &
Married & 25000-34999 & 30000 & 2.15 & 10 & Own & NotWorking & 86.6 & NA
& NA & 171.8 & 29.30 & NA & 25.0\_to\_29.9 & 64 & 153 & 59 & 148 & 58 &
154 & 58 & 152 & 60 & 331.92 & 1.68 & 5.59 & 82 & 0.385 & NA & NA & Yes
& 60 & Vgood & 0 & 0 & None & None & NA & NA & NA & 8 & No & No & NA &
More\_4\_hr & 0\_to\_1\_hr & NA & NA & Yes & 2 & 312 & No & Yes & Smoker
& 16 & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
58234 & 2009\_10 & male & 53 & 50-59 & 640 & White & NA & College Grad &
Married & more 99999 & 100000 & 5.00 & 8 & Own & Working & 69.6 & NA &
NA & 179.7 & 21.55 & NA & 18.5\_to\_24.9 & 96 & 125 & 72 & 128 & 72 &
126 & 72 & 124 & 72 & NA & 1.76 & 3.78 & 60 & 1.364 & NA & NA & Yes & 21
& Fair & 0 & 0 & None & None & NA & NA & NA & 7 & No & Yes & 5 & NA & NA
& NA & NA & Yes & 1 & 364 & NA & No & Non-Smoker & NA & No & NA & No &
NA & No & Yes & 21 & 1 & 1 & No & Heterosexual & NA \\
64654 & 2011\_12 & male & 44 & 40-49 & NA & White & White & High School
& Married & 35000-44999 & 40000 & 2.31 & 5 & Rent & Working & 104.2 & NA
& NA & 181.7 & 31.60 & NA & 30.0\_plus & 70 & 118 & 78 & 116 & 78 & 116
& 78 & 120 & 78 & 484.92 & 0.96 & 5.64 & 125 & 0.448 & NA & NA & No & NA
& Good & 5 & 3 & None & None & NA & NA & NA & 7 & No & Yes & NA &
More\_4\_hr & 1\_hr & NA & NA & Yes & 2 & 1 & Yes & Yes & Smoker & 28 &
No & NA & No & NA & No & Yes & 14 & 7 & 1 & No & Heterosexual & NA \\
57702 & 2009\_10 & female & 51 & 50-59 & 617 & Black & NA & 9 - 11th
Grade & Divorced & NA & NA & NA & 7 & Rent & Working & 78.3 & NA & NA &
172.9 & 26.19 & NA & 25.0\_to\_29.9 & 64 & 118 & 67 & 126 & 68 & 120 &
66 & 116 & 68 & NA & 1.68 & 4.09 & 275 & 1.627 & NA & NA & No & NA &
Excellent & 0 & 0 & None & None & 4 & 4 & 20 & 8 & No & Yes & 3 & NA &
NA & NA & NA & No & NA & NA & NA & No & Non-Smoker & NA & No & NA & No &
NA & No & No & NA & 0 & 0 & No & NA & NA \\
59427 & 2009\_10 & male & 54 & 50-59 & 658 & White & NA & 9 - 11th Grade
& Married & 75000-99999 & 87500 & 3.40 & 5 & Own & NotWorking & 78.5 &
NA & NA & 191.2 & 21.47 & NA & 18.5\_to\_24.9 & 50 & 143 & 69 & 150 & 72
& 144 & 72 & 142 & 66 & NA & 0.78 & 5.69 & 78 & 0.619 & NA & NA & No &
NA & Good & 5 & 0 & None & None & NA & NA & NA & 4 & Yes & No & NA & NA
& NA & NA & NA & Yes & NA & 0 & Yes & Yes & Smoker & 21 & No & NA & No &
NA & No & Yes & 17 & 5 & 3 & No & Heterosexual & NA \\
57812 & 2009\_10 & female & 31 & 30-39 & 379 & White & NA & College Grad
& NeverMarried & 55000-64999 & 60000 & 5.00 & 5 & Own & Working & 64.5 &
NA & NA & 166.9 & 23.16 & NA & 18.5\_to\_24.9 & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & 1.73 & 4.27 & 350 & 2.713 & NA & NA & No & NA &
Good & 2 & 4 & None & Several & NA & NA & NA & 6 & No & No & NA & NA &
NA & NA & NA & Yes & 2 & 104 & NA & No & Non-Smoker & NA & Yes & 21 & No
& NA & No & Yes & 16 & 7 & 1 & No & Heterosexual & No \\

\end{longtable}

\subsubsection{Solution}\label{solution-70}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = BMI of an American

\(\mu\) = mean BMI of Americans

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the confidence interval
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  A random sample of 50 BMI levels was taken. Check: A random sample was
  taken from the NHANES data frame using r Studio
\item
  The population of BMI levels is normally distributed. Check:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{BMI, }\AttributeTok{data=}\NormalTok{sample\_NHANES\_50, }\AttributeTok{title=}\StringTok{"BMI of an American"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Body Mass Index"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Estimation_files/figure-pdf/fig-BMI-density-1.pdf}

}

\caption{\label{fig-BMI-density}Density Plot of BMI from NHANES sample}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{BMI, }\AttributeTok{data=}\NormalTok{sample\_NHANES\_50, }\AttributeTok{title=}\StringTok{"BMI of an American"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Estimation_files/figure-pdf/fig-BMI-qq-1.pdf}

}

\caption{\label{fig-BMI-qq}Normal quantile Plot of BMI from NHANES
sample}

\end{figure}%

The density plot looks somewhat skewed right and the normal quantile
plot looks somewhat linear. There doesn't seem to be strong evidence
that the sample comes from a population that is normally distributed.
However, since the sample is moderate to large, the t-test is robust to
this condition not being met. So the results of the test are probably
valid.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic and confidence interval
\end{enumerate}

On r Studio, the command would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{BMI, }\AttributeTok{data=}\NormalTok{ sample\_NHANES\_50, }\AttributeTok{conf.level=}\FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  BMI
t = 30.684, df = 48, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 24.77401 28.24844
sample estimates:
mean of x 
 26.51122 
\end{verbatim}

The sample statistic is the mean of \(x\) in the output, and confidence
interval is under the words 95 percent confidence interval.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: You are 95\% confident that
  \(24.87190<\mu<28.71422\) contains the mean BMI of Americans.
\item
  Real World Interpretation: The mean BMI of Americans is between 24.87
  and 28.71 \(kg/m^2\).
\end{enumerate}

Notice that in example the chapter 7, you were asked if the mean BMI of
Americans was different from Australians' mean BMI of 27.2 \(kg/m^2\).
The interval that
\hyperref[example-confidence-interval-for-the-population-mean-1]{Example:
Confidence Interval for the Population Mean} calculated does contain the
value of 27.2. So you can't say that Americans' mean BMI and
Australians' mean BMI are different.This means that you can just use
confidence intervals and not conduct hypothesis tests at all if you
prefer.

Note: When creating this book, the random samples may change. So the
answers may be different from what is said in the interpretations. This
shows sampling variability, so it was not adjusted to show that this
could happen.

\subsection{Example: Confidence Interval for the Population
Mean}\label{example-confidence-interval-for-the-population-mean-1}

The data in Table~\ref{tbl-Expectancy} are the life expectancies for all
people in European countries (\textbackslash{}``WHO life
expectancy,\textbackslash{}'' 2013). The data in
Table~\ref{tbl-Expectancy-male} filtered the data frame for just males
and just year 2000. The year 2000 was randomly chosen as the year to
use. Estimate the mean life expectancy for a man in Europe at the 99\%
level.

Code book for data frame Expectancy is below Table~\ref{tbl-Expectancy}.

\subsubsection{Solution}\label{solution-71}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variable and the parameter in words.
\end{enumerate}

\(x\) = life expectancy for a European man

\(\mu\) = mean life expectancy for European men

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the confidence interval
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of 53 life expectancies of European men in 2000
  was taken.

  Check: The data is actually all of the life expectancies for every
  country that is considered part of Europe by the World Health
  Organization in the year 2000. Since the year 2000 was picked at
  random, then the sample is a random sample.
\item
  State: The distribution of life expectancies of European men in 2000
  is normally distributed.

  Check:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{expect, }\AttributeTok{data=}\NormalTok{Expectancy\_male, }\AttributeTok{title=}\StringTok{"Life Expectancy of a male"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Life Expectancy of a Male"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Estimation_files/figure-pdf/fig-expanctancy-male-density1-1.pdf}

}

\caption{\label{fig-expanctancy-male-density1}Density Plot of Life
Expectancy of Males in Europe in 2000}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{expect, }\AttributeTok{data=}\NormalTok{Expectancy\_male, }\AttributeTok{title=}\StringTok{"Male Life Expectancy"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Estimation_files/figure-pdf/fig-expanctancy-male-quantile1-1.pdf}

}

\caption{\label{fig-expanctancy-male-quantile1}Quantile Plot of Life
Expectancy of Males in Europe in 2000}

\end{figure}%

This sample does not appear to come from a population that is normally
distributed. This sample is moderate to large, so it is good that the
t-test is robust.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the sample statistic and confidence interval
\end{enumerate}

On rStudio, the command would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{expect, }\AttributeTok{data=}\NormalTok{Expectancy\_male, }\AttributeTok{conf.level=}\FloatTok{0.99}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  expect
t = 90.919, df = 52, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 0
99 percent confidence interval:
 68.60071 72.75778
sample estimates:
mean of x 
 70.67925 
\end{verbatim}

Sample statistic is 70.68 years, and the confidence interval is
\(68.60071<\mu<72.75778\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: You are 99\% confident that
  \(68.60071<\mu<72.75778\) contains the mean life expectancy of
  European men.
\item
  Real World Interpretation: The mean life expectancy of European men is
  between 68.60 and 72.76 years.
\end{enumerate}

\subsection{Homework for One-Sample Interval for the Mean
Section}\label{homework-for-one-sample-interval-for-the-mean-section}

\textbf{In each problem show all steps of the confidence interval. If
some of the conditions are not met, note that the results of the
interval may not be correct and then continue the process of the
confidence interval.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The Kyoto Protocol was signed in 1997, and required countries to start
  reducing their carbon emissions. The protocol became enforceable in
  February 2005. Table~\ref{tbl-Emission} contains a random sample of
  CO2 emissions in 2010 (CO2 emissions (metric tons per capita), 2018).
  Find a 99\% confidence interval for the mean CO-2 emissions in 2010.
\end{enumerate}

Code book for data frame Emission is below Table~\ref{tbl-Emission}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  The amount of sugar in a Krispy Kream glazed donut is 10 g. Many
  people feel that cereal is a healthier alternative for children over
  glazed donuts. Table~\ref{tbl-Sugar} contains the amount of sugar in a
  sample of cereal that is geared towards children (breakfast cereal,
  2019). Estimate the mean amount of sugar in children's cereal at the
  95\% confidence level.
\end{enumerate}

Code book for data frame Sugar is below Table~\ref{tbl-Sugar}.

A new data frame will need to be created of just cereal for children. It
is Table~\ref{tbl-Sugar_children}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The FDA regulates that fish that is consumed is allowed to contain 1.0
  mg/kg of mercury. In Florida, bass fish were collected in 53 different
  lakes to measure the health of the lakes. The data frame of
  measurements from Florida lakes is in Table~\ref{tbl-Mercury} (NISER
  081107 ID Data, 2019). Calculate with 90\% confidence the mean amount
  of mercury in fish in Florida lakes. Is there too much mercury in the
  fish in Florida?
\end{enumerate}

Code book for data frame Mercury is below Table~\ref{tbl-Mercury}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The data frame Table~\ref{tbl-Pulse} contains various variables about
  a person including their pulse rates before the subject exercised and
  after the subject ran in place for one minute. Estimate the mean pulse
  rate before exercise of females who do drink alcohol with a 95\% level
  of confidence?
\end{enumerate}

Code book for data frame Pulse below Table~\ref{tbl-Pulse}.

A new data frame with just females who drink alcohol is
Table~\ref{tbl-Females} from chapter 7.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The economic dynamism is an index of productive growth in dollars.
  Economic data for many countries are in Table~\ref{tbl-Economics}
  (SOCR Data 2008 World CountriesRankings, 2019).
\end{enumerate}

Code book for data frame Economics is below Table~\ref{tbl-Economics}.

A data frame that contains only middle income countries was created in
chapter 7 and is Table~\ref{tbl-Middle_economics}. Find a 95\%
confidence interval for the mean economic dynamism for middle income
countries.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Table~\ref{tbl-Fert_prenatal} contains the percentage of woman
  receiving prenatal care in a sample of countries over several years.
  (births per woman), 2019). Estimate the average percentage of women
  receiving prenatal care in 2009 (p2009) with a 95\% confidence
  interval?
\end{enumerate}

Code book for Data frame Fert\_prenatal is below
Table~\ref{tbl-Fert_prenatal}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Maintaining your balance may get harder as you grow older. A study was
  conducted to see how steady the elderly is on their feet. They had the
  subjects stand on a force platform and have them react to a noise. The
  force platform then measured how much they swayed forward and
  backward, and the data is in Table~\ref{tbl-Sway} (Maintaining Balance
  while Concentrating, 2019). Find the mean forward/backward sway of
  elderly person? Use a 95\% confidence level. Follow the filtering
  methods in other homework problems to create a data frame for only
  Elderly.
\end{enumerate}

Code book for data frame Sway is below Table~\ref{tbl-Sway}.

\bookmarksetup{startatroot}

\chapter{Two Sample Inference}\label{two-sample-inference}

Chapter 7 discussed methods of hypothesis testing about one-population
parameters. Chapter 8 discussed methods of estimating population
parameters from one sample using confidence intervals. This chapter will
look at methods of confidence intervals and hypothesis testing for two
populations. Since there are two populations, there are two random
variables, two means or proportions, and two samples (though with paired
samples you usually consider there to be one sample with pairs
collected). Examples of where you would do this are:

Testing and estimating the difference in testosterone levels of men
before and after they had children (Gettler, McDade, Feranil \& Kuzawa,
2011).

Testing the claim that a diet works by looking at the weight before and
after subjects are on the diet.

Estimating the difference in proportion of those who approve of
President Obama in the age group 18 to 26 year old and the 55 and over
age group.

All of these are examples of hypothesis tests or confidence intervals
for two populations. The methods to conduct these hypothesis tests and
confidence intervals will be explored in this chapter. As a reminder,
all hypothesis tests are the same process. The only thing that changes
is the formula that you use and the conditions. Confidence intervals are
also the same process, except that the formula is different.

\section{Two Proportions}\label{two-proportions}

There are times you want to test a claim about two population
proportions or construct a confidence interval estimate of the
difference between two population proportions. As with all other
hypothesis tests and confidence intervals, the process is the same
though the formulas and conditions are different.

\subsection{Hypothesis Test for Two Population Proportion (2-Prop
Test)}\label{hypothesis-test-for-two-population-proportion-2-prop-test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

\(x_1\) = number of successes from group 1

\(x_2\) = number of successes from group 2

\(p_1\) = proportion of successes in group 1

\(p_2\) = proportion of successes in group 2

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:p_1=p_2\)

\(H_a: p_1\ne p_2\). the \(\ne\) can be replaced with \(<\) or \(>\)
depending on the question.

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A simple random sample of size \(n_1\) is taken from population
  1, and a simple random sample of size \(n_2\) is taken from population
  2. Check: describe how each sample was collected.
\item
  State: The samples are independent. Check: describe why the two
  samples are independent.
\item
  State: The properties for the binomial distribution are satisfied for
  both populations. Check: describe how each population meets all the
  properties.
\item
  State: The sampling distribution of \(\hat{p_1}\) can be approximated
  as a normal distribution. Check: To determine the sampling
  distribution of \(\hat{p_1}\), you need to show that \(p_1*n_1\ge5\)
  and \(q_1*n_1\ge5\) where \(q_1=1-p_1\). If this requirement is true,
  then the sampling distribution of \(\hat{p_1}\) is well approximated
  by a normal curve. State: The sampling distribution of \(\hat{p_2}\)
  can be approximated as a normal distribution. Check: To determine the
  sampling distribution of \(\hat{p_2}\), you need to show that
  \(p_2*n_2\ge 5\) and \(q_2*n_2\ge 5\) where \(q_2=1-p_2\). If this
  requirement is true, then the sampling distribution of \(\hat{p_2}\)
  is well approximated by a normal curve. However, if you do not know
  \(p_1\) and \(p_2\), you will need to use \(\hat{p_1}\) and
  \(\hat{p_2}\) instead. This is not perfect, but it is the best you can
  do.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistics, test statistic, and p-value
\end{enumerate}

On rStudio, use the command

prop.test(c(x1,x2), c(n1, n2)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject or fail to reject \(H_o\). The rule is:
if the p-value \(<\alpha\), then reject \(H_o\). If the p-value
\(\ge \alpha\), then fail to reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

\subsection{Confidence Interval for the Difference Between Two
Population Proportion (2-Prop
Interval)}\label{confidence-interval-for-the-difference-between-two-population-proportion-2-prop-interval}

The confidence interval for the difference in proportions has the same
random variables and proportions and the same conditions as the
hypothesis test for two proportions. If you have already completed the
hypothesis test, then you do not need to state them again. If you
haven't completed the hypothesis test, then state the random variables
and proportions and state and check the conditions before completing the
confidence interval step.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Find the sample statistics and the confidence interval
\end{enumerate}

The confidence interval estimate of the difference is found using the
following command in r Studio:

prop.test(c(x1,x2), c(n1, n2), conf.level=C) Type C as a decimal

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Statistical Interpretation: In general this looks like, ``You are C\%
  confident that the confidence interval contains the true difference in
  proportions.''
\item
  Real World Interpretation: This is where you state how much more (or
  less) the first proportion is from the second proportion.
\end{enumerate}

\subsection{Example: Hypothesis Test for Two Population
Proportions}\label{example-hypothesis-test-for-two-population-proportions}

Do husbands cheat on their wives in a different proportion from the
proportion of wives cheat on their husbands (``Statistics brain,''
2013)? Suppose you take a group of 1000 randomly selected husbands and
find that 231 had cheated on their wives. Suppose in a group of 1200
randomly selected wives, 176 cheated on their husbands. Do the data show
that the proportion of husbands who cheat on their wives is different
from the proportion of wives who cheat on their husbands. Test at the
5\% level.

\subsubsection{Solution}\label{solution-72}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

\(x_1\) = number of husbands who cheat on his wife

\(x_2\) = number of wives who cheat on her husband

\(p_1\) = proportion of husbands who cheat on his wife

\(p_2\) = proportion of wives who cheat on her husband

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o: p_1=p_2\)

\(H_a: p_1\ne p_2\)

level of significance is \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for a hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A simple random sample of 1000 responses about cheating from
  husbands is taken. Check: This was stated in the problem. State: A
  simple random sample of 1200 responses about cheating from wives is
  taken. Check: This was stated in the problem.
\item
  State: The samples are independent. Check: The samples are
  independent. This is true since the samples involved different
  genders.
\item
  State: The properties of the binomial distribution are satisfied in
  both populations. Check: This is true since there are only two
  responses, there are a fixed number of trials, the probability of a
  success is the same, and the trials are independent.
\item
  State: The sampling distributions of \(\hat{p_1}\) and \(\hat{p_2}\)
  can be approximated with a normal distribution. Check: \(n_1*p_1\),
  \(n_2*p_2\), \(n_1*q_1\), and \(n_2*q_2\) are all greater than or
  equal to 5. So both sampling distributions of \(\hat{p_1}\) and
  \(\hat{p_2}\) can be approximated with a normal distribution.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistics, test statistic, and p-value
\end{enumerate}

On r use the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{231}\NormalTok{,}\DecValTok{176}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{1200}\NormalTok{))}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    2-sample test for equality of proportions with continuity correction

data:  c out of c231 out of 1000176 out of 1200
X-squared = 25.173, df = 1, p-value = 5.241e-07
alternative hypothesis: two.sided
95 percent confidence interval:
 0.05050705 0.11815962
sample estimates:
   prop 1    prop 2 
0.2310000 0.1466667 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Reject \(H_o\), since the p-value is less than 5\%.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is enough evidence to support that the proportion of husbands
having affairs is different from the proportion of wives having affairs.

\subsection{Example: Confidence Interval for Two Population
Proportions}\label{example-confidence-interval-for-two-population-proportions}

What is the difference in proportion that husbands cheat on their wives
than wives cheat on the husbands (``Statistics brain,'' 2013)? Suppose
you take a group of 1000 randomly selected husbands and find that 231
had cheated on their wives. Suppose in a group of 1200 randomly selected
wives, 176 cheated on their husbands. Estimate the difference in the
proportion of husbands and wives who cheat on their spouses using a 95\%
confidence level.

\subsubsection{Solution}\label{solution-73}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

These were stated in
\hyperref[example-hypothesis-test-for-two-population-proportions]{Example:
Hypothesis Test for Two Population Proportions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the confidence interval
\end{enumerate}

The conditions were stated and checked in
\hyperref[example-hypothesis-test-for-two-population-proportions]{Example:
Hypothesis Test for Two Population Proportions}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the sample statistics and the confidence interval
\end{enumerate}

On r use the command:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{prop.test}\NormalTok{(}\FunctionTok{c}\NormalTok{(}\DecValTok{231}\NormalTok{,}\DecValTok{176}\NormalTok{), }\FunctionTok{c}\NormalTok{(}\DecValTok{1000}\NormalTok{, }\DecValTok{1200}\NormalTok{), }\AttributeTok{conf.level =}\NormalTok{ .}\DecValTok{95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    2-sample test for equality of proportions with continuity correction

data:  c out of c231 out of 1000176 out of 1200
X-squared = 25.173, df = 1, p-value = 5.241e-07
alternative hypothesis: two.sided
95 percent confidence interval:
 0.05050705 0.11815962
sample estimates:
   prop 1    prop 2 
0.2310000 0.1466667 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: You are 95\% confident that
  \(0.05050705<p_1-p_2<0.11815962\) contains the true difference in
  proportions.
\item
  Real World Interpretation: The proportion of husbands who cheat on
  their wives is anywhere from 5.05\% to 11.82\% higher than the
  proportion of wives who cheat on their husband.
\end{enumerate}

\subsection{Homework for Two Proportions
Section}\label{homework-for-two-proportions-section}

\textbf{In each problem show all steps of the hypothesis test or
confidence interval. If some of the conditions are not met, note that
the results of the test or interval may not be correct and then continue
the process of the hypothesis test or confidence interval.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Many high school students take the AP tests in different subject
  areas. In 2007, of the 144,796 students who took the biology exam
  84,199 of them were female. In that same year, of the 211,693 students
  who took the calculus AB exam 102,598 of them were female (``AP exam
  scores,'' 2013). Is there enough evidence to show that the proportion
  of female students taking the biology exam is different than the
  proportion of female students taking the calculus AB exam? Test at the
  5\% level.
\item
  Many high school students take the AP tests in different subject
  areas. In 2007, of the 144,796 students who took the biology exam
  84,199 of them were female. In that same year, of the 211,693 students
  who took the calculus AB exam 102,598 of them were female (``AP exam
  scores,'' 2013). Estimate the difference in the proportion of female
  students taking the biology exam and female students taking the
  calculus AB exam using a 90\% confidence level.
\item
  Many high school students take the AP tests in different subject
  areas. In 2007, of the 211,693 students who took the calculus AB exam
  102,598 of them were female and 109,095 of them were male (``AP exam
  scores,'' 2013). Is there enough evidence to show that the proportion
  of female students taking the calculus AB exam is different from the
  proportion of male students taking the calculus AB exam? Test at the
  5\% level.
\item
  Many high school students take the AP tests in different subject
  areas. In 2007, of the 211,693 students who took the calculus AB exam
  102,598 of them were female and 109,095 of them were male (``AP exam
  scores,'' 2013). Estimate using a 90\% level the difference in
  proportion of female students taking the calculus AB exam versus male
  students taking the calculus AB exam.
\item
  Are there more children diagnosed with Autism Spectrum Disorder (ASD)
  in states that have larger urban areas over states that are mostly
  rural? In the state of Pennsylvania, a fairly urban state, there are
  245 eight year old diagnosed with ASD out of 18,440 eight year old
  evaluated. In the state of Utah, a fairly rural state, there are 45
  eight year old diagnosed with ASD out of 2,123 eight year old
  evaluated (``Autism and developmental,'' 2008). Is there enough
  evidence to show that the proportion of children diagnosed with ASD in
  Pennsylvania is different than the proportion in Utah? Test at the 1\%
  level.
\item
  Are there more children diagnosed with Autism Spectrum Disorder (ASD)
  in states that have larger urban areas over states that are mostly
  rural? In the state of Pennsylvania, a fairly urban state, there are
  245 eight year old diagnosed with ASD out of 18,440 eight year old
  evaluated. In the state of Utah, a fairly rural state, there are 45
  eight year old diagnosed with ASD out of 2,123 eight year old
  evaluated (``Autism and developmental,'' 2008). Estimate the
  difference in proportion of children diagnosed with ASD between
  Pennsylvania and Utah. Use a 98\% confidence level.
\item
  A child dying from an accidental poisoning is a terrible incident. Is
  it more likely that a male child will get into poison than a female
  child? To find this out, data was collected that showed that out of
  1830 children between the ages one and four who pass away from
  poisoning, 1031 were males and 799 were females (Flanagan, Rooney \&
  Griffiths, 2005). Do the data show that there is different proportion
  of male children dying of poisoning than female children? Test at the
  1\% level.
\item
  A child dying from an accidental poisoning is a terrible incident. Is
  it more likely that a male child will get into poison than a female
  child? To find this out, data was collected that showed that out of
  1830 children between the ages one and four who pass away from
  poisoning, 1031 were males and 799 were females (Flanagan, Rooney \&
  Griffiths, 2005). Compute a 99\% confidence interval for the
  difference in proportions of poisoning deaths of male and female
  children ages one to four.
\end{enumerate}

\section{Paired Samples for Two
Means}\label{paired-samples-for-two-means}

Are two populations the same? Is the average height of men taller than
the average height of women? Is the mean weight less after a diet than
before?

You can compare populations by comparing their means. You take a sample
from each population and compare the statistics.

Anytime you compare two populations you need to know if the samples are
independent or dependent. The formulas you use are different for
different types of samples.

If how you choose one sample has no effect on the way you choose the
other sample, the two samples are \textbf{independent}. The way to think
about it is that in independent samples, the observations from one
sample are overall different from the observations from the other
sample. This will mean that sample one has no affect on sample two. The
sample values from one sample are not related or paired with values from
the other sample.

If you choose the samples so that a measurement in one sample is paired
with a measurement from the other sample, the samples are
\textbf{dependent} or \textbf{matched} or \textbf{paired}. (Often a
before and after situation.) You want to make sure the there is a
meaning for pairing data values from one sample with a specific data
value from the other sample. One way to think about it is that in
dependent samples, the observations from one sample are the same
observations from the other sample, though there can be other reasons to
pair values. This makes the sample values from each sample paired.

In tidy data, remember each row is a unit of observation, and each
column is a variable. In paired samples, you would have two variables
that you are working with. In independent samples, you would have a
variable that distinguishes an observation from another observation. As
an example, in the Pulse data frame, consider the variables
pulse\_before and pulse\_after. Since they are measured off the same
observation, then comparing the two variables would be a paired samples
analysis. However, consider the pulse\_after and whether a person smokes
would be comparing the variable pulse\_after against the variable smokes
to see if smoking effects a person's pulse rate after exercise. In this
case, the observations would be different based on smoking yes or
smoking no. Consider the variable smoking to be the factor that one is
interested in seeing how it effects pulse rate in the data frame
Table~\ref{tbl-Pulse}.

\subsection{Example: Independent or Dependent
Samples}\label{example-independent-or-dependent-samples}

Determine if the following are dependent or independent samples.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Randomly choose 5 men and 6 women and compare their heights
\item
  Choose 10 men and weigh them. Give them a new diet drug and later
  weigh them again.
\item
  Take 10 people and measure the strength of their dominant arm and
  their non-dominant arm.
\end{enumerate}

\subsubsection{Solution}\label{solution-74}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  Randomly choose 5 men and 6 women and compare their heights

  Independent, since there is no reason that one value belongs to
  another. The units of observations are not the same for both samples.
  The units of observations are definitely different. A way to think
  about this is that the knowledge that a man is chosen in one sample
  does not give any information about any of the woman chosen in the
  other sample.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\item
  Choose 10 men and weigh them. Give them a new diet drug and later
  weigh them again.

  Dependent, since each person's before weight can be matched with their
  after weight. The units of observations are the same for both samples.
  A way to think about this is that the knowledge that a person weighs
  400 pounds at the beginning will tell you something about their weight
  after the diet drug.
\item
  Take 10 people and measure the strength of their dominant arm and
  their non-dominant arm.

  Dependent, since you can match the two arm strengths. The units of
  observations are the same for both samples. So the knowledge of one
  person's dominant arm strength will tell you something about the
  strength of their non-dominant arm.
\end{enumerate}

To analyze data when there are matched or paired samples, called
dependent samples, you conduct a paired t-test. Since the samples are
matched, you can find the difference between the values of the two
random variables.

\subsection{Hypothesis Test for Two Sample Paired
t-Test}\label{hypothesis-test-for-two-sample-paired-t-test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

\(x_1\) = random variable 1

\(x_2\) = random variable 2

\(\mu_1\) = mean of random variable 1

\(\mu_2\) = mean of random variable 2

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

The hypotheses would be

\(H_o:\mu_1=\mu_2\) or\(H_o:\mu_1-\mu_2=0\)

\(H_a:\mu_1\ne \mu_2\) or \(H_a:\mu_1-\mu_2\ne0\)

However, since you are finding the differences, then you can actually
think of \(\mu_1-\mu_2=\mu_d\).

So the hypotheses could become

\(H_o:\mu_d=0\)

\(H_a:\mu_d\ne 0\) Remember, you can replace \(\ne\) with \(<\) or
\(>\).

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of \(n\) pairs is taken. Check: state how the
  sample was collected.
\item
  Check: The population of the difference between random variables is
  normally distributed. Check: In this case the population you are
  interested in has to do with the differences that you find. It does
  not matter if each random variable is normally distributed. It is only
  important if the differences you find are normally distributed. Just
  as before, the t-test is fairly robust to the condition if the sample
  size is large. This means that if this condition isn't met, but your
  sample size is quite large, then the results of the t-test are valid.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

Realize that a paired test is a one sample t-test on the difference
between two variables. So you are running a one-sample t-test on a new
variable known as the difference variable. You need to create this
difference variable by creating a new data frame. This is done on
rStudio by doing the following command (The following shows how to
create the variable difference for pulse\_after-pulse\_before on the
data frame Pulse. Change the variables used and data frame used to your
data frame and variables):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pulse}\OtherTok{\textless{}{-}}
\NormalTok{  Pulse }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{difference=}\NormalTok{pulse\_after}\SpecialCharTok{{-}}\NormalTok{pulse\_before) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Pulse))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0426}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0851}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0957}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0426}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1383}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1277}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0532}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1170}}@{}}

\caption{\label{tbl-Pulse_Difference}Pulse Data frame with Difference
Column Added}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
smokes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alcohol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
exercise
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ran
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_before
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_after
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
year
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
difference
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
170 & 68 & 22 & male & yes & yes & moderate & sat & 70 & 71 & 93 & 1 \\
182 & 75 & 26 & male & yes & yes & moderate & sat & 80 & 76 & 93 & -4 \\
180 & 85 & 19 & male & yes & yes & moderate & ran & 68 & 125 & 95 &
57 \\
182 & 85 & 20 & male & yes & yes & low & sat & 70 & 68 & 95 & -2 \\
167 & 70 & 22 & male & yes & yes & low & sat & 92 & 84 & 96 & -8 \\
178 & 86 & 21 & male & yes & yes & low & sat & 76 & 80 & 98 & 4 \\

\end{longtable}

Notice rStudio added a new variable called difference to the data frame
Table~\ref{tbl-Pulse_Difference}. Now to conduct a paired t-test use the
rStudio command

t.test(\textasciitilde difference\_variable, data=Data\_Frame)

Note: if the \(H_a\) is \textless, then the command becomes

t.test(\textasciitilde difference\_variable, data=Data\_Frame,
alternative=``less'')

Similarly for \textgreater{} put alternative=``greater''

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject \(H_o\) or fail to reject \(H_o\). The
rule is: if the p-value \(<\alpha\), then reject \(H_o\). If the p-value
\(\ge\alpha\), then fail to reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

\subsection{Confidence Interval for Difference in Means from Paired
Samples
(t-Interval)}\label{confidence-interval-for-difference-in-means-from-paired-samples-t-interval}

The confidence interval for the difference in means has the same random
variables and means and the same conditions as the hypothesis test for
two paired samples. If you have already completed the hypothesis test,
then you do not need to state them again. If you haven't completed the
hypothesis test, then state the random variables and means, and state
and check the conditions before completing the confidence interval step.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Find the sample statistic and confidence interval. Again, you will
  need to create a new data frame with a difference variable. Then on
  rStudio the command is

  t.test(\textasciitilde difference\_variable, data=Data\_Frame,
  conf.level=C) Type C as a decimal
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Statistical Interpretation: In general this looks like, ``You are C\%
  confident that the statement contains the true mean difference.''
\item
  Real World Interpretation: This is where you state what interval
  contains the true mean difference.
\end{enumerate}

\subsection{Example: Hypothesis Test for Paired
Samples}\label{example-hypothesis-test-for-paired-samples}

Is the pulse rate after exercise different from the pulse rate before
exercise for a woman who drinks alcohol? Use the data frame
Table~\ref{tbl-Pulse}. Test at the 5\% level.

\textbf{Code book for data frame Pulse below Table~\ref{tbl-Pulse}.}

\subsubsection{Solution}\label{solution-75}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

\(x_1\) = pulse of a smoking woman who drinks alcohol after exercise

\(x_2\) = pulse of a smoking woman who drinks alcohol before exercise

\(\mu_1\) = mean pulse of a smoking woman who drinks alcohol after
exercise

\(\mu_2\) = mean pulse of a smoking woman who drinks alcohol after
exercise

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o: \mu_1=\mu_2\)

\(H_a: \mu_1\ne \mu_2\)

level of significance, \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of 110 pairs of pulse rates after and before
  exercise was taken. Check: The data frame says that the data was
  collected from students in classes for several years. Though this was
  not a random sample, it is probably a representative sample.
\item
  State: The population of the difference in after and before pulse
  rates is normally distributed. Check: To see if this is true, look at
  the density plot and the normal quantile plot for the difference
  between after and before. This variable must be created before the
  density plot and normal quantile plot can be created. The data frame
  Table~\ref{tbl-Females} is females who drink alcohol.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pulse\_female}\OtherTok{\textless{}{-}} 
\NormalTok{  Pulse }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(gender}\SpecialCharTok{==}\StringTok{"female"}\NormalTok{, alcohol}\SpecialCharTok{==}\StringTok{"yes"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Pulse\_female))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0426}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0851}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0957}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0426}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1383}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1277}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0532}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1170}}@{}}

\caption{\label{tbl-Females}Pulse Rates Before and After Exercise of
Females who do drink Alcohol with Difference}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
smokes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alcohol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
exercise
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ran
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_before
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_after
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
year
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
difference
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
165 & 60 & 19 & female & yes & yes & low & ran & 88 & 120 & 98 & 32 \\
163 & 47 & 23 & female & yes & yes & low & ran & 71 & 125 & 98 & 54 \\
173 & 57 & 18 & female & no & yes & moderate & sat & 86 & 88 & 93 & 2 \\
179 & 58 & 19 & female & no & yes & moderate & ran & 82 & 150 & 93 &
68 \\
167 & 62 & 18 & female & no & yes & high & ran & 96 & 176 & 93 & 80 \\
173 & 64 & 18 & female & no & yes & low & sat & 90 & 88 & 93 & -2 \\

\end{longtable}

Now mutate Table~\ref{tbl-Females} data frame to include a difference
variable.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Pulse\_female}\OtherTok{\textless{}{-}}
\NormalTok{  Pulse\_female }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{difference=}\NormalTok{pulse\_after}\SpecialCharTok{{-}}\NormalTok{pulse\_before) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Pulse\_female))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0426}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0745}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0851}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0957}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0426}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1383}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1277}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.0532}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 22\tabcolsep) * \real{0.1170}}@{}}

\caption{\label{tbl-Females_difference}Pulse Rates Before and After
Exercise of Females who do drink Alcohol with Difference}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedleft
height
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
weight
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
age
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
gender
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
smokes
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
alcohol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
exercise
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
ran
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_before
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
pulse\_after
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
year
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
difference
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
165 & 60 & 19 & female & yes & yes & low & ran & 88 & 120 & 98 & 32 \\
163 & 47 & 23 & female & yes & yes & low & ran & 71 & 125 & 98 & 54 \\
173 & 57 & 18 & female & no & yes & moderate & sat & 86 & 88 & 93 & 2 \\
179 & 58 & 19 & female & no & yes & moderate & ran & 82 & 150 & 93 &
68 \\
167 & 62 & 18 & female & no & yes & high & ran & 96 & 176 & 93 & 80 \\
173 & 64 & 18 & female & no & yes & low & sat & 90 & 88 & 93 & -2 \\

\end{longtable}

Using Table~\ref{tbl-Females_difference} create a density plot and
normal quantile plot on the difference variable.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{difference, }\AttributeTok{data=}\NormalTok{Pulse\_female, }\AttributeTok{title =} \StringTok{"Difference in Pulse Rates for Females who drink Alcohol"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Difference Between Before and After"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Two-Sample-Inference_files/figure-pdf/fig-pulse-female-density-1.pdf}

}

\caption{\label{fig-pulse-female-density}Density plot of differences in
pulse rates}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{difference, }\AttributeTok{data=}\NormalTok{Pulse\_female, }\AttributeTok{title =} \StringTok{"Difference in Pulse Rates for Females who drink Alcohol"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Two-Sample-Inference_files/figure-pdf/fig-pulse-female-quartile-1.pdf}

}

\caption{\label{fig-pulse-female-quartile}Normal Quantile Plot of
Differences in Pulse Rates}

\end{figure}%

The density plot is not symmetrical and the normal quantile plot on the
differences is not linear. So you cannot assume that the distribution of
the difference in pulse rates is normal. It is good that the t-test is
robust if there is a large sample. The sample is of size 110, so that
should be adequate to assume the conclusion is valid.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value On r Studio,
  use the command:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{difference, }\AttributeTok{data=}\NormalTok{Pulse\_female)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  difference
t = 4.1353, df = 26, p-value = 0.0003283
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 11.51152 34.26625
sample estimates:
mean of x 
 22.88889 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Since the p-value \textless{} 0.05, reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is enough evidence to support that there is a difference in pulse
rate before and after exercise of females who smoke.

\subsection{Example: Hypothesis Test for Paired
Samples}\label{example-hypothesis-test-for-paired-samples-1}

The New Zealand Air Force purchased a batch of flight helmets. They then
found out that the helmets didn't fit. In order to make sure that they
order the correct size helmets, they measured the head size of recruits.
To save money, they wanted to use cardboard calipers, but were not sure
if they will be accurate enough. So they took 18 recruits and measured
their heads with the cardboard calipers and also with metal calipers.
The data frame is in Table~\ref{tbl-Helmet} (Helmet Sizes for New
Zealand Airforce, 2019). Do the data provide enough evidence to show
that there is a difference in measurements between the cardboard and
metal calipers? Use a 5\% level of significance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Helmet}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/helmet.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Helmet))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rr@{}}

\caption{\label{tbl-Helmet}Helmet Head Measurments}

\tabularnewline

\toprule\noalign{}
Cardboard & Metal \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
146 & 145 \\
151 & 153 \\
163 & 161 \\
152 & 151 \\
151 & 145 \\
151 & 150 \\

\end{longtable}

\textbf{Code book for data frame Helmet}

\textbf{Description} After purchasing a batch of flight helmets that did
not fit the heads of many pilots, the NZ Airforce decided to measure the
head sizes of all recruits. Before this was carried out, information was
collected to determine the feasibility of using cheap cardboard calipers
to make the measurements, instead of metal ones which were expensive and
uncomfortable. The data lists the head diameters of 18 recruits measured
once using cardboard calipers and again using metal calipers. One
question is whether there is any systematic difference between the two
sets of calipers. One might also ask whether there is more variability
in the cardboard calipers measurement than that of the metal calipers.

This data frame contains the following columns:

Cardboard: measurement using cardboard calipers (cm)

Metal: measurement using metal calipers (cm)

Source Helmet Sizes for New Zealand Airforce. (n.d.). Retrieved July 20,
2019, from http://www.statsci.org/data/oz/nzhelmet.html

References Data courtesy of Dr Stephen Legg. Seber and Lee (1998). Page
545.

\subsubsection{Solution}\label{solution-76}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

\(x_1\) = head measurement of recruit using cardboard caliper

\(x_2\) = head measurement of recruit using metal caliper

\(\mu_1\)= mean head measurement of recruit using cardboard caliper

\(\mu_2\) = mean head measurement of recruit using metal caliper

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\mu_1=\mu_2\)

\(H_a:\mu_1\ne \mu_2\)

level of significance, \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of 18 pairs of head measures of recruits with
  cardboard and metal caliper was taken. Check: This was not stated, but
  probably could be safely assumed.
\item
  State: The population of the difference in head measurements between
  cardboard and metal calipers is normally distributed. Check: First
  create the difference variable, then the density plot and normal
  quantile plot.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Helmet}\OtherTok{\textless{}{-}}
\NormalTok{  Helmet }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{difference=}\NormalTok{Cardboard}\SpecialCharTok{{-}}\NormalTok{Metal) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Helmet))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrr@{}}

\caption{\label{tbl-Helmet-difference}Helmet Head Measurments}

\tabularnewline

\toprule\noalign{}
Cardboard & Metal & difference \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
146 & 145 & 1 \\
151 & 153 & -2 \\
163 & 161 & 2 \\
152 & 151 & 1 \\
151 & 145 & 6 \\
151 & 150 & 1 \\

\end{longtable}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{difference, }\AttributeTok{data=}\NormalTok{Helmet, }\AttributeTok{title=}\StringTok{"Differences in Head Measurements"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Difference Between Cardboard and Metal"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Two-Sample-Inference_files/figure-pdf/fig-helmet-density-1.pdf}

}

\caption{\label{fig-helmet-density}Density plot of differences in head
measurements}

\end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{difference, }\AttributeTok{data=}\NormalTok{Helmet, }\AttributeTok{title=}\StringTok{"Differences in Head Measurements"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Two-Sample-Inference_files/figure-pdf/fig-helmet-qq-1.pdf}

}

\caption{\label{fig-helmet-qq}Normal Quantile Plot of Differences in
Head Measurements}

\end{figure}%

This density plot Figure~\ref{fig-helmet-density} looks somewhat bell
shaped. The normal quantile plot Figure~\ref{fig-helmet-qq} on the
differences looks somewhat linear. So you can assume that the
distribution of the difference in weights is normal.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

Using rStudio the command is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{difference, }\AttributeTok{data=}\NormalTok{Helmet)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  difference
t = 3.1854, df = 17, p-value = 0.005415
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 0.5440163 2.6782060
sample estimates:
mean of x 
 1.611111 
\end{verbatim}

The sample statistic is 1.6111, the test statistic is 3.1854, and the
p-value is 0.005415.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Since the p-value \(<\) 0.05, reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is enough evidence to support that the mean head measurements
using the cardboard calipers are not the same as when using the metal
calipers. So it looks like the New Zealand Air Force shouldn't use the
cardboard calipers.

\subsection{Example: Confidence Interval for Paired
Samples}\label{example-confidence-interval-for-paired-samples}

The New Zealand Air Force purchased a batch of flight helmets. They then
found out that the helmets didn't fit. In order to make sure that they
order the correct size helmets, they measured the head size of recruits.
To save money, they wanted to use cardboard calipers, but were not sure
if they will be accurate enough. So they took 18 recruits and measured
their heads with the cardboard calipers and also with metal calipers.
The data frame is in Table~\ref{tbl-Helmet} (Helmet Sizes for New
Zealand Airforce, 2019). Estimate the difference in measurements between
the cardboard and metal calipers using a 95\% confidence interval.

\subsubsection{Solution}\label{solution-77}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

These were stated in
\hyperref[example-hypothesis-test-for-paired-samples-1]{Example:
Hypothesis Test for Paired Samples}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the confidence interval
\end{enumerate}

The conditions were stated and checked in
\hyperref[example-hypothesis-test-for-paired-samples-1]{Example:
Hypothesis Test for Paired Samples}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the sample statistic and confidence interval
\end{enumerate}

Using the data frame Table~\ref{tbl-Helmet-difference} the rStudio the
command is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{difference, }\AttributeTok{data=}\NormalTok{Helmet, }\AttributeTok{conf.leve=}\FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  difference
t = 3.1854, df = 17, p-value = 0.005415
alternative hypothesis: true mean is not equal to 0
95 percent confidence interval:
 0.5440163 2.6782060
sample estimates:
mean of x 
 1.611111 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: You are 95\% confidence that
  \(0.5440163<\mu_1-\mu_2<2.6782060\) contains the true mean difference
  in head measurement between using the cardboard and metal calibers.
\item
  Real World Interpretation: The mean head measurement using the
  cardboard calibers is anywhere from 0.54 cm to 2.68 cm more than the
  head measurement using the metal calibers.
\end{enumerate}

Examples 9.2.6 and 9.2.7 use the same data set, but one is conducting a
hypothesis test and the other is conducting a confidence interval.
Notice that the hypothesis test's conclusion was to reject and say that
there was a difference in the means, and the confidence interval does
not contain the number 0. If the confidence interval did contain the
number 0, then that would mean that the two means could be the same.
Since the interval did not contain 0, then you could say that the means
are different just as in the hypothesis test. This means that the
hypothesis test and the confidence interval can produce the same
interpretation. Do be careful though, you can run a hypothesis test with
a particular significance level and a confidence interval with a
confidence level that is not compatible with your significance level.
This will mean that the conclusion from the confidence interval would
not be the same as with a hypothesis test. So if you want to estimate
the mean difference, then conduct a confidence interval. If you want to
show that the means are different, then conduct a hypothesis test. As a
reminder, the American Statistical Association (ASA) suggests not
conducting hypothesis tests and just create confidence intervals.

\subsection{Homework for Paired Samples for Two Means
Section}\label{homework-for-paired-samples-for-two-means-section}

\textbf{In each problem show all steps of the hypothesis test or
confidence interval. If some of the conditions are not met, note that
the results of the test or interval may not be correct and then continue
the process of the hypothesis test or confidence interval.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The cholesterol level of patients who had heart attacks was measured
  multiple times after the heart attack. The researchers want to see if
  the cholesterol level of patients who have heart attacks changes as
  the time since their heart attack increases. The data is in
  Table~\ref{tbl-Cholesterol}. Do the data show that the mean
  cholesterol level of patients that have had a heart attack changes as
  the time increases since their heart attack? Use day2 and day4
  variables to answer the question. Test at the 1\% level.
\end{enumerate}

\textbf{Code book for Data Frame Cholesterol is below
Table~\ref{tbl-Helmet-difference}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  The cholesterol level of patients who had heart attacks was measured
  multiple times after the heart attack. The researchers want to see if
  the cholesterol level of patients who have heart attacks changes as
  the time since their heart attack increases. The data is in
  Table~\ref{tbl-Helmet-difference}. Calculate a 98\% confidence
  interval for the mean difference in cholesterol levels from day two to
  day four.
\item
  All Fresh Seafood is a wholesale fish company based on the east coast
  of the U.S. Catalina Offshore Products is a wholesale fish company
  based on the west coast of the U.S. Table~\ref{tbl-Price-seafood}
  contains prices from both companies for specific fish types
  (\textbackslash{}``Seafood online,\textbackslash{}'' 2013)
  (\textbackslash{}``Buy sushi grade,\textbackslash{}'' 2013). Do the
  data provide enough evidence to show that fish cost different from
  west coast fish wholesaler and east coast wholesaler? Test at the 5\%
  level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Price }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/price.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Price))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-Price-seafood}Wholesale Prices of Fish in Dollars}

\tabularnewline

\toprule\noalign{}
fish & east & west \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Cod & 19.99 & 17.99 \\
Tilapi & 6.00 & 13.99 \\
Farmed Salmon & 19.99 & 22.99 \\
Organic Salmon & 24.99 & 24.99 \\
Grouper Fillet & 29.99 & 19.99 \\
Tuna & 28.99 & 31.99 \\

\end{longtable}

\textbf{Code book for data frame Price}

\textbf{Description} Price of fish was collected from two websites. One
for Catalina Offshore Products (west coast) and the other for All Fresh
Seafood (east coast) in 2013.

This data frame contains the following columns:

fish: type of fish for sale

east: price of fish from east coast supplier (\$)

west: price of fish from west coast supplier (\$)

Source Seafood online. (2013, November 20). Retrieved from
http://www.allfreshseafood.com/

Buy sushi grade fish online. (2013, November 20). Retrieved from
http://www.catalinaop.com/

References Websites of Catalina Offshore Products and All Fresh Seafood

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  All Fresh Seafood is a wholesale fish company based on the east coast
  of the U.S. Catalina Offshore Products is a wholesale fish company
  based on the west coast of the U.S. Table~\ref{tbl-Price-seafood}
  contains prices from both companies for specific fish types
  (\textbackslash{}``Seafood online,\textbackslash{}'' 2013)
  (\textbackslash{}``Buy sushi grade,\textbackslash{}'' 2013). Find a
  95\% confidence interval for the mean difference in wholesale price
  between the east coast and west coast suppliers.
\item
  The British Department of Transportation studied to see if people
  avoid driving or shopping, or have more accidents on Friday the 13th.
  They collected data from different locations (Friday the 13th, 2019).
  The data for each location on the two different dates is in
  Table~\ref{tbl-Traffic}. Do the data show that on average different
  number of people are engaged in activities on Friday the 13th? Test at
  the 5\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Traffic }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/traffic.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Traffic))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lllrrl@{}}

\caption{\label{tbl-Traffic}Traffic Count}

\tabularnewline

\toprule\noalign{}
source & year & month & X6th & X13th & location \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
traffic & 1990, & July & 139246 & 138548 & 7 to 8 \\
traffic & 1990, & July & 134012 & 132908 & 9 to 10 \\
traffic & 1991, & September & 137055 & 136018 & 7 to 8 \\
traffic & 1991, & September & 133732 & 131843 & 9 to 10 \\
traffic & 1991, & December & 123552 & 121641 & 7 to 8 \\
traffic & 1991, & December & 121139 & 118723 & 9 to 10 \\

\end{longtable}

\textbf{Code book for data frame Traffic}

\textbf{Description} This file consists of three separate data sets, all
of which address the issues of how superstitions regarding Friday the
13th affect human behavior, and whether Friday the 13th is an unlucky
day. Scanlon, et al.~collected data on traffic and shopping patterns and
accident frequency for Fridays the 6th and 13th between October of 1989
and November of 1992.

For the first data set, the researchers obtained information from the
British Department of Transport regarding the traffic flows between
junctions 7 to 8 and junctions 9 to 10 of the M25 motorway. They
collected the numbers of shoppers in nine different supermarkets in
southeast England for the second data set. The third data set contains
numbers of emergency admissions to hospitals due to transport accidents.

We present the three data sets in a combined format, with the variable
``Data set'' as an identifier that may be used to separate them.

This data frame contains the following columns:

source: which data set the data were obtained from

year: which year the data was collected from

Month: the month that the Friday was in

x6th: Number of cars passing through junction (traffic data set),
shoppers for each supermarket (shopping data set), or admissions due to
transport accidents (accident data set) on Friday the 6th

x13th: Number of cars passing through junction (traffic data set),
shoppers for each supermarket (shopping data set), or admissions due to
transport accidents (accident data set) on Friday the 13th

location: Motorway junction (traffic data set), supermarket location
(shopping data set) or hospital (accident data set) to which the data
correspond

Source (n.d.). Retrieved from
https://www3.nd.edu/\textasciitilde busiforc/handouts/Data and Stories/t
test/Friday The Thirteenth/Friday The Thirteenth Data.html

References Scanlon, T.J., Luben, R.N., Scanlon, F.L., Singleton, N.
(1993), ``Is Friday the 13th Bad For Your Health?,'' BMJ, 307,
1584-1586.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  The British Department of Transportation studied to see if people
  avoid driving or shopping, or have more accidents on Friday the 13th.
  They collected data from different locations (Friday the 13th, 2019).
  The data for each location on the two different dates is in
  Table~\ref{tbl-Traffic}. Do the data show that on average different
  number of people are engaged in activities on Friday the 13th?
  Estimate the mean difference in activity count between the 6th and the
  13th using a 95\% level.
\item
  To determine if Reiki is an effective method for treating pain, a
  pilot study was carried out where a certified second-degree Reiki
  therapist provided treatment on volunteers. Pain was measured using a
  visual analogue scale (VAS) and a likert scale immediately before and
  after the Reiki treatment (Olson \& Hanson, 1997). The data is in
  Table~\ref{tbl-Reiki}. Do the data show that Reiki treatment reduces
  pain? Test at the 5\% level.
\end{enumerate}

\textbf{Code book for data frame Reiki is below Table~\ref{tbl-Reiki}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\item
  To determine if Reiki is an effective method for treating pain, a
  pilot study was carried out where a certified second-degree Reiki
  therapist provided treatment on volunteers. Pain was measured using a
  visual analogue scale (VAS) and a likert scale immediately before and
  after the Reiki treatment (Olson \& Hanson, 1997). The data is in
  Table~\ref{tbl-Reiki}. Compute a 90\% confidence level for the mean
  difference in VAS score from before and after Reiki treatment.
\item
  The female labor force participation rates (FLFPR) of women in
  countries from 1990 to 2018 are in table 9.2.8.5 (Labor force
  participation rate, female (\% of female population ages 15+) (modeled
  ILO estimate), 2019). Do the data show that the mean female labor
  force participation rate in 1990 is different from that in the 2018
  using a 5\% level of significance?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Labor }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/labor.csv"}\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Labor))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0393}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0393}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0785}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0604}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0242}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0242}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 64\tabcolsep) * \real{0.0272}}@{}}

\caption{\label{tbl-Labor}Female Labor Force Participation Rates}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Country.Name
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Country.Code
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Region
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
IncomeGroup
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1990
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1991
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1992
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1993
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1994
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1995
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1996
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1997
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1998
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y1999
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2000
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2001
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2002
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2003
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2004
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2005
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2006
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2007
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2008
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2009
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2010
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2011
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2012
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2013
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2014
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2015
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2016
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2017
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
y2018
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Aruba & ABW & Latin America \& Caribbean & High income & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
Afghanistan & AFG & South Asia & Low income & 43.11500 & 43.12400 &
43.12900 & 43.07200 & 43.00300 & 43.01700 & 42.77000 & 42.55400 &
42.41300 & 42.3340 & 42.27400 & 42.53900 & 42.89900 & 43.28600 &
43.66100 & 44.02500 & 43.59700 & 43.19200 & 42.8730 & 42.70900 &
42.73500 & 43.32800 & 44.11700 & 45.03900 & 46.01700 & 47.00100 &
47.76600 & 48.47400 & 48.66000 \\
Angola & AGO & Sub-Saharan Africa & Lower middle income & 74.94500 &
74.87900 & 74.82600 & 74.78200 & 74.77000 & 74.78400 & 74.78300 &
74.80600 & 74.84600 & 74.8940 & 74.94100 & 74.96200 & 74.98400 &
75.01100 & 75.04800 & 75.09400 & 75.12600 & 75.16500 & 75.2090 &
75.25600 & 75.30700 & 75.34400 & 75.38900 & 75.43300 & 75.46500 &
75.47900 & 75.47000 & 75.45100 & 75.41200 \\
Albania & ALB & Europe \& Central Asia & Upper middle income & 53.77100
& 56.29600 & 56.68700 & 55.74700 & 54.90400 & 53.74600 & 53.07500 &
53.81200 & 53.15400 & 52.2540 & 51.76900 & 51.11000 & 50.67900 &
49.75900 & 48.87800 & 48.05100 & 47.38900 & 46.80300 & 46.2690 &
44.94500 & 45.69300 & 47.10400 & 48.80600 & 44.65000 & 44.78900 &
47.67600 & 47.45900 & 47.31200 & 47.19100 \\
Andorra & AND & Europe \& Central Asia & High income & NA & NA & NA & NA
& NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA &
NA & NA & NA & NA & NA & NA & NA & NA & NA & NA & NA \\
Arab World & ARB & & & 19.18997 & 19.24094 & 19.13159 & 19.29515 &
19.64479 & 19.66156 & 19.51602 & 19.27293 & 19.07511 & 19.5351 &
19.59284 & 19.52237 & 19.08892 & 19.32403 & 19.44488 & 19.53444 &
19.68183 & 20.17107 & 19.8473 & 20.05784 & 20.17166 & 20.27703 &
20.46453 & 20.76731 & 20.70378 & 20.51515 & 20.61605 & 20.56842 &
20.58152 \\

\end{longtable}

\textbf{Code book for data frame Labor}

\textbf{Description} Labor force participation rate, female (\% of
female population ages 15+)

This data frame contains the following columns:

Country Name: The name of a country around the world

Country Code: The 3 letter country code

Region: The location of the country in the world

IncomeGroup: The World Bank's income classification

y1990-y2018: Labor force participation rate, female (\% of female
population ages 15+) for the years 100--2018

Source Labor force participation rate, female (\% of female population
ages 15 ) (modeled ILO estimate). (n.d.). Retrieved July 20, 2019, from
https://data.worldbank.org/indicator/SL.TLF.CACT.FE.ZS

References International Labour Organization, ILOSTAT database. Data
retrieved in April 2019.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\item
  The female labor force participation rates (FLFPR) of women in
  countries from 1990 to 2018 are in Table~\ref{tbl-Labor} (Labor force
  participation rate, female (\% of female population ages 15+) (modeled
  ILO estimate), 2019). Estimate the mean difference in the female labor
  force participation rate in 1990 to 2018 using a 95\% confidence
  level?
\item
  Is the pulse rate after exercise different from the pulse rate before
  exercise for a man who drinks alcohol but doesn't smoke? Use the data
  frame Pulse Table~\ref{tbl-Pulse}. Test at the 5\% level.
\end{enumerate}

\textbf{Code book for data frame Pulse is below Table~\ref{tbl-Pulse}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{11}
\tightlist
\item
  Table~\ref{tbl-Pulse} contains pulse rates Compute a 95\% confidence
  interval for the mean difference in pulse rates from before and after
  exercise for males who drink but do not smoke.
\end{enumerate}

\section{Independent Samples for Two
Means}\label{independent-samples-for-two-means}

This section will look at how to analyze when two samples are collected
that are independent. As with all other hypothesis tests and confidence
intervals, the process is the same though the formulas and conditions
are different.

\subsection{Hypothesis Test for the Difference in Means from Two
Independent
Samples}\label{hypothesis-test-for-the-difference-in-means-from-two-independent-samples}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

\(x_1\) = random variable 1

\(x_2\)= random variable 2

\(\mu_1\)= mean of random variable 1

\(\mu_2\)= mean of random variable 2

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

The hypotheses would be

\(H_o:\mu_1=\mu_2\)

\(H_a:\mu_1\ne \mu_2\), the \(\ne\) can be replaced with \(<\) or \(>\)

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of size \(n_1\) is taken from population 1. A
  random sample of size \(n_2\) is taken from population 2. Check:
  describe how both samples are collected. Note: the samples do not need
  to be the same size, but the test is more robust if they are.
\item
  State: The two samples are independent. Check: describe whey the
  samples are independent of each other.
\item
  State: Population 1 is normally distributed. Population 2 is normally
  distributed. Check: draw the density graph and normal quantile plot
  for both samples and discuss if they meet the criteria. Just as
  before, the t-test is fairly robust to the condition if the sample
  size is large. This means that if this condition isn't met, but your
  sample sizes are quite large, then the results of the t-test are
  valid.
\item
  State: The population variances are unknown and not assumed to be
  equal. The old condition is that the variances are equal. However,
  this condition is no longer a condition that most statisticians use.
  This is because it isn't really realistic to assume that the variances
  are equal. So just assume the condition of the variances being unknown
  and not assumed to be equal is true, and it will not be checked.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

The command using r is

t.test(variable\textasciitilde factor, data=Data\_Frame)

Note: if the \(H_a\) is \textless, then the command becomes

t.test(variable\textasciitilde factor, data=Data\_Frame,
alternative=``less'')

Similarly for \textgreater{} put alternative=``greater''

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject or fail to reject \(H_0\). The rule is:
if the p-value \(<\alpha\), then reject \(H_o\). If the p-value
\(\ge \alpha\), then fail to reject \(H_o\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

\subsection{Confidence Interval for the Difference in Means from Two
Independent
Samples}\label{confidence-interval-for-the-difference-in-means-from-two-independent-samples}

The confidence interval for the difference in means has the same random
variables and means and the same conditions as the hypothesis test for
independent samples. If you have already completed the hypothesis test,
then you do not need to state them again. If you haven't completed the
hypothesis test, then state the random variables and means and state and
check the conditions before completing the confidence interval step.

Find the sample statistic and confidence interval

On r Studio, the command is

t.test(variable\textasciitilde factor, data=Data\_Frame, conf.level=C)
type C as a decimal

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Statistical Interpretation: In general this looks like, ``You are C\%
  confident that the interval contains the true mean difference.''
\item
  Real World Interpretation: This is where you state what interval
  contains the true difference in means, though often you state how much
  more (or less) the first mean is from the second mean.
\end{enumerate}

\subsection{Example: Hypothesis Test for Two
Means}\label{example-hypothesis-test-for-two-means}

The cholesterol level of people vary for many reasons. The question is
do people with diabetes have different cholesterol levels from people
who do not have diabetes? Use the NHANES data frame. Test at the 5\%
level.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(NHANES) }\CommentTok{\#displays the names of the variables in a data frame}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "ID"               "SurveyYr"         "Gender"           "Age"             
 [5] "AgeDecade"        "AgeMonths"        "Race1"            "Race3"           
 [9] "Education"        "MaritalStatus"    "HHIncome"         "HHIncomeMid"     
[13] "Poverty"          "HomeRooms"        "HomeOwn"          "Work"            
[17] "Weight"           "Length"           "HeadCirc"         "Height"          
[21] "BMI"              "BMICatUnder20yrs" "BMI_WHO"          "Pulse"           
[25] "BPSysAve"         "BPDiaAve"         "BPSys1"           "BPDia1"          
[29] "BPSys2"           "BPDia2"           "BPSys3"           "BPDia3"          
[33] "Testosterone"     "DirectChol"       "TotChol"          "UrineVol1"       
[37] "UrineFlow1"       "UrineVol2"        "UrineFlow2"       "Diabetes"        
[41] "DiabetesAge"      "HealthGen"        "DaysPhysHlthBad"  "DaysMentHlthBad" 
[45] "LittleInterest"   "Depressed"        "nPregnancies"     "nBabies"         
[49] "Age1stBaby"       "SleepHrsNight"    "SleepTrouble"     "PhysActive"      
[53] "PhysActiveDays"   "TVHrsDay"         "CompHrsDay"       "TVHrsDayChild"   
[57] "CompHrsDayChild"  "Alcohol12PlusYr"  "AlcoholDay"       "AlcoholYear"     
[61] "SmokeNow"         "Smoke100"         "Smoke100n"        "SmokeAge"        
[65] "Marijuana"        "AgeFirstMarij"    "RegularMarij"     "AgeRegMarij"     
[69] "HardDrugs"        "SexEver"          "SexAge"           "SexNumPartnLife" 
[73] "SexNumPartYear"   "SameSex"          "SexOrientation"   "PregnantNow"     
\end{verbatim}

\textbf{Code book for data frame NHANES type help(``NHANES'') in the r
Console.}

\subsubsection{Solution}\label{solution-78}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

\(x_1\) = Cholesterol level of people with diabetes

\(x_2\) = Cholesterol level of people without diabetes

\(\mu_1\) = mean cholesterol level of people with diabetes

\(\mu_2\) = mean cholesterol level of people without diabetes

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

The hypotheses would be

\(H_o: \mu_1=\mu_2\)

\(H_a: \mu_1 \ne \mu_2\)

level of significance, \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of cholesterol levels of people with diabetes
  is taken. A random sample of cholesterol levels of people without
  diabetes is taken.

  Check: The NHANES data frame uses cluster sampling which incorporates
  random sampling, so the sample is probably representative. This
  condition has been met.
\item
  State: The two samples are independent.

  Check: This is because either they were dealing with people who have
  diabetes or not.
\item
  State: Population of all cholesterol levels of people who have
  diabetes is normally distributed. Population of all cholesterol levels
  of people without diabetes is normally distributed.

  Check:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{NHANES\_no\_NA}\OtherTok{\textless{}{-}} 
\NormalTok{  NHANES }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{drop\_na}\NormalTok{(Diabetes) }
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{TotChol}\SpecialCharTok{|}\NormalTok{Diabetes, }\AttributeTok{data=}\NormalTok{NHANES\_no\_NA, }\AttributeTok{title =} \StringTok{"Cholesterol of a person with and without Diabetes"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Total Cholesterol"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Two-Sample-Inference_files/figure-pdf/fig-desnity-facet-diabetes-1.pdf}

}

\caption{\label{fig-desnity-facet-diabetes}Density Plot of Cholesterol
of a person with and without Diabetes}

\end{figure}%

Both the yes group and the no group look somewhat bell shaped.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{TotChol}\SpecialCharTok{|}\NormalTok{Diabetes, }\AttributeTok{data=}\NormalTok{NHANES\_no\_NA, }\AttributeTok{title =} \StringTok{"Cholesterol of a person with and without Diabetes"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Two-Sample-Inference_files/figure-pdf/fig-quantile-facet-diabetes-1.pdf}

}

\caption{\label{fig-quantile-facet-diabetes}quantile Plot of Cholesterol
of a person with and without Diabetes}

\end{figure}%

Both the yes group and the no group look somewhat linear.

The population of all cholesterol levels of people who have diabetes is
probably normally distributed. The population of all cholesterol levels
of people who do not have diabetes is probably normally distributed.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

The variable is cholesterol (TotChol) and separating based on if a
person has diabetes or not. So the factor is Diabetes. Using r Studio
the command would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(TotChol}\SpecialCharTok{\textasciitilde{}}\NormalTok{Diabetes, }\AttributeTok{data=}\NormalTok{NHANES) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  TotChol by Diabetes
t = 2.4286, df = 809.7, p-value = 0.01537
alternative hypothesis: true difference in means between group No and group Yes is not equal to 0
95 percent confidence interval:
 0.02105115 0.19851114
sample estimates:
 mean in group No mean in group Yes 
         4.887936          4.778155 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Reject \(H_o\) since the p-value \(<\alpha\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is enough evidence to support that people who have diabetes have
different cholesterol levels on average from people who do not have
diabetes.

\subsection{Example: Confidence Interval in Two
Samples}\label{example-confidence-interval-in-two-samples}

The cholesterol level of people vary for many reasons. The question is
how different is the cholesterol levels of people with diabetes from
people who do not have diabetes? Use the NHANES data frame. Compute a
95\% confidence interval.

\subsubsection{Solution}\label{solution-79}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

These were stated in
\hyperref[example-hypothesis-test-for-two-means-1]{Example: Hypothesis
Test for Two Means}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

The conditions were stated and checked in
\hyperref[example-hypothesis-test-for-two-means-1]{Example: Hypothesis
Test for Two Means}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the sample statistic and confidence interval
\end{enumerate}

The variable is cholesterol (TotChol) and separating based on if a
person has diabetes or not. So the factor is Diabetes. Using rStudio the
command would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(TotChol}\SpecialCharTok{\textasciitilde{}}\NormalTok{Diabetes, }\AttributeTok{data=}\NormalTok{NHANES, }\AttributeTok{conf.level=}\FloatTok{0.95}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  TotChol by Diabetes
t = 2.4286, df = 809.7, p-value = 0.01537
alternative hypothesis: true difference in means between group No and group Yes is not equal to 0
95 percent confidence interval:
 0.02105115 0.19851114
sample estimates:
 mean in group No mean in group Yes 
         4.887936          4.778155 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: You are 95\% confident that the interval
  \(0.02105115<\mu_1-\mu_2<0.19851114\) contains the true difference in
  means.
\item
  Real World Interpretation: The mean cholesterol level for people with
  diabetes is anywhere from 0.021 mmol/L to 0.199 mmol/L more than the
  mean cholesterol level for people without diabetes.
\end{enumerate}

\subsection{Example: Hypothesis Test for Two
Means}\label{example-hypothesis-test-for-two-means-1}

The amount of sodium in beef and poultry hot dogs was measured.
(\textbackslash{}``SOCR 012708 id,\textbackslash{}'' 2013). The data is
in Table~\ref{tbl-Hotdog}. Is there enough evidence to show that beef
has different amounts of sodium on average than poultry hot dogs? Use a
5\% level of significance.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Hotdog}\OtherTok{\textless{}{-}}\FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/hotdog\_beef\_poultry.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Hotdog))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-Hotdog}Hot dog Data}

\tabularnewline

\toprule\noalign{}
type & calories & sodium \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Beef & 186 & 495 \\
Beef & 181 & 477 \\
Beef & 176 & 425 \\
Beef & 149 & 322 \\
Beef & 184 & 482 \\
Beef & 190 & 587 \\

\end{longtable}

\textbf{Code book for data frame Hot dog}

\textbf{Description} Results of a laboratory analysis of calories and
sodium content of major hot dog brands. Researchers for Consumer Reports
analyzed three types of hot dog: beef, poultry, and meat (mostly pork
and beef, but up to 15\% poultry meat). The meat was left off this data
frame so a two-sample t-test could be performed.

This data frame contains the following columns:

type: Type of hot dog (beef or poultry)

calories: Calories per hot dog

sodium: Milligrams of sodium per hot dog

Source SOCR 012708 id data hotdogs. (2013, November 13). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_012708\_ID\_Data\_HotDogs

References SOCR Home page: http://www.socr.ucla.edu

\subsubsection{Solution}\label{solution-80}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words.
\end{enumerate}

\(x_1\) = sodium level in beef hot dogs

\(x_2\) = sodium level in poultry hot dogs

\(\mu_1\) = mean sodium level in beef hot dogs

\(\mu_2\) = mean sodium level in poultry hot dogs

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

The hypotheses would be

\(H_o:\mu_1=\mu_2\)

\(H_o:\mu_1\ne \mu_2\)

level of significance: \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of 20 sodium levels in beef hot dogs is taken.
  A random sample of 20 sodium levels in poultry hot dogs.

  Check: The code does not state if either sample was randomly selected,
  but since Consumer Reports performed the test, it is safe to assume
  the samples were both random.
\item
  State: The two samples are independent.

  Check: These are different types of hot dogs so this is true.
\item
  State; Population of all sodium levels in beef hot dogs is normally
  distributed. Population of all sodium levels in poultry hot dogs is
  normally distributed.

  Check:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sodium}\SpecialCharTok{|}\NormalTok{type, }\AttributeTok{data=}\NormalTok{Hotdog, }\AttributeTok{title=}\StringTok{"Sodium amount in Hot Dogs facetted by Type of Meat"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Total Sodium Level"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Two-Sample-Inference_files/figure-pdf/fig-density-Hotdog-1.pdf}

}

\caption{\label{fig-density-Hotdog}Density Plot of Sodium Amount in Hot
Dogs facetted by Type of Meat}

\end{figure}%

The density plot for beef hot dogs looks somewhat bell shaped, but the
density plot for poultry hot dogs does not look bell shaped.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{sodium}\SpecialCharTok{|}\NormalTok{type, }\AttributeTok{data=}\NormalTok{Hotdog, }\AttributeTok{title=}\StringTok{"Sodium amoount in Hot Dogs facetted by Type of Meat"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Two-Sample-Inference_files/figure-pdf/fig-quantile-Hotdog-1.pdf}

}

\caption{\label{fig-quantile-Hotdog}Quantile Plot of Sodium Amount in
Hot Dogs facetted by Type of Meat}

\end{figure}%

The normal quantile plot Figure~\ref{fig-density-Hotdog} for the sodium
level in beef hot dogs looks somewhat linear. The normal quantile plot
Figure~\ref{fig-quantile-Hotdog} for the sodium level in poultry hot
dogs does not look linear. The population of all sodium levels in beef
hot dogs may be normally distributed, but the population of all sodium
levels in poultry hot dogs is probably not normally distributed. The
sample size is not very large so the results of the test may not be
valid. A larger sample would be a good idea.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the sample statistic, test statistic, and p-value
\end{enumerate}

Using rStudio the variable is sodium levels (sodium) in different types
of hot dogs. So the factor is type. The command is

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(sodium}\SpecialCharTok{\textasciitilde{}}\NormalTok{type, }\AttributeTok{data=}\NormalTok{Hotdog)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  sodium by type
t = -1.8798, df = 34.983, p-value = 0.06848
alternative hypothesis: true difference in means between group Beef and group Poultry is not equal to 0
95 percent confidence interval:
 -120.325706    4.625706
sample estimates:
   mean in group Beef mean in group Poultry 
               401.15                459.00 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  Conclusion: Fail to reject \(H_o\) since the p-value \(\ge \alpha\).
\item
  Interpretation
\end{enumerate}

This is not enough evidence to support that beef hot dogs' sodium level
is different from poultry hot dogs. (Though do realize that the
population conditions is not valid, so this interpretation may be
invalid.)

\subsection{Example: Confidence Interval for Two Independent
Samples}\label{example-confidence-interval-for-two-independent-samples}

The amount of sodium in beef and poultry hot dogs was measured. (``SOCR
012708 id,'' 2013). The data is in Table~\ref{tbl-Hotdog}. Find a 95\%
confidence interval for the mean difference in sodium levels between
beef and poultry hot dogs.

\subsubsection{Solution}\label{solution-81}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  State the random variables and the parameters in words.

  These were stated in
  \hyperref[example-hypothesis-test-for-two-means-1]{Example: Hypothesis
  Test for Two Means}.
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

The conditions were stated and checked in
\hyperref[example-hypothesis-test-for-two-means-1]{Example: Hypothesis
Test for Two Means}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the sample statistic and confidence interval Using r Studio the
  variable is sodium levels (sodium) in different types of hot dogs. So
  the factor is type. The command is
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{t.test}\NormalTok{(sodium}\SpecialCharTok{\textasciitilde{}}\NormalTok{type, }\AttributeTok{data=}\NormalTok{Hotdog, }\AttributeTok{conf.level=}\FloatTok{0.95}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Welch Two Sample t-test

data:  sodium by type
t = -1.8798, df = 34.983, p-value = 0.06848
alternative hypothesis: true difference in means between group Beef and group Poultry is not equal to 0
95 percent confidence interval:
 -120.325706    4.625706
sample estimates:
   mean in group Beef mean in group Poultry 
               401.15                459.00 
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  Statistical Interpretation: You are 95\% confident that the interval
  \(-120.325706<\mu_1-\mu_2<4.625706\) contains the true difference in
  mean sodium level between beef and poultry hot dogs.
\item
  Real World Interpretation: The mean sodium level of beef hot dogs is
  anywhere from 120.33 mg less than the mean sodium level of poultry hot
  dogs to 4.63 mg more. (The negative sign on the lower limit implies
  that the first mean is less than the second mean. The positive sign on
  the upper limit implies that the first mean is greater than the second
  mean.)
\end{enumerate}

Do realize that the population conditions is not valid, so this
interpretation may be invalid.

\subsection{Homework for Independent Samples for Two Means
Section}\label{homework-for-independent-samples-for-two-means-section}

\textbf{In each problem show all steps of the hypothesis test or
confidence interval. If some of the conditions are not met, note that
the results of the test or interval may not be correct and then continue
the process of the hypothesis test or confidence interval.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The NHANES data contains many variables. One variable is the income of
  households derived from the middle income of different income
  categories. The variable is called HHIncomeMid. Is there enough
  evidence to show that the mean income of males is different from the
  mean income of females? Test at the 1\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{names}\NormalTok{(NHANES)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
 [1] "ID"               "SurveyYr"         "Gender"           "Age"             
 [5] "AgeDecade"        "AgeMonths"        "Race1"            "Race3"           
 [9] "Education"        "MaritalStatus"    "HHIncome"         "HHIncomeMid"     
[13] "Poverty"          "HomeRooms"        "HomeOwn"          "Work"            
[17] "Weight"           "Length"           "HeadCirc"         "Height"          
[21] "BMI"              "BMICatUnder20yrs" "BMI_WHO"          "Pulse"           
[25] "BPSysAve"         "BPDiaAve"         "BPSys1"           "BPDia1"          
[29] "BPSys2"           "BPDia2"           "BPSys3"           "BPDia3"          
[33] "Testosterone"     "DirectChol"       "TotChol"          "UrineVol1"       
[37] "UrineFlow1"       "UrineVol2"        "UrineFlow2"       "Diabetes"        
[41] "DiabetesAge"      "HealthGen"        "DaysPhysHlthBad"  "DaysMentHlthBad" 
[45] "LittleInterest"   "Depressed"        "nPregnancies"     "nBabies"         
[49] "Age1stBaby"       "SleepHrsNight"    "SleepTrouble"     "PhysActive"      
[53] "PhysActiveDays"   "TVHrsDay"         "CompHrsDay"       "TVHrsDayChild"   
[57] "CompHrsDayChild"  "Alcohol12PlusYr"  "AlcoholDay"       "AlcoholYear"     
[61] "SmokeNow"         "Smoke100"         "Smoke100n"        "SmokeAge"        
[65] "Marijuana"        "AgeFirstMarij"    "RegularMarij"     "AgeRegMarij"     
[69] "HardDrugs"        "SexEver"          "SexAge"           "SexNumPartnLife" 
[73] "SexNumPartYear"   "SameSex"          "SexOrientation"   "PregnantNow"     
\end{verbatim}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  The NHANES data contains many variables. One variable is the income of
  households derived from the middle income of different income
  categories. The variable is called HHIncomeMid. Estimate with 95\%
  confidence the mean difference in incomes between males and females in
  the U.S.
\item
  A study was conducted that measured the total brain volume (TBV) of
  patients that had schizophrenia and patients that do not have
  schizophrenia. Table~\ref{tbl-Brain} contains the TBV of the all
  patients (``SOCR data oct2009,\textbackslash{}'' 2013). Is there
  enough evidence to show that the patients with schizophrenia have a
  different TBV on average than a patient without schizophrenia? Test at
  the 10\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Brain }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/brain.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Brain))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lr@{}}

\caption{\label{tbl-Brain}Total Brain Volume of Patients}

\tabularnewline

\toprule\noalign{}
type & volume \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
n & 1663407 \\
n & 1583940 \\
n & 1299470 \\
n & 1535137 \\
n & 1431890 \\
n & 1578698 \\

\end{longtable}

\textbf{Code book for data frame Brain}

\textbf{Description} A study to measure the total brain volume (TBV) (in
) of patients that had schizophrenia and patients that do not have
schizophrenia.

This data frame contains the following columns:

type: whether the patient had schizophrenia (s) or did not have
schizophrenia (n)

volume: the total brain volume of a patient.(\(mm^3\))

Source SOCR data Oct2009 id ni. (2013, November 16). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_Oct2009\_ID\_NI

References ``SOCR data nips,'' 2013

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\item
  A study was conducted that measured the total brain volume (TBV) of
  patients that had schizophrenia and patients that do not have
  schizophrenia. Table~\ref{tbl-Brain} contains the TBV of the all
  patients (``SOCR data oct2009,'' 2013). Is there enough evidence to
  show that the patients with schizophrenia have a different TBV on
  average than a patient without schizophrenia? Test at the 10\% level.
  Compute a 90\% confidence interval for the difference in TBV of
  patients with Schizophrenia and patients without Schizophrenia.
\item
  The lengths (in kilometers) of rivers on the South Island of New
  Zealand and what body of water they flow into are listed in
  Table~\ref{tbl-Length} (Lee, 1994). Do the data provide enough
  evidence to show on average that the rivers that travel to the Pacific
  Ocean are different length than the rivers that travel to the Tasman
  Sea? Use a 5\% level of significance.
\end{enumerate}

\textbf{Code book for data frame Length below Table~\ref{tbl-Length}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\item
  The lengths (in kilometers) of rivers on the South Island of New
  Zealand and what body of water they flow into are listed in
  Table~\ref{tbl-Length} (Lee, 1994). Estimate the difference in mean
  lengths of rivers between rivers in New Zealand that travel to the
  Pacific Ocean and ones that travel to the Tasman Sea. Use a 95\%
  confidence level.
\item
  A vitamin K shot is given to infants soon after birth. Nurses at
  Northbay Healthcare were involved in a study to see if how they handle
  the infants could reduce the pain the infants feel
  (\textbackslash{}``SOCR data nips,\textbackslash{}'' 2013). The data
  frame is in Table~\ref{tbl-Crying}. Is there enough evidence to show
  that infants cried a different amount on average when they are held by
  their mothers than if held using conventional methods? Test at the 5\%
  level.
\end{enumerate}

\subsubsection{Table: Crying Time of Infants Given Shots Using New
Methods}\label{table-crying-time-of-infants-given-shots-using-new-methods}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Crying}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/crying.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Crying))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lr@{}}

\caption{\label{tbl-Crying}Crying Time of Infants Given Shots Using New
Methods}

\tabularnewline

\toprule\noalign{}
method & crying \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
convent & 63 \\
convent & 0 \\
convent & 2 \\
convent & 46 \\
convent & 33 \\
convent & 33 \\

\end{longtable}

\textbf{Code book for data frame Crying}

\textbf{Description} Nurses at Northbay Healthcare were involved in a
study to see if how they handle the infants could reduce the pain the
infants feel. One of the measurements taken was how long, in seconds,
the infant cried after being given the shot. A random sample was taken
from the group that was given the shot using conventional methods, and a
random sample was taken from the group that was given the shot where the
mother held the infant prior to and during the shot.

This data frame contains the following columns:

method: whether the infant was given the conventional method (convent)
or the new method (new) prior to being given the vitamin K shot.

crying: how long the infant cried after given a vitamin K shot.
(seconds)

Source SOCR data nips infantvitK shotdata. (2013, November 16).
Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_NIPS\_InfantVitK\_ShotData

References \textbackslash{}``SOCR data nips,\textbackslash{}'' 2013

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  A vitamin K shot is given to infants soon after birth. Nurses at
  Northbay Healthcare were involved in a study to see if how they handle
  the infants could reduce the pain the infants feel
  (\textbackslash{}``SOCR data nips,\textbackslash{}'' 2013). The data
  frame is in Table~\ref{tbl-Crying}. Calculate a 95\% confidence
  interval for the mean difference in mean crying time after being given
  a vitamin K shot between infants held using conventional methods and
  infants held by their mothers.
\end{enumerate}

\section{Which Analysis Should You
Conduct?}\label{which-analysis-should-you-conduct}

One of the most important concept that you need to understand is
deciding which analysis you should conduct for a particular situation.
To help you to figure out the analysis to conduct, there are a series of
questions you should ask yourself.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Does the problem deal with mean or proportion?
\end{enumerate}

Sometimes the problem states explicitly the words mean or proportion,
but other times you have to figure it out based on the information you
are given. If you counted number of individuals that responded in the
affirmative to a question, then you are dealing with proportion. If you
measured something, then you are dealing with mean.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Does the problem have one or two samples?
\end{enumerate}

So look to see if one group was measured or if two groups were measured.
You need to decide if the problem describes collecting data from one
group or from two groups, or if you are comparing two different groups.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  If you have two samples, then you need to determine if the samples are
  independent or dependent.
\end{enumerate}

If the individuals are different for both samples, then most likely the
samples are independent. If you can't tell, then determine if a data
value from the first sample influences the data value in the second
sample. In other words, can you pair data values together so you can
find the difference, and that difference has meaning. If the answer is
yes, then the samples are paired. Otherwise, the samples are
independent.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Does the situation involve a hypothesis test or a confidence interval?
\end{enumerate}

If the problem talks about ``do the data show'', ``is there evidence
of'', ``test to see'', then you are doing a hypothesis test. If the
problem talks about ``find the value'', ``estimate the'' or ``find the
interval'', then you are doing a confidence interval.

So if you have a situation that has two samples, independent samples,
involving the mean, and is a hypothesis test, then you have a two-sample
independent t-test. Now you look up the conditions and the technology
process for doing this test. Every hypothesis test involves the same six
steps, and you just have to use the correct conditions and calculations.
Every confidence interval has the same five steps, and again you just
need to use the correct conditions and calculations. So this is why it
is so important to figure out what analysis you should conduct.

\bookmarksetup{startatroot}

\chapter{Regression}\label{regression}

The previous chapter looked at comparing populations to see if there is
a difference between the two. That involved two random variables that
are similar measures. This chapter will look at two random variables
that do not need to be similar measures, and see if there is a
relationship between the two or more variables. To do this, you look at
regression, which finds the linear relationship, and correlation, which
measures the strength of a linear relationship.

Please note: there are many other types of relationships besides linear
that can be found for the data. This book will only explore linear, but
realize that there are other relationships that can be used to describe
data.

\section{Regression}\label{regression-1}

When comparing different variables, two questions come to mind: ``Is
there a relationship between two variables?'' and ``How strong is that
relationship?'' These questions can be answered using
\textbf{regression} and \textbf{correlation}. Regression answers whether
there is a relationship (again this book will explore linear only) and
correlation answers how strong the linear relationship is. The variable
that are used to explain the change in the other variable is called the
\textbf{explanatory variable} while the variable that is changing is
called the \textbf{response variable}. Other variables that help to
explain the changes are known as \textbf{covariates}. To introduce the
concepts of regression and correlation it is easier to look at a set of
data.

\subsection{Example: Determining if there is a
Relationship}\label{example-determining-if-there-is-a-relationship}

Is there a relationship between the alcohol content and the number of
calories in 12-ounce beer? To determine if there is one, we explore a
sample of 227 beers' alcohol content and their calories (Find Out How
Many Calories in Beer?, 2019). Table~\ref{tbl-Beer} shows the first five
rows of the dataset.

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.3816}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1974}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1184}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1053}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.1184}}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 10\tabcolsep) * \real{0.0789}}@{}}

\caption{\label{tbl-Beer}Alcohol and Calorie Content in Beer}

\tabularnewline

\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
beer
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
brewery
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
location
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
alcohol
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
calories
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedleft
carbs
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
American Amber Lager & Straub Brewery & domestic & 0.04 & 136 & 10.5 \\
American Lager & Straub Brewery & domestic & 0.04 & 132 & 10.5 \\
American Light & Straub Brewery & domestic & 0.03 & 96 & 7.6 \\
Anchor Porter & Anchor & domestic & 0.06 & 209 & NA \\
Anchor Steam & Anchor & domestic & 0.05 & 153 & 16.0 \\
Anheuser Busch Natural Light & Anheuser Busch & domestic & 0.04 & 95 &
3.2 \\

\end{longtable}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, arc=.35mm, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, colframe=quarto-callout-note-color-frame, opacityback=0, colback=white, rightrule=.15mm, breakable]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-note-color}{\faInfo}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\vspace{-3mm}\textbf{\includegraphics[width=0.22917in,height=\textheight]{Rguroo_Icons/R_circle_logo.pdf}
Click here for instructions to import and view the Beer dataset in your
Rguroo account}\vspace{3mm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to your Rguroo account.
\item
  Open the \textbf{Data} toolbox.
\item
  Click on \Dpd{Data Import} dropdown and select
  \Fun{Dataset Repository}.
\item
  In the searchbox on the top type in the keyword \Typein{kozak}. Then
  select the ``\emph{Statistics Using Technology - Kozak}'\,'
  repository.
\item
  In the middle \emph{searchbox} type in \Typein{beer} and select the
  \Data{beer\_data} dataset.
\item
  Click the \Button{Import} button. The dataset will be imported to your
  Rguroo account.
\item
  Click the \Button{Close} button to close the Rguroo dialog.
\item
  To view the dataset, click on the \Data{beer\_data} under the
  \textbf{Data} toolbox.
\end{enumerate}

\end{minipage}%
\end{tcolorbox}

\textbf{Code book for data frame Beer}

\textbf{Description} Collection of the most popular beers from large
breweries. The data is of the calories, carbs and alcohol of a specific
beer. The data is shown for a 12 ounce serving. The collection includes
both domestic and import beer. For the imported beers the information is
per 12 oz. serving even though many imports come in pints.

This dataset contains the following columns:

beer: The name of the beer.

brewery: the brewery that brews the beer.

location: whether the beer is brewed in the U.S. (domestic) or brewed in
another country (import).

alcohol: the alcohol content of the beer.

calories: the number of calories in the beer.

carbs: the amount of carbohydrates in the beer (g).

Source Find Out How Many Calories in Beer? (n.d.). Retrieved July 21,
2019, from \href{https://www.beer100.com/beer-calories/}{}

References (Find Out How Many Calories in Beer?, 2019)

\subsubsection{Solution}\label{solution-82}

To aid in figuring out if there is a relationship, it helps to draw a
scatter plot of the data. First it is helpful to state the random
variables, and since in an algebra class the variables are represented
as \(x\) and \(y\), those labels will be used here. It helps to state
which variable is \(x\) and which is \(y\).

State random variables

\(x\) = alcohol content in the beer

\(y\) = calories in 12 ounce beer

\begin{tcolorbox}[enhanced jigsaw, left=2mm, arc=.35mm, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, colframe=quarto-callout-note-color-frame, opacityback=0, colback=white, rightrule=.15mm, breakable]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-note-color}{\faInfo}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\vspace{-3mm}\textbf{\includegraphics[width=0.22917in,height=\textheight]{Rguroo_Icons/R_circle_logo.pdf}
Click here to see how to create the scatter plot of calories vs.~alcohol
content in Rguroo}\vspace{3mm}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Go to your Rguroo account. Make sure that you have already imported
  the \Data{beer\_data} dataset by following the instructions given
  above.
\item
  Open the \textbf{Plots} toolbox.
\item
  Click on the \Dpd{Create Plot} dropdown and select \Fun{Scatterplot}.
  This opend the Scatterplot dialog.
\item
  From the \Dpd{Dataset} dropdown select the \Data{beer\_data} dataset.
\item
  From the \Dpd{Predictor (x)} dropdown select the \Var{alcohol}
  variable.
\item
  From the \Dpd{Response (y)} dropdown select the \Var{calories}
  variable.
\item
  (Optional) In the \textbf{Label} section of the dialog

  \begin{itemize}
  \tightlist
  \item
    type in \Typein{Calories versus Alcohol Content in Beer} in the
    \Des{Title} textbox.
  \item
    type in \Typein{Alcohol Content} in the \Des{X-Axis} textbox.
  \item
    type in \Typein{Number of Calories} in the \Des{Y-Axis} textbox.
  \end{itemize}
\item
  Click the Preview icon
  \includegraphics{Rguroo_Icons/preview_inline.png} to see a preview of
  the scatter plot.
\end{enumerate}

\begin{tcolorbox}[enhanced jigsaw, left=2mm, arc=.35mm, leftrule=.75mm, bottomrule=.15mm, toprule=.15mm, colframe=quarto-callout-note-color-frame, opacityback=0, colback=white, rightrule=.15mm, breakable]
\begin{minipage}[t]{5.5mm}
\textcolor{quarto-callout-note-color}{\faInfo}
\end{minipage}%
\begin{minipage}[t]{\textwidth - 5.5mm}

\vspace{-3mm}\textbf{Click here to see the Rguroo dialog}\vspace{3mm}

\begin{figure}[H]

{\centering \includegraphics[width=5.20833in,height=\textheight]{Rguroo_images/Regression/Scatterplot_dialog.png}

}

\caption{Scatterplot dialog in Rguroo}

\end{figure}%

\end{minipage}%
\end{tcolorbox}

\end{minipage}%
\end{tcolorbox}

\begin{figure}

\centering{

\includegraphics{Regression_files/figure-pdf/fig-Beer-point-1.pdf}

}

\caption{\label{fig-Beer-point}Calories versus Alcohol Content in Beer}

\end{figure}%

This scatter plot looks fairly linear.

To find the equation for the linear relationship, the process of
regression is used to find the line that best fits the data (sometimes
called the best fitting line). The process is to draw the line through
the data and then find the vertical distance from a point to the line.
These are called the \textbf{residuals}. The regression line is the line
that makes the square of the residuals as small as possible, so the
regression line is also sometimes called the least squares line. The
regression line on the scatter plot is displayed in
Figure~\ref{fig-Beer-point-line}.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_point}\NormalTok{(alcohol}\SpecialCharTok{\textasciitilde{}}\NormalTok{calories, }\AttributeTok{data=}\NormalTok{Beer, }\AttributeTok{title=}\StringTok{"Calories versus Alcohol Content in Beer"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Alcohol Content"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Number of Calories"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{gf\_lm}\NormalTok{()}
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Regression_files/figure-pdf/fig-Beer-point-line-1.pdf}

}

\caption{\label{fig-Beer-point-line}Scatter Plot of Beer Data with
Regression Line}

\end{figure}%

\subsection{Find the regression equation (also known as best fitting
line or least squares
line)}\label{find-the-regression-equation-also-known-as-best-fitting-line-or-least-squares-line}

Given a collection of paired sample data, the regression equation is
\(\hat{y}=mx+b\)

where the slope = \(m\) and \(y\)-intercept = \((0,b)\)

To find the linear model using r Studio, use the command:

lm(response\_variable\textasciitilde explanatory\_variable,
data=Data\_Frame

The \textbf{residuals} are the difference between the actual values and
the estimated values.

The \textbf{independent variable}, also called the \textbf{explanatory
variable} or \textbf{predictor variable}, is the \(x\)-value in the
equation. The independent variable is the one that you use to predict
what the other variable is. The \textbf{dependent variable} depends on
what independent value you pick. It also responds to the explanatory
variable and is usually called the \textbf{response variable}. In the
alcohol content and calorie example, it makes slightly more sense to say
that you would use the alcohol content on a beer to predict the number
of calories in the beer.

Conditions of the regression line when creating a regression line from a
sample:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  The set of ordered pairs is a random sample from the population of all
  such possible pairs.
\item
  For each fixed value of \(x\), the \(y\)-values have a normal
  distribution. All of the \(y\) distributions have the same variance,
  and for a given \(x\)-value, the distribution of \(y\)-values has a
  mean that lies on the least squares line. You also assume that for a
  fixed \(y\), each \(x\) has its own normal distribution. This is
  difficult to figure out, so you can use the following to determine if
  you have a normal distribution.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  Look to see if the scatter plot has a linear pattern.
\item
  Examine the residuals to see if there is randomness in the residuals.
  If there is a pattern to the residuals, then there is an issue in the
  data.
\end{enumerate}

\subsection{Example: Find the Equation of the Regression
Line}\label{example-find-the-equation-of-the-regression-line}

Is there a relationship between the alcohol content and the number of
calories in 12-ounce beer? To determine if there is one a sample of
beer's alcohol content and calories (Find Out How Many Calories in
Beer?, 2019), is in Table~\ref{tbl-Beer}.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the regression equation between alcohol content and calories.
\item
  Use the regression equation to find the number of calories when the
  alcohol content is 7.00\%.
\item
  Use the regression equation to find the number of calories when the
  alcohol content is 14\%.
\end{enumerate}

\subsubsection{Solution}\label{solution-83}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the regression equation between alcohol content and calories.
\end{enumerate}

State random variables

\(x\) = alcohol content in the beer

\(y\) = calories in 12 ounce beer

To find the regression equation using rStudio for this example, the
command would be

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{lm}\NormalTok{(calories}\SpecialCharTok{\textasciitilde{}}\NormalTok{alcohol, }\AttributeTok{data=}\NormalTok{Beer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = calories ~ alcohol, data = Beer)

Coefficients:
(Intercept)      alcohol  
      14.53      2672.36  
\end{verbatim}

From this you can see that the y-intercept is 14.53 and the slope is
2672.36. So the regression equation is \(\hat{y}=2672x+14.3\).

Remember, this is an estimate for the true regression. A different
sample would produce a different estimate.

Conditions check:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  State: A random sample of alcohol content and calories was taken.

  Check: There is no guarantee that this was a random sample. The data
  was collected off of a website, and the website does not say how the
  data was obtained. However, it is a collection of most popular beers
  from large breweries, so it may be alright that it isn't a random
  sample.
\item
  State: The distribution for each calorie value is normally distributed
  for every value of alcohol content in the beer.
\end{enumerate}

Check:

\begin{enumerate}
\def\labelenumi{\roman{enumi}.}
\item
  From Figure~\ref{fig-Beer-point}, the scatter plot looks fairly
  linear.
\item
  To graph the residuals, first the residuals need to be calculated.
  This means that for every \(x\) value, you need to calculate the \(y\)
  value that can be found from the regression equation. Then subtract
  the \(y\) value that was measured from this calculated \(y\) value.
  This is \(\hat{y}-y\). Luckily, rStudio will calculate these for you.
  The command is:
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_out\_beer}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(calories}\SpecialCharTok{\textasciitilde{}}\NormalTok{alcohol, }\AttributeTok{data=}\NormalTok{Beer) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(lm\_out\_beer))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}r@{}}

\caption{\label{tbl-Beer-residuals}Residuals for Alcohol and Calorie
Content in Beer}

\tabularnewline

\toprule\noalign{}
x \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
14.5785382 \\
10.5785382 \\
1.3021259 \\
34.1313626 \\
4.8549504 \\
-26.4214618 \\
-17.8686374 \\
-27.4214618 \\
6.8549504 \\
20.1313626 \\
28.8549504 \\
20.1313626 \\
14.8549504 \\
5.8549504 \\
14.8549504 \\
4.8549504 \\
22.8549504 \\
21.1313626 \\
18.1313626 \\
-18.1450496 \\
-51.8686374 \\
-33.1450496 \\
-11.4214618 \\
-5.4214618 \\
-37.8686374 \\
-3.1450496 \\
-28.8686374 \\
-22.4214618 \\
-12.9742863 \\
-15.1450496 \\
-5.8686374 \\
-26.4214618 \\
16.5785382 \\
-0.8686374 \\
0.8549504 \\
-17.4214618 \\
13.8549504 \\
5.1313626 \\
-11.5922251 \\
3.8549504 \\
-6.3158129 \\
9.8549504 \\
-9.8686374 \\
-22.2101640 \\
-24.8686374 \\
15.9605993 \\
32.2370115 \\
-10.1450496 \\
-4.5922251 \\
9.6841871 \\
-20.8686374 \\
-7.3158129 \\
-11.8686374 \\
-13.5922251 \\
-19.1450496 \\
6.2370115 \\
-17.1450496 \\
-0.1450496 \\
13.8549504 \\
-18.8686374 \\
-0.1450496 \\
13.8549504 \\
-42.8686374 \\
-25.8686374 \\
-18.4214618 \\
-4.1450496 \\
-4.1450496 \\
-11.4214618 \\
-10.4214618 \\
-18.4214618 \\
-32.8686374 \\
0.8549504 \\
14.8549504 \\
3.8549504 \\
21.8549504 \\
17.8549504 \\
16.8549504 \\
-16.4214618 \\
-11.4214618 \\
11.8549504 \\
11.8549504 \\
-17.8686374 \\
-3.1450496 \\
17.8549504 \\
6.8549504 \\
3.8549504 \\
-11.4214618 \\
26.8549504 \\
-8.4214618 \\
-26.4214618 \\
-17.8686374 \\
-17.8686374 \\
-11.4214618 \\
-5.1450496 \\
-11.4214618 \\
-30.6978741 \\
-5.1450496 \\
-11.4214618 \\
-25.4214618 \\
6.5785382 \\
-11.4214618 \\
-11.4214618 \\
-11.4214618 \\
6.5785382 \\
-23.4214618 \\
-30.8686374 \\
30.1313626 \\
-1.5922251 \\
-1.5922251 \\
-8.1450496 \\
11.8549504 \\
6.8549504 \\
-13.3158129 \\
-3.1450496 \\
55.4728892 \\
-7.4214618 \\
-2.1450496 \\
-14.8686374 \\
-26.3158129 \\
-8.3158129 \\
-2.1450496 \\
-4.1450496 \\
-27.6978741 \\
25.8549504 \\
-1.1450496 \\
4.1313626 \\
-13.5922251 \\
-6.1450496 \\
-28.1450496 \\
11.8549504 \\
11.8549504 \\
17.8549504 \\
46.8549504 \\
0.1313626 \\
-2.4214618 \\
-3.1450496 \\
-6.1450496 \\
-2.1450496 \\
-11.4214618 \\
10.1313626 \\
15.1313626 \\
48.2370115 \\
12.4077749 \\
8.8549504 \\
15.1313626 \\
13.4077749 \\
29.4077749 \\
0.1313626 \\
16.4077749 \\
19.1313626 \\
50.1313626 \\
9.8549504 \\
31.5785382 \\
4.8549504 \\
-25.1450496 \\
-6.3158129 \\
-6.3158129 \\
-14.8686374 \\
0.8549504 \\
-8.4214618 \\
20.1313626 \\
-1.1450496 \\
14.8549504 \\
7.8549504 \\
1.8549504 \\
13.5785382 \\
2.8549504 \\
12.8549504 \\
-1.1450496 \\
-3.1450496 \\
1.8549504 \\
13.5785382 \\
-23.4214618 \\
13.5785382 \\
-22.4214618 \\
-2.1450496 \\
-14.8686374 \\
-2.1450496 \\
-5.1450496 \\
-3.9742863 \\
-2.1450496 \\
6.5785382 \\
24.5785382 \\
7.8549504 \\
-0.1450496 \\
0.8549504 \\
7.8549504 \\
18.4077749 \\
7.8549504 \\
16.5785382 \\
11.8549504 \\
-6.1450496 \\
-3.1450496 \\
31.5785382 \\
-6.1450496 \\
1.8549504 \\
21.8549504 \\
-22.4214618 \\
-13.5922251 \\
4.8549504 \\
14.8549504 \\
4.8549504 \\
13.2370115 \\
4.8549504 \\
-4.8686374 \\
14.0257137 \\
4.8549504 \\
-38.1450496 \\
-6.1450496 \\
38.5785382 \\
4.8549504 \\
9.8549504 \\
25.8549504 \\
9.5785382 \\
-16.5922251 \\
-0.1450496 \\
1.8549504 \\
-10.1450496 \\
31.5785382 \\
6.5785382 \\
-41.5922251 \\
3.4077749 \\
-2.1450496 \\
4.8549504 \\
20.5785382 \\
11.8549504 \\
4.8549504 \\

\end{longtable}

lm\_out\_beer saves the linear model into a variable called
lm\_out\_beer

residuals(lm\_out\_beer) finds the residuals for each \(x\) value based
on the linear model, lm\_out\_beer, and displays them.

Now graph the residuals to see if there is any pattern to the residuals.
The command in rStudio is:

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_point}\NormalTok{(}\FunctionTok{residuals}\NormalTok{(lm\_out\_beer)}\SpecialCharTok{\textasciitilde{}}\NormalTok{alcohol, }\AttributeTok{data=}\NormalTok{Beer, }\AttributeTok{title=}\StringTok{"Residuals for Calories in beer"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"Alcohol Content"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Residuals for Number of Calories"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Regression_files/figure-pdf/fig-Beer-residuals-1.pdf}

}

\caption{\label{fig-Beer-residuals}Residual Plot of Beer Data}

\end{figure}%

The residual versus the \(x-\)values plot
Figure~\ref{fig-Beer-residuals} looks fairly random.

It appears that the distribution for calories is a normal distribution.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Use the regression equation to find the number of calories when the
  alcohol content is 7.00\%.
\end{enumerate}

\(\hat{y}=2672*0.07+14.3=201.34\) If you are drinking a beer that is
7.00\% alcohol content, then it is probably close to 200 calories.

The mean number of calories is 154.5 calories. This value of 201 seems
like a better estimate than the mean when looking at the data since all
the beers with 7\% alcohol have between 160 and 231 calories. The
regression equation is a better estimate than just the mean.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Use the regression equation to find the number of calories when the
  alcohol content is 14\%.
\end{enumerate}

\(\hat{y}=2672*0.14+14.3=388.38\)

If you are drinking a beer that is 14\% alcohol content, then it has
probably close to 389 calories. Since 12\% alcohol beer has 330 calories
you might think that 14\% would have more calories than this. This
estimate is what is called \textbf{extrapolation}. It is not a good idea
to predict values that are far outside the range of the original data.
This is because you can never be sure that the regression equation is
valid for data outside the original data.

Notice, that the 7.00\% value falls into the range of the original
\(x\)-values. The processes of predicting values using an \(x\) within
the range of original \(x\)-values is called \textbf{interpolating}. The
14.00\% value is outside the range of original \(x\)-values. Using an
\(x\)-value that is outside the range of the original \(x\)-values is
called \textbf{extrapolating}. When predicting values using
interpolation, you can usually feel pretty confident that that value
will be close to the true value. When you extrapolate, you are not
really sure that the predicted value is close to the true value. This is
because when you interpolate, you know the equation that predicts, but
when you extrapolate, you are not really sure that your relationship is
still valid. The relationship could in fact change for different
\(x\)-values.

An example of this is when you use regression to come up with an
equation to predict the growth of a city, like Flagstaff, AZ. Based on
analysis it was determined that the population of Flagstaff would be
well over 50,000 by 1995. However, when a census was undertaken in 1995,
the population was less than 50,000. This is because they extrapolated
and the growth factor they were using had obviously changed from the
early 1990's. Growth factors can change for many reasons, such as
employment growth, employment stagnation, disease, articles saying great
place to live, etc. Realize that when you extrapolate, your predicted
value may not be anywhere close to the actual value that you observe.

What does the slope mean in the context of this problem?

The calories increase 26.72 calories for every 1\% increase in alcohol
content.

The \(y\)-intercept in many cases is meaningless. In this case, it means
that if a drink has 0 alcohol content, then it would have 14.3 calories.
This may be reasonable, but remember this value is an extrapolation so
it may be wrong.

Consider the residuals again. According to the data, a beer with 7.0\%
alcohol has between 160 and 231 calories. The predicted value is 201
calories. This variation means that the actual value was between 40
calories below and 30 calories above the predicted value. That isn't
that far off. Some of the actual values differ by a large amount from
the predicted value. This is due to variability in the response
variable. The larger the residuals the less the model explains the
variability in the response variable. There needs to be a way to
calculate how well the model explains the variability in the response
variable. This will be explored in the next section.

On last thing to look at here, is that you may wonder if import beer has
a different amount of calories than domestic beer. The location is a
co-variate, a third variable that affects the calories, and you can
graph a scatter plot that separates based on the co-variate. Here is how
to do this in r Studio.

(ref:Beer10a-point-lm) Calories versus Alcohol Content Separated by
Location of Beer Data

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_point}\NormalTok{(calories}\SpecialCharTok{\textasciitilde{}}\NormalTok{alcohol, }\AttributeTok{data=}\NormalTok{Beer, }\AttributeTok{title=}\StringTok{"Calories versus Alcohol Content of Beer separated by Location"}\NormalTok{, }\AttributeTok{color=}\SpecialCharTok{\textasciitilde{}}\NormalTok{location, }\AttributeTok{xlab=}\StringTok{"Alcohol Content"}\NormalTok{, }\AttributeTok{ylab=}\StringTok{"Number of calories"}\NormalTok{) }\SpecialCharTok{|\textgreater{}}
  \FunctionTok{gf\_lm}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\begin{figure}[H]

\centering{

\includegraphics{Regression_files/figure-pdf/fig-Beer-point-lm-1.pdf}

}

\caption{\label{fig-Beer-point-lm}Calories versus Alcohol Content of
Beer separated by Location}

\end{figure}%

Looking at the scatter plot, there doesn't appear to be an affect from
domestic or import. This is what is nice about scatter plots, You can
visually see a possible relationships.

\subsection{Homework for Regression
Section}\label{homework-for-regression-section}

\textbf{For each problem, state the random variables. The Data Frame in
this section are used in the homework for sections 10.2 and 10.3 also.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When an anthropologist finds skeletal remains, they need to figure out
  the height of the person. The height of a person (in cm) and the
  length of their metacarpal bone 1 (in mm) were collected and are in
  Table~\ref{tbl-Metacarpal} (\textbackslash{}``Prediction of
  height,\textbackslash{}'' 2013). Create a scatter plot and find a
  regression equation between the height of a person and the length of
  their metacarpal. Then use the regression equation to find the height
  of a person for a metacarpal length of 44 mm and for a metacarpal
  length of 55 mm. Which height that you calculated do you think is
  closer to the true height of the person? Why?
\end{enumerate}

\textbf{Code book for Data Frame Metacarpal is below
Table~\ref{tbl-Metacarpal}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Table~\ref{tbl-House} contains the value of the house and the amount
  of rental income in a year that the house brings in
  (\textbackslash{}``Capital and rental,\textbackslash{}'' 2013). Create
  a scatter plot and find a regression equation between house value and
  rental income. Then use the regression equation to find the rental
  income of a house worth \textbackslash\$230,000 and for a house worth
  \textbackslash\$400,000. Which rental income that you calculated do
  you think is closer to the true rental income? Why?
\end{enumerate}

\textbf{Code book for Data Frame House is below Table~\ref{tbl-House}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The World Bank collects information on the life expectancy of a person
  in each country (\textbackslash{}``Life expectancy
  at,\textbackslash{}'' 2013) and the fertility rate per woman in the
  country (\textbackslash{}``Fertility rate,\textbackslash{}'' 2013).
  The Data Frame for countries for the year 2011 are in
  Table~\ref{tbl-Fertility}. Create a scatter plot of the Data Frame and
  find a linear regression equation between fertility rate and life
  expectancy in 2011. Then use the regression equation to find the life
  expectancy for a country that has a fertility rate of 2.7 and for a
  country with fertility rate of 8.1. Which life expectancy that you
  calculated do you think is closer to the true life expectancy? Why?
\end{enumerate}

\textbf{Code book for Data Frame Fertility is below
Table~\ref{tbl-Fertility}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The World Bank collected data on the percentage of gross domestic
  product (GDP) that a country spends on health expenditures (Current
  health expenditure (\% of GDP), 2019), the fertility rate of the
  country (Fertility rate, total (births per woman), 2019), and the
  percentage of woman receiving prenatal care (Pregnant women receiving
  prenatal care (\%), 2019). The Data Frame for the countries where this
  information is available in Table~\ref{tbl-Fert_prenatal}. Create a
  scatter plot of the Data Frame and find a regression equation between
  percentage spent on health expenditure and the percentage of women
  receiving prenatal care. Then use the regression equation to find the
  percent of women receiving prenatal care for a country that spends
  5.0\% of GDP on health expenditure and for a country that spends
  12.0\% of GDP. Which prenatal care percentage that you calculated do
  you think is closer to the true percentage? Why?
\end{enumerate}

\textbf{Code book for Data Frame Fert\_prenatal is below
Table~\ref{tbl-Fert_prenatal}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The height and weight of baseball players are in
  Table~\ref{tbl-Baseball} (\textbackslash{}``MLB
  heightsweights,\textbackslash{}'' 2013). Create a scatter plot and
  find a regression equation between height and weight of baseball
  players. Then use the regression equation to find the weight of a
  baseball player that is 75 inches tall and for a baseball player that
  is 68 inches tall. Which weight that you calculated do you think is
  closer to the true weight? Why?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Baseball }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/baseball.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Baseball))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rrr@{}}

\caption{\label{tbl-Baseball}Heights and Weights of Baseball Players}

\tabularnewline

\toprule\noalign{}
player & height & weight \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 65.78 & 112.99 \\
2 & 71.52 & 136.49 \\
3 & 69.40 & 153.03 \\
4 & 68.22 & 142.34 \\
5 & 67.79 & 144.30 \\
6 & 68.70 & 123.30 \\

\end{longtable}

\textbf{Code book for Data Frame Baseball}

\textbf{Description}

The heights and weights of MLB players.

Format

This Data Frame contains the following columns:

Player: Player in the sample

height: height of baseball player (inches)

weight: weight of baseball player (pounds)

Source MLB heightsweights. (2013, November 16). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_MLB\_HeightsWeights

References SOCR Data Frame of MLB Heights Weights from UCLA.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Different species have different body weights and brain weights are in
  Table~\ref{tbl-Body}.
  (\textbackslash{}``Brain2bodyweight,\textbackslash{}'' 2013). Create a
  scatter plot and find a regression equation between body weights and
  brain weights. Then use the regression equation to find the brain
  weight for a species that has a body weight of 62 kg and for a species
  that has a body weight of 180,000 kg. Which brain weight that you
  calculated do you think is closer to the true brain weight? Why?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Body }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/body.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Body))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrr@{}}

\caption{\label{tbl-Body}Body Weights and Brain Weights of Species}

\tabularnewline

\toprule\noalign{}
species & bodyweight & brainweight & brainbodyproportion \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Newborn\_Human & 3.20 & 0.3749848 & 0.1171828 \\
Adult\_Human & 73.00 & 1.3499816 & 0.0184929 \\
Pithecanthropus\_Man & 70.00 & 0.9250109 & 0.0132144 \\
Squirrel & 0.80 & 0.0076204 & 0.0095254 \\
Hamster & 0.15 & 0.0014061 & 0.0093742 \\
Chimpanzee & 50.00 & 0.4199812 & 0.0083996 \\

\end{longtable}

\textbf{Code book for Data Frame Body}

\textbf{Description}

The body weight, brain weight, and brain/body proportion of different
species of animals.

Format

This Data Frame contains the following columns:

species: species of animal

bodyweight: the body weight of the species (kg)

brainweight: the brain weight of the species (kg)

brainbodyproportion: the ratio of brain weight to body weight of the
species

Source Brain2bodyweight. (2013, November 16). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_Brain2BodyWeight

References SOCR Data of species body weights and brain weights from
UCLA.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  A sample of hot dogs was taken and the amount of sodium (in mg) and
  calories were measured. (\textbackslash{}``Data
  hotdogs,\textbackslash{}'' 2013) The Data Frame are in
  Table~\ref{tbl-Hotdog}. Create a scatter plot and find a regression
  equation between amount of calories and amount of sodium. Then use the
  regression equation to find the amount of sodium a hot dog has if it
  is 170 calories and if it is 120 calories. Which sodium level that you
  calculated do you think is closer to the true sodium level? Why?
\end{enumerate}

\textbf{Code book for data frame Hotdog is below
Table~\ref{tbl-Hotdog}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Per capita income in 1960 dollars for European countries and the
  percent of the labor force that works in agriculture in 1960 are in
  Table~\ref{tbl-Agriculture} (\textbackslash{}``OECD economic
  development,\textbackslash{}'' 2013). Create a scatter plot and find a
  regression equation between percent of labor force in agriculture and
  per capita income. Then use the regression equation to find the per
  capita income in a country that has 21 percent of labor in agriculture
  and in a country that has 2 percent of labor in agriculture. Which per
  capita income that you calculated do you think is closer to the true
  income? Why?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Agriculture }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/agriculture.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Agriculture))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}

\caption{\label{tbl-Agriculture}Percent of Labor in Agriculture and Per
Capita Income for European Countries}

\tabularnewline

\toprule\noalign{}
country & percapita & agriculture & industry & services \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
SWEEDEN & 1644 & 14 & 53 & 33 \\
SWITZERLAND & 1361 & 11 & 56 & 33 \\
LUXEMBOURG & 1242 & 15 & 51 & 34 \\
U. KINGDOM & 1105 & 4 & 56 & 40 \\
DENMARK & 1049 & 18 & 45 & 37 \\
W. GERMANY & 1035 & 15 & 60 & 25 \\

\end{longtable}

\textbf{Code book for Data Frame Agriculture}

\textbf{Description}

The per capita income and percent in different industries in European
countries

Format

This Data Frame contains the following columns:

country: country in Europe

percapita: per captia income

agriculture: percentage of workforce in agriculture

industry: percentage of workforce in industry

services: percentage of workforce in services

Source OECD economic development. (2013, December 04). Retrieved from
http://lib.stat.cmu.edu/DASL/Datafiles/oecdat.html

References Data And Story Library

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  Cigarette smoking and cancer have been linked. The number of deaths
  per one hundred thousand from bladder cancer and the number of
  cigarettes sold per capita in 1960 are in table
  \textbackslash\#10.1.11 (\textbackslash{}``Smoking and
  cancer,\textbackslash{}'' 2013). Create a scatter plot and find a
  regression equation between number of cigarettes smoked and number of
  deaths from bladder cancer. Then use the regression equation to find
  the number of deaths from bladder cancer when the cigarette sales were
  20 per capita and when the cigarette sales were 6 per capita. Which
  number of deaths that you calculated do you think is closer to the
  true number? Why?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cancer }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/cancer\_1.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Cancer))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrr@{}}

\caption{\label{tbl-Cancer}Number of Cigarettes and Number of Bladder
Cancer Deaths}

\tabularnewline

\toprule\noalign{}
state & cig & bladder & lung & kidney & leukemia \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
AL & 18.20 & 2.90 & 17.05 & 1.59 & 6.15 \\
AZ & 25.82 & 3.52 & 19.80 & 2.75 & 6.61 \\
AR & 18.24 & 2.99 & 15.98 & 2.02 & 6.94 \\
CA & 28.60 & 4.46 & 22.07 & 2.66 & 7.06 \\
CT & 31.10 & 5.11 & 22.83 & 3.35 & 7.20 \\
DE & 33.60 & 4.78 & 24.55 & 3.36 & 6.45 \\

\end{longtable}

\textbf{Code book for Data Frame Cancer}

\textbf{Description} This data frame contains the number of cigarette
sales (per Capita), number of cancer Deaths (per 100 Thousand) from
bladder, lung, kidney, and leukemia.

Format

This Data Frame contains the following columns:

state: state in US

cig: the number of cigarette sales (per capita)

bladder: number of deaths per 100 thousand from bladder cancer

lung: number of deaths per 100 thousand from lung cancer

kidney: number of deaths per 100 thousand from kidney cancer

leukemia: number of deaths per 100 thousand from leukemia

Source Smoking and cancer. (2013, December 04). Retrieved from
http://lib.stat.cmu.edu/DASL/Datafiles/cigcancerdat.html

References Data And Story Library

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  The weight of a car can influence the mileage that the car can obtain.
  A random sample of cars' weights and mileage was collected and are in
  Table~\ref{tbl-Cars} (\textbackslash{}``us auto
  mileage,\textbackslash{}'' 2019). Create a scatter plot and find a
  regression equation between weight of cars and mileage. Then use the
  regression equation to find the mileage on a car that weighs 3800
  pounds and on a car that weighs 2000 pounds. Which mileage that you
  calculated do you think is closer to the true mileage? Why?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cars}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/cars.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Cars))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrrr@{}}

\caption{\label{tbl-Cars}Weights and Mileages of Cars}

\tabularnewline

\toprule\noalign{}
make & vol & hp & mpg & sp & wt \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
GM/GeoMetroXF1 & 89 & 49 & 65.4 & 96 & 17.5 \\
GM/GeoMetro & 92 & 55 & 56.0 & 97 & 20.0 \\
GM/GeoMetroLSI & 92 & 55 & 55.9 & 97 & 20.0 \\
SuzukiSwift & 92 & 70 & 49.0 & 105 & 20.0 \\
DaihatsuCharade & 92 & 53 & 46.5 & 96 & 20.0 \\
GM/GeoSprintTurbo & 89 & 70 & 46.2 & 105 & 20.0 \\

\end{longtable}

\textbf{Code book for Data Frame Cars}

\textbf{Description} Variation in gasoline mileage among makes and
models of automobiles is influenced substantially by the weight and
horsepower of the vehicles. When miles per gallon and horsepower are
transformed to logarithms, the linearity of the regression is improved.
A negative second order term is required to fit the logarithmic mileage
to weight relation. If the logged variables are standardized, the
coefficients of the first order terms indicate the standard units change
in log mileage per one standard unit change in the predictor variable at
the (logarithmic) mean. This change is constant in the case of mileage
to horsepower, but not for mileage to weight. The coefficient of the
second order weight term indicates the change in standardized slope
associated with a one standard deviation increase in the logarithm of
weight.

Format

This Data Frame contains the following columns:

make: the type of car

vol: cubic feet of cab space

hp: engine horsepower

npg: the average mileage of the car

sp: top speed (mph)

wt: the weight of the car (100 pounds)

Source (n.d.). Retrieved July 21, 2019, from
https://www3.nd.edu/\textasciitilde busiforc/handouts/Data and
Stories/regression/us auto mileage/usautomileage.html

References R.M. Heavenrich, J.D. Murrell, and K.H. Hellman, Light Duty
Automotive Technology and Fuel Economy Trends Through 1991, U.S.
Environmental Protection Agency, 1991 (EPA/AA/CTAB/91-02).

**\textbackslash{} **

\section{Correlation}\label{correlation}

A \textbf{correlation} exists between two variables when the values of
one variable are somehow associated with the values of the other
variable. Correlation is a word used in every day life and many people
think they understand what it means. What it really means is a measure
of how strong the linear relationship is between two variables. However,
it doesn't give you a strong indication of how well the relationship
explains the variability in the response variable, and it doesn't work
for more than two variables. There is a way to look at how much of the
relationship explains the variability in the response variable.

There are many types of patterns one can see in the data. Common types
of patterns are linear, exponential, logarithmic, or periodic. To see
this pattern, you can draw a scatter plot of the data.This course will
mostly deal with linear, but the other patterns exist.

There is some variability in the response variable values, such as
calories. Some of the variation in calories is due to alcohol content
and some is due to other factors. How much of the variation in the
calories is due to alcohol content?

When considering this question, you want to look at how much of the
variation in calories is explained by alcohol content and how much is
explained by other variables. Realize that some of the changes in
calories have to do with other ingredients. You can have two beers at
the same alcohol content, but one beer has higher calories because of
the other ingredients. Some variability is explained by the model and
some variability is not explained. Together, both of these give the
total variability. This is (total variation)=(explained
variation)+(unexplained variation)

The proportion of the variation that is explained by the model is
\(R^2=\frac{\text{explained variation}}{\text{total variation}}\)

This is known as the \textbf{coefficient of determination}.

To find the coefficient of determination, on r Studio, the command is to
first name the linear model to be something and then you can find the
summary of that named linear model. The next example shows how to find
this.

\subsection{\texorpdfstring{Example: Find the coefficient of
determination
\(R^2\)}{Example: Find the coefficient of determination R\^{}2}}\label{example-find-the-coefficient-of-determination-r2}

How well does the alcohol content of a beer explain the variability in
the number of calories in 12-ounce beer? To determine this a sample of
beer's alcohol content and calories (Find Out How Many Calories in
Beer?, 2019), is in Table~\ref{tbl-Beer}.

\subsubsection{Solution}\label{solution-84}

State random variables

\(x\) = alcohol content in the beer

\(y\) = calories in 12 ounce beer

First find and name the linear model, how about lm\_out\_beer, so you
can refer to it. Then use the command

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_out\_beer}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(calories}\SpecialCharTok{\textasciitilde{}}\NormalTok{alcohol, }\AttributeTok{data=}\NormalTok{Beer) }\CommentTok{\#creates the linear model and calls it lm\_out\_beer }
\FunctionTok{rsquared}\NormalTok{(lm\_out\_beer) }\CommentTok{\#finds the rsquared of the linear model created.}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.8164314
\end{verbatim}

The \(R^2=0.8164\). Thus, 81.64\% of the variation in calories is
explained to the linear relationship between alcohol content and
calories. The other 18.36\% of the variation is due to other factors. A
really good coefficient of determination has a very small, unexplained
part. So this means you probably don't need to look for a co-variate.

This is all you have to do to find how much of the variability in the
response variable is explained by the linear model.

If you do want to talk about correlation, correlation is another measure
of the strength of the linear relationship. The symbol for the
coefficient correlation is \(r\). To find \(r\), it is the square root
of \(R^2\). So \(r=\sqrt{R^2}\). So all you need to do is take the
square root of the answer you already found. The only thing is that when
you take a square root of both sides of an equation you need the \(\pm\)
symbol. This is because you don't know if you want the positive or
negative root. However, remember to read graphs from left to right, the
same as you read words. If the graph goes up the correlation is positive
and if the graph goes down the correlation is negative. Just affix the
correct sign to the \(r\) value. You can also use this command and r
will give you the correct sign.

cor(respose\_variable\textasciitilde explanatory\_variable,
data=Data\_Frame)

The \textbf{linear correlation coefficient} is a number that describes
the strength of the linear relationship between the two variables. It is
also called the Pearson correlation coefficient after Karl Pearson who
developed it.

\subsection{Interpretation of the correlation
coefficient}\label{interpretation-of-the-correlation-coefficient}

\(r\) is always between \(-1\) and 1. \(r\) = \(-1\) means there is a
perfect negative linear correlation and \(r\) = 1 means there is a
perfect positive correlation. The closer \(r\) is to 1 or \(-1\), the
stronger the correlation. The closer \(r\) is to 0, the weaker the
correlation. CAREFUL: \(r\) = 0 does not mean there is no correlation.
It just means there is \textbf{no linear correlation}. There might be a
very strong curved pattern.

\subsection{\texorpdfstring{Example: Calculating the Linear Correlation
Coefficient,
\(r\)}{Example: Calculating the Linear Correlation Coefficient, r}}\label{example-calculating-the-linear-correlation-coefficient-r}

How strong is the positive relationship between the alcohol content and
the number of calories in 12-ounce beer? To determine if there is a
linear correlation, a random sample was taken of beer's alcohol content
and calories for several different beers (\textbackslash{}``Calories in
beer,,\textbackslash{}'' 2011), and the Data Frame are in
Table~\ref{tbl-Beer}. Find the correlation coefficient and interpret
that value.

\subsubsection{Solution}\label{solution-85}

State random variables

\(x\) = alcohol content in the beer

\(y\) = calories in 12 ounce beer

From example 10.2.1, \(R^2=0.8164\). The correlation coefficient =
\(r=\sqrt{r^2}=\sqrt{0.8164}=0.9035486\), or use this command in r

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{cor}\NormalTok{(calories}\SpecialCharTok{\textasciitilde{}}\NormalTok{alcohol, }\AttributeTok{data=}\NormalTok{Beer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.9035659
\end{verbatim}

The scatter graph went up from left to right, so the correlation
coefficient is positive. Since 0.90 is close to 1, then that means there
is a fairly strong positive correlation.

\subsection{Causation}\label{causation}

One common mistake people make is to assume that because there is a
correlation, then one variable causes the other. This is usually not the
case. That would be like saying the amount of alcohol in the beer causes
it to have a certain number of calories. However, fermentation of sugars
is what causes the alcohol content. The more sugars you have, the more
alcohol can be made, and the more sugar, the higher the calories. It is
actually the amount of sugar that causes both. Do not confuse the idea
of correlation with the concept of causation. Just because two variables
are correlated does not mean one causes the other to happen. However,
the new theory is showing that if you have a relationship between two
variables and a strong correlation, and you can show that there are no
other variables that could explain the change, then you can show
causation. This is how doctors have shown that smoking causes kidney
cancer. Just realize that proving that one caused the other is a
difficult process, and causation should not be just assumed.

\subsection{Example: Correlation Versus
Causation}\label{example-correlation-versus-causation}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  A study showed a strong linear correlation between per capita beer
  consumption and teacher's salaries. Does giving a teacher a raise
  cause people to buy more beer? Does buying more beer cause teachers to
  get a raise?
\item
  A study shows that there is a correlation between people who have had
  a root canal and those that have cancer. Does that mean having a root
  canal causes cancer?
\end{enumerate}

\subsubsection{Solution}\label{solution-86}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  A study showed a strong linear correlation between per capita beer
  consumption and teacher's salaries. Does giving a teacher a raise
  cause people to buy more beer? Does buying more beer cause teachers to
  get a raise?

  There is probably some other factor causing both of them to increase
  at the same time. Think about this: In a town where people have little
  extra money, they won't have money for beer and they won't give
  teachers raises. In another town where people have more extra money to
  spend it will be easier for them to buy more beer and they would be
  more willing to give teachers raises.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\item
  A study shows that there is a correlation between people who have had
  a root canal and those that have cancer. Does that mean having a root
  canal causes cancer?

  Just because there is positive correlation doesn't mean that one
  caused the other. It turns out that there is a positive correlation
  between eating carrots and cancer, but that doesn't mean that eating
  carrots causes cancer. In other words, there are lots of relationships
  you can find between two variables, but that doesn't mean that one
  caused the other.
\end{enumerate}

Remember a correlation only means a pattern exists. It does not mean
that one variable causes the other variable to change.

\subsection{Homework for Correlation
Section}\label{homework-for-correlation-section}

\textbf{For each problem, state the random variables.The Data Frame in
this section are in section 10.1 and will be used in section 10.3.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When an anthropologist finds skeletal remains, they need to figure out
  the height of the person. The height of a person (in cm) and the
  length of their metacarpal bone 1 (in mm) were collected and are in
  Table~\ref{tbl-Metacarpal} (\textbackslash{}``Prediction of
  height,\textbackslash{}'' 2013). Find the coefficient of determination
  and the correlation coefficient, then interpret both.
\end{enumerate}

\textbf{Code book for Data Frame Metacarpal is below}
Table~\ref{tbl-Metacarpal}.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Table~\ref{tbl-House} contains the value of the house and the amount
  of rental income in a year that the house brings in
  (\textbackslash{}``Capital and rental,\textbackslash{}'' 2013). Find
  the coefficient of determination and the correlation coefficient, then
  interpret both.
\end{enumerate}

\textbf{Code book for Data Frame House is below Table~\ref{tbl-House}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The World Bank collects information on the life expectancy of a person
  in each country (\textbackslash{}``Life expectancy
  at,\textbackslash{}'' 2013) and the fertility rate per woman in the
  country (\textbackslash{}``Fertility rate,\textbackslash{}'' 2013).
  The Data Frame for countries for the year 2011 are in
  Table~\ref{tbl-Fertility}. Find the coefficient of determination and
  the correlation coefficient, then interpret both.
\end{enumerate}

\textbf{Code book for Data Frame Fertility is below
Table~\ref{tbl-Fertility}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The World Bank collected data on the percentage of gross domestic
  product (GDP) that a country spends on health expenditures (Current
  health expenditure (\% of GDP), 2019), the fertility rate of the
  country (Fertility rate, total (births per woman), 2019), and the
  percentage of woman receiving prenatal care (Pregnant women receiving
  prenatal care (\%), 2019). The Data Frame for the countries where this
  information is available in Table~\ref{tbl-Fert_prenatal}. Find the
  coefficient of determination and the correlation coefficient between
  the percentage spent on health expenditure and the percentage of women
  receiving prenatal care, then interpret both.
\end{enumerate}

\textbf{Code book for Data Frame Fert\_prenatal is below
Table~\ref{tbl-Fert_prenatal}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The height and weight of baseball players are in
  Table~\ref{tbl-Baseball} (\textbackslash{}``MLB
  heightsweights,\textbackslash{}'' 2013). Find the coefficient of
  determination and the correlation coefficient, then interpret both.
\end{enumerate}

\textbf{Code book for Data Frame Baseball is below
Table~\ref{tbl-Baseball}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Different species have different body weights and brain weights are in
  Table~\ref{tbl-Body}
  (\textbackslash{}``Brain2bodyweight,\textbackslash{}'' 2013). Find the
  coefficient of determination and the correlation coefficient, then
  interpret both.
\end{enumerate}

\textbf{Code book for Data Frame Body is below Table~\ref{tbl-Body}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  A sample of hot dogs was taken and the amount of sodium (in mg) and
  calories were measured. (\textbackslash{}``Data
  hotdogs,\textbackslash{}'' 2013) The Data Frame are in
  Table~\ref{tbl-Hotdog}. Find the coefficient of determination and the
  correlation coefficient, then interpret both.
\end{enumerate}

\textbf{Code book for data frame Hotdog is below
Table~\ref{tbl-Hotdog}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Per capita income in 1960 dollars for European countries and the
  percent of the labor force that works in agriculture in 1960 are in
  Table~\ref{tbl-Agriculture} (\textbackslash{}``OECD economic
  development,\textbackslash{}'' 2013). Find the coefficient of
  determination and the correlation coefficient, then interpret both.
\end{enumerate}

\textbf{Code book for Data Frame Agriculture is
Table~\ref{tbl-Agriculture}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  Cigarette smoking and cancer have been linked. The number of deaths
  per one hundred thousand from bladder cancer and the number of
  cigarettes sold per capita in 1960 are in Table~\ref{tbl-Cancer}
  (\textbackslash{}``Smoking and cancer,\textbackslash{}'' 2013). Find
  the coefficient of determination and the correlation coefficient, then
  interpret both.
\end{enumerate}

\textbf{Code book for Data Frame Cancer is below
Table~\ref{tbl-Cancer}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  The weight of a car can influence the mileage that the car can obtain.
  A random sample of cars' weights and mileage was collected and are in
  Table~\ref{tbl-Cars} (\textbackslash{}``us auto
  mileage,\textbackslash{}'' 2019). Find the coefficient of
  determination and the correlation coefficient, then interpret both.
\end{enumerate}

\textbf{Code book for Data Frame Cars is below Table~\ref{tbl-Cars}.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{10}
\item
  There is a correlation between police expenditure and crime rate. Does
  this mean that spending more money on police causes the crime rate to
  decrease? Explain your answer.
\item
  There is a correlation between tobacco sales and alcohol sales. Does
  that mean that using tobacco causes a person to also drink alcohol?
  Explain your answer.
\item
  There is a correlation between the average temperature in a location
  and the mortality rate from breast cancer. Does that mean that higher
  temperatures cause more women to die of breast cancer? Explain your
  answer.
\item
  There is a correlation between the length of time a tableware company
  polishes a dish and the price of the dish. Does that mean that the
  time a plate is polished determines the price of the dish? Explain
  your answer.
\end{enumerate}

\section{Inference for Regression and
Correlation}\label{inference-for-regression-and-correlation}

The idea behind regression is to find an equation that relates the
response variable to the explanatory variables, and then use that
equation to predict values of the response variable from values of the
explanatory variables. But how do you now how good that estimate is?
First you need a measure of the variability in the estimate, called the
standard error of the estimate. The definition of the \textbf{standard
error of the estimate} is how much error is in the estimate from the
regression equation.

\textbf{Standard Error of the Estimate} formula
\(S_e=\sum{(\hat{y}-y)^2}\)

This formula is hard to work with but luckily r Studio will calculate it
for you. In fact the command has already been used, so this information
is already found. The command is

summary(lm\_out) \#lm\_out is the name you gave the linear model

\subsection{Example: Finding the Standard Error of the
Estimate}\label{example-finding-the-standard-error-of-the-estimate}

Is there a relationship between the alcohol content and the number of
calories in 12-ounce beer? To determine if there is a sample of beer's
alcohol content and calories (Find Out How Many Calories in Beer?,
2019), in Table~\ref{tbl-Beer}. Find the standard error of the estimate.

\textbf{Code book for data frame Beer is below Table~\ref{tbl-Beer}.}

\subsubsection{Solution}\label{solution-87}

\(x\) = alcohol content in the beer

\(y\) = calories in 12 ounce beer

First save the linear model with a name like lm\_out\_beer. Then use
summary(lm\_out\_beer) to find the standard error of the estimate.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_out\_beer}\OtherTok{=}\FunctionTok{lm}\NormalTok{(calories}\SpecialCharTok{\textasciitilde{}}\NormalTok{alcohol, }\AttributeTok{data=}\NormalTok{Beer) }
\FunctionTok{summary}\NormalTok{(lm\_out\_beer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = calories ~ alcohol, data = Beer)

Residuals:
    Min      1Q  Median      3Q     Max 
-51.869 -11.421  -0.145  11.855  55.473 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   14.527      4.577   3.174  0.00171 ** 
alcohol     2672.359     84.478  31.634  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 17.62 on 225 degrees of freedom
Multiple R-squared:  0.8164,    Adjusted R-squared:  0.8156 
F-statistic:  1001 on 1 and 225 DF,  p-value: < 2.2e-16
\end{verbatim}

The standard error of the estimate is the residual standard error in the
output. So the standard error of the estimate for the number of calories
in a beer related to alcohol content is \(S_e=17.62\)

\subsection{Prediction Interval}\label{prediction-interval}

Using the regression equation you can predict the number of calories
from the alcohol content. However, you only find one value. The problem
is that beers vary a bit in calories even if they have the same alcohol
content. It would be nice to have a range instead of a single value. The
range is called a prediction interval.

\subsection{\texorpdfstring{Prediction Interval for an Individual
\(y\)}{Prediction Interval for an Individual y}}\label{prediction-interval-for-an-individual-y}

Given the fixed value \(x\), the prediction interval for an individual
\(y\) is \(\hat{y}\pm E\) where \(E\) is the error of the estimate.

r will produce the prediction interval for you. The process follows

To calculate the linear model. Note this may already been done.

lm\_out = lm(explanatory\_variable\textasciitilde{} response\_variable)

The following will compute a prediction interval. For the prediction
variable set to a particular value (put that value in place of the word
value), at a particular C level (given as a decimal).

predict(lm\_out, newdata=list(prediction\_variable = value),
interval=``prediction'', level=C)

\subsection{Example \textbackslash\#10.3.2: Find the Prediction
Interval}\label{example-10.3.2-find-the-prediction-interval}

Is there a relationship between the alcohol content and the number of
calories in 12-ounce beer? To determine if there is a sample of beer's
alcohol content and calories (Find Out How Many Calories in Beer?,
2019), in Table~\ref{tbl-Beer}. Find a 95\% prediction interval for the
number of calories when the alcohol content is 7.0\%. content and
calories (\textbackslash{}``Calories in beer,,\textbackslash{}'' 2011).
The Data Frame is in Table~\ref{tbl-Beer}.

\subsubsection{Solution}\label{solution-88}

\(x\) = alcohol content in the beer

\(y\) = calories in 12 ounce beer

Computing the prediction interval using r Studio

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{predict}\NormalTok{(lm\_out\_beer, }\AttributeTok{newdata=}\FunctionTok{list}\NormalTok{(}\AttributeTok{alcohol=}\FloatTok{0.07}\NormalTok{), }\AttributeTok{interval =} \StringTok{"prediction"}\NormalTok{, }\AttributeTok{level=}\FloatTok{0.95}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
       fit      lwr     upr
1 201.5922 166.6664 236.518
\end{verbatim}

the fit = 201.5922 is the number of calories in the beer when the
alcohol content is 7.0\%. The prediction interval is between the lower
(lwr) and upper (upr). The prediction interval is 166.6664 and 236.528.
That means that 95\% of all beer that has 7.0\% alcohol has between 167
and 237 calories.

\subsection{Hypothesis Test for Correlation or
Slope:}\label{hypothesis-test-for-correlation-or-slope}

How do you really say you have a correlation or a linear relationship?
Can you test to see if there really is a correlation or linear
relationship? Of course, the answer is yes. The hypothesis test for
correlation is as follows.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables in words.
\end{enumerate}

\(x\) = explanatory variable

\(y\) = response variable

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o: \text{there is not a correlation or linear relationship}\)

\(H_a: \text{there is a correlation or linear relationship}\)

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

The conditions for the hypothesis test are the same conditions for
regression and correlation.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the test statistic and p-value
\end{enumerate}

The test statistic and p-value is already in the summary(lm\_out)
output.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject \(H_o\) or fail to reject \(H_o\). The
rule is: if the p-value \(<\alpha\), then reject \(H_o\). If the p-value
\(\ge \alpha\), then fail to reject \(H_o\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

\subsection{Example: Testing the Claim of a Linear
Correlation}\label{example-testing-the-claim-of-a-linear-correlation}

Is there a linear relationship, or correlation, between the alcohol
content and the number of calories in 12-ounce beer? To determine if
there is a sample of beer's alcohol content and calories (Find Out How
Many Calories in Beer?, 2019), in Table~\ref{tbl-Beer}. Test at the 5\%
level.

\subsubsection{Solution}\label{solution-89}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables in words.
\end{enumerate}

\(x\) = alcohol content in the beer

\(y\) = calories in 12 ounce beer

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o: \text{there is not a correlation or linear relationship}\)

\(H_a: \text{there is a correlation or linear relationship}\)

level of significance \(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

The conditions for the hypothesis test were already checked in
\hyperref[example-find-the-equation-of-the-regression-line]{Example:
Find the Equation of the Regression Line}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Find the test statistic and p-value
\end{enumerate}

The command on r Studio is

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{lm\_out\_beer}\OtherTok{\textless{}{-}}\FunctionTok{lm}\NormalTok{(calories}\SpecialCharTok{\textasciitilde{}}\NormalTok{alcohol, }\AttributeTok{data=}\NormalTok{Beer) }
\FunctionTok{summary}\NormalTok{(lm\_out\_beer)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

Call:
lm(formula = calories ~ alcohol, data = Beer)

Residuals:
    Min      1Q  Median      3Q     Max 
-51.869 -11.421  -0.145  11.855  55.473 

Coefficients:
            Estimate Std. Error t value Pr(>|t|)    
(Intercept)   14.527      4.577   3.174  0.00171 ** 
alcohol     2672.359     84.478  31.634  < 2e-16 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

Residual standard error: 17.62 on 225 degrees of freedom
Multiple R-squared:  0.8164,    Adjusted R-squared:  0.8156 
F-statistic:  1001 on 1 and 225 DF,  p-value: < 2.2e-16
\end{verbatim}

The test statistic is the t value in the table for the explanatory
variable. In this case that is 31.634. The p-value is the
Pr(\textgreater\textbar t\textbar) which is \(2.2X10^{-16}\).

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Reject \(H_o\) since the p-value is less than 0.05.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is enough evidence to show that there is a correlation and linear
relationship between alcohol content and number of calories in a
12-ounce bottle of beer.

\subsection{Homework for Inference for Regression and Correlation
Section}\label{homework-for-inference-for-regression-and-correlation-section}

\textbf{For each problem, state the random variables. The Data Frame in
this section are in the homework for section 10.1 and were also used in
section 10.2.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  When an anthropologist finds skeletal remains, they need to figure out
  the height of the person. The height of a person (in cm) and the
  length of their metacarpal bone 1 (in mm) were collected and are in
  Table~\ref{tbl-Metacarpal} (\textbackslash{}``Prediction of
  height,\textbackslash{}'' 2013).
\end{enumerate}

\textbf{Code book for Data Frame Metacarpal is below
Table~\ref{tbl-Metacarpal}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 99\% prediction interval for height of a person with a
  metacarpal length of 44 mm.
\item
  Test at the 1\% level for a correlation or linear relationship between
  length of metacarpal bone 1 and height of a person.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Table~\ref{tbl-House} contains the value of the house and the amount
  of rental income in a year that the house brings in
  (\textbackslash{}``Capital and rental,\textbackslash{}'' 2013).
\end{enumerate}

\textbf{Code book for Data Frame House is below Table~\ref{tbl-House}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 95\% prediction interval for the rental income on a house
  worth \textbackslash\$230,000.
\item
  Test at the 5\% level for a correlation or linear relationship between
  house value and rental amount.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  The World Bank collects information on the life expectancy of a person
  in each country (\textbackslash{}``Life expectancy
  at,\textbackslash{}'' 2013) and the fertility rate per woman in the
  country (\textbackslash{}``Fertility rate,\textbackslash{}'' 2013).
  The Data Frame for countries for the year 2011 is in
  Table~\ref{tbl-Fertility}.
\end{enumerate}

\textbf{Code book for Data Frame Fertility is below
Table~\ref{tbl-Fertility}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 99\% prediction interval for the life expectancy for a
  country that has a fertility rate of 2.7.
\item
  Test at the 1\% level for a correlation or linear relationship between
  fertility rate and life expectancy.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  The World Bank collected data on the percentage of gross domestic
  product (GDP) that a country spends on health expenditures (Current
  health expenditure (\% of GDP), 2019), the fertility rate of the
  country (Fertility rate, total (births per woman), 2019), and the
  percentage of woman receiving prenatal care (Pregnant women receiving
  prenatal care (\%), 2019). The Data Frame for the countries where this
  information is available in Table~\ref{tbl-Fert_prenatal}.
\end{enumerate}

\textbf{Code book for Data Frame Fert\_prenatal is below
Table~\ref{tbl-Fert_prenatal}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 95\% prediction interval for the percentage of woman
  receiving prenatal care for a country that spends 5.0 \% of GDP on
  health expenditure.
\item
  Test at the 5\% level for a correlation or linear relationship between
  percentage spent on health expenditure and the percentage of women
  receiving prenatal care.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The height and weight of baseball players are in
  Table~\ref{tbl-Baseball} (\textbackslash{}``MLB
  heightsweights,\textbackslash{}'' 2013).
\end{enumerate}

\textbf{Code book for Data Frame Baseball is below
Table~\ref{tbl-Baseball}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 95\% prediction interval for the weight of a baseball player
  that is 75 inches tall.
\item
  Test at the 5\% level for a correlation or linear relationship between
  height and weight of baseball players.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Different species have different body weights and brain weights are in
  Table~\ref{tbl-Body}
  (\textbackslash{}``Brain2bodyweight,\textbackslash{}'' 2013).
\end{enumerate}

\textbf{Code book for Data Frame Body is below Table~\ref{tbl-Body}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 99\% prediction interval for the brain weight for a species
  that has a body weight of 62 kg.
\item
  Test at the 1\% level for a correlation or linear relationship between
  body weights and brain weights.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  A sample of hot dogs was taken and the amount of sodium (in mg) and
  calories were measured. (\textbackslash{}``Data
  hotdogs,\textbackslash{}'' 2013) The Data Frame are in
  Table~\ref{tbl-Hotdog}.
\end{enumerate}

\textbf{Code book for data frame Hotdog is below
Table~\ref{tbl-Hotdog}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 95\% prediction interval for the amount of sodium a beef hot
  dog has if it is 170 calories.
\item
  Test at the 5\% level for a correlation or linear relationship between
  amount of calories and amount of sodium.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{7}
\tightlist
\item
  Per capita income in 1960 dollars for European countries and the
  percent of the labor force that works in agriculture in 1960 are in
  Table~\ref{tbl-Agriculture} (\textbackslash{}``OECD economic
  development,\textbackslash{}'' 2013).
\end{enumerate}

\textbf{Code book for Data Frame Agriculture is below
Table~\ref{tbl-Agriculture}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 90\% prediction interval for the per capita income in a
  country that has 21 percent of labor in agriculture.
\item
  Test at the 5\% level for a correlation or linear relationship between
  percent of labor force in agriculture and per capita income.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{8}
\tightlist
\item
  Cigarette smoking and cancer have been linked. The number of deaths
  per one hundred thousand from bladder cancer and the number of
  cigarettes sold per capita in 1960 are in Table~\ref{tbl-Cancer}
  (``Smoking and cancer,'' 2013).
\end{enumerate}

\textbf{Code book for Data Frame Cancer is below
Table~\ref{tbl-Cancer}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 99\% prediction interval for the number of deaths from
  bladder cancer when the cigarette sales were 20 per capita.
\item
  Test at the 1\% level for a correlation or linear relationship between
  cigarette smoking and deaths of bladder cancer.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{9}
\tightlist
\item
  The weight of a car can influence the mileage that the car can obtain.
  A random sample of cars' weights and mileage was collected and are in
  Table~\ref{tbl-Cars} (``us auto mileage,'' 2019).
\end{enumerate}

\textbf{Code book for Data Frame Cars is below Table~\ref{tbl-Cars}.}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  Find the standard error of the estimate.
\item
  Compute a 95\% prediction interval for the mileage on a car that
  weighs 3800 pounds.
\item
  Test at the 5\% level for a correlation or linear relationship between
  the weight of cars and mileage.
\end{enumerate}

\bookmarksetup{startatroot}

\chapter{Chi-squared and ANOVA Tests}\label{chi-squared-and-anova-tests}

This chapter presents material on three more hypothesis tests. One is
used to determine significant relationship between two qualitative
variables, the second is used to determine if the sample data has a
particular distribution, and the last is used to determine significant
relationships between means of 3 or more samples.

\section{Chi-Square Test for
Independence}\label{chi-square-test-for-independence}

Remember, qualitative data is where you collect data on individuals that
are categories or names. Then you would count how many of the
individuals had particular qualities. An example is that there is a
theory that there is a relationship between breastfeeding and autism. To
determine if there is a relationship, researchers could collect the time
period that a mother breastfed her child and if that child was diagnosed
with autism. Then you would have a table containing this information.
Now you want to know if each cell is independent of each other cell.
Remember, independence says that one event does not affect another
event. Here it means that having autism is independent of being
breastfed. What you really want is to see if they are not independent.
In other words, does one affect the other? If you were to do a
hypothesis test, this is your alternative hypothesis and the null
hypothesis is that they are independent. There is a hypothesis test for
this and it is called the \textbf{Chi-Square Test for Independence}.
Technically it should be called the Chi-Square Test for Dependence, but
for historical reasons it is known as the test for independence. Just as
with previous hypothesis tests, all the steps are the same except for
the conditions and the test statistic.

\subsection{Hypothesis Test for Chi-Square
Test}\label{hypothesis-test-for-chi-square-test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\) the two variables are independent (this means that the one
variable is not affected by the other)

\(H_a:\) the two variables are dependent (this means that the one
variable is affected by the other)

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample is taken. Check: describe the process used to
  collect sample.
\item
  State: Expected frequencies for each cell are greater than or equal to
  5, which means \(E\ge5\). Check: The expected frequencies, \(E\), will
  be calculated later.
\end{enumerate}

3. Find the test statistic and p-value

Finding the test statistic involves several steps. First the data is
collected and counted, and then it is organized into a table (in a table
each entry is called a cell). These values are known as the observed
frequencies, which the symbol for an observed frequency is \(O\). Each
table is made up of rows and columns. Then each row is totaled to give a
row total and each column is totaled to give a column total.

The null hypothesis is that the variables are independent. Using the
multiplication rule for independent events you can calculate the
probability of being one value of the first variable, \(A\), and one
value of the second variable, \(B\) (the probability of a particular
cell). Remember in a hypothesis test, you assume that is true, the two
variables are assumed to be independent.

Now you want to find out how many individuals you expect to be in a
certain cell. To find the expected frequencies, you just need to
multiply the probability of that cell times the total number of
individuals. Do not round the expected frequencies.

If the variables are independent the expected frequencies and the
observed frequencies should be the same. The test statistic here will
involve looking at the difference between the expected frequency and the
observed frequency for each cell. Then you want to find the ``total
difference'' of all of these differences. The larger the total, the
smaller the chances that you could find that test statistic given that
the condition of independence is true. That means that the condition of
independence is not true. How do you find the test statistic? First find
the differences between the observed and expected frequencies. Because
some of these differences will be positive and some will be negative,
you need to square these differences. These squares could be large just
because the frequencies are large, you need to divide by the expected
frequencies to scale them. Then finally add up all of these fractional
values. This is the test statistic.

Test Statistic:

Using r: See
\hyperref[example-hypothesis-test-with-chi-square-test]{Example:
Hypothesis Test with Chi-Square Test} for the process

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject \(H_o\) or fail to reject \(H_o\). The
rule is: if the p-value \(<\alpha\), then reject \(H_o\). If the
p-value\(\ge \alpha\), then fail to reject \(H_o\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

\subsubsection{Example: Hypothesis Test with Chi-Square
Test}\label{example-hypothesis-test-with-chi-square-test}

Is there a relationship between autism and breastfeeding? To determine
if there is, a researcher asked mothers of autistic and non-autistic
children to say what time period they breastfed their children. The data
is in \hyperref[autism-versus-breastfeeding]{Autism Versus
Breastfeeding} (Schultz, Klonoff-Cohen, Wingard, Askhoomoff, Macera, Ji
\& Bacher, 2006). Do the data provide enough evidence to support that
breastfeeding and autism are independent? Test at the 1\% level.

\paragraph{Autism Versus
Breastfeeding}\label{autism-versus-breastfeeding}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 8\tabcolsep) * \real{0.2000}}@{}}
\caption{Breast Feeding}\tabularnewline
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Autism
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Not Breast Feed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Breast Feed less than 2 months
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Breast Feed 2 to 6 months
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Breast Feed more than 6 months
\end{minipage} \\
\midrule\noalign{}
\endfirsthead
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
Autism
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Not Breast Feed
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Breast Feed less than 2 months
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Breast Feed 2 to 6 months
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Breast Feed more than 6 months
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
yes & 241 & 198 & 164 & 215 \\
no & 20 & 25 & 27 & 44 \\
\end{longtable}

To put this data into r, use the following commands:

If you have the data frame instead of the summary table as in this
example, you can use the tally command to create the table in r. Just
save the tally command with a name.

\subsubsection{Solution}\label{solution-90}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o\): Breastfeeding and autism are independent

\(H_a\): Breastfeeding and autism are dependent

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of breastfeeding time frames and autism
  incidence was taken. Check: this was stated in the problem.
\item
  State: Expected frequencies for each cell are greater than or equal to
  5, \(E\ge 5\). Check: See step 3. All expected frequencies are more
  than 5.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the test statistic and p-value On rStudio, the command is
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(autism\_table) }\CommentTok{\#calculates the test statistics and p{-}value }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Pearson's Chi-squared test

data:  autism_table
X-squared = 11.217, df = 3, p-value = 0.01061
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(autism\_table)}\SpecialCharTok{$}\NormalTok{expected }\CommentTok{\# shows all the expected frequencies }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
         [,1]      [,2]      [,3]      [,4]
yes 228.58458 195.30407 167.27837 226.83298
no   32.41542  27.69593  23.72163  32.16702
\end{verbatim}

The test statistic is 11.217 and the p-value is 0.01061.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Conclusion
\end{enumerate}

Fail to reject \(H_o\) since the p-value is more than 0.01.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Interpretation
\end{enumerate}

There is not enough evidence to support that breastfeeding and autism
are dependent. This means that you cannot say whether a child is
breastfed or not will indicate if that the child will be diagnosed with
autism.

\subsection{Homework for Chi-Square Test for Independence
Section}\label{homework-for-chi-square-test-for-independence-section}

\textbf{In each problem show all steps of the hypothesis test. If some
of the conditions are not met, note that the results of the test may not
be correct and then continue the process of the hypothesis test.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  The number of people who survived the Titanic based on class and sex
  is in Table~\ref{tbl-Titanic_table} (``Encyclopedia Titanica,'' 2013).
  Is there enough evidence to show that the class and the sex of a
  person who survived the Titanic are independent? Test at the 5\%
  level.
\end{enumerate}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-Titanic_table}Surviving the Titanic}

\tabularnewline

\toprule\noalign{}
& female & male \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
first & 134 & 59 \\
second & 94 & 25 \\
third & 80 & 58 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Researchers watched groups of dolphins off the coast of Ireland in
  1998 to determine what activities the dolphins partake in at certain
  times of the day (``Activities of dolphin,'' 2013). The numbers in
  Table~\ref{tbl-Dolphin_table} represent the number of groups of
  dolphins that were partaking in an activity at certain times of days.
  Is there enough evidence to show that the activity and the time period
  are independent for dolphins? Test at the 1\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Dolphin}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/dolphins.csv"}\NormalTok{) }
\NormalTok{Dolphin\_table}\OtherTok{\textless{}{-}}\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{activity}\SpecialCharTok{+}\NormalTok{period, }\AttributeTok{data=}\NormalTok{Dolphin)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Dolphin\_table)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}

\caption{\label{tbl-Dolphin_table}Dolphin Activity}

\tabularnewline

\toprule\noalign{}
& Afternoon & Evening & Morning & Noon \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Feed & 0 & 56 & 28 & 4 \\
Social & 9 & 10 & 38 & 5 \\
Travel & 14 & 13 & 6 & 6 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Is there a relationship between autism and what an infant is fed? To
  determine if there is, a researcher asked mothers of autistic and
  non-autistic children to say what they fed their infant. The data is
  in Table~\ref{tbl-Feeding_table} (Schultz, Klonoff-Cohen, Wingard,
  Askhoomoff, Macera, Ji \& Bacher, 2006). Do the data provide enough
  evidence to show that that what an infant is fed and autism are
  independent? Breast-feeding (BF), Formula with DHA/ARA (For with), and
  Formula without DHA/ARA (Form without)Test at the 1\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Feeding}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Mothers.csv"}\NormalTok{) }
\NormalTok{Feeding\_table}\OtherTok{\textless{}{-}}\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{autism}\SpecialCharTok{+}\NormalTok{feeding, }\AttributeTok{data=}\NormalTok{Feeding)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Feeding\_table)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrr@{}}

\caption{\label{tbl-Feeding_table}Autism Versus Breastfeeding}

\tabularnewline

\toprule\noalign{}
& breast & formula\_with & formula\_without \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
no & 6 & 22 & 10 \\
yes & 12 & 39 & 65 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Students at multiple grade schools were asked what their personal goal
  (get good grades, be popular, be good at sports) was and how important
  good grades were to them (1 very important and 4 least important). The
  data is in Table~\ref{tbl-Goal_Grades_table} (``Popular kids
  datafile,'' 2013). Do the data provide enough evidence to show that
  goal attainment and importance of grades are independent? Test at the
  5\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Goal}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Popular\_Kids\_clean.csv"}\NormalTok{) }
\NormalTok{Goal\_Grades\_table}\OtherTok{\textless{}{-}}\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Goals}\SpecialCharTok{+}\NormalTok{Grades, }\AttributeTok{data=}\NormalTok{Goal)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Goal\_Grades\_table)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}

\caption{\label{tbl-Goal_Grades_table}Personal Goal and Importance of
Grades}

\tabularnewline

\toprule\noalign{}
& 1 & 2 & 3 & 4 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Grades & 70 & 66 & 55 & 56 \\
Popular & 14 & 33 & 45 & 49 \\
Sports & 10 & 24 & 33 & 23 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Students at multiple grade schools were asked what their personal goal
  (get good grades, be popular, be good at sports) was and how important
  being good at sports were to them (1 very important and 4 least
  important). The data is in Table~\ref{tbl-Goal_Sports_table}
  (``Popular kids datafile,'' 2013). Do the data provide enough evidence
  to show that goal attainment and importance of sports are independent?
  Test at the 5\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Goal}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Popular\_Kids\_clean.csv"}\NormalTok{) }
\NormalTok{Goal\_Sports\_table}\OtherTok{\textless{}{-}}\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Goals}\SpecialCharTok{+}\NormalTok{Sports, }\AttributeTok{data=}\NormalTok{Goal)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Goal\_Sports\_table)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}

\caption{\label{tbl-Goal_Sports_table}Personal Goal and Importance of
Sports}

\tabularnewline

\toprule\noalign{}
& 1 & 2 & 3 & 4 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Grades & 83 & 81 & 55 & 28 \\
Popular & 32 & 49 & 43 & 17 \\
Sports & 50 & 24 & 14 & 2 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Students at multiple grade schools were asked what their personal goal
  (get good grades, be popular, be good at sports) was and how important
  having good looks were to them (1 very important and 4 least
  important). The data is in Table~\ref{tbl-Goal_Looks_table} (``Popular
  kids datafile,'' 2013). Do the data provide enough evidence to show
  that goal attainment and importance of looks are independent? Test at
  the 5\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Goal}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Popular\_Kids\_clean.csv"}\NormalTok{) }
\NormalTok{Goal\_Looks\_table}\OtherTok{\textless{}{-}}\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Goals}\SpecialCharTok{+}\NormalTok{Looks, }\AttributeTok{data=}\NormalTok{Goal)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Goal\_Looks\_table)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}

\caption{\label{tbl-Goal_Looks_table}Personal Goal and Importance of
Looks}

\tabularnewline

\toprule\noalign{}
& 1 & 2 & 3 & 4 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Grades & 80 & 66 & 66 & 35 \\
Popular & 81 & 30 & 18 & 12 \\
Sports & 24 & 30 & 17 & 19 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{6}
\tightlist
\item
  Students at multiple grade schools were asked what their personal goal
  (get good grades, be popular, be good at sports) was and how important
  having money were to them (1 very important and 4 least important).
  The data is in Table~\ref{tbl-Goal_Money_table} (``Popular kids
  datafile,'' 2013). Do the data provide enough evidence to show that
  goal attainment and importance of money are independent? Test at the
  5\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Goal}\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Popular\_Kids\_clean.csv"}\NormalTok{) }
\NormalTok{Goal\_Money\_table}\OtherTok{\textless{}{-}}\FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Goals}\SpecialCharTok{+}\NormalTok{Money, }\AttributeTok{data=}\NormalTok{Goal)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Goal\_Money\_table)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrrrr@{}}

\caption{\label{tbl-Goal_Money_table}Personal Goal and Importance of
Money}

\tabularnewline

\toprule\noalign{}
& 1 & 2 & 3 & 4 \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Grades & 14 & 34 & 71 & 128 \\
Popular & 14 & 29 & 35 & 63 \\
Sports & 6 & 12 & 26 & 46 \\

\end{longtable}

\section{Chi-Square Goodness of Fit}\label{chi-square-goodness-of-fit}

In probability, you calculated probabilities using both experimental and
theoretical methods. There are times when it is important to determine
how well the experimental values match the theoretical values. An
example of this is if you wish to verify if a die is fair. To determine
if observed values fit the expected values, you want to see if the
difference between observed values and expected values is large enough
to say that the test statistic is unlikely to happen if you assume that
the observed values fit the expected values. The test statistic in this
case is also the chi-square. The process is the same as for the
chi-square test for independence.

\subsection{Hypothesis Test for Goodness of Fit
Test}\label{hypothesis-test-for-goodness-of-fit-test}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\) The data are consistent with a specific distribution

\(H_a:\) The data are not consistent with a specific distribution

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample is taken. Check: State how the sample is
  collected.
\item
  State: Expected frequencies for each cell are greater than or equal to
  5. Check: The expected frequencies, *E*, will be calculated later.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the test statistic and p-value
\end{enumerate}

Using rStudio see example 11.2.1

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject \(H_o\) or fail to reject \(H_o\). The
rule is: if the p-value \(<\alpha\), then reject \(H_o\). If the p-value
\(\ge \alpha\), then fail to reject \(H_0\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

\subsection{Example: Goodness of Fit
Test}\label{example-goodness-of-fit-test}

Suppose you have a die that you are curious if it is fair or not. If it
is fair then the proportion for each value should be the same. You need
to find the observed frequencies and to accomplish this you roll the die
500 times and count how often each side comes up. The data is in table
Table~\ref{tbl-Die}. Do the data show that the die is fair? Test at the
5\% level.

\begin{longtable}[]{@{}lr@{}}

\caption{\label{tbl-Die}Observed Frequencies of Die for sides 1 through
6}

\tabularnewline

\toprule\noalign{}
Sides & observed\ldots.c.78..87..87..76..85..87. \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1 & 78 \\
2 & 87 \\
3 & 87 \\
4 & 76 \\
5 & 85 \\
6 & 87 \\

\end{longtable}

\subsubsection{Solution}\label{solution-91}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\) The observed frequencies are consistent with the distribution
for fair die (the die is fair)

\(H_a:\) The observed frequencies are not consistent with the
distribution for fair die (the die is not fair)

\(\alpha=0.05\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  State: A random sample is taken. check: This is true since each throw
  of a die is a random event.
\item
  State: Expected frequencies for each cell are greater than or equal to
  5. Check: See step 3.
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Find the test statistic and p-value
\end{enumerate}

On rStudio, this would be

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{fair\_die}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{1}\SpecialCharTok{/}\DecValTok{6}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{6}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{6}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{6}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{6}\NormalTok{, }\DecValTok{1}\SpecialCharTok{/}\DecValTok{6}\NormalTok{) }
\NormalTok{observed}\OtherTok{\textless{}{-}}\FunctionTok{c}\NormalTok{(}\DecValTok{78}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{76}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{87}\NormalTok{) }
\FunctionTok{chisq.test}\NormalTok{(observed, }\AttributeTok{p=}\NormalTok{fair\_die) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Chi-squared test for given probabilities

data:  observed
X-squared = 1.504, df = 5, p-value = 0.9126
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{chisq.test}\NormalTok{(observed, }\AttributeTok{p=}\NormalTok{fair\_die)}\SpecialCharTok{$}\NormalTok{expected }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 83.33333 83.33333 83.33333 83.33333 83.33333 83.33333
\end{verbatim}

Test Statistic: The test statistic is 1.504. The p-value is 0.9126.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  Conclusion
\end{enumerate}

Fail to reject \(H_o\) since the p-value is greater than 0.05.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Interpretation
\end{enumerate}

There is not enough evidence to support that the die is not consistent
with the distribution for a fair die. There is not enough evidence to
support that the die is not fair.

\subsection{Homework for Chi-Square Goodness of Fit
Section}\label{homework-for-chi-square-goodness-of-fit-section}

\textbf{In each problem show all steps of the hypothesis test. If some
of the conditions are not met, note that the results of the test may not
be correct and then continue the process of the hypothesis test.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  According to the M\&M candy company, the expected proportion can be
  found in Table~\ref{tbl-MandM}. In addition, the table contains the
  number of M\&M's of each color that were found in a case of candy
  (Madison, 2013). At the 5\% level, do the observed frequencies support
  the claim of M\&M?
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{MandM }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Color =} \FunctionTok{c}\NormalTok{(}\StringTok{"Blue"}\NormalTok{, }\StringTok{"Brown"}\NormalTok{, }\StringTok{"Green"}\NormalTok{, }\StringTok{"Orange"}\NormalTok{, }\StringTok{"Red"}\NormalTok{, }\StringTok{"Yellow"}\NormalTok{),}
  \AttributeTok{Observed =} \FunctionTok{c}\NormalTok{(}\DecValTok{78}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{87}\NormalTok{, }\DecValTok{76}\NormalTok{, }\DecValTok{85}\NormalTok{, }\DecValTok{87}\NormalTok{),}
  \AttributeTok{Expected =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.24}\NormalTok{, }\FloatTok{0.13}\NormalTok{, }\FloatTok{0.16}\NormalTok{, }\FloatTok{0.20}\NormalTok{, }\FloatTok{0.13}\NormalTok{, }\FloatTok{0.14}\NormalTok{)}
\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(MandM)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lrr@{}}

\caption{\label{tbl-MandM}M\&M Observed and Expected}

\tabularnewline

\toprule\noalign{}
Color & Observed & Expected \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Blue & 78 & 0.24 \\
Brown & 87 & 0.13 \\
Green & 87 & 0.16 \\
Orange & 76 & 0.20 \\
Red & 85 & 0.13 \\
Yellow & 87 & 0.14 \\

\end{longtable}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\item
  Eyeglassomatic manufactures eyeglasses for different retailers. The
  data frame is in Table~\ref{tbl-Defects}. They test to see how many
  defective lenses they made the time period of January 1 to March 31.
  Table~\ref{tbl-Defect_table} gives the defect and the number of
  defects.

  \textbf{Code book for Data Frame Defects} below
  Table~\ref{tbl-Defects}.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Defect\_table }\OtherTok{\textless{}{-}} \FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{type, }\AttributeTok{data=}\NormalTok{Defects)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Defect\_table)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lr@{}}

\caption{\label{tbl-Defect_table}Number of Defective Lenses}

\tabularnewline

\toprule\noalign{}
type & Freq \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
axis & 1838 \\
big & 1105 \\
chamfer & 1596 \\
cracks & 1546 \\
flaked & 1992 \\
height & 1130 \\
intern & 976 \\
lost & 976 \\
pd & 1398 \\
scratch & 5865 \\
shape & 1485 \\
small & 4613 \\
spot & 1371 \\

\end{longtable}

Do the data support the notion that each defect type occurs in the same
proportion? Test at the 5\% level.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\item
  On occasion, medical studies need to model the proportion of the
  population that has a disease and compare that to observed frequencies
  of the disease actually occurring. Suppose the end-stage renal failure
  in south-west Wales was collected for different age groups. Do the
  data in Table~\ref{tbl-Renal} show that the observed frequencies are
  in agreement with proportion of people in each age group (Boyle,
  Flowerdew \& Williams, 1997)? Test at the 1\% level.

  \textbf{Table Renal Failure Frequencies}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Renal }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Age\_group =} \FunctionTok{c}\NormalTok{(}\StringTok{"16{-}29"}\NormalTok{, }\StringTok{"30{-}44"}\NormalTok{, }\StringTok{"45{-}59"}\NormalTok{, }\StringTok{"60{-}75"}\NormalTok{, }\StringTok{"75+"}\NormalTok{),}
  \AttributeTok{Observed =} \FunctionTok{c}\NormalTok{(}\DecValTok{32}\NormalTok{, }\DecValTok{66}\NormalTok{, }\DecValTok{132}\NormalTok{, }\DecValTok{218}\NormalTok{, }\DecValTok{91}\NormalTok{),}
  \AttributeTok{Expected =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.23}\NormalTok{, }\FloatTok{0.25}\NormalTok{, }\FloatTok{0.22}\NormalTok{, }\FloatTok{0.21}\NormalTok{, }\FloatTok{0.09}\NormalTok{)}
\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Renal)}
\end{Highlighting}
\end{Shaded}

  \begin{longtable}[]{@{}lrr@{}}

  \caption{\label{tbl-Renal}Renal Failure Frequencies}

  \tabularnewline

  \toprule\noalign{}
  Age\_group & Observed & Expected \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  16-29 & 32 & 0.23 \\
  30-44 & 66 & 0.25 \\
  45-59 & 132 & 0.22 \\
  60-75 & 218 & 0.21 \\
  75+ & 91 & 0.09 \\

  \end{longtable}
\item
  In Africa in 2011, the number of deaths of a female from
  cardiovascular disease for different age groups are in
  Table~\ref{tbl-Deaths} (``Global health observatory,'' 2013). In
  addition, the proportion of deaths of females from all causes for the
  same age groups are also in table Deaths of Females for Different Age
  Groups. Do the data show that the death from cardiovascular disease
  are in the same proportion as all deaths for the different age groups?
  Test at the 5\% level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Deaths }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Age =} \FunctionTok{c}\NormalTok{(}\StringTok{"5{-}14"}\NormalTok{, }\StringTok{"14{-}29"}\NormalTok{, }\StringTok{"30{-}49"}\NormalTok{, }\StringTok{"50{-}69"}\NormalTok{),}
  \AttributeTok{Observed =} \FunctionTok{c}\NormalTok{(}\DecValTok{8}\NormalTok{, }\DecValTok{16}\NormalTok{, }\DecValTok{56}\NormalTok{, }\DecValTok{433}\NormalTok{),}
  \AttributeTok{Expected =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.10}\NormalTok{, }\FloatTok{0.12}\NormalTok{, }\FloatTok{0.226}\NormalTok{, }\FloatTok{0.52}\NormalTok{)}
\NormalTok{)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Deaths)}
\end{Highlighting}
\end{Shaded}

  \begin{longtable}[]{@{}lrr@{}}

  \caption{\label{tbl-Deaths}Deaths of Females for Different Age Groups}

  \tabularnewline

  \toprule\noalign{}
  Age & Observed & Expected \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  5-14 & 8 & 0.100 \\
  14-29 & 16 & 0.120 \\
  30-49 & 56 & 0.226 \\
  50-69 & 433 & 0.520 \\

  \end{longtable}
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\item
  In Australia in 1995, there was a question of whether indigenous
  people are more likely to die in prison than non-indigenous people. To
  figure out, the data in Table~\ref{tbl-Prisoners} was collected.
  (``Aboriginal deaths in,'' 2013). Do the data show that indigenous
  people die in the same proportion as non-indigenous people? Test at
  the 1\% level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Prisoners }\OtherTok{\textless{}{-}} \FunctionTok{data.frame}\NormalTok{(}
  \AttributeTok{Died =} \FunctionTok{c}\NormalTok{(}\StringTok{"Yes"}\NormalTok{, }\StringTok{"No"}\NormalTok{),}
  \AttributeTok{Observed =} \FunctionTok{c}\NormalTok{(}\DecValTok{17}\NormalTok{, }\DecValTok{3890}\NormalTok{),}
  \AttributeTok{Expected =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.003}\NormalTok{, }\FloatTok{0.997}\NormalTok{))}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Prisoners)}
\end{Highlighting}
\end{Shaded}

  \begin{longtable}[]{@{}lrr@{}}

  \caption{\label{tbl-Prisoners}Death of Indigenous Prisoners}

  \tabularnewline

  \toprule\noalign{}
  Died & Observed & Expected \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  Yes & 17 & 0.003 \\
  No & 3890 & 0.997 \\

  \end{longtable}
\item
  A project conducted by the Australian Federal Office of Road Safety
  asked people many questions about their cars. One question was the
  reason that a person chooses a given car, and that data is in
  Table~\ref{tbl-Car_table} (``Car preferences,'' 2013).
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Car\_table }\OtherTok{\textless{}{-}} \FunctionTok{tally}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{Reason, }\AttributeTok{data=}\NormalTok{Car\_pref)}
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(Car\_table)}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}lr@{}}

\caption{\label{tbl-Car_table}Reason for Choosing a Car}

\tabularnewline

\toprule\noalign{}
Reason & Freq \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
comfort & 47 \\
cost & 46 \\
looks & 27 \\
performance & 34 \\
reliability & 62 \\
safety & 84 \\

\end{longtable}

Do the data show that the frequencies observed substantiate the claim
that the reasons for choosing a car are equally likely? Test at the 5\%
level.

\section{Analysis of Variance (ANOVA)}\label{analysis-of-variance-anova}

There are times where you want to compare three or more population
means. One idea is to just test different combinations of two means. The
problem with that is that your chance for a type I error increases.
Instead you need a process for analyzing all of them at the same time.
This process is known as \textbf{analysis of variance (ANOVA)}. The test
statistic for the ANOVA is fairly complicated, you will want to use
technology to find the test statistic and p-value. The test statistic is
distributed as an F-distribution, which is skewed right and depends on
degrees of freedom. Since you will use technology to find these, the
distribution and the test statistic will not be presented. Remember, all
hypothesis tests are the same process. Note that to obtain a
statistically significant result there need only be a difference between
any two of the \(k\) means.

Before conducting the hypothesis test, it is helpful to look at the
means and standard deviations for each data set. If the sample means
with consideration of the sample standard deviations are different, it
may mean that some of the population means are different. However, do
realize that if they are different, it doesn't provide enough evidence
to show the population means are different. Calculating the sample
statistics just gives you an idea that conducting the hypothesis test is
a good idea.

\subsection{\texorpdfstring{Hypothesis test using ANOVA to compare \(k\)
means}{Hypothesis test using ANOVA to compare k means}}\label{hypothesis-test-using-anova-to-compare-k-means}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  State the random variables and the parameters in words
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\) all the means are the same

\(H_a:\) at least two of the means are different

Also, state your \(\alpha\) level here.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

a. State: A random sample of size is taken from each population. Check:
discuss how the samples were taken.

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\setcounter{enumi}{1}
\item
  State: All the samples are independent of each other. Check: Discuss
  how they are all independent.
\item
  State: Each population is normally distributed. Check: Create density
  plots and normal quantile plot of each sample. Note: the ANOVA test is
  fairly robust to the condition especially if the sample sizes are
  fairly close to each other. Unless the populations are really not
  normally distributed and the sample sizes are close to each other,
  then this is a loose condition.
\item
  State: The population variances are all equal. Check: See if the
  sample variances are close to each other. If the sample sizes are
  close to each other, then this is a loose condition.
\end{enumerate}

4. Find the test statistic and p-value

The test statistic is \(F\). To find the test statistic, use technology
such r Studio.

The test statistic, \(F\), is distributed as an F-distribution, where
both degrees of freedom are needed in this distribution. The p-value is
also calculated r Studio.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

This is where you write reject \(H_O\) or fail to reject \(H_O\). The
rule is: if the p-value \(<\alpha\), then reject \(H_o\). If the p-value
\(\ge \alpha\), then fail to reject \(H_o\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

This is where you interpret in real world terms the conclusion to the
test. The conclusion for a hypothesis test is that you either have
enough evidence to support \(H_a\), or you do not have enough evidence
to support \(H_a\).

If you do in fact reject \(H_o\), then you know that at least two of the
means are different. The next question you might ask is which are
different? You can look at the sample means, but realize that these only
give a preliminary result. To actually determine which means are
different, you need to conduct other tests. Some of these tests are the
range test, multiple comparison tests, Duncan test, Student-Newman-Keuls
test, Tukey test, Scheffé test, Dunnett test, least significant
different test, and the Bonferroni test. There is no consensus on which
test to use.

\subsection{Example: Hypothesis Test Involving Several
Means}\label{example-hypothesis-test-involving-several-means}

Cancer is a terrible disease. Surviving may depend on the type of cancer
the person has. To see if the mean survival time for several types of
cancer are different, data was collected on the survival time in days of
patients with one of these cancer in advanced stage. The data is in
Table~\ref{tbl-Cancer} (``Cancer survival story,'' 2013). (Please
realize that this data is from 1978. There have been many advances in
cancer treatment, so do not use this data as an indication of survival
rates from these cancers.) Do the data indicate that at least two of the
mean survival time for these types of cancer are not all equal? Test at
the 1\% level.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Cancer }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/cancer.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Cancer))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rl@{}}

\caption{\label{tbl-Cancer}Survival Times in Days of Five Cancer Types}

\tabularnewline

\toprule\noalign{}
survival & organ \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
124 & Stomach \\
42 & Stomach \\
25 & Stomach \\
45 & Stomach \\
412 & Stomach \\
51 & Stomach \\

\end{longtable}

\textbf{Code book for data frame Cancer}

\textbf{Description} Survival time for several types of cancer was
collected.

This data frame contains the following columns:

survival: survival times (months)

organ: the organ that the cancer is in

Source Cancer survival story. (2013, December 04). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Stories/CancerSurvival.html\textgreater{}

References \textless http://lib.stat.cmu.edu/DASL\textgreater{}

\subsubsection{Solution}\label{solution-92}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  State the random variables and the parameters in words
\end{enumerate}

\(x_1:\) survival time of patient with Stomach cancer

\(x_2:\) survival time of patient with Bronchus (lung) cancer

\(x_3:\) survival time of patient with Colon cancer

\(x_4:\) survival time of patient with Ovarian cancer

\(x_5:\) survival time of patient with Breast cancer

\(\mu_1:\) mean survival time of patient with Stomach cancer

\(\mu_2:\) mean survival time of patient with Bronchus (lung) cancer

\(\mu_3:\) mean survival time of patient with Colon cancer

\(\mu_4:\) mean survival time of patient with Ovarian cancer

\(\mu_5:\) mean survival time of patient with Brest cancer

Now before conducting the hypothesis test, look at the means and
standard deviations. There appears to be a difference between at least
two of the means, but realize that the standard deviations are very
different. The difference you see may not be significant.

Notice the sample sizes are not the same.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  State the null and alternative hypotheses and the level of
  significance
\end{enumerate}

\(H_o:\) all the means are equal

\(H_a:\) some of the means are different

\(\alpha=0.01\)

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  State and check the conditions for the hypothesis test
\end{enumerate}

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\item
  State: A random sample of 13 survival times from stomach cancer was
  taken. A random sample of 17 survival times from bronchus cancer was
  taken. A random sample of 17 survival times from colon cancer was
  taken. A random sample of 6 survival times from ovarian cancer was
  taken. A random sample of 11 survival times from breast cancer was
  taken.

  Check: These statements may not be true. This information was not
  shared as to whether the samples were random or not but it may be safe
  to assume that.
\item
  State: The samples are all independent.

  Check: Since the individuals have different cancers, then the samples
  are independent.
\item
  State: Population of all survival times from stomach cancer is
  normally distributed. Population of all survival times from bronchus
  cancer is normally distributed. Population of all survival times from
  colon cancer is normally distributed. Population of all survival times
  from ovarian cancer is normally distributed. Population of all
  survival times from breast cancer is normally distributed.

  Check: Looking at the density plots and normal quantile plots for each
  sample, it appears that none of the populations are normally
  distributed. The sample sizes are somewhat different for the problem.
  This condition may not be true.

  (ref:cancer-density--graphs-cap) Density Plot of Survival Times for
  Different Cancers

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_density}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{survival}\SpecialCharTok{|}\NormalTok{organ, }\AttributeTok{data=}\NormalTok{Cancer, }\AttributeTok{title=}\StringTok{"Survival times for Different Cancers"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Survival Times"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{figure}[H]

  \centering{

  \includegraphics{Chi-Square-and-ANOVA-Tests_files/figure-pdf/fig-Cancer-density-1.pdf}

  }

  \caption{\label{fig-Cancer-density}Density Plot of Survival Times for
  Different Cancers}

  \end{figure}%

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{gf\_qq}\NormalTok{(}\SpecialCharTok{\textasciitilde{}}\NormalTok{survival}\SpecialCharTok{|}\NormalTok{organ, }\AttributeTok{data=}\NormalTok{Cancer, }\AttributeTok{title=}\StringTok{"Survival times for Different Cancers"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

  \begin{figure}[H]

  \centering{

  \includegraphics{Chi-Square-and-ANOVA-Tests_files/figure-pdf/fig-Cancer-quantile-1.pdf}

  }

  \caption{\label{fig-Cancer-quantile}Quantile Plot of Survival Times
  for Different Cancers}

  \end{figure}%
\item
  State: The population variances are all equal.

  Check: The sample standard deviations are approximately 346.3, 209.9,
  427.2, 1098.6, and 1239.0 respectively. This condition does not appear
  to be met, since the sample standard deviations are very different.
  The sample sizes are somewhat different for the problem. This
  condition may not be true.
\end{enumerate}

4. Find the test statistic and p-value

To find the test statistic and p-value on r Studio, the commands would
be:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{results}\OtherTok{=}\FunctionTok{aov}\NormalTok{(survival}\SpecialCharTok{\textasciitilde{}}\NormalTok{organ, }\AttributeTok{data=}\NormalTok{Cancer) }
\FunctionTok{summary}\NormalTok{(results)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
            Df   Sum Sq Mean Sq F value   Pr(>F)    
organ        4 11535761 2883940   6.433 0.000229 ***
Residuals   59 26448144  448274                     
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
\end{verbatim}

The test statistic is F = 6.433 and the p-value = 0.000229.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  Conclusion
\end{enumerate}

Reject \(H_o:\) since the p-value is less than 0.01.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{5}
\tightlist
\item
  Interpretation
\end{enumerate}

There is enough evidence to support that at least two of the mean
survival times from different cancers are not equal.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{df\_stats}\NormalTok{(survival}\SpecialCharTok{\textasciitilde{}}\NormalTok{organ, }\AttributeTok{data=}\NormalTok{Cancer, mean)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
  response    organ      mean
1 survival   Breast 1395.9091
2 survival Bronchus  211.5882
3 survival    Colon  457.4118
4 survival    Ovary  884.3333
5 survival  Stomach  286.0000
\end{verbatim}

By examination of the means, it appears that the mean survival time for
breast cancer is different from the mean survival times for both stomach
and bronchus cancers. It may also be different for the mean survival
time for colon cancer. The others may not be different enough to
actually say for sure.

\subsection{Homework for Analysis of Variance (ANOVA)
Section}\label{homework-for-analysis-of-variance-anova-section}

\textbf{In each problem show all steps of the hypothesis test. If some
of the conditions are not met, note that the results of the test may not
be correct and then continue the process of the hypothesis test.}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Cuckoo birds are in the habit of laying their eggs in other birds'
  nest. The other birds adopt and hatch the eggs. The lengths (in cm) of
  cuckoo birds' eggs in the other species nests were measured and are in
  Table~\ref{tbl-Eggs} (``Cuckoo eggs in,'' 2013). Do the data show that
  the mean length of cuckoo bird's eggs is not all the same when put
  into different nests? Test at the 5\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Eggs }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Birds\_eggs.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Eggs))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rl@{}}

\caption{\label{tbl-Eggs}Lengths of Cuckoo Bird Eggs in Different
Species Nest}

\tabularnewline

\toprule\noalign{}
length & bird \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
19.65 & Meadow \\
20.05 & Meadow \\
20.65 & Meadow \\
20.85 & Meadow \\
21.65 & Meadow \\
21.65 & Meadow \\

\end{longtable}

\textbf{Code book for data frame Eggs}

\textbf{Description} Cuckoo birds are in the habit of laying their eggs
in other birds' nest. The other birds adopt and hatch the eggs. The
lengths (in cm) of cuckoo birds' eggs in the other species nests were
measured

This data frame contains the following columns:

length: length of cuckoo bird's eggs in other species nets (cm)

bird: bids where eggs were found in their nests. The birds are Meadow
Pipit, Tree Pipit, Hedge Sparrow, Robin, Pied Wagtail, and Wren

Source Cuckoo eggs in nest of other birds. (2013, December 04).
Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Stories/cuckoo.html\textgreater{}

References SOCR Home page:
\textless http://www.socr.ucla.edu\textgreater{}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{1}
\tightlist
\item
  Levi-Strauss Co manufactures clothing. The quality control department
  measures weekly values of different suppliers for the percentage
  difference of waste between the layout on the computer and the actual
  waste when the clothing is made (called run-up). The data is in
  Table~\ref{tbl-Levi} (``Waste run up,'' 2013). Do the data show that
  there is a difference between some of the suppliers? Test at the 1\%
  level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Levi }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Levi\_jeans.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Levi))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rl@{}}

\caption{\label{tbl-Levi}Run-ups for Different Plants Making Levi
Strauss Clothing}

\tabularnewline

\toprule\noalign{}
run\_up & plant \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
1.2 & Plant\_1 \\
10.1 & Plant\_1 \\
-2.0 & Plant\_1 \\
1.5 & Plant\_1 \\
-3.0 & Plant\_1 \\
-0.7 & Plant\_1 \\

\end{longtable}

\textbf{Code book for data frame Levi}

\textbf{Description} Levi-Strauss Co manufactures clothing. The quality
control department measures weekly values of different suppliers for the
percentage difference of waste between the layout on the computer and
the actual waste when the clothing is made (called run-up).

This data frame contains the following columns:

run\_up: percentage difference of waste between the layout on the
computer and the actual waste when the clothing is made. There are some
negative values because sometimes the supplier is able to layout the
pattern better than the computer

plant: Which suppliers

Source Waste run up. (2013, December 04). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Stories/wasterunup.html\textgreater{}

References \textless http://lib.stat.cmu.edu/DASL\textgreater{}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{2}
\tightlist
\item
  Several magazines were grouped into three categories based on what
  level of education of their readers the magazines are geared towards:
  high, medium, or low level. Then random samples of the magazines were
  selected to determine the number of three-plus-syllable words were in
  the advertising copy, and the data is in Table~\ref{tbl-Advertising}
  (``Magazine ads readability,'' 2013). Is there enough evidence to show
  that the mean number of three-plus-syllable words in advertising copy
  is different for at least two of the education levels? Test at the 5\%
  level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Advertising }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/three\_syllable\_words.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Advertising))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rl@{}}

\caption{\label{tbl-Advertising}Number of Three Plus Syllable Words in
Advertising Copy}

\tabularnewline

\toprule\noalign{}
number & education \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
34 & High \\
21 & High \\
37 & High \\
31 & High \\
10 & High \\
24 & High \\

\end{longtable}

\textbf{Code book for data frame Advertising}

\textbf{Description} Several magazines were grouped into three
categories based on what level of education of their readers the
magazines are geared towards: high, medium, or low level. Then random
samples of the magazines were selected to determine the number of
three-plus-syllable words were in the advertising copy

This data frame contains the following columns:

number: number of three=plus-syllable words in advertising copy

education: level of education the magazine is geared towards: high,
medium, or low

Source Magazine ads readability. (2013, December 04). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Datafiles/magadsdat.html\textgreater{}

References \textless http://lib.stat.cmu.edu/DASL\textgreater{}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{3}
\tightlist
\item
  A study was undertaken to see how accurate food labeling for calories
  on food that is considered reduced calorie. The group measured the
  amount of calories for each item of food and then found the percent
  difference between measured and labeled food. The group also looked at
  food that was nationally advertised, regionally distributed, or
  locally prepared. The data is in Table~\ref{tbl-Food} (``Calories
  datafile,'' 2013). Do the data indicate that at least two of the mean
  percent differences between the three groups are different? Test at
  the 5\% level.
\end{enumerate}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{Food }\OtherTok{\textless{}{-}} \FunctionTok{read.csv}\NormalTok{( }\StringTok{"https://krkozak.github.io/MAT160/Food\_calories\_percent\_diff.csv"}\NormalTok{) }
\NormalTok{knitr}\SpecialCharTok{::}\FunctionTok{kable}\NormalTok{(}\FunctionTok{head}\NormalTok{(Food))}
\end{Highlighting}
\end{Shaded}

\begin{longtable}[]{@{}rl@{}}

\caption{\label{tbl-Food}Percent Differences Between Measured and
Labeled Food}

\tabularnewline

\toprule\noalign{}
percent\_diff & food \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
2 & national \\
-28 & national \\
-6 & national \\
8 & national \\
6 & national \\
-1 & national \\

\end{longtable}

\textbf{Code book for data frame Food}

\textbf{Description} A study was undertaken to see how accurate food
labeling for calories on food that is considered reduced calorie. The
group measured the amount of calories for each item of food and then
found the percent difference between measured and labeled food. The
group also looked at food that was nationally advertised, regionally
distributed, or locally prepared.

This data frame contains the following columns:

percent\_diff: percent difference between the number of calories that
are measured in the food and the amount that is labeled on the food.

food: Where the food is created: nationally advertised, regionally
distributed, or locally prepared.

Source Calories datafile. (2013, December 07). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Datafiles/Calories.html\textgreater{}

References \textless http://lib.stat.cmu.edu/DASL\textgreater{}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\setcounter{enumi}{4}
\tightlist
\item
  The amount of sodium (in mg) in different types of hot dogs is in
  Table~\ref{tbl-Food} (``Hot dogs story,'' 2013). Is there sufficient
  evidence to show that the mean amount of sodium in the types of hot
  dogs are not all equal? Test at the 5\% level.
\end{enumerate}

\textbf{Code book for data frame Food is below Table~\ref{tbl-Food}.}

\bookmarksetup{startatroot}

\chapter{References}\label{references}

\section{Data Sources:}\label{data-sources}

(n.d.). Retrieved July 18, 2019, from
https://www.idvbook.com/teaching-aid/data-sets/the-breakfast-cereal-data-set/
The Best Kids' Cereal. (n.d.). Retrieved July 18, 2019, from
https://www.ranker.com/list/best-kids-cereal/ranker-food

(n.d.). Retrieved from
https://www3.nd.edu/\textasciitilde busiforc/handouts/Data and Stories/t
test/Friday The Thirteenth/Friday The Thirteenth Data.html

(n.d.). Retrieved July 21, 2019, from
https://www3.nd.edu/\textasciitilde busiforc/handouts/Data and
Stories/regression/us auto mileage/usautomileage.html

http://apps.who.int/gho/athena/data/download.xsl?format=xml\&target=GHO/WHOSIS\_000001\&profile=excel\&filter=COUNTRY:;SEX:;REGION:EUR

Aboriginal deaths in custody. (2013, September 26). Retrieved from
\textless http://www.statsci.org/data/oz/custody.html

Activities of Dolphin Groups. (n.d.). Retrieved July 12, 2019, from
http://www.statsci.org/data/general/dolpacti.html

Advanced Solutions International, Inc.~(n.d.). Retrieved July 16, 2019,
from
https://www.amstat.org/asa/News/ASA-Calls-Time-on-Statistically-Significant-in-Science-Research.aspx

Annual maximums of daily rainfall in Sydney. (2013, September 25).
Retrieved from
\textless http://www.statsci.org/data/oz/sydrain.html\textgreater{}

AP exam scores. (2013, November 20). Retrieved from
\textless http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_Dinov\_030708\_APExamScores\textgreater{}

Appliance life expectancy. (2013, November 8). Retrieved from
\textless http://www.mrappliance.com/expert/life-guide/\textgreater{}

Australian Human Rights Commission, (1996). Indigenous deaths in custody
1989 - 1996. Retrieved from website:
\textless http://www.humanrights.gov.au/publications/indigenous-deaths-custody\textgreater{}

Bhat, R., \& Kushtagi, P. (2006). A re-look at the duration of human
pregnancy. Singapore Med J., 47(12), 1044-8. Retrieved from
\textless http://www.ncbi.nlm.nih.gov/pubmed/17139400\textgreater{}

Boyle, P., Flowerdew, R., \& Williams, A. (1997). Evaluating the
goodness of fit in models of sparse medical data: A simulation approach.
International Journal of Epidemiology, 26(3), 651-656. Retrieved from
\textless http://ije.oxfordjournals.org/content/26/3/651.full.pdf\textgreater{}
html

Brain2bodyweight. (2013, November 16). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_Brain2BodyWeight

Buy sushi grade fish online. (2013, November 20). Retrieved from
http://www.catalinaop.com/

Cancer survival story. (2013, December 04). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Stories/CancerSurvival.html\textgreater{}

Calories datafile. (2013, December 07). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Datafiles/Calories.html\textgreater{}

Capital and rental values of Auckland properties. (2013, September 26).
Retrieved from http://www.statsci.org/data/oz/rentcap.html

Car Preferences. (n.d.). Retrieved July 11, 2019, from
http://www.statsci.org/data/oz/carprefs.html

CDC-data and statistics, autism spectrum disorders - ncbdd. (2013,
October 21). Retrieved from
\textless http://www.cdc.gov/ncbddd/autism/data.html\textgreater{}

CDC features - new data on autism spectrum disorders. (2013, November
26). Retrieved from
\textless http://www.cdc.gov/features/countingautism/\textgreater{}

Center for Disease Control and Prevention, Prevalence of Autism Spectrum
Disorders - Autism and Developmental Disabilities Monitoring Network.
(2008). Autism and developmental disabilities monitoring network-2012.
Retrieved from website:
\textless http://www.cdc.gov/ncbddd/autism/documents/ADDM-2012-Community-Report.pdf\textgreater{}

CO2 emissions (metric tons per capita). (n.d.). Retrieved July 18, 2019,
from https://data.worldbank.org/indicator/EN.ATM.CO2E.PC

College Board, SAT. (2012). Total group profile report. Retrieved from
website:
\textless http://media.collegeboard.com/digitalServices/pdf/research/TotalGroup-2012.pdf\textgreater{}

Consumer Price Index Data from 1913 to 2019. (2019, June 12). Retrieved
July 10, 2019, from
https://www.usinflationcalculator.com/inflation/consumer-price-index-and-annual-percent-changes-from-1913-to-2008/

CPS News Releases. (n.d.). Retrieved July 8, 2019, from
https://www.bls.gov/cps/

Cuckoo eggs in nest of other birds. (2013, December 04). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Stories/cuckoo.html\textgreater{}

Current health expenditure (\% of GDP). (n.d.). Retrieved July 9, 2019,
from https://data.worldbank.org/indicator/SH.XPD.CHEX.GD.ZS

11 little-known facts about left-handers. (2013, October 21). Retrieved
from
\textless http://www.huffingtonpost.com/2012/10/29/left-handed-facts-lefties\_n\_2005864.html\textgreater{}

Education by age datafile. (2013, December 05). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Datafiles/Educationbyage.html\textgreater{}

Encyclopedia Titanica. (2013, November 09). Retrieved from
\textless http://www.encyclopedia-titanica.org/\textgreater{}

Deaths from firearms. (2013, September 26). Retrieved from
http://www.statsci.org/data/oz/firearms.html

Flanagan, R., Rooney, C., \& Griffiths, C. (2005). Fatal poisoning in
childhood, england \& wales 1968-2000. Forensic Science International,
148:121-129, Retrieved from
\textless http://www.cdc.gov/nchs/data/ice/fatal\_poisoning\_child.pdf\textgreater{}

Federal Trade Commission, (2008). Consumer fraud and identity theft
complaint data: January-December 2007. Retrieved from website:
\textless http://www.ftc.gov/opa/2008/02/fraud.pdf\textgreater{}

Fertility rate, total (births per woman). (n.d.). Retrieved July 8,
2019, from https://data.worldbank.org/indicator/SP.DYN.TFRT.IN

Find Out How Many Calories in Beer? (n.d.). Retrieved July 21, 2019,
from https://www.beer100.com/beer-calories/

Gettler, L. T., McDade, T. W., Feranil, A. B., \& Kuzawa, C. W. (2011).
Longitudinal evidence that fatherhood decreases testosterone in human
males. The Proceedings of the National Academy of Sciences, PNAS 2011,
doi: 10.1073/pnas.1105403108

Global health observatory data respository. (2013, October 09).
Retrieved from
{[}http://apps.who.int/gho/athena/data/download.xsl?format=xml\&target=GHO/MORT\textbackslash\textbackslash\textbackslash\_400\&profile=excel\&filter=AGEGROUP:YEARS05-14;AGEGROUP:YEARS15-29;AGEGROUP:YEARS30-49;AGEGROUP:YEARS50-69;AGEGROUP:YEARS70{]}(http://apps.who.int/gho/athena/data/download.xsl?format=xml\&target=GHO/MORT\textbackslash\_400\&profile=excel\&filter=AGEGROUP:YEARS05-14;AGEGROUP:YEARS15-29;AGEGROUP:YEARS30-49;AGEGROUP:YEARS50-69;AGEGROUP:YEARS70)\{.uri\}
;MGHEREG:REG6\_AFR;GHECAUSES:\textbackslash;SEX:\textbackslash{}

Health Insurance Market Place Retrieved from website:
http://aspe.hhs.gov/health/reports/2013/marketplacepremiums/ib\_premiumslandscape.pdf

Helmet Sizes for New Zealand Airforce. (n.d.). Retrieved July 20, 2019,
from http://www.statsci.org/data/oz/nzhelmet.html

Ho, P. M., Bryson, C. L., \& Rumsfeld, J. S. (2009). Medication
adherence. Circulation, 119(23), 3028-3035. Retrieved from
\textless http://circ.ahajournals.org/content/119/23/3028\textgreater{}

Households by age of householder and size of household: 1990 to 2010.
(2013, October 19). Retrieved from
\textless http://www.census.gov/compendia/statab/2012/tables/12s0062.pdf\textgreater{}

Janssen, P. A., Thiessen, P., Klein, M. C., Whitfield, M. F., MacNab, Y.
C., \& Cullis-Kuhl, S. C. (2007). Standards for the measurement of birth
weight, length and head circumference at term in neonates of european,
chinese and south asian ancestry. Open Medicine, 1(2), e74-e88.
Retrieved from
\textless http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2802014/\textgreater{}

John Matic provided the data from a company he worked with. The
company's name is fictitious, but the data is from an actual company.

Kiama blowhole eruptions. (2013, September 25). Retrieved from
\textless http://www.statsci.org/data/oz/kiama.html\textgreater{}

Kozak K (2019). Survey results form surveys collected in statistics
class at Coconino Community College.

Kuulasmaa, K., Hense, H., \& Tolonen, H. World Health Organization
(WHO), WHO Monica Project. (1998). Quality assessment of data on blood
pressure in the who monica project (ISSN 2242-1246). Retrieved from WHO
MONICA Project e-publications website:
\textless http://www.thl.fi/publications/monica/bp/bpqa.htm\textgreater{}

Labor force participation rate, female (\% of female population ages 15
) (modeled ILO estimate). (n.d.). Retrieved July 20, 2019, from
https://data.worldbank.org/indicator/SL.TLF.CACT.FE.ZS

Lange TL, Royals HE, Connor LL (1993) Influence of water chemistry on
mercury concentration in largemouth bass from Florida lakes. Trans Am
Fish Soc 122:74-84. Michael K. Saiki, Darell G. Slotton, Thomas W. May,
Shaun M. Ayers, and Charles N. Alpers (2000) Summary of Total Mercury
Concentrations in Fillets of Selected Sport Fishes Collected during
2000--2003 from Lake Natoma, Sacramento County, California (Raw data is
included in appendix), U.S. Geological Survey Data Series 103, 1-21.
NISER 081107 ID Data. (n.d.). Retrieved July 18, 2019, from
http://wiki.stat.ucla.edu/socr/index.php/NISER\_081107\_ID\_Data

Lawes, C., Hoorn, S., Law, M., \& Rodgers, A. (2004). High cholesterol.
In M. Ezzati, A. Lopez, A. Rodgers \& C. Murray (Eds.), Comparative
Quantification of Health Risks (1 ed., Vol. 1, pp.~391-496). Retrieved
from
\textless http://www.who.int/publications/cra/chapters/volume1/0391-0496.pdf\textgreater{}

Lee, A. (1994). Data analysis: An introduction based on r. Auckland.
Retrieved from
\textless http://www.statsci.org/data/oz/nzrivers.html\textgreater{}

Life expectancy at birth. (2013, October 14). Retrieved from
http://data.worldbank.org/indicator/SP.DYN.LE00.IN

Life expectancy in southeast Asia. (2013, September 23). Retrieved from
\textless http://apps.who.int/gho/data/node.main.688\textgreater{}

Madison, J. (2013, October 15). M\&M's color distribution analysis.
Retrieved from
\textless http://joshmadison.com/2007/12/02/mms-color-distribution-analysis/\textgreater{}

Magazine ads readability. (2013, December 04). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Datafiles/magadsdat.html\textgreater{}

M\&M's Color Distribution Analysis. (n.d.). Retrieved July 11, 2019,
from https://joshmadison.com/2007/12/02/mms-color-distribution-analysis/

Maintaining Balance while Concentrating. (n.d.). Retrieved July 19,
2019, from http://www.statsci.org/data/general/balaconc.html

MLB heightsweights. (2013, November 16). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_MLB\_HeightsWeights

OECD economic development. (2013, December 04). Retrieved from
http://lib.stat.cmu.edu/DASL/Datafiles/oecdat.html

Olson, K., \& Hanson, J. (1997). Using reiki to manage pain: a
preliminary report. Cancer Prev Control, 1(2), 108-13. Retrieved from
\textless http://www.ncbi.nlm.nih.gov/pubmed/9765732\textgreater{}

Ovegard, M., Berndt, K., \& Lunneryd, S. (2012). Condition indices of
Atlantic cod (gadus morhua) biased by capturing method. ICES Journal of
Marine Science, doi: 10.1093/icesjms/fss145

Popular kids datafile. (2013, December 05). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Datafiles/PopularKids.html\textgreater{}

Population density (people per sq. km of land area). (n.d.). Retrieved
July 9, 2019, from https://data.worldbank.org/indicator/EN.POP.DNST

Population reference bureau. (2013, October 8). Retrieved from
\textless http://www.prb.org/DataFinder/Topic/Rankings.aspx?ind=25\textgreater{}

Prediction of Height from Metacarpal Bone Length. (n.d.). Retrieved July
9, 2019, from http://www.statsci.org/data/general/stature.html

Pregnant women receiving prenatal care (\%). (n.d.). Retrieved July 9,
2019, from https://data.worldbank.org/indicator/SH.STA.ANVC.ZS

Pulse rates before and after exercise. (2013, September 25). Retrieved
from \textless http://www.statsci.org/data/oz/ms212.html\textgreater{}

Reserve Bank of Australia. (2019, May 13). Statistical Tables. Retrieved
July 10, 2019, from https://www.rba.gov.au/statistics/tables/

Ryan, B. F., Joiner, B. L., \& Ryan, Jr, T. A. (1985). Cholesterol
levels after heart attack. Retrieved from
\textless http://www.statsci.org/data/general/cholest.html\textgreater{}

Sanchez, Y. W. (2016, October 20). Poll: Arizona voters still favor
legalizing marijuana. Retrieved from
https://www.azcentral.com/story/news/politics/elections/2016/10/20/poll-arizona-marijuana-legalization-proposition-205/92417690/

Schultz, S. T., Klonoff-Cohen, H. S., Wingard, D. L., Askhoomoff, N. A.,
Macera, C. A., Ji, M., \& Bacher, C. (2006). Breastfeeding, infant
formula supplementation, and autistic disorder: the results of a parent
survey. International Breastfeeding Journal, 1(16), doi:
10.1186/1746-4358-1-16

Seafood online. (2013, November 20). Retrieved from
http://www.allfreshseafood.com/

Smoking and cancer. (2013, December 04). Retrieved from
http://lib.stat.cmu.edu/DASL/Datafiles/cigcancerdat.html

SOCR 012708 id data hotdogs. (2013, November 13). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_012708\_ID\_Data\_HotDogs

SOCR Data 2008 World CountriesRankings. (n.d.). Retrieved July 19, 2019,
from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_2008\_World\_CountriesRankings\#SOCR\_Data\_-\_Ranking\_of\_the\_top\_100\_Countries\_based\_on\_Political.2C\_Economic.2C\_Health.2C\_and\_Quality-of-Life\_Factors

SOCR data Oct2009 id ni. (2013, November 16). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_Oct2009\_ID\_NI

SOCR data nips infantvitK shotdata. (2013, November 16). Retrieved from
http://wiki.stat.ucla.edu/socr/index.php/SOCR\_Data\_NIPS\_InfantVitK\_ShotData

Staff nurse - RN salary. (2013, November 08). Retrieved from
\textless http://www1.salary.com/Staff-Nurse-RN-salary.html\textgreater{}

Time between nerve pulses. (2019, July 3). Retrieved from
\textless http://www.statsci.org/data/general/nerve.html\textgreater{}

Time of passages of play in rugby. (2013, September 25). Retrieved from
\textless http://www.statsci.org/data/oz/rugby.html\textgreater{}

Tuition and Fees, 1998-99 Through 2018-19. (2018, December 31).
Retrieved from https://www.chronicle.com/interactives/tuition-and-fees

U.S. Census Bureau, Current Population Survey, Annual Social and
Economic Supplements.

US Department of Agriculture, Agricultural Research Service. (2012).
What we eat in America. Retrieved from website:
\textless http://www.ars.usda.gov/Services/docs.htm?docid=18349\textgreater{}

US Department of Commerce, \& Noaa. (2016, November 15). 1950 Oklahoma
Tornadoes. Retrieved from
https://www.weather.gov/oun/tornadodata-ok-1950

UV radiation: Burden of disease by country. (2013, September 4).
Retrieved from
\textless http://apps.who.int/gho/data/node.main.165?lang=en\textgreater{}

Waste run up. (2013, December 04). Retrieved from
\textless http://lib.stat.cmu.edu/DASL/Stories/wasterunup.html\textgreater{}

What percentage of people have green eyes?. (2013, October 21).
Retrieved from
\textless http://www.ask.com/question/what-percentage-of-people-have-green-eyes\textgreater{}




\end{document}
