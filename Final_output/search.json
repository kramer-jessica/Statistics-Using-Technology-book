[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics Using Technology: Rguroo Edition",
    "section": "",
    "text": "Preface\nThis is test of my macros: Inline examples (should be colored): - The variable is x. - The defined term is standard deviation. - The descriptor is Frequency. - The dataset is Cars. - The dropdown selection is Mean. - The function is lm. - The dialog name is Descriptive Statistics. - The repository is Rguroo Datasets. - The answer is 42.\nI hope you find this book useful in teaching statistics. When writing this book, I tried to follow the GAISE Standards (GAISE recommendations.\nTo this end, I ask students to interpret the results of their calculations. I incorporated the use of technology (R Studio) for most calculations. Because of that you will not find me using any of the computational formulas for standard deviations or correlation and regression since I prefer students understand the concept of these quantities. Also, because I utilize technology you will not find the standard normal table, Student’s t-table, binomial table, chi-square distribution table, and F-distribution table in the book. Another difference between this book and other statistics books is the order of hypothesis testing and confidence intervals. Most books present confidence intervals first and then hypothesis tests. I find that presenting hypothesis testing first and then confidence intervals is more understandable for students. Lastly, I have de-emphasized the use of the z-test. In fact, I only use it to introduce hypothesis testing, and never utilize it again. Two samples should be emphasized over one sample test. Lastly, to aid student understanding and interest, most of the homework and examples utilize real data with multiple variables. The beauty of multiple variables, is that you can ask the students to investigate different analysis with different variables. This way students can work with data and come up with connections of asking questions and using data to answer the questions. Again, I hope you find this book useful for your introductory statistics class.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#preface",
    "href": "index.html#preface",
    "title": "Statistics Using Technology: Rguroo Edition",
    "section": "",
    "text": "Teach statistical thinking.\nFocus on conceptual understanding.\nIntegrate real data with a context and a purpose.\nFoster active learning.\nUse technology to explore concepts and analyze data.\nUse assessments to improve and evaluate student learning",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#mathematical-knowledge-assumed",
    "href": "index.html#mathematical-knowledge-assumed",
    "title": "Statistics Using Technology: Rguroo Edition",
    "section": "Mathematical Knowledge Assumed",
    "text": "Mathematical Knowledge Assumed\nI want to make a comment about the mathematical knowledge that I assumed the students possess. The course for which I wrote this book has a higher prerequisite than most introductory statistics books. However, I do feel that students can read and understand this book as long as they can read critically. I do not show how to create most of the graphs, but all graphs are created with R Studio. So I hope the mathematical level is appropriate for your course.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#technology-used",
    "href": "index.html#technology-used",
    "title": "Statistics Using Technology: Rguroo Edition",
    "section": "Technology Used",
    "text": "Technology Used\nThe technology that I utilized for creating the graphs and statistical analysis is R Studio. This is a statistical software that are used by statisticians and so using it gives students skills they may need in the future. Please feel free to use any other technology that is more appropriate for your students. Do make sure that you use some technology. I worked on the StatPREP project and there are Little Apps that can be used to explore data. There are also activities that can be used in your classes that utilize the Little Apps on the website.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#acknowledgments",
    "href": "index.html#acknowledgments",
    "title": "Statistics Using Technology: Rguroo Edition",
    "section": "Acknowledgments",
    "text": "Acknowledgments\nI would like to thank the following people for taking their valuable time to review the book. Their comments and insights improved this book immensely.\n\nDaniel Kaplan, Macalester College\nJane Tanner, Onondaga Community College\nRob Farinelli, College of Southern Maryland\nCarrie Kinnison, retired engineer\nSean Simpson, Westchester Community College\nKim Sonier, Coconino Community College\nJim Ham, Delta College\nBrian Birgen, Wartburg College\nChristopher Cunningham, Elgin Community College\nKendra Feinstein, Tacoma Community College\nDavid Straayer, Tacoma Community College\nStudents of Coconino Community College\nStudents of Elgin Community College\nStudents of Tacoma Community College\nStudents of Wartburg College\n\nI also want to thank Coconino Community College for granting me a sabbatical so that I would have the time to write the book. On a personal note, I wanted to thank my brother, John Matic, his wife Jenelle, and their children Hannah and Eli for their hospitality when writing the first edition. In addition to allowing my family access to their home, John provided numerous examples and data sets for business applications in this book. I inadvertently left this thank you out of the first edition of the book, His help and his family’s hospitality were invaluable to me. Lastly, I want to thank my husband Rich and my son Dylan for supporting me in this project. Without their love and support, I would not have been able to complete the book.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#new-to-the-fourth-edition",
    "href": "index.html#new-to-the-fourth-edition",
    "title": "Statistics Using Technology: Rguroo Edition",
    "section": "New to the Fourth Edition",
    "text": "New to the Fourth Edition\nThe additions to this edition mostly involve format changes and other edits to make the textbook more accessible for students with visual disabilities. Have a textbook that is accessible to all is very important to me, so please let me know if more changes need to be made. Minor changes and corrections were also made. One change is that every hypothesis test and confidence interval has assumptions that must be true to make the inference valid. Instead of calling them assumptions though, I decided to call them conditions to remove confusion about other assumptions.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#packages-needed-for-r-studio",
    "href": "index.html#packages-needed-for-r-studio",
    "title": "Statistics Using Technology: Rguroo Edition",
    "section": "Packages Needed for r studio",
    "text": "Packages Needed for r studio\nYou will need the following packages installed and loaded in r Studio: arm, HNANES, MASS, mosaic, Weighted.Desc.Stat.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#license",
    "href": "index.html#license",
    "title": "Statistics Using Technology: Rguroo Edition",
    "section": "License",
    "text": "License\nCreative Commons Attribution Sharealike.\n2025 Kathryn Kozak\n\nISBN:",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "Statistical Basics.html",
    "href": "Statistical Basics.html",
    "title": "1  Statistical Basics",
    "section": "",
    "text": "1.1 What is Statistics?\nStatistics is the study of how to collect, organize, analyze, and interpret data collected from a group.\nThere are two branches of statistics. One is called descriptive statistics, which is where you collect and organize data. The other is called inferential statistics, which is where you analyze and interpret data. First you need to look at descriptive statistics since you will use the descriptive statistics when making inferences.\nTo understand how to create descriptive statistics and then conduct inferences, there are a few definitions that you need to look at. Note, many of the words that are defined have common definitions that are used in non-statistical terminology. In statistics, some have slightly different definitions. It is important that you notice the difference and utilize the statistical definitions.\nThe first thing to decide in a statistical study is whom you want to measure and what you want to measure. You always want to make sure that you can answer the question of whom you measured and what you measured. The who is known as the observation and the what is the variable(s).\nobservation, or simply observations: a person or object that you are interested in finding out information about.\nVariable: the measurement or observation of the observation\nHaving the observation and the variables is part of picture of a data set or data frame. To make a data set or data frame into what is called tidy data, it should be organized in a way that each row of the data frame is an observation, and the variables should be well defined and are easily identified. An example of a data frame that is tidy data is:\nTable 1.1: Example of a Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nchidren\nmfr\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarbo\nsugars\npotass\nvitamins\nshelf\nweight\ncups\nrating\n\n\n\n\n100%_Bran\nN\nN\nC\n70\n4\n1\n130\n10.0\n5.0\n6\n280\n25\n3\n1\n0.33\n68.40297\n\n\n100%_Natural_Bran\nN\nQ\nC\n120\n3\n5\n15\n2.0\n8.0\n8\n135\n0\n3\n1\n1.00\n33.98368\n\n\nAll-Bran\nN\nK\nC\n70\n4\n1\n260\n9.0\n7.0\n5\n320\n25\n3\n1\n0.33\n59.42551\n\n\nAll-Bran_with_Extra_Fiber\nN\nK\nC\n50\n4\n0\n140\n14.0\n8.0\n0\n330\n25\n3\n1\n0.50\n93.70491\n\n\nAlmond_Delight\nN\nR\nC\n110\n2\n2\n200\n1.0\n14.0\n8\n-1\n25\n3\n1\n0.75\n34.38484\n\n\nApple_Cinnamon_Cheerios\nY\nG\nC\n110\n2\n2\n180\n1.5\n10.5\n10\n70\n25\n1\n1\n0.75\n29.50954\nCollecting multiple variables from one observation makes sense. If you wanted to figure out the diameter of breast height of Ponderosa Pine trees in the Coconino National Forest, you need to physically measure a bunch of trees. While you are measuring the diameter, you might also want to measure the height of the tree, if the tree has a bark beetle infestation, the estimated age of the tree, the color of the bark, and how many branches it has. You may only want to estimate the average diameter at breast height, but now you have the ability to estimate other quantities too. No sense walking all over the forest and only measure one thing.\nA large data frame is one that has at least 5 variables and at least 1000 units of observations. If a data frame only has 3 variables and 500 rows, that doesn’t make it not usable. The 1000 observations and 5 variables is just a guideline to work with.\nIf you put the observation and the variable into one statement, then you obtain a population.\nPopulation: set of all values of the variable for the entire group of units of observations\nNotice, the population answers who you want to measure and what you want to measure. Make sure that your population always answers both of these questions. If it doesn’t, then you haven’t given someone who is reading your study the entire picture. As an example, if you just say that you are going to collect data from the senators in the U.S. Congress, you haven’t told your reader want you are going to collect. Do you want to know their income, their highest degree earned, their voting record, their age, their political party, their gender, their marital status, or how they feel about a particular issue? Without telling what you want to measure, your reader has no idea what your study is actually about.\nSometimes the population is very easy to collect. Such as if you are interested in finding the average age of all of the current senators in the U.S. Congress, there are only 100 senators. This wouldn’t be hard to find. However, if instead you were interested in knowing the average age that a senator in the U.S. Congress first took office for all senators that ever served in the U.S. Congress, then this would be a bit more work. It is still doable, but it would take a bit of time to collect. But what if you are interested in finding the average diameter of breast height of all of the Ponderosa Pine trees in the Coconino National Forest? This would be impossible to actually collect. What do you do in these cases? Instead of collecting the entire population, you take a smaller group of the population, kind of a snap shot of the population. This smaller group is called a sample.\nSample: a subset from the population. It looks just like the population, but contains less data.\nIn today of big data, there is some confusion between really large data frames and populations. The population is a theoretical concept and even if you have a very large data frame, that doesn’t mean you have the population. Most populations are not actually able to be collected. They are considered an ideal that you are trying to make decisions about.\nHow you collect your sample can determine how accurate the results of your study are. There are many ways to collect samples. Some of them create better samples than others. No sampling method is perfect, but some are better than others. Sampling techniques will be discussed later. For now, realize that every time you take a sample you will find different data values. The sample is a snapshot of the population, and there is more information than is in the picture. The idea is to try to collect a sample that gives you an accurate picture, but you will never know for sure if your picture is the correct picture. Unlike previous mathematics classes where there was always one right answer, in statistics there can be many answers, and you don’t know which are right.\nOnce you have your data frame, either from a population or a sample, you need to know how you want to summarize the data. As an example, suppose you are interested in finding the proportion of people who like a candidate, the average height a plant grows to using a new fertilizer, or the variability of the test scores. Understanding how you want to summarize the data helps to determine the type of data you want to collect. Since the population is what we are interested in, then you want to calculate a number from the population. This is known as a parameter. As mentioned already, you can’t really collect the entire population. Even though this is the number you are interested in, you can’t really calculate it. Instead you use a number calculated from the sample, called a statistic, to estimate the parameter. Since no sample is exactly the same, the statistic values are going to be different from sample to sample. They estimate the value of the parameter, but again, you do not know for sure if your answer is correct.\nParameter: a number calculated from the population. Usually denoted with a Greek letter. This number is a fixed, unknown number that you want to find.\nStatistic: a number calculated from the sample. Usually denoted with letters from the Latin alphabet, though sometimes there is a Greek letter with a (called a hat) above it. Since you can find samples, it is readily known, though it changes depending on the sample taken. It is used to estimate the parameter value.\nOne last concept to mention is that there are two different types of variables – qualitative (categorical) and quantitative (numerical). Each type of variable has different parameters and statistics that you find. It is important to know the difference between them.\nQualitative or categorical variable: answer is a word or name that describes a quality of the observation\nQuantitative or numerical variable: answer is a number, something that can be counted or measured from the observation",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Basics</span>"
    ]
  },
  {
    "objectID": "Statistical Basics.html#what-is-statistics",
    "href": "Statistical Basics.html#what-is-statistics",
    "title": "1  Statistical Basics",
    "section": "",
    "text": "1.1.1 Example: Stating Definitions for Qualitative Variable\nIn 2010, the Pew Research Center questioned 1500 adults in the U.S. to estimate the proportion of the population favoring marijuana use for medical purposes. It was found that 73% are in favor of using marijuana for medical purposes. State the observation, variable, population, and sample.\n\n1.1.1.1 Solution\nObservation: a U.S. adult\nVariable: the response to the question “should marijuana be used for medical purposes?” This is qualitative data since you are recording a person’s response — yes or no.\nPopulation: set of responses of all adults in the U.S.\nSample: set of responses of 1500 adults in the U.S.\nParameter: proportion of all U.S. Adults who favor marijuana for medical purposes\nStatistic — proportion of 1500 U.S. Adults who favor marijuana for medical purposes\n\n\n\n1.1.2 Example: Stating Definitions for Qualitative Variable\nA parking control officer records the manufacturer of every \\(5^{th}\\) car in the college parking lot in order to determine the most common manufacturer. State the observation, variable, population, and sample.\n\n1.1.2.1 Solution\nObservation: a car in the college parking lot\nVariable: the name of the manufacturer. This is qualitative data since you are recording a car type.\nPopulation: set of names of the manufacturer of all cars in the college parking lot.\nSample: set of names of the manufacturer of the a particular number of cars in college parking lot\nParameter: proportion of each car type of all cars in the college parking lot\nStatistic: proportion of each car type a particular number of cars in the college parking lot\n\n\n\n1.1.3 Example: Stating Definitions for Quantitative Variable\nA biologist wants to estimate the average height of a plant that is given a new plant food. She gives 10 plants the new plant food and measures the plant height on day 50. State the observation, variable, population, and sample.\n\n1.1.3.1 Solution\nObservation: a plant given the new plant food\nVariable: the height of the plant on day 50 (Note: it is not the average height since you cannot measure an average – it is calculated from data.) This is quantitative data since you will have a number.\nPopulation: set of heights on day 50 of all plants when the new plant food is used\nSample: set of heights on day 50 of 10 plants when the new plant food is used\nParameter: average height on day 50 of all plants when the new plant food is used\nStatistic: average height on day 50 of 10 plants when the new plant food is used\nNote: in Example: Stating Definitions for Qualitative Variable, you most likely will be comparing the new plant food to an old plant food. So you would have more units of observations, but the plants given the new plant food are what you are interested in in this case. You may also want to have measurements on other days after you give the plant food. In your data frame you would need to have many variables besides just the height of the plant on day 50. Examples of variables would be plant_number, fertilizer (yes or no), height on day 20, height on day 30, height on day 50, and so forth. One other comment, you variable names should make sense to your reader, and be one word for ease in analyzing by a computer program.\n\n\n\n1.1.4 Example: Stating Definitions for Quantitative Variable\nA doctor wants to see if a new treatment for cancer extends the life expectancy of a patient versus the old treatment. She gives one group of 25 cancer patients the new treatment and another group of 25 the old treatment. She then measures the life expectancy of each of the patients. State the units of observations, variables, populations, and samples.\n\n1.1.4.1 Solution\nIn this example there are two observations, two variables, two populations, and two samples.\nObservation 1: cancer patient given new treatment\nObservation 2: cancer patient given old treatment\nVariable 1: life expectancy when given new treatment. This is quantitative data since you will have a number.\nVariable 2: life expectancy when given old treatment. This is quantitative data since you will have a number.\nPopulation 1: set of life expectancies of all cancer patients given new treatment\nPopulation 2: set of life expectancies of all cancer patients given old treatment\nSample 1: set of life expectancies of 25 cancer patients given new treatment\nSample 2: set of life expectancies of 25 cancer patients given old treatment\nParameter 1: average life expectancy of all cancer patients given new treatment\nParameter 2: average life expectancy of all cancer patients given old treatment\nStatistic 1: average life expectancy of 25 cancer patients given new treatment\nStatistic 2: average life expectancy of 25 cancer patients given old treatment\nThere are different types of quantitative variables, called discrete or continuous. The difference is in how many values can the data have. If you can actually count the number of data values (even if you are counting to infinity), then the variable is called discrete. If it is not possible to count the number of data values, then the variable is called continuous.\nDiscrete data can only take on particular values like integers. Discrete data are usually things you count.\nContinuous data can take on any value. Continuous data are usually things you measure.\n\n\n\n1.1.5 Example: Discrete or Continuous\nClassify the quantitative variable as discrete or continuous.\n\nThe weight of a cat.\nThe number of fleas on a cat.\nThe size of a shoe.\n\n\n1.1.5.1 Solution\n\nThe weight of a cat.\nThis is continuous since it is something you measure.\nThe number of fleas on a cat.\nThis is discrete since it is something you count.\nThe size of a shoe.\nThis is discrete since you can only be certain values, such as 7, 7.5, 8, 8.5, 9. You can’t buy a 9.73 shoe.\n\nThere are also are four measurement scales for different types of data with each building on the ones below it. They are:\n\n\n\n1.1.6 Measurement Scales:\nNominal: data is just a name or category. There is no order to any data and since there are no numbers, you cannot do any arithmetic on this level of data. Examples of this are gender, car name, ethnicity, and race.\nOrdinal: data that is nominal, but you can now put the data in order, since one value is more or less than another value. You cannot do arithmetic on this data, but you can now put data values in order. Examples of this are grades (A, B, C, D, F), place value in a race (1st, 2nd, 3rd), and size of a drink (small, medium, large).\nInterval: data that is ordinal, but you can now subtract one value from another and that subtraction makes sense. You can do arithmetic on this data, but only addition and subtraction. Examples of this are temperature and time on a clock.\nRatio: data that is interval, but you can now divide one value by another and that ratio makes sense. You can now do all arithmetic on this data. Examples of this are height, weight, distance, and length of time.\nNominal and ordinal data come from qualitative variables. Interval and ratio data come from quantitative variables.\nMost people have a hard time deciding if the data are nominal, ordinal, interval, or ratio. First, if the variable is qualitative (words instead of numbers) then it is either nominal or ordinal. Now ask yourself if you can put the data in a particular order. If you can it is ordinal. Otherwise, it is nominal. If the variable is quantitative (numbers), then it is either interval or ratio. For ratio data, a value of 0 means there is no measurement. This is known as the absolute zero. If there is an absolute zero in the data, then it means it is ratio. If there is no absolute zero, then the data are interval. An example of an absolute zero is if you have \\$0 in your bank account, then you are without money. The amount of money in your bank account is ratio data. Word of caution: sometimes ordinal data is displayed using numbers, such as 5 being strongly agree, and 1 being strongly disagree. These numbers are not really numbers. Instead they are used to assign numerical values to ordinal data. In reality you should not perform any computations on this data, though many people do. If there are numbers, make sure the numbers are inherent numbers, and not numbers that were assigned.\n\n\n1.1.7 Example: Measurement Scale\nState which measurement scale each is.\n\nTime of first class\nHair color\nLength of time to take a test\nAge groupings (baby, toddler, adolescent, teenager, adult, elderly)\n\n\n1.1.7.1 Solution\n\nTime of first class\nThis is interval since it is a number, but 0 o’clock means midnight and not the absence of time.\nHair color\nThis is nominal since it is not a number, and there is no specific order for hair color.\nLength of time to take a test.\n\nThis is ratio since it is a number, and if you take 0 minutes to take a test, it means you didn’t take any time to complete it.\n\nAge groupings (baby, toddler, adolescent, teenager, adult, elderly)\nThis is ordinal since it is not a number, but you could put the data in order from youngest to oldest or the other way around.\n\n\n\n\n1.1.8 Homework for What is Statistics Section\n\nSuppose you want to know how Arizona workers age 16 or older travel to work. To estimate the percentage of people who use the different modes of travel, you take a sample containing 500 Arizona workers age 16 or older. State the observation, variable, population, sample, parameter, and statistic.\nYou wish to estimate the mean cholesterol levels of patients two days after they had a heart attack. To estimate the mean you collect data from 28 heart patients. State the observation, variable, population, sample, parameter, and statistic.\nPrint-O-Matic would like to estimate their mean salary of all employees. To accomplish this they collect the salary of 19 employees. State the observation, variable, population, sample, parameter, and statistic.\nTo estimate the percentage of households in Connecticut which use fuel oil as a heating source, a researcher collects information from 1000 Connecticut households about what fuel is their heating source. State the observation, variable, population, sample, parameter, and statistic.\nThe U.S. Census Bureau needs to estimate the median income of males in the U.S., they collect incomes from 2500 males. State the observation, variable, population, sample, parameter, and statistic.\nThe U.S. Census Bureau needs to estimate the median income of females in the U.S., they collect incomes from 3500 females. State the observation, variable, population, sample, parameter, and statistic.\nEyeglassmatic manufactures eyeglasses and they would like to know the percentage of each defect type made. They review 25,891 defects and classify each defect that is made. State the observation, variable, population, sample, parameter, and statistic.\nThe World Health Organization wishes to estimate the mean density of people per square kilometer, they collect data on 56 countries. State the observation, variable, population, sample, parameter, and statistic\nState the measurement scale for each.\n\n\n\nCholesterol level\nDefect type\nTime of first class\nOpinion on a 5 point scale, with 5 being strongly agree and 1 being strongly disagree\n\n\n\nState the measurement scale for each.\n\n\n\nTemperature in degrees Celsius\nIce cream flavors available\nPain levels on a scale from 1 to 10, 10 being the worst pain ever\nSalary of employees",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Basics</span>"
    ]
  },
  {
    "objectID": "Statistical Basics.html#sampling-methods",
    "href": "Statistical Basics.html#sampling-methods",
    "title": "1  Statistical Basics",
    "section": "1.2 Sampling Methods",
    "text": "1.2 Sampling Methods\nAs stated before, if you want to know something about a population, it is often impossible or impractical to examine the whole population. It might be too expensive in terms of time or money. It might be impractical — you can’t test all batteries for their length of lifetime because there wouldn’t be any batteries left to sell. You need to look at a sample. Hopefully the sample behaves the same as the population.\nWhen you choose a sample you want it to be as similar to the population as possible. If you want to test a new painkiller for adults you would want the sample to include people who are fat, skinny, old, young, healthy, not healthy, male, female, etc.\nThere are many ways to collect a sample. None are perfect, and you are not guaranteed to collect a representative sample. That is unfortunately the limitations of sampling. However, there are several techniques that can result in samples that give you a semi-accurate picture of the population. Just remember to be aware that the sample may not be representative. As an example, you can take a random sample of a group of people that are equally males and females, yet by chance everyone you choose is female. If this happens, it may be a good idea to collect a new sample if you have the time and money. There are many sampling techniques, though only four will be presented here.\nThe simplest, and the type that is desired for is a simple random sample. This is where you pick the sample such that every sample has the same chance of being chosen. This type of sample is actually hard to collect, since it is sometimes difficult to obtain a complete list of all observations. There are many cases where you cannot conduct a truly random sample. However, you can get as close as you can.\nNow suppose you are interested in what type of music people like. It might not make sense to try to find the most popular type of music preferred by everyone in the U.S. You probably don’t like the same music as your parents. The answers vary so much you probably couldn’t find an answer for everyone all at once. It might make sense to look at people in different age groups, or people of different ethnicities. This is called a stratified sample. The issue with this sample type is that sometimes people subdivide the population too much. It is best to just have one stratification. Also, a stratified sample has similar problems that a simple random sample has.\nIf your population has some order in it, then you could do a systematic sample. This is popular in manufacturing. The problem is that it is possible to miss a manufacturing mistake because of how this sample is taken.\nIf you are collecting polling data based on location, then a cluster sample that divides the population based on geographical means would be the easiest sample to conduct. The problem is that if you are looking for opinions of people, and people who live in the same region may have similar opinions. As you can see each of the sampling techniques have pluses and minuses.\nOne last type of sample that is sometimes conducted is called a convenience sample. This sample is not one that should be conducted since the idea of a convenience sample is that the sample is collected using the most convenient process for the researcher. The researcher may ask people who they know or who are easy to get a old of, and it is in no way representative of the population.\nA simple random sample (SRS) of size n is a sample that is selected from a population in a way that ensures that every different possible sample of size n has the same chance of being selected. Also, every observation associated with the population has the same chance of being selected.\nWays to select a simple random sample:\n\nPut all names in a hat and draw a certain number of names out.\nAssign each observation a number and use a random number table or a calculator or computer to randomly select the observations that will be measured.\n\n\n1.2.1 Example: Choosing a Simple Random Sample\nDescribe how to take a simple random sample from a classroom.\n\n1.2.1.1 Solution\nGive each student in the class a number. Using a random number generator you could then pick the number of students you want to pick.\n\n\n\n1.2.2 Example: How Not to Choose a Simple Random Sample\nYou want to choose 5 students out of a class of 20. Give some examples of samples that are *not* simple random samples.\n\n1.2.2.1 Solution\nChoose 5 students from the front row. The people in the last row have no chance of being selected. Choose the 5 shortest students. The tallest students have no chance of being selected. Ask your friend to pick numbers that have been assigned to each student. Your friend may prefer certain numbers and picks those. This is not known by your friend, but this happens.\n\n\n\n1.2.3 Example: How to Choose a Simple Random Sample using R\nYou want to take a simple random sample of size 10 from a data frame known as NHANES Table 1.2, use these steps:\n\nlibrary(\"NHANES\") # turns on the package NHANES in R\nsample_NHANES&lt;- # gives the new sample a name\n  NHANES |&gt; # states the dataframe to collect from\n  slice_sample(n=10) # creates a random sample and saves it as Sample_NHANES\noptions(width = 60)\nknitr::kable(sample_NHANES) #displays the sample just created\n\n\n\nTable 1.2: Random Sample of size 10 from NHANES\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n70448\n2011_12\nmale\n29\n20-29\nNA\nMexican\nMexican\nHigh School\nSeparated\n25000-34999\n30000\n0.89\n6\nOwn\nWorking\n90.1\nNA\nNA\n175.4\n29.30\nNA\n25.0_to_29.9\n76\n127\n64\n130\n66\n128\n66\n126\n62\n396.83\n1.16\n5.56\n123\n1.382\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n7\nYes\nYes\nNA\n0_hrs\n2_hr\nNA\nNA\nYes\n5\n12\nNo\nYes\nSmoker\n18\nNo\nNA\nNo\nNA\nNo\nYes\n19\n3\n1\nNo\nHeterosexual\nNA\n\n\n70647\n2011_12\nfemale\n38\n30-39\nNA\nOther\nOther\nCollege Grad\nMarried\n45000-54999\n50000\n1.91\n4\nRent\nWorking\n110.1\nNA\nNA\n176.1\n35.50\nNA\n30.0_plus\n78\n127\n79\n130\n82\n130\n82\n124\n76\n38.37\n1.06\n4.63\n17\n0.195\n190\n1.387\nNo\nNA\nFair\n2\n4\nNone\nNone\n8\n4\n21\n4\nNo\nNo\n5\n0_hrs\n3_hr\nNA\nNA\nYes\nNA\n0\nNA\nNo\nNon-Smoker\nNA\nYes\n20\nNo\nNA\nYes\nYes\n20\n11\n1\nYes\nHeterosexual\nNo\n\n\n62684\n2011_12\nfemale\n29\n20-29\nNA\nWhite\nWhite\nSome College\nMarried\n65000-74999\n70000\n2.10\n7\nRent\nWorking\n96.3\nNA\nNA\n168.4\n34.00\nNA\n30.0_plus\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n25.79\n1.06\n4.27\n110\n1.897\nNA\nNA\nNo\nNA\nGood\n0\n0\nSeveral\nNone\n2\n2\n19\n7\nNo\nYes\nNA\n3_hr\n0_to_1_hr\nNA\nNA\nYes\n4\n104\nNA\nNo\nNon-Smoker\nNA\nYes\n16\nYes\n18\nNo\nYes\n16\n5\n1\nYes\nHeterosexual\nNo\n\n\n68625\n2011_12\nmale\n24\n20-29\nNA\nMexican\nMexican\nSome College\nNeverMarried\nNA\nNA\nNA\n7\nOwn\nLooking\n71.6\nNA\nNA\n168.3\n25.30\nNA\n25.0_to_29.9\n60\n104\n45\n110\n58\n102\n46\n106\n44\n340.60\n1.29\n4.99\n320\n1.006\nNA\nNA\nNo\nNA\nGood\n9\n0\nSeveral\nNone\nNA\nNA\nNA\n8\nNo\nNo\n2\n3_hr\n4_hr\nNA\nNA\nNo\n3\n2\nYes\nYes\nSmoker\n21\nYes\n15\nYes\n16\nYes\nYes\n20\n2\n1\nNo\nHeterosexual\nNA\n\n\n67190\n2011_12\nfemale\n36\n30-39\nNA\nWhite\nWhite\nCollege Grad\nMarried\n75000-99999\n87500\n4.50\n8\nOwn\nWorking\n93.1\nNA\nNA\n184.5\n27.30\nNA\n25.0_to_29.9\n60\n129\n66\n128\n70\n130\n66\n128\n66\n17.70\n1.24\n3.85\n117\n1.481\nNA\nNA\nNo\nNA\nFair\n20\n10\nNone\nSeveral\n1\n1\nNA\n7\nYes\nYes\n3\n0_to_1_hr\n1_hr\nNA\nNA\nYes\nNA\n0\nYes\nYes\nSmoker\nNA\nYes\n17\nYes\n17\nNo\nYes\n20\n4\n3\nNo\nHeterosexual\nNo\n\n\n68693\n2011_12\nmale\n28\n20-29\nNA\nWhite\nWhite\nSome College\nNeverMarried\nmore 99999\n100000\n4.14\n8\nOwn\nWorking\n80.9\nNA\nNA\n177.1\n25.80\nNA\n25.0_to_29.9\n58\n132\n74\n134\n72\n130\n72\n134\n76\n653.19\n1.84\n4.55\n51\n0.464\nNA\nNA\nNo\nNA\nExcellent\n0\n0\nNone\nNone\nNA\nNA\nNA\n7\nNo\nYes\nNA\n0_to_1_hr\n4_hr\nNA\nNA\nNo\nNA\n0\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nNo\nNA\n0\n0\nNo\nHeterosexual\nNA\n\n\n71091\n2011_12\nfemale\n39\n30-39\nNA\nHispanic\nHispanic\nSome College\nLivePartner\n75000-99999\n87500\n3.77\n4\nRent\nWorking\n77.5\nNA\nNA\n154.9\n32.30\nNA\n30.0_plus\n74\n106\n82\n106\n82\nNA\nNA\n106\n82\n15.91\n1.47\n5.79\n226\n1.527\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n6\nYes\nNo\n4\n2_hr\n0_to_1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n53515\n2009_10\nmale\n41\n40-49\n503\nWhite\nNA\nHigh School\nMarried\n75000-99999\n87500\n4.08\n9\nOwn\nWorking\n105.1\nNA\nNA\n182.5\n31.56\nNA\n30.0_plus\n70\n140\n72\n142\n78\n140\n76\n140\n68\nNA\n1.09\n6.59\n155\n0.608\nNA\nNA\nNo\nNA\nGood\n0\n3\nSeveral\nSeveral\nNA\nNA\nNA\n6\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n1\n156\nNo\nYes\nSmoker\n24\nYes\n15\nNo\nNA\nYes\nYes\n16\n5\n1\nNo\nHeterosexual\nNA\n\n\n52773\n2009_10\nfemale\n64\n60-69\n776\nWhite\nNA\nSome College\nMarried\n45000-54999\n50000\n3.43\n10\nOwn\nNotWorking\n102.9\nNA\nNA\n162.6\n38.92\nNA\n30.0_plus\n68\n120\n44\n126\n52\n118\n48\n122\n40\nNA\n1.01\n5.79\n38\n0.576\n110\n0.973\nNo\nNA\nGood\n0\n0\nNone\nNone\n3\n2\n23\n7\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\n23\n1\nNA\nNo\nNA\nNA\n\n\n60266\n2009_10\nmale\n14\n10-19\n175\nWhite\nNA\nNA\nNA\nmore 99999\n100000\n3.55\n9\nOwn\nNA\n78.6\nNA\nNA\n174.1\n25.93\nNA\n25.0_to_29.9\n58\n125\n35\n124\n42\n126\n36\n124\n34\nNA\n0.96\n2.53\n88\n0.595\nNA\nNA\nNo\nNA\nFair\n0\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n7\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nStratified sampling is where you break the population into groups called strata, then take a simple random sample from each strata.\nFor example:\n\nIf you want to look at musical preference, you could divide the observations into age groups and then conduct simple random samples inside each group.\nIf you want to calculate the average price of textbooks, you could divide the observations into groups by major and then conduct simple random samples inside each group.\n\n\n\n1.2.4 Example: How to Choose a Stratified Sample using R\nTo take a stratified sample using rStudio of size 20 from NHANES Table 1.3 using race as the strata, use these steps:\n\nlibrary(\"NHANES\") # turns on the package NHANES in R\nsample_NHANES&lt;- # gives the new sample a name\n  NHANES |&gt; # states the dataframe to collect from\n  group_by(Race1) |&gt; # tells what variable is the strata\n  slice_sample(n=20) # takes the random sample within each strata\noptions(width = 60)\nknitr::kable(sample_NHANES) #displays the sample just created\n\n\n\nTable 1.3: Stratafied Sample of size 100 from NHANES with Race as the Strata\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n59793\n2009_10\nmale\n3\n0-9\n40\nBlack\nNA\nNA\nNA\n20000-24999\n22500\n0.93\n6\nRent\nNA\nNA\n100.0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n68140\n2011_12\nmale\n30\n30-39\nNA\nBlack\nBlack\nHigh School\nNeverMarried\n0-4999\n2500\n0.00\n6\nOther\nLooking\n76.9\nNA\nNA\n175.7\n24.90\nNA\n18.5_to_24.9\n52\n116\n68\n118\n70\n114\n68\n118\n68\n463.36\n1.16\n3.80\n145\n0.224\nNA\nNA\nNo\nNA\nVgood\n0\n1\nNone\nNone\nNA\nNA\nNA\n8\nNo\nYes\n5\nMore_4_hr\n4_hr\nNA\nNA\nYes\n4\n72\nYes\nYes\nSmoker\n18\nYes\n13\nYes\n14\nNo\nYes\n14\n35\n6\nNo\nHeterosexual\nNA\n\n\n68913\n2011_12\nfemale\n27\n20-29\nNA\nBlack\nBlack\n9 - 11th Grade\nMarried\n0-4999\n2500\n0.10\n7\nRent\nNotWorking\n67.0\nNA\nNA\n150.5\n29.60\nNA\n25.0_to_29.9\n56\n111\n59\n104\n58\n108\n58\n114\n60\n36.96\n1.76\n4.94\n20\n0.286\n66\n0.423\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n8\nNo\nNo\nNA\n0_to_1_hr\n1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n71488\n2011_12\nfemale\n41\n40-49\nNA\nBlack\nBlack\nHigh School\nMarried\n20000-24999\n22500\n1.08\n3\nRent\nWorking\n110.4\nNA\nNA\n165.8\n40.20\nNA\n30.0_plus\n74\n107\n65\n102\n64\n104\n64\n110\n66\n18.65\n1.22\n3.78\n212\n1.005\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\n4\n2\n24\n8\nNo\nYes\n5\n3_hr\n4_hr\nNA\nNA\nYes\n2\n3\nNo\nYes\nSmoker\n21\nYes\n14\nYes\n21\nNo\nYes\n12\n10\n1\nNo\nHeterosexual\nNo\n\n\n64585\n2011_12\nfemale\n42\n40-49\nNA\nBlack\nBlack\nSome College\nMarried\n45000-54999\n50000\n1.95\n5\nOwn\nWorking\n79.6\nNA\nNA\n161.6\n30.50\nNA\n30.0_plus\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7\nYes\nNo\nNA\n3_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nUnknown\n\n\n54092\n2009_10\nfemale\n68\n60-69\n826\nBlack\nNA\nSome College\nMarried\n55000-64999\n60000\n4.26\n6\nOwn\nNotWorking\n60.9\nNA\nNA\n157.9\n24.43\nNA\n18.5_to_24.9\n40\n136\n76\n140\n78\n136\n76\nNA\nNA\nNA\n1.89\n4.03\n82\n0.646\nNA\nNA\nNo\nNA\nGood\n0\n2\nSeveral\nNone\n2\n2\n22\n5\nNo\nYes\n2\nNA\nNA\nNA\nNA\nNo\nNA\n0\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\n14\n3\nNA\nNo\nNA\nNA\n\n\n64272\n2011_12\nfemale\n19\n10-19\nNA\nBlack\nBlack\nNA\nNA\n25000-34999\n30000\n1.34\n5\nRent\nNotWorking\n79.9\nNA\nNA\n163.8\n29.80\nOverWeight\n25.0_to_29.9\n84\n103\n61\n104\n64\n102\n62\n104\n60\n39.29\n2.69\n7.03\n53\n0.351\nNA\nNA\nNo\nNA\nGood\n0\n3\nNone\nNone\nNA\nNA\nNA\n5\nNo\nNo\nNA\n0_to_1_hr\n3_hr\nNA\nNA\nYes\n2\n24\nNA\nNA\nNA\nNA\nYes\n12\nNo\nNA\nNo\nYes\n11\n4\n2\nNo\nHeterosexual\nNA\n\n\n53507\n2009_10\nfemale\n28\n20-29\n345\nBlack\nNA\nSome College\nNeverMarried\n35000-44999\n40000\n1.55\n9\nOwn\nNotWorking\n139.8\nNA\nNA\n168.1\n49.47\nNA\n30.0_plus\n82\n110\n69\n114\n68\n114\n70\n106\n68\nNA\n1.47\n4.68\n132\n0.754\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nYes\n5\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nNo\nNA\n0\n0\nNo\nHeterosexual\nNo\n\n\n67089\n2011_12\nfemale\n29\n20-29\nNA\nBlack\nBlack\nSome College\nNeverMarried\n25000-34999\n30000\n1.39\n5\nRent\nWorking\n93.9\nNA\nNA\n153.7\n39.70\nNA\n30.0_plus\n74\n119\n81\n120\n76\n120\n80\n118\n82\n25.80\n1.50\n5.66\n75\n0.487\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7\nNo\nYes\nNA\n2_hr\n1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n70845\n2011_12\nfemale\n21\n20-29\nNA\nBlack\nBlack\nSome College\nNeverMarried\nNA\nNA\nNA\n9\nOwn\nNotWorking\n51.2\nNA\nNA\n156.9\n20.80\nNA\n18.5_to_24.9\n98\n106\n59\n112\n52\n106\n56\n106\n62\n91.01\n2.46\n5.51\n274\n3.558\nNA\nNA\nNo\nNA\nFair\n0\n0\nNone\nNone\n3\nNA\nNA\n8\nNo\nYes\n2\nMore_4_hr\n1_hr\nNA\nNA\nYes\n2\n108\nNo\nYes\nSmoker\n18\nYes\n18\nNo\nNA\nNo\nYes\n18\n4\n1\nNo\nHeterosexual\nYes\n\n\n53982\n2009_10\nfemale\n0\n0-9\n11\nBlack\nNA\nNA\nNA\n45000-54999\n50000\n2.27\n10\nOwn\nNA\n10.6\n76.4\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n65103\n2011_12\nmale\n12\n10-19\nNA\nBlack\nBlack\nNA\nNA\n75000-99999\n87500\n2.96\n4\nRent\nNA\n63.7\nNA\nNA\n170.3\n22.00\nOverWeight\n18.5_to_24.9\n70\n120\n68\n118\n62\n116\n78\n124\n58\n472.79\n1.19\n3.36\n37\n0.425\n146\n1.364\nNo\nNA\nGood\n3\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n5\nMore_4_hr\n0_to_1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n63071\n2011_12\nfemale\n53\n50-59\nNA\nBlack\nBlack\nCollege Grad\nWidowed\n25000-34999\n30000\n1.72\n6\nOwn\nWorking\n77.7\nNA\nNA\n161.3\n29.90\nNA\n25.0_to_29.9\n46\n140\n84\n152\n86\n142\n86\n138\n82\n7.53\n1.37\n4.68\n30\n0.294\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\n2\n2\n26\n6\nNo\nYes\n3\n2_hr\n1_hr\nNA\nNA\nYes\n2\n24\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n17\n1\n0\nNo\nHeterosexual\nNA\n\n\n55965\n2009_10\nfemale\n63\n60-69\n759\nBlack\nNA\nSome College\nNeverMarried\n15000-19999\n17500\n1.62\n4\nRent\nLooking\n73.2\nNA\nNA\n169.0\n25.63\nNA\n25.0_to_29.9\n76\n114\n80\n130\n80\n114\n80\nNA\nNA\nNA\n1.06\n5.77\n18\n0.111\n319\n3.009\nYes\n51\nVgood\n0\n0\nNone\nNone\n1\nNA\nNA\n6\nNo\nYes\n4\nNA\nNA\nNA\nNA\nYes\n3\n10\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\n18\n40\nNA\nNo\nNA\nNA\n\n\n59351\n2009_10\nfemale\n15\n10-19\n186\nBlack\nNA\nNA\nNA\n75000-99999\n87500\n3.40\n6\nOwn\nNA\n54.9\nNA\nNA\n160.2\n21.39\nNA\n18.5_to_24.9\n68\n99\n56\n106\n58\n98\n54\n100\n58\nNA\n1.81\n4.63\n31\n0.170\n97\n1.090\nNo\nNA\nGood\n0\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n68818\n2011_12\nmale\n62\n60-69\nNA\nBlack\nBlack\nHigh School\nDivorced\n25000-34999\n30000\n2.75\n5\nOwn\nNotWorking\n114.2\nNA\nNA\n181.0\n34.90\nNA\n30.0_plus\n68\n99\n63\n106\n64\n100\n64\n98\n62\n463.04\n1.37\n3.23\n116\n0.364\nNA\nNA\nYes\n56\nFair\n15\n0\nSeveral\nNone\nNA\nNA\nNA\n4\nNo\nYes\nNA\n0_to_1_hr\n0_to_1_hr\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\n15\n48\nNA\nNo\nNA\nNA\n\n\n64268\n2011_12\nfemale\n11\n10-19\nNA\nBlack\nBlack\nNA\nNA\n5000-9999\n7500\n0.38\n5\nRent\nNA\n81.8\nNA\nNA\n148.0\n37.30\nObese\n30.0_plus\n82\n106\n58\n108\n56\n106\n54\n106\n62\n5.08\n1.06\n3.72\n80\n0.784\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4_hr\n2_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n54682\n2009_10\nmale\n19\n10-19\n239\nBlack\nNA\nNA\nNA\n75000-99999\n87500\n3.49\n6\nOwn\nNotWorking\n87.1\nNA\nNA\n188.6\n24.49\nNA\n18.5_to_24.9\n62\n123\n63\n122\n64\n126\n60\n120\n66\nNA\n2.22\n4.76\n130\n0.199\nNA\nNA\nNo\nNA\nExcellent\n0\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nYes\n4\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n13\n20\n3\nNo\nHeterosexual\nNA\n\n\n55730\n2009_10\nmale\n66\n60-69\n793\nBlack\nNA\nCollege Grad\nMarried\n65000-74999\n70000\n3.55\n8\nOwn\nNotWorking\n62.5\nNA\nNA\n161.3\n24.02\nNA\n18.5_to_24.9\n78\n133\n71\n134\n74\n134\n72\n132\n70\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\nNA\nNA\nNA\n9\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n104\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\n15\n23\nNA\nNo\nNA\nNA\n\n\n52591\n2009_10\nmale\n63\n60-69\n767\nBlack\nNA\nCollege Grad\nNeverMarried\nNA\nNA\nNA\n6\nOwn\nWorking\n84.4\nNA\nNA\n176.5\n27.09\nNA\n25.0_to_29.9\n52\n106\n71\n114\n80\n106\n74\n106\n68\nNA\n1.68\n5.56\n86\n0.925\nNA\nNA\nNo\nNA\nVgood\n30\n0\nNone\nNone\nNA\nNA\nNA\n6\nNo\nYes\n7\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\n16\n10\nNA\nNo\nNA\nNA\n\n\n55845\n2009_10\nfemale\n32\n30-39\n388\nHispanic\nNA\n8th Grade\nMarried\n20000-24999\n22500\n0.78\n2\nRent\nNotWorking\n58.0\nNA\nNA\n150.3\n25.67\nNA\n25.0_to_29.9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1.06\n4.68\n69\n0.515\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n8\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n65072\n2011_12\nfemale\n27\n20-29\nNA\nHispanic\nHispanic\nCollege Grad\nNeverMarried\n75000-99999\n87500\n5.00\n5\nRent\nWorking\n67.8\nNA\nNA\n157.3\n27.40\nNA\n25.0_to_29.9\n80\n108\n61\n106\n60\n112\n62\n104\n60\n46.18\n1.58\n5.25\n243\n3.682\nNA\nNA\nNo\nNA\nExcellent\n0\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nNo\n4\n2_hr\n4_hr\nNA\nNA\nYes\n1\n24\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n19\n3\n0\nNo\nHeterosexual\nNo\n\n\n61670\n2009_10\nmale\n4\n0-9\n51\nHispanic\nNA\nNA\nNA\n10000-14999\n12500\n0.64\n7\nRent\nNA\n16.2\nNA\nNA\n102.1\n15.54\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n60922\n2009_10\nmale\n39\n30-39\n469\nHispanic\nNA\n8th Grade\nMarried\n25000-34999\n30000\n1.08\n4\nOwn\nWorking\n104.5\nNA\nNA\n164.7\n38.52\nNA\n30.0_plus\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0.88\n4.34\n139\nNA\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n65315\n2011_12\nmale\n7\n0-9\nNA\nHispanic\nHispanic\nNA\nNA\n75000-99999\n87500\n3.58\n8\nOwn\nNA\n22.6\nNA\nNA\n118.7\n16.00\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1.36\n1.50\n4.14\n46\n0.329\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n3\n2_hr\n1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n64514\n2011_12\nfemale\n31\n30-39\nNA\nHispanic\nHispanic\n9 - 11th Grade\nLivePartner\n15000-19999\n17500\n0.81\n4\nRent\nLooking\n53.0\nNA\nNA\n153.9\n22.40\nNA\n18.5_to_24.9\n88\n107\n73\n114\n66\n108\n78\n106\n68\n34.30\n1.11\n4.91\n17\n0.128\n160\n1.905\nNo\nNA\nGood\n0\n13\nSeveral\nMost\n2\n0\nNA\n6\nNo\nNo\nNA\n3_hr\n0_to_1_hr\nNA\nNA\nYes\n4\n24\nYes\nYes\nSmoker\n18\nYes\n16\nYes\n20\nNo\nYes\n16\n13\n1\nNo\nHeterosexual\nNo\n\n\n62137\n2009_10\nfemale\n34\n30-39\n410\nHispanic\nNA\n8th Grade\nNeverMarried\n45000-54999\n50000\n1.16\n4\nRent\nWorking\n98.3\nNA\nNA\n159.3\n38.74\nNA\n30.0_plus\n74\n106\n66\n108\n68\n106\n66\n106\n66\nNA\n0.88\n3.54\n253\n1.664\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\n2\n2\n24\n8\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNo\n1\n3\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n17\n3\n3\nNo\nHeterosexual\nNo\n\n\n69869\n2011_12\nfemale\n36\n30-39\nNA\nHispanic\nHispanic\n8th Grade\nLivePartner\n20000-24999\n22500\n0.59\n7\nOwn\nNotWorking\n65.1\nNA\nNA\n164.6\n24.00\nNA\n18.5_to_24.9\n82\n99\n63\n104\n62\n104\n64\n94\n62\n24.00\n1.84\n4.76\n60\n0.531\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7\nNo\nNo\nNA\n1_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n65553\n2011_12\nfemale\n42\n40-49\nNA\nHispanic\nHispanic\n9 - 11th Grade\nSeparated\nNA\nNA\n0.46\n3\nOther\nWorking\n56.0\nNA\nNA\n153.8\n23.70\nNA\n18.5_to_24.9\n86\n118\n74\n124\n72\n116\n72\n120\n76\n21.93\n1.24\n3.26\n193\n0.666\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n6\nNo\nNo\nNA\n1_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n69838\n2011_12\nmale\n31\n30-39\nNA\nHispanic\nHispanic\nHigh School\nMarried\n25000-34999\n30000\n1.35\n4\nRent\nWorking\n155.4\nNA\nNA\n181.3\n47.30\nNA\n30.0_plus\n96\n115\n85\n116\n86\n116\n84\n114\n86\n162.85\n1.19\n6.03\n88\n0.331\nNA\nNA\nNo\nNA\nFair\n0\n0\nNone\nNone\nNA\nNA\nNA\n4\nYes\nNo\n7\n3_hr\n2_hr\nNA\nNA\nYes\n3\n9\nNA\nNo\nNon-Smoker\nNA\nYes\n23\nNo\nNA\nNo\nYes\n17\n25\n3\nNo\nHeterosexual\nNA\n\n\n54474\n2009_10\nmale\n33\n30-39\n405\nHispanic\nNA\nSome College\nMarried\nNA\nNA\nNA\n13\nOwn\nWorking\n79.0\nNA\nNA\n171.8\n26.77\nNA\n25.0_to_29.9\n50\n131\n71\n128\n66\n132\n68\n130\n74\nNA\n1.32\n4.40\n139\n1.219\nNA\nNA\nNo\nNA\nGood\n0\n2\nSeveral\nSeveral\nNA\nNA\nNA\n6\nYes\nYes\n2\nNA\nNA\nNA\nNA\nYes\n12\n24\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n15\n5\n1\nNo\nHeterosexual\nNA\n\n\n56614\n2009_10\nmale\n28\n20-29\n337\nHispanic\nNA\nSome College\nNeverMarried\n25000-34999\n30000\n0.79\n1\nRent\nWorking\n79.9\nNA\nNA\n168.9\n28.01\nNA\n25.0_to_29.9\n74\n109\n49\n110\n52\n110\n50\n108\n48\nNA\n0.75\n3.98\n236\n1.858\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n6\nNo\nYes\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\nYes\nSmoker\n12\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n62468\n2011_12\nfemale\n0\n0-9\n8\nHispanic\nHispanic\nNA\nNA\n20000-24999\n22500\n0.89\n4\nRent\nNA\n7.9\n68.0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n53990\n2009_10\nmale\n45\n40-49\n551\nHispanic\nNA\n8th Grade\nLivePartner\n20000-24999\n22500\n0.60\n7\nRent\nWorking\n92.8\nNA\nNA\n179.2\n28.90\nNA\n25.0_to_29.9\n84\n115\n68\n118\n66\n114\n66\n116\n70\nNA\n1.19\n5.46\n29\n0.725\nNA\nNA\nNo\nNA\nExcellent\n0\n0\nNone\nNone\nNA\nNA\nNA\n6\nNo\nYes\n7\nNA\nNA\nNA\nNA\nYes\nNA\n0\nYes\nYes\nSmoker\n25\nNo\nNA\nNo\nNA\nNo\nYes\n14\nNA\nNA\nNo\nHeterosexual\nNA\n\n\n63450\n2011_12\nmale\n6\n0-9\nNA\nHispanic\nHispanic\nNA\nNA\n75000-99999\n87500\n4.71\n9\nOwn\nNA\n21.3\nNA\nNA\n121.6\n14.40\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n55\n0.274\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n1_hr\n1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n63153\n2011_12\nmale\n2\n0-9\nNA\nHispanic\nHispanic\nNA\nNA\n5000-9999\n7500\n0.42\n4\nRent\nNA\n12.6\n88.2\nNA\n89.0\n15.90\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n3_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n52621\n2009_10\nmale\n35\n30-39\n424\nHispanic\nNA\nHigh School\nLivePartner\n25000-34999\n30000\n1.18\n3\nRent\nWorking\n88.2\nNA\nNA\n164.1\n32.75\nNA\n30.0_plus\n74\n126\n68\n130\n74\n126\n68\n126\n68\nNA\n0.59\n4.91\n90\n0.634\nNA\nNA\nNo\nNA\nVgood\n0\n1\nNone\nNone\nNA\nNA\nNA\n6\nNo\nYes\n1\nNA\nNA\nNA\nNA\nYes\n1\n12\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n16\n4\n1\nNo\nHeterosexual\nNA\n\n\n68664\n2011_12\nmale\n6\n0-9\nNA\nHispanic\nHispanic\nNA\nNA\nmore 99999\n100000\n5.00\n4\nOwn\nNA\n25.2\nNA\nNA\n118.4\n18.00\nOverWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1.11\n1.34\n3.93\n53\n0.373\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1_hr\n0_to_1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51794\n2009_10\nmale\n13\n10-19\n167\nHispanic\nNA\nNA\nNA\n25000-34999\n30000\n0.97\n4\nRent\nNA\n74.0\nNA\nNA\n173.7\n24.53\nNA\n18.5_to_24.9\n96\n127\n68\n122\n70\n126\n66\n128\n70\nNA\n1.01\n3.90\n281\n1.873\nNA\nNA\nNo\nNA\nFair\n0\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n54041\n2009_10\nmale\n52\n50-59\n625\nHispanic\nNA\nSome College\nSeparated\n45000-54999\n50000\n4.16\n3\nRent\nWorking\n92.5\nNA\nNA\n176.5\n29.69\nNA\n25.0_to_29.9\n54\n106\n61\n108\n66\n108\n58\n104\n64\nNA\n1.14\n4.99\n97\n0.655\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nNo\nNA\nNo\nNA\nNo\nYes\n17\n3\n0\nNo\nHeterosexual\nNA\n\n\n52004\n2009_10\nfemale\n12\n10-19\n147\nMexican\nNA\nNA\nNA\n35000-44999\n40000\n2.75\n6\nRent\nNA\n73.0\nNA\nNA\n159.6\n28.66\nNA\n25.0_to_29.9\n78\n106\n53\n104\n60\n110\n52\n102\n54\nNA\n1.42\n4.45\n56\n1.931\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n62781\n2011_12\nfemale\n3\n0-9\nNA\nMexican\nMexican\nNA\nNA\n20000-24999\n22500\n0.71\n6\nRent\nNA\n13.1\n94.6\nNA\n94.8\n14.60\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n59364\n2009_10\nfemale\n25\n20-29\n310\nMexican\nNA\nSome College\nNeverMarried\n75000-99999\n87500\n3.69\n2\nRent\nWorking\n80.6\nNA\nNA\n158.8\n31.96\nNA\n30.0_plus\n68\n117\n86\n114\n88\n116\n86\n118\n86\nNA\n1.68\n4.68\n210\n0.383\nNA\nNA\nNo\nNA\nExcellent\n5\n4\nSeveral\nSeveral\n1\nNA\nNA\n6\nNo\nYes\n3\nNA\nNA\nNA\nNA\nYes\n2\n104\nYes\nYes\nSmoker\n17\nYes\n17\nYes\n17\nNo\nYes\n15\n10\n1\nYes\nBisexual\nNo\n\n\n56130\n2009_10\nfemale\n16\n10-19\n193\nMexican\nNA\nNA\nNA\n25000-34999\n30000\n1.45\n5\nRent\nNotWorking\n69.4\nNA\nNA\n161.0\n26.77\nNA\n25.0_to_29.9\n88\n101\n53\n100\n54\n102\n50\n100\n56\nNA\n1.47\n4.06\n295\n0.251\nNA\nNA\nNo\nNA\nPoor\n0\n0\nNA\nNA\nNA\nNA\nNA\n5\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n66466\n2011_12\nmale\n6\n0-9\nNA\nMexican\nMexican\nNA\nNA\n35000-44999\n40000\n1.42\n5\nRent\nNA\n30.0\nNA\nNA\n126.2\n18.80\nObese\n18.5_to_24.9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5.52\n1.66\n4.63\n34\n0.191\n34\n0.276\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2_hr\n0_to_1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n68625\n2011_12\nmale\n24\n20-29\nNA\nMexican\nMexican\nSome College\nNeverMarried\nNA\nNA\nNA\n7\nOwn\nLooking\n71.6\nNA\nNA\n168.3\n25.30\nNA\n25.0_to_29.9\n60\n104\n45\n110\n58\n102\n46\n106\n44\n340.60\n1.29\n4.99\n320\n1.006\nNA\nNA\nNo\nNA\nGood\n9\n0\nSeveral\nNone\nNA\nNA\nNA\n8\nNo\nNo\nNA\n3_hr\n4_hr\nNA\nNA\nNo\n3\n2\nYes\nYes\nSmoker\n21\nYes\n15\nYes\n16\nYes\nYes\n20\n2\n1\nNo\nHeterosexual\nNA\n\n\n62656\n2011_12\nfemale\n0\n0-9\n9\nMexican\nMexican\nNA\nNA\n15000-19999\n17500\n0.73\n4\nRent\nNA\n9.0\n74.1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n56783\n2009_10\nfemale\n65\n60-69\n781\nMexican\nNA\n8th Grade\nMarried\n25000-34999\n30000\n0.68\n4\nOwn\nNotWorking\n104.3\nNA\nNA\n152.8\n44.67\nNA\n30.0_plus\n60\n134\n84\n112\n82\nNA\nNA\n134\n84\nNA\nNA\nNA\n40\n0.678\nNA\nNA\nYes\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nYes\nSmoker\n25\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n61844\n2009_10\nfemale\n0\n0-9\n11\nMexican\nNA\nNA\nNA\n15000-19999\n17500\n0.54\n7\nOther\nNA\n11.1\n74.9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n58321\n2009_10\nfemale\n61\n60-69\n733\nMexican\nNA\n8th Grade\nSeparated\n15000-19999\n17500\n0.88\n5\nRent\nNotWorking\n68.3\nNA\nNA\n147.9\n31.22\nNA\n30.0_plus\n64\n122\n57\n132\n62\n116\n56\n128\n58\nNA\n0.83\n4.01\n95\n0.247\nNA\nNA\nYes\n49\nFair\n30\n30\nMost\nMost\n10\n7\n14\n5\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n59057\n2009_10\nfemale\n2\n0-9\n35\nMexican\nNA\nNA\nNA\n5000-9999\n7500\n0.31\n5\nRent\nNA\n19.7\n96.8\nNA\n94.7\n21.97\nNA\n18.5_to_24.9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n71679\n2011_12\nfemale\n27\n20-29\nNA\nMexican\nMexican\n9 - 11th Grade\nNeverMarried\n15000-19999\n17500\n0.61\n3\nRent\nWorking\n65.7\nNA\nNA\n166.1\n23.80\nNA\n18.5_to_24.9\n98\n110\n55\n108\n54\n110\n52\n110\n58\n29.51\n1.27\n3.49\n73\n0.213\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n8\nNo\nNo\nNA\n4_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n51752\n2009_10\nfemale\n78\n70+\n937\nMexican\nNA\n8th Grade\nWidowed\n5000-9999\n7500\n0.70\n4\nOwn\nNotWorking\n51.6\nNA\nNA\n151.0\n22.63\nNA\n18.5_to_24.9\n66\n133\n61\n130\n60\n132\n58\n134\n64\nNA\n1.37\n4.37\n61\nNA\nNA\nNA\nYes\n40\nPoor\nNA\n0\nNA\nSeveral\nNA\nNA\nNA\n9\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nYes\nYes\nSmoker\n35\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n55519\n2009_10\nmale\n2\n0-9\n24\nMexican\nNA\nNA\nNA\n10000-14999\n12500\n0.79\n5\nRent\nNA\n13.0\n88.8\nNA\n87.6\n16.94\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n57470\n2009_10\nmale\n44\n40-49\n538\nMexican\nNA\n9 - 11th Grade\nSeparated\n25000-34999\n30000\n1.13\n5\nRent\nWorking\n95.4\nNA\nNA\n171.6\n32.40\nNA\n30.0_plus\n70\n102\n63\n100\n66\n104\n62\n100\n64\nNA\n0.96\n6.39\n80\nNA\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n9\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n52\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nYes\nYes\n21\n6\n3\nNo\nHeterosexual\nNA\n\n\n70333\n2011_12\nfemale\n20\n20-29\nNA\nMexican\nMexican\nSome College\nNeverMarried\nmore 99999\n100000\n5.00\n10\nOwn\nWorking\n50.9\nNA\nNA\n168.1\n18.00\nNA\n12.0_18.5\n110\n97\n80\n102\n90\n96\n78\n98\n82\n50.45\n2.30\n5.48\n292\nNA\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\nNA\nNA\nNA\n7\nNo\nNo\n3\n1_hr\n0_to_1_hr\nNA\nNA\nNo\n2\n3\nNA\nNo\nNon-Smoker\nNA\nYes\n18\nNo\nNA\nNo\nYes\n18\n1\n1\nNo\nHeterosexual\nNo\n\n\n59842\n2009_10\nmale\n23\n20-29\n283\nMexican\nNA\n8th Grade\nMarried\n25000-34999\n30000\n1.37\n5\nRent\nWorking\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n58\n116\n64\n114\n64\n118\n62\n114\n66\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nYes\nSmoker\n10\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n71596\n2011_12\nfemale\n19\n10-19\nNA\nMexican\nMexican\nNA\nNA\n20000-24999\n22500\n0.68\n6\nRent\nNotWorking\n50.9\nNA\nNA\n159.6\n20.00\nNormWeight\n18.5_to_24.9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n11.76\n1.03\n3.28\n187\n1.247\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7\nNo\nNo\n2\n2_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n63224\n2011_12\nfemale\n31\n30-39\nNA\nMexican\nMexican\n9 - 11th Grade\nMarried\n35000-44999\n40000\n0.51\n5\nOwn\nWorking\n70.9\nNA\nNA\n155.6\n29.30\nNA\n25.0_to_29.9\n66\n112\n62\n112\n60\n112\n62\n112\n62\n23.98\n1.37\n4.29\n156\n1.793\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\n4\n4\n16\n8\nNo\nYes\n3\n2_hr\n0_hrs\nNA\nNA\nYes\n1\n3\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n15\n1\n1\nNo\nHeterosexual\nNo\n\n\n70494\n2011_12\nmale\n25\n20-29\nNA\nMexican\nMexican\nHigh School\nNeverMarried\n35000-44999\n40000\n1.97\n5\nOwn\nLooking\n104.9\nNA\nNA\n168.1\n37.10\nNA\n30.0_plus\n74\n132\n84\n140\n92\n134\n82\n130\n86\n217.00\n0.93\n5.74\n181\n1.403\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nSeveral\nNA\nNA\nNA\n7\nNo\nNo\nNA\n1_hr\n0_to_1_hr\nNA\nNA\nYes\n6\n52\nNA\nNo\nNon-Smoker\nNA\nYes\n16\nNo\nNA\nNo\nYes\n15\n4\n1\nNo\nHeterosexual\nNA\n\n\n68754\n2011_12\nmale\n57\n50-59\nNA\nWhite\nWhite\nSome College\nLivePartner\n75000-99999\n87500\n3.25\n5\nOwn\nNotWorking\n86.1\nNA\nNA\n192.8\n23.20\nNA\n18.5_to_24.9\n82\n124\n85\n124\n84\n122\n84\n126\n86\n437.72\n1.58\n5.72\n132\n0.273\nNA\nNA\nYes\nNA\nVgood\n2\n0\nNone\nNone\nNA\nNA\nNA\n6\nNo\nNo\nNA\nMore_4_hr\n0_hrs\nNA\nNA\nYes\n7\n156\nYes\nYes\nSmoker\n36\nYes\n17\nYes\n20\nYes\nYes\n19\n35\n2\nNo\nHeterosexual\nNA\n\n\n68953\n2011_12\nfemale\n32\n30-39\nNA\nWhite\nWhite\nHigh School\nDivorced\n15000-19999\n17500\n0.78\n5\nOwn\nWorking\n70.3\nNA\nNA\n149.3\n31.50\nNA\n30.0_plus\n70\n123\n79\n112\n80\n120\n78\n126\n80\n9.26\n1.06\n6.28\n141\n0.892\nNA\nNA\nYes\nNA\nExcellent\n0\n2\nNone\nNone\n3\n3\n21\n8\nNo\nYes\nNA\n1_hr\n1_hr\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n31\nYes\n20\nNo\nNA\nNo\nYes\n16\n3\n1\nNo\nHeterosexual\nNo\n\n\n55597\n2009_10\nmale\n30\n30-39\n368\nWhite\nNA\nSome College\nLivePartner\nmore 99999\n100000\n4.62\n7\nOwn\nWorking\n81.3\nNA\nNA\n176.5\n26.10\nNA\n25.0_to_29.9\n82\n118\n53\n112\n66\n116\n54\n120\n52\nNA\n1.53\n4.81\n282\n5.529\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7\nNo\nYes\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n63098\n2011_12\nfemale\n10\n10-19\nNA\nWhite\nWhite\nNA\nNA\n75000-99999\n87500\n2.63\n6\nRent\nNA\n38.8\nNA\nNA\n144.3\n18.60\nNormWeight\n18.5_to_24.9\n80\n107\n53\n102\n48\n108\n50\n106\n56\n16.30\n1.71\n4.27\n69\n0.489\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n2_hr\n0_to_1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n62571\n2011_12\nfemale\n32\n30-39\nNA\nWhite\nWhite\nCollege Grad\nNeverMarried\n75000-99999\n87500\n3.06\n10\nOwn\nNotWorking\n87.2\nNA\nNA\n177.1\n27.80\nNA\n25.0_to_29.9\n72\n101\n64\n102\n62\n100\n68\n102\n60\n49.93\n1.47\n4.34\n276\n1.272\nNA\nNA\nNo\nNA\nExcellent\n0\n0\nNone\nNone\n1\nNA\nNA\n7\nNo\nYes\n1\n0_to_1_hr\n1_hr\nNA\nNA\nYes\n1\n24\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n24\n7\n1\nNo\nHeterosexual\nNo\n\n\n67879\n2011_12\nfemale\n45\n40-49\nNA\nWhite\nWhite\nSome College\nMarried\nmore 99999\n100000\n5.00\n7\nRent\nNotWorking\n68.6\nNA\nNA\n167.1\n24.60\nNA\n18.5_to_24.9\n52\n110\n67\n108\n64\n112\n66\n108\n68\n19.99\n1.76\n5.64\n25\n0.106\n54\n0.514\nNo\nNA\nVgood\n0\n0\nNone\nNone\n3\n3\n23\n6\nYes\nYes\nNA\n3_hr\n2_hr\nNA\nNA\nYes\n2\n2\nNo\nYes\nSmoker\n18\nYes\n16\nNo\nNA\nNo\nYes\n17\n1\n1\nNo\nHeterosexual\nNA\n\n\n64880\n2011_12\nmale\n60\n60-69\nNA\nWhite\nWhite\nHigh School\nMarried\nmore 99999\n100000\n5.00\n11\nOwn\nWorking\n57.2\nNA\nNA\n162.0\n21.80\nNA\n18.5_to_24.9\n68\n152\n78\n154\n82\n144\n78\n160\n78\n318.63\n1.78\n5.90\n118\n0.849\nNA\nNA\nNo\nNA\nVgood\n0\n10\nMost\nSeveral\nNA\nNA\nNA\n7\nNo\nNo\n1\n0_hrs\n3_hr\nNA\nNA\nYes\n3\n364\nYes\nYes\nSmoker\n17\nNA\nNA\nNA\nNA\nNo\nYes\n20\n20\nNA\nNo\nNA\nNA\n\n\n52731\n2009_10\nfemale\n47\n40-49\n571\nWhite\nNA\nHigh School\nMarried\n55000-64999\n60000\n4.12\n5\nOwn\nWorking\n59.7\nNA\nNA\n163.6\n22.31\nNA\n18.5_to_24.9\n88\n120\n80\n124\n82\n120\n80\n120\n80\nNA\n2.12\n5.79\n235\n2.670\nNA\nNA\nNo\nNA\nVgood\n2\n0\nNone\nNone\nNA\nNA\nNA\n7\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n3\n52\nNA\nNo\nNon-Smoker\nNA\nYes\n19\nNo\nNA\nNo\nYes\n18\n3\n1\nNo\nHeterosexual\nNA\n\n\n57907\n2009_10\nmale\n48\n40-49\n586\nWhite\nNA\nCollege Grad\nMarried\nmore 99999\n100000\n5.00\n4\nRent\nWorking\n82.1\nNA\nNA\n179.2\n25.57\nNA\n25.0_to_29.9\n60\n108\n79\n110\n78\n110\n78\n106\n80\nNA\n1.29\n5.66\n141\n0.966\nNA\nNA\nNo\nNA\nExcellent\n2\n0\nNone\nNone\nNA\nNA\nNA\n7\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n3\n104\nNA\nNo\nNon-Smoker\nNA\nYes\n14\nYes\n20\nYes\nYes\n14\n50\n1\nNo\nHeterosexual\nNA\n\n\n69934\n2011_12\nfemale\n56\n50-59\nNA\nWhite\nWhite\nCollege Grad\nMarried\nmore 99999\n100000\n5.00\n8\nOwn\nNotWorking\n88.5\nNA\nNA\n182.0\n26.70\nNA\n25.0_to_29.9\n64\n95\n69\n96\n68\n96\n66\n94\n72\n24.64\n1.78\n5.61\n446\n2.549\nNA\nNA\nNo\nNA\nGood\n7\n0\nNone\nNone\n3\n1\nNA\n8\nYes\nYes\nNA\n2_hr\n1_hr\nNA\nNA\nYes\n1\n364\nNo\nYes\nSmoker\n22\nYes\n23\nNo\nNA\nYes\nYes\n20\n10\n1\nYes\nHeterosexual\nNA\n\n\n65540\n2011_12\nmale\n9\n0-9\nNA\nWhite\nWhite\nNA\nNA\n55000-64999\n60000\n2.60\n10\nOwn\nNA\n34.6\nNA\nNA\n135.7\n18.80\nNormWeight\n18.5_to_24.9\n72\n93\n0\n92\n38\n94\n0\n92\n0\n3.89\n1.73\n4.37\n326\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\nMore_4_hr\n1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n67264\n2011_12\nfemale\n77\n70+\nNA\nWhite\nWhite\n9 - 11th Grade\nWidowed\n15000-19999\n17500\n1.61\n5\nOwn\nNotWorking\n64.5\nNA\nNA\n163.2\n24.20\nNA\n18.5_to_24.9\n88\n124\n56\n122\n56\n126\n56\n122\n56\n19.39\n1.37\n5.30\n95\n1.583\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\n2\n2\n19\n8\nYes\nYes\n6\n4_hr\n0_hrs\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n54800\n2009_10\nmale\n41\n40-49\n501\nWhite\nNA\nSome College\nSeparated\n55000-64999\n60000\n5.00\n4\nRent\nWorking\n84.1\nNA\nNA\n170.6\n28.90\nNA\n25.0_to_29.9\n80\n111\n76\n112\n72\n108\n72\n114\n80\nNA\n1.32\n6.52\n272\n6.800\nNA\nNA\nNo\nNA\nGood\n0\n3\nNone\nSeveral\nNA\nNA\nNA\n7\nYes\nYes\n3\nNA\nNA\nNA\nNA\nYes\n6\n104\nNo\nYes\nSmoker\n36\nYes\n13\nYes\n15\nYes\nYes\n13\n25\n1\nNo\nHeterosexual\nNA\n\n\n66873\n2011_12\nfemale\n54\n50-59\nNA\nWhite\nWhite\nSome College\nMarried\nNA\nNA\nNA\n6\nOwn\nWorking\n149.8\nNA\nNA\n159.1\n59.20\nNA\n30.0_plus\n60\n197\n96\n212\n96\n198\n100\n196\n92\n40.73\n1.11\n6.70\n55\n0.573\nNA\nNA\nNo\nNA\nGood\n4\n0\nNone\nNone\n4\n2\n24\n7\nNo\nNo\n5\n3_hr\n1_hr\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n18\n1\n1\nNo\nHeterosexual\nNA\n\n\n56296\n2009_10\nmale\n2\n0-9\n33\nWhite\nNA\nNA\nNA\n25000-34999\n30000\n1.32\n5\nOwn\nNA\n12.6\n89.6\nNA\n85.9\n17.08\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n3\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n53257\n2009_10\nmale\n6\n0-9\n76\nWhite\nNA\nNA\nNA\nmore 99999\n100000\n5.00\n11\nOwn\nNA\n22.8\nNA\nNA\n119.6\n15.94\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1.27\n4.84\n86\n1.012\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n68953\n2011_12\nfemale\n32\n30-39\nNA\nWhite\nWhite\nHigh School\nDivorced\n15000-19999\n17500\n0.78\n5\nOwn\nWorking\n70.3\nNA\nNA\n149.3\n31.50\nNA\n30.0_plus\n70\n123\n79\n112\n80\n120\n78\n126\n80\n9.26\n1.06\n6.28\n141\n0.892\nNA\nNA\nYes\nNA\nExcellent\n0\n2\nNone\nNone\n3\n3\n21\n8\nNo\nYes\nNA\n1_hr\n1_hr\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n31\nYes\n20\nNo\nNA\nNo\nYes\n16\n3\n1\nNo\nHeterosexual\nNo\n\n\n57381\n2009_10\nfemale\n15\n10-19\n188\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n3.63\n10\nOwn\nNA\n57.6\nNA\nNA\n170.5\n19.81\nNA\n18.5_to_24.9\n84\n99\n62\n100\n68\n98\n58\n100\n66\nNA\n1.63\n4.50\n12\n0.308\n306\n2.372\nNo\nNA\nGood\n0\n10\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n61087\n2009_10\nfemale\n8\n0-9\n105\nWhite\nNA\nNA\nNA\n25000-34999\n30000\n1.01\n6\nRent\nNA\n25.7\nNA\nNA\n126.3\n16.11\nNA\n12.0_18.5\n78\n93\n55\n96\n58\n96\n56\n90\n54\nNA\n1.16\n4.37\n96\n1.171\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n64965\n2011_12\nmale\n26\n20-29\nNA\nWhite\nWhite\nSome College\nMarried\nmore 99999\n100000\n4.97\n7\nOwn\nWorking\n82.6\nNA\nNA\n160.0\n32.30\nNA\n30.0_plus\n78\n102\n57\n104\n62\n104\n58\n100\n56\n368.00\n0.75\n4.63\n124\n0.629\nNA\nNA\nNo\nNA\nGood\n0\n20\nNone\nSeveral\nNA\nNA\nNA\n5\nNo\nNo\n1\n4_hr\n0_to_1_hr\nNA\nNA\nYes\n8\n36\nYes\nYes\nSmoker\n16\nNo\nNA\nNo\nNA\nNo\nYes\n17\n6\n1\nNo\nHeterosexual\nNA\n\n\n52217\n2009_10\nfemale\n48\n40-49\n579\nOther\nNA\nSome College\nMarried\nNA\nNA\nNA\n10\nOwn\nNotWorking\n92.7\nNA\nNA\n161.6\n35.50\nNA\n30.0_plus\n70\n143\n66\n142\n66\n146\n68\n140\n64\nNA\n1.40\n4.86\n71\n0.504\nNA\nNA\nYes\n46\nFair\n4\n30\nSeveral\nSeveral\n6\n4\n23\n6\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n12\n5\nNA\nNo\nNon-Smoker\nNA\nYes\n22\nNo\nNA\nNo\nYes\n18\n8\n0\nNo\nHeterosexual\nNA\n\n\n59747\n2009_10\nmale\n28\n20-29\n343\nOther\nNA\nCollege Grad\nNeverMarried\n55000-64999\n60000\n3.50\n7\nOwn\nNotWorking\n75.8\nNA\nNA\n172.3\n25.53\nNA\n25.0_to_29.9\n76\n127\n91\n126\n92\n126\n92\n128\n90\nNA\n1.37\n5.69\n37\n0.301\n395\n3.015\nNo\nNA\nGood\n0\n30\nMost\nMost\nNA\nNA\nNA\n8\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNo\nNA\n0\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nNo\nNA\n0\n0\nNo\nHeterosexual\nNA\n\n\n64811\n2011_12\nfemale\n16\n10-19\nNA\nOther\nAsian\nNA\nNA\n55000-64999\n60000\n2.39\n6\nOwn\nNotWorking\n60.0\nNA\nNA\n151.0\n26.30\nOverWeight\n25.0_to_29.9\n86\n111\n55\n106\n60\n112\n46\n110\n64\nNA\nNA\nNA\n33\n0.209\n173\n4.023\nNo\nNA\nGood\n3\n0\nNA\nNA\nNA\nNA\nNA\n7\nNo\nYes\nNA\n0_hrs\n2_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n71316\n2011_12\nmale\n57\n50-59\nNA\nOther\nOther\n9 - 11th Grade\nMarried\n65000-74999\n70000\n3.13\n5\nOwn\nWorking\n70.8\nNA\nNA\n169.3\n24.70\nNA\n18.5_to_24.9\n48\n113\n65\n114\n66\n116\n66\n110\n64\n599.00\n1.14\n3.88\n97\n0.890\nNA\nNA\nNo\nNA\nVgood\n4\n0\nNone\nNone\nNA\nNA\nNA\n7\nNo\nNo\n2\n2_hr\n0_to_1_hr\nNA\nNA\nYes\n12\n24\nYes\nYes\nSmoker\n25\nYes\n25\nYes\n25\nNo\nYes\n16\n3\n1\nNo\nHeterosexual\nNA\n\n\n67385\n2011_12\nmale\n47\n40-49\nNA\nOther\nOther\nHigh School\nNeverMarried\nNA\nNA\n1.16\n5\nRent\nNotWorking\n66.7\nNA\nNA\n168.1\n23.60\nNA\n18.5_to_24.9\n76\n114\n60\n110\n62\n114\n60\n114\n60\n152.99\n1.55\n5.20\n300\n0.896\nNA\nNA\nNo\nNA\nVgood\n0\n4\nNone\nNone\nNA\nNA\nNA\n7\nNo\nYes\n1\n4_hr\n0_hrs\nNA\nNA\nYes\n3\n84\nYes\nYes\nSmoker\n22\nYes\n35\nYes\n35\nNo\nYes\n15\n12\n1\nNo\nHeterosexual\nNA\n\n\n56216\n2009_10\nfemale\n50\n50-59\n603\nOther\nNA\nCollege Grad\nMarried\nNA\nNA\nNA\n6\nOwn\nNotWorking\n67.0\nNA\nNA\n171.7\n22.73\nNA\n18.5_to_24.9\n70\n117\n60\n114\n62\n118\n66\n116\n54\nNA\nNA\nNA\n48\n1.200\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7\nNo\nYes\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nYes\nSmoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n55272\n2009_10\nmale\n66\n60-69\n800\nOther\nNA\nCollege Grad\nMarried\nmore 99999\n100000\n5.00\n6\nOwn\nWorking\n128.8\nNA\nNA\n170.3\n44.41\nNA\n30.0_plus\n60\n125\n55\n138\n46\n124\n56\n126\n54\nNA\n1.53\n4.11\n41\n0.199\nNA\nNA\nYes\n62\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n7\nNo\nYes\n3\nNA\nNA\nNA\nNA\nNo\n1\n1\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\n18\n6\nNA\nNo\nNA\nNA\n\n\n55288\n2009_10\nmale\n1\n0-9\n16\nOther\nNA\nNA\nNA\n75000-99999\n87500\n4.37\n6\nOwn\nNA\n10.8\n78.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n67328\n2011_12\nfemale\n6\n0-9\nNA\nOther\nAsian\nNA\nNA\nmore 99999\n100000\n3.92\n6\nOwn\nNA\n19.6\nNA\nNA\n116.3\n14.50\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n68\n0.562\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2_hr\n1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n69402\n2011_12\nfemale\n59\n50-59\nNA\nOther\nAsian\nCollege Grad\nNeverMarried\n75000-99999\n87500\n5.00\n4\nRent\nWorking\n65.7\nNA\nNA\n158.4\n26.20\nNA\n25.0_to_29.9\n70\n104\n62\n104\n56\n104\n62\nNA\nNA\n17.99\n1.58\n5.53\n172\n0.217\nNA\nNA\nNo\nNA\nExcellent\n21\n0\nNone\nNone\nNA\nNA\nNA\n5\nNo\nYes\n2\n3_hr\n0_hrs\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nNo\nNA\n0\n0\nNo\nNA\nNA\n\n\n68380\n2011_12\nfemale\n43\n40-49\nNA\nOther\nAsian\nCollege Grad\nMarried\nmore 99999\n100000\n5.00\n8\nOwn\nWorking\n68.4\nNA\nNA\n158.0\n27.40\nNA\n25.0_to_29.9\n78\n106\n73\n110\n74\n104\n74\n108\n72\n10.64\n0.96\n4.24\n230\n4.894\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\n2\n2\n22\n7\nNo\nYes\n4\n0_to_1_hr\n1_hr\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n20\n1\n1\nNo\nHeterosexual\nNo\n\n\n63523\n2011_12\nfemale\n22\n20-29\nNA\nOther\nOther\nCollege Grad\nNeverMarried\n75000-99999\n87500\n3.95\n4\nRent\nWorking\n50.4\nNA\nNA\n155.1\n21.00\nNA\n18.5_to_24.9\n90\n105\n72\n104\n68\n108\n72\n102\n72\n72.33\n1.78\n6.88\n18\n0.321\n303\n3.092\nNo\nNA\nGood\n0\n15\nSeveral\nSeveral\nNA\nNA\nNA\n7\nNo\nNo\n7\n2_hr\nMore_4_hr\nNA\nNA\nYes\n2\n104\nNA\nNo\nNon-Smoker\nNA\nYes\n18\nNo\nNA\nNo\nYes\n18\n15\n3\nNo\nHeterosexual\nNo\n\n\n62789\n2011_12\nfemale\n26\n20-29\nNA\nOther\nAsian\nSome College\nLivePartner\n25000-34999\n30000\n2.20\n3\nRent\nNotWorking\n88.5\nNA\nNA\n166.8\n31.80\nNA\n30.0_plus\n78\n106\n78\n108\n78\n106\n78\n106\n78\n27.15\n1.27\n4.01\n39\n0.424\n360\n3.077\nNo\nNA\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n8\nYes\nYes\nNA\n0_to_1_hr\nMore_4_hr\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n24\n2\n1\nNo\nHeterosexual\nNo\n\n\n70736\n2011_12\nfemale\n32\n30-39\nNA\nOther\nOther\nCollege Grad\nSeparated\nmore 99999\n100000\n5.00\n9\nOwn\nWorking\n71.4\nNA\nNA\n175.3\n23.20\nNA\n18.5_to_24.9\n82\n116\n52\n120\n62\n120\n54\n112\n50\n22.48\n2.12\n5.07\n21\n0.750\n246\n3.905\nNo\nNA\nExcellent\n0\n5\nNone\nSeveral\nNA\nNA\nNA\n6\nYes\nYes\n7\n2_hr\n3_hr\nNA\nNA\nYes\n1\n12\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n29\n3\n0\nNo\nHeterosexual\nNo\n\n\n56164\n2009_10\nfemale\n62\n60-69\n754\nOther\nNA\nSome College\nSeparated\n10000-14999\n12500\n0.92\n8\nOwn\nNotWorking\n109.1\nNA\nNA\n158.3\n43.54\nNA\n30.0_plus\n62\n141\n86\n144\n80\n138\n84\n144\n88\nNA\n1.66\n6.47\n190\n3.220\nNA\nNA\nNo\nNA\nExcellent\n0\n15\nMost\nMost\n3\n3\n30\n8\nNo\nYes\n2\nNA\nNA\nNA\nNA\nYes\n1\n12\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNo\nYes\n29\n1\nNA\nNo\nNA\nNA\n\n\n68856\n2011_12\nfemale\n14\n10-19\nNA\nOther\nOther\nNA\nNA\n75000-99999\n87500\n3.16\n9\nOwn\nNA\n70.8\nNA\nNA\n156.6\n28.90\nObese\n25.0_to_29.9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n32.48\n1.66\n4.19\n41\n0.297\n237\n3.950\nNo\nNA\nVgood\n0\n2\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0_to_1_hr\n1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n62053\n2009_10\nfemale\n27\n20-29\n334\nOther\nNA\nSome College\nNeverMarried\nNA\nNA\nNA\n8\nOwn\nWorking\n69.1\nNA\nNA\n173.4\n22.98\nNA\n18.5_to_24.9\n62\n100\n66\n102\n60\n100\n62\n100\n70\nNA\n1.55\n3.98\n193\n0.885\nNA\nNA\nNo\nNA\nGood\n0\n3\nNone\nNone\nNA\nNA\nNA\n5\nYes\nYes\n1\nNA\nNA\nNA\nNA\nYes\n3\n48\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n58185\n2009_10\nmale\n69\n60-69\n828\nOther\nNA\n9 - 11th Grade\nMarried\nmore 99999\n100000\n4.21\n6\nOwn\nNotWorking\n53.7\nNA\nNA\n156.7\n21.87\nNA\n18.5_to_24.9\n76\n130\n21\n128\n42\n128\n42\n132\n0\nNA\n0.98\n3.70\n109\n1.557\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n6\nNo\nYes\n7\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n70253\n2011_12\nmale\n0\n0-9\n8\nOther\nOther\nNA\nNA\n0-4999\n2500\n0.03\n4\nOther\nNA\n9.3\n70.3\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n65718\n2011_12\nfemale\n11\n10-19\nNA\nOther\nOther\nNA\nNA\nmore 99999\n100000\n4.83\n6\nOwn\nNA\n35.2\nNA\nNA\n143.8\n17.00\nNormWeight\n12.0_18.5\n72\n94\n65\n94\n60\n94\n64\n94\n66\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7\n2_hr\n1_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nSystematic sampling is where you randomly choose a starting place then select every \\(k^{th}\\) observation to measure.\nFor example:\n\nYou select every \\(5^{th}\\) item on an assembly line\nYou select every \\(10^{th}\\) name on the list\n\nYou select every \\(3^{rd}\\) customer that comes into the store.\nMake sure you randomly select the starting point. Also, if you want a sample with 100 units of observations, and you have a population that has 10,000 units of observation, then you would want to select every 10,000/100=100 units of observations.\nCluster sampling is where you break the population into groups called clusters. Randomly pick some clusters then poll all observations in those clusters.\nFor example:\n\nA large city wants to poll all businesses in the city. They divide the city into sections (clusters), maybe a square block for each section, and use a random number generator to pick some of the clusters. Then they poll all businesses in each chosen cluster.\nYou want to measure whether a tree in the forest is infected with bark beetles. Instead of having to walk all over the forest, you divide the forest up into sectors (clusters), and then randomly pick the sectors (clusters) that you will travel to. Then record whether a tree is infected or not for every tree in that sector (cluster).\n\nMany people confuse stratified sampling and cluster sampling. In stratified sampling you use all the groups and some of the members in each group. Cluster sampling is the other way around. It uses some of the groups and all the members in each group.\nThe four sampling techniques that were presented all have advantages and disadvantages. There is another sampling technique that is sometimes utilized because either the researcher doesn’t know better, or it is easier to do. This sampling technique is known as a convenience sample. This sample will not result in a representative sample, and should be avoided.\nConvenience sample is one where the researcher picks observations to be included that are easy for the researcher to collect.\n\nAn example of a convenience sample is if you want to know the opinion of people about the criminal justice system, and you stand on a street corner near the county court house, and questioning the first 10 people who walk by. The people who walk by the county court house are most likely involved in some fashion with the criminal justice system, and their opinion would not represent the opinions of all observations.\n\nOn a rare occasion, you do want to collect the entire population. In which case you conduct a census.\nA census is when every observation is measured.\n\n\n1.2.5 Example: Sampling type\n\nBanner Health is a several state nonprofit chain of hospitals. Management wants to assess the incident of complications after surgery. They wish to use a sample of surgery patients. Several sampling techniques are described below. Categorize each technique as simple random sample, stratified sample, systematic sample, cluster sample, or convenience sampling.\n\n\n\nObtain a list of patients who had surgery at all Banner Health facilities. Divide the patients according to type of surgery. Draw simple random samples from each group.\nObtain a list of patients who had surgery at all Banner Health facilities. Number these patients, and then use a random number table to obtain the sample.\nRandomly select some Banner Health facilities from each of the seven states, and then include all the patients on the surgery lists of the states.\nAt the beginning of the year, instruct each Banner Health facility to record any complications from every 100^th^ surgery.\nInstruct each Banner Health facilities to record any complications from 20 surgeries this week and send in the results.\n1.2.5.1 Solution\n\n\n\nObtain a list of patients who had surgery at all Banner Health facilities. Divide the patients according to type of surgery. Draw simple random samples from each group.\nThis is a stratified sample since the patients where separated into different stratum and then random samples were taken from each strata. The problem with this is that some types of surgeries may have more chances for complications than others. Of course, the stratified sample would show you this.\nObtain a list of patients who had surgery at all Banner Health facilities. Number these patients, and then use a random number table to obtain the sample.\nThis is a random sample since each patient has the same chance of being chosen. The problem with this one is that it will take a while to collect the data.\nRandomly select some Banner Health facilities from each of the seven states, and then include all the patients on the surgery lists of the states.\nThis is a cluster sample since all patients are questioned in each of the selected hospitals. The problem with this is that you could have by chance selected hospitals that have no complications.\nAt the beginning of the year, instruct each Banner Health facility to record any complications from every 100^th^ surgery.\nThis is a systematic sample since they selected every \\(100^{th}\\) surgery. The problem with this is that if every \\(90^{th}\\) surgery has complications, you wouldn’t see this come up in the data.\nInstruct each Banner Health facilities to record any complications from 20 surgeries this week and send in the results.\nThis is a convenience sample since they left it up to the facility how to do it. The problem with convenience samples is that the person collecting the data will probably collect data from surgeries that had no complications.\n\n\n\n1.2.6 Homework for Sampling Methods Section\n\nResearchers want to collect cholesterol levels of U.S. patients who had a heart attack two days prior. The following are different sampling techniques that the researcher could use. Classify each as simple random sample, stratified sample, systematic sample, cluster sample, or convenience sample.\n\nThe researchers randomly select 5 hospitals in the U.S. then measure the cholesterol levels of all the heart attack patients in each of those hospitals.\nThe researchers list all of the heart attack patients and measure the cholesterol level of every \\(25^{th}\\) person on the list.\nThe researchers go to one hospital on a given day and measure the cholesterol level of the heart attack patients at that time.\nThe researchers list all of the heart attack patients. They then measure the cholesterol levels of randomly selected patients.\nThe researchers divide the heart attack patients based on race, and then measure the cholesterol levels of randomly selected patients in each race grouping.\n\nThe quality control officer at a manufacturing plant needs to determine what percentage of items in a batch are defective. The following are different sampling techniques that could be used by the officer. Classify each as simple random sample, stratified sample, systematic sample, cluster sample, or convenience sample.\n\n\n\nThe officer lists all of the batches in a given month. The number of defective items is counted in randomly selected batches.\nThe officer takes the first 10 batches and counts the number of defective items.\nThe officer groups the batches made in a month into which shift they are made. The number of defective items is counted in randomly selected batches in each shift.\nThe officer chooses every \\(15^{th}\\) batch off the line and counts the number of defective items in each chosen batch.\n\nThe officer divides the batches made in a month into which day they were made. Then certain days are picked and every batch made that day is counted to determine the number of defective items.\n\nYou wish to determine the GPA of students at your school. Describe what process you would go through to collect a sample if you use a simple random sample.\nYou wish to determine the GPA of students at your school. Describe what process you would go through to collect a sample if you use a stratified sample.\nYou wish to determine the GPA of students at your school. Describe what process you would go through to collect a sample if you use a systematic sample.\nYou wish to determine the GPA of students at your school. Describe what process you would go through to collect a sample if you use a cluster sample.\nYou wish to determine the GPA of students at your school. Describe what process you would go through to collect a sample if you use a convenience sample.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Basics</span>"
    ]
  },
  {
    "objectID": "Statistical Basics.html#experimental-design",
    "href": "Statistical Basics.html#experimental-design",
    "title": "1  Statistical Basics",
    "section": "1.3 Experimental Design",
    "text": "1.3 Experimental Design\nThe section is an introduction to experimental design. This is how to actually design an experiment or a survey so that they are statistical sound. Experimental design is a very involved process, so this is just a small introduction.\n\n1.3.1 Guidelines for planning a statistical study\n\nIdentify the observations that you are interested in. Realize that you can only make conclusions for these observations. As an example, if you use a fertilizer on a certain genus of plant, you can’t say how the fertilizer will work on any other types of plants. However, if you diversify too much, then you may not be able to tell if there really is an improvement since you have too many factors to consider.\nSpecify the variable. You want to make sure this is something that you can measure, and make sure that you control for all other factors too. As an example, if you are trying to determine if a fertilizer works by measuring the height of the plants on a particular day, you need to make sure you can control how much fertilizer you put on the plants (which would be your treatment), and make sure that all the plants receive the same amount of sunlight, water, and temperature.\nSpecify the population. This is important in order for you know what conclusions you can make and what observations you are making the conclusions about.\nSpecify the method for taking measurements or making observations.\nDetermine if you are taking a census or sample. If taking a sample, decide on the sampling method.\nCollect the data.\nUse appropriate descriptive statistics methods and make decisions using appropriate inferential statistics methods.\nNote any concerns you might have about your data collection methods and list any recommendations for future.\n\nThere are two types of studies:\nAn observational study is when the investigator collects data merely by watching or asking questions. Nothing is change or controlled\nAn experiment is when the investigator changes a variable or imposes a treatment to determine its effect.\n\n\n1.3.2 Example: Observational Study or Experiment\nState if the following is an observational study or an experiment.\n\nPoll students to see if they favor increasing tuition.\nGive some students a tutor to see if grades improve.\n\n\n1.3.2.1 Solution\n\nPoll students to see if they favor increasing tuition.\nThis is an observational study. You are only asking a question.\nGive some students a tutor to see if grades improve.\nThis is an experiment. The tutor is the treatment.\n\n\n\n\n1.3.3 Survey\nMany observational studies involve surveys. A survey uses questions to collect the data and needs to be written so that there is no bias.\n\n\n1.3.4 Experiment Options\nIn an experiment, there are different options.\nRandomized two-treatment experiment: in this experiment, there are two treatments, and observations are randomly placed into the two groups. Either both groups get a treatment, or one group gets a treatment and the other gets either nothing or a placebo. The group getting either an old treatment, no treatment or a placebo is called the control group. The group getting the treatment is called the treatment group. The idea of the placebo is that a person thinks they are receiving a treatment, but in reality they are receiving a sugar pill or fake treatment. Doing this helps to account for the placebo effect, which is where a person’s mind makes their body respond to a treatment because they think they are taking the treatment when they are not really taking the treatment. Note, not every experiment needs a placebo, such when using animals or plants. Also, you can’t always use a placebo or no treatment. As an example, if you are testing a new blood pressure medication you can’t give a person with high blood pressure a placebo or no treatment because of moral reasons.\nRandomized Block Design: a block is a group of subjects that are similar, but the blocks differ from each other. Then randomly assign treatments to subjects inside each block. An example would be separating students into full-time versus part-time, and then randomly picking a certain number full-time students to get the treatment and a certain number part-time students to get the treatment. This way some of each type of student gets the treatment and some do not.\nRigorously Controlled Design: carefully assign subjects to different treatment groups, so that those given each treatment are similar in ways that are important to the experiment. An example would be if you want to have a full-time student who is male, takes only night classes, has a full-time job, and has children in one treatment group, then you need to have the same type of student getting the other treatment. This type of design is hard to implement since you don’t know how many differentiation you would use, and should be avoided.\nMatched Pairs Design: the treatments are given to two groups that can be matched up with each other in some ways. One example would be to measure the effectiveness of a muscle relaxer cream on the right arm and the left arm of observations, and then for each observation you can match up their right arm measurement with their left arm. Another example of this would be before and after experiments, such as weight before and weight after a diet.\nNo matter which experiment type you conduct, you should also consider the following:\nReplication: repetition of an experiment on more than one observation so you can make sure that the sample is large enough to distinguish true effects from random effects. It is also the ability for someone else to duplicate the results of the experiment.\nBlind study is where the subject used in the study does not know which treatment they are getting or if they are getting the treatment or a placebo.\nDouble-blind study is where neither the subject used in the study nor the researcher knows who is getting which treatment or who is getting the treatment and who is getting the placebo. This is important so that there can be no bias created by either the subject or the researcher.\nOne last consideration is the time period that you are collecting the data over. There are three types of time periods that you can consider.\nCross-sectional study: data observed, measured, or collected at one point in time.\nRetrospective (or case-control) study: data collected from the past using records, interviews, and other similar artifacts.\nProspective (or longitudinal or cohort) study: data collected in the future from groups sharing common factors.\n\n\n1.3.5 Homework for Experimental Design Section\n\nYou want to determine if cinnamon reduces a person’s insulin sensitivity. You give patients who are insulin sensitive a certain amount of cinnamon and then measure their glucose levels. Is this an observation or an experiment? Why?\nYou want to determine if eating more fruits reduces a person’s chance of developing cancer. You watch people over the years and ask them to tell you how many servings of fruit they eat each day. You then record who develops cancer. Is this an observation or an experiment? Why?\nA researcher wants to evaluate whether countries with lower fertility rates have a higher life expectancy. They collect the fertility rates and the life expectancies of countries around the world. Is this an observation or an experiment? Why?\nTo evaluate whether a new fertilizer improves plant growth more than the old fertilizer, the fertilizer developer gives some plants the new fertilizer and others the old fertilizer. Is this an observation or an experiment? Why?\nA researcher designs an experiment to determine if a new drug lowers the blood pressure of patients with high blood pressure. The patients are randomly selected to be in the study and they randomly pick which group to be in. Is this a randomized experiment? Why or why not?\nDoctors trying to see if a new stent works longer for kidney patients, asks patients if they are willing to have one of two different stents put in. During the procedure the doctor decides which stent to put in based on which one is on hand at the time. Is this a randomized experiment? Why or why not?\nA researcher wants to determine if diet and exercise together helps people lose weight over just exercising. The researcher solicits volunteers to be part of the study, randomly picks which volunteers are in the study, and then lets each volunteer decide if they want to be in the diet and exercise group or the exercise only group. Is this a randomized experiment? Why or why not?\nTo determine if lack of exercise reduces flexibility in the knee joint, physical therapists ask for volunteers to join their trials. They then randomly select the volunteers to be in the group that exercises and to be in the group that doesn’t exercise. Is this a randomized experiment? Why or why not?\nYou collect the weights of tagged fish in a tank. You then put an extra protein fish food in water for the fish and then measure their weight a month later. Are the two samples matched pairs or not? Why or why not?\nA mathematics instructor wants to see if a computer homework system improves the scores of the students in the class. The instructor teaches two different sections of the same course. One section utilizes the computer homework system and the other section completes homework with paper and pencil. Are the two samples matched pairs or not? Why or why not?\nA business manager wants to see if a new procedure improves the processing time for a task. The manager measures the processing time of the employees then trains the employees using the new procedure. Then each employee performs the task again and the processing time is measured again. Are the two samples matched pairs or not? Why or why not?\nThe prices of generic items are compared to the prices of the equivalent named brand items. Are the two samples matched pairs or not? Why or why not?\nA doctor gives some of the patients a new drug for treating acne and the rest of the patients receive the old drug. Neither the patient nor the doctor knows who is getting which drug. Is this a blind experiment, double blind experiment, or neither? Why?\nOne group is told to exercise and one group is told to not exercise. Is this a blind experiment, double blind experiment, or neither? Why?\nThe researchers at a hospital want to see if a new surgery procedure has a better recovery time than the old procedure. The patients are not told which procedure that was used on them, but the surgeons obviously did know. Is this a blind experiment, double blind experiment, or neither? Why?\nTo determine if a new medication reduces headache pain, some patients are given the new medication and others are given a placebo. Neither the researchers nor the patients know who is taking the real medication and who is taking the placebo. Is this a blind experiment, double blind experiment, or neither? Why?\nA new study is underway to track the eating and exercise patterns of people at different time periods in the future, and see who is afflicted with cancer later in life. Is this a cross-sectional study, a retrospective study, or a prospective study? Why?\nTo determine if a new medication reduces headache pain, some patients are given the new medication and others are given a placebo. The pain levels of a patient are then recorded. Is this a cross-sectional study, a retrospective study, or a prospective study? Why?\nTo see if there is a link between smoking and bladder cancer, patients with bladder cancer are asked if they currently smoke or if they smoked in the past. Is this a cross-sectional study, a retrospective study, or a prospective study? Why?\nThe Nurses Health Survey was a survey where nurses were asked to record their eating habits over a period of time, and their general health was recorded. Is this a cross-sectional study, a retrospective study, or a prospective study? Why?\nConsider a question that you would like to answer. Describe how you would design your own experiment. Make sure you state the question you would like to answer, then determine if an experiment or an observation is to be done, decide if the question needs one or two samples, if two samples are the samples matched, if this is a randomized experiment, if there is any blinding, and if this is a cross-sectional, retrospective, or prospective study.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Basics</span>"
    ]
  },
  {
    "objectID": "Statistical Basics.html#how-not-to-do-statistics",
    "href": "Statistical Basics.html#how-not-to-do-statistics",
    "title": "1  Statistical Basics",
    "section": "1.4 How Not to Do Statistics",
    "text": "1.4 How Not to Do Statistics\nMany studies are conducted and conclusions are made. However, there are occasions where the study is not conducted in the correct manner or the conclusion is not correctly made based on the data. There are many things that you should question when you read a study. There are many reasons for the study to have bias in it. Bias is where a study may have a certain slant or preference for a certain result. The following are a list of some of the questions or issues you should consider to help decide if there is bias in a study.\nOne of the first issues you should ask is who funded the study. If the entity that sponsored the study stands to gain either profits or notoriety from the results, then you should question the results. It doesn’t mean that the results are wrong, but you should scrutinize them on your own to make sure they are sound. As an example if a study says that genetically modified foods are safe, and the study was funded by a company that sells genetically modified food, then one may question the validity of the study. Since the company funds the study and their profits rely on people buying their food, there may be bias.\nAn experiment could have lurking or confounding variables when you cannot rule out the possibility that the observed effect is due to some other variable rather than the factor being studied. An example of this is when you give fertilizer to some plants and no fertilizer to others, but the no fertilizer plants also are placed in a location that doesn’t receive direct sunlight. You won’t know if the plants that received the fertilizer grew taller because of the fertilizer or the sunlight. Make sure you design experiments to eliminate the effects of confounding variables by controlling all the factors that you can.\nOver generalization is where you do a study on one group and then try to say that it will happen on all groups. An example is doing cancer treatments on rats. Just because the treatment works on rats does not mean it will work on humans. Another example is that until recently most FDA medication testing had been done on white males of a particular age. There is no way to know how the medication affects other genders, ethnic groups, age groups, and races. The new FDA guidelines stresses using subjects from different groups.\nCause and effect is where people decide that one variable causes the other just because the variables are related. Unless the study was done as an experiment where a variable was controlled, you cannot say that one variable caused the other. There is the possibility that another variable caused both to change. As an example, there is a relationship between number of drownings at the beach and ice cream sales. This does not mean that ice cream sales increasing causes people to drown. Most likely the cause for both increasing is the heat.\nSampling error: This is the difference between the sample results and the true population results. This is unavoidable, and results in the fact that samples are different from each other. As an example, if you take a sample of 5 people’s height in your class, you will get 5 numbers. If you take another sample of 5 people’s heights in your class, you will likely get 5 different numbers.\nNon-sampling error: This is where the sample is collected poorly either through a biased sample or through error in measurements. Care should be taken to avoid this error.\nLastly, there should be care taken in considering the difference between statistical significance versus practical significance. This is a major issue in statistics. Something could be statistically significance, which means that a statistical test shows there is evidence to show what you are trying to prove. However, in practice it doesn’t mean much or there are other issues to consider. As an example, suppose you find that a new drug for high blood pressure does reduce the blood pressure of patients. When you look at the improvement it actually doesn’t amount to a large difference. Even though statistically there is a change, it may not be worth marketing the product because it really isn’t that big of a change. Another consideration is that you find the blood pressure medication does improve a person’s blood pressure, but it has serious side effects or it costs a great deal for a prescription. In this case, it wouldn’t be practical to use it. In both cases, the study is shown to be statistically significant, but practically you don’t want to use the medication. The main thing to remember in a statistical study is that the statistics is only part of the process. You also want to make sure that there is practical significance. One more comment on statistical significance, the American Statistical Association (ASA) recently came out with a statement, “Based on our review of the articles in this special issue and the broader literature, we conclude that it is time to stop using the term ‘statistically significant’ entirely.” (Advanced Solutions International, Inc, 2019) Though the ASA suggests not using this term anymore, there are many studies that have been done in the past that uses this term, so it is presented here. However, it is not a term that should be use and will be down played in the rest of this book.\nSurveys have their own areas of bias that can occur. A few of the issues with surveys are in the wording of the questions, the ordering of the questions, the manner the survey is conducted, and the response rate of the survey.\nThe wording of the questions can cause hidden bias, which is where the questions are asked in a way that makes a person respond a certain way. An example is that a poll was done where people were asked if they believe that there should be an amendment to the constitution protecting a woman’s right to choose. About 60% of all people questioned said yes. Another poll was done where people were asked if they believe that there should be an amendment to the constitution protecting the life of an unborn child. About 60% of all people questioned said yes. These two questions deal with the same issue, though giving different results, but how the question was asked affected the outcome.\nThe ordering of the question can also cause hidden bias. An example of this is if you were asked if there should be a fine for texting while driving, but proceeding that question is the question asking if you text while drive. By asking a person if they actually partake in the activity, that person now personalizes the question and that might affect how they answer the next question of creating the fine.\nNon-response is where you send out a survey but not everyone returns the survey. You can calculate the response rate by dividing the number of returns by the number of surveys sent. Most response rates are around 30-50%. A response rate less than 30% is very poor and the results of the survey are not valid. To reduce non-response, it is better to conduct the surveys in person, though these are very expensive. Phones are the next best way to conduct surveys, emails can be effective, and physical mailings are the least desirable way to conduct surveys.\nVoluntary response is where people are asked to respond via phone, email or online. The problem with these is that only people who really care about the topic are likely to call or email. These surveys are not scientific and the results from these surveys are not valid. Note: all studies involve volunteers. The difference between a voluntary response survey and a scientific study is that in a scientific study the researchers ask the subjects to be involved, while in a voluntary response survey the subjects become involved on their own choosing.\n\n1.4.1 Example: Bias in a Study\nSuppose a mathematics department at a community college would like to assess whether computer-based homework improves students’ test scores. They use computer-based homework in one classroom with one teacher and use traditional paper and pencil homework in a different classroom with a different teacher. The students using the computer-based homework had higher test scores. What is wrong with this experiment?\n\n1.4.1.1 Solution\nSince there were different teachers, you do not know if the better test scores are because of the teacher or the computer-based homework. A better design would be have the same teacher teach both classes. The control group would utilize traditional paper and pencil homework and the treatment group would utilize the computer-based homework. Both classes would have the same teacher, and the students would be split between the two classes randomly. The only difference between the two groups should be the homework method. Of course, there is still variability between the students, but utilizing the same teacher will reduce any other confounding variables.\n\n\n\n1.4.2 Example: Cause and Effect\nDetermine if the one variable did cause the change in the other variable.\n\nCinnamon was giving to a group of people who have diabetes, and then their blood glucose levels were measured a time period later. All other factors for each person were kept the same. Their glucose levels went down. Did the cinnamon cause the reduction?\nThere is a link between spray on tanning products and lung cancer. Does that mean that spray on tanning products cause lung cancer?\n\n\n1.4.2.1 Solution\n\nCinnamon was giving to a group of people who have diabetes, and then their blood glucose levels were measured a time period later. All other factors for each person were kept the same. Their glucose levels went down. Did the cinnamon cause the reduction?\nSince this was a study where the use of cinnamon was controlled, and all other factors were kept constant from person to person, then any changes in glucose levels can be attributed to the use of cinnamon.\nThere is a link between spray on tanning products and lung cancer. Does that mean that spray on tanning products cause lung cancer?\nSince there is only a link, and not a study controlling the use of the tanning spray, then you cannot say that increased use causes lung cancer. You can say that there is a link, and that there could be a cause, but you cannot say for sure that the spray causes the cancer.\n\n\n\n\n1.4.3 Example: Generalizations\n\nA researcher conducts a study on the use of ibuprofen on humans and finds that it is safe. Does that mean that all species can use ibuprofen?\nAspirin has been used for years to bring down fevers in humans. Originally it was tested on white males between the ages of 25 and 40 and found to be safe. Is it safe to give to everyone?\n\n\n1.4.3.1 Solution\n\nA researcher conducts a study on the use of ibuprofen on humans and finds that it is safe. Does that mean that all species can use ibuprofen?\nNo. Just because a drug is safe to use on one species doesn’t mean it is safe to use for all species. In fact, ibuprofen is toxic to cats.\nAspirin has been used for years to bring down fevers in humans. Originally it was tested on white males between the ages of 25 and 40 and found to be safe. Is it safe to give to everyone?\nNo. Just because one age group can use it doesn’t mean it is safe to use for all age groups. In fact, there has been a link between giving a child under the age of 19 aspirin when they have a fever and Reye’s syndrome.\n\n\n\n\n1.4.4 Homework for How Not to Do Statistics Section\n\nSuppose there is a study where a researcher conducts an experiment to show that deep breathing exercises helps to lower blood pressure. The researcher takes two groups of people and has one group to perform deep breathing exercises and a series of aerobic exercises every day and the other group was asked to refrain from any exercises. The researcher found that the group performing the deep breathing exercises and the aerobic exercises had lower blood pressure. Discuss any issue with this study.\nSuppose a car dealership offers a low interest rate and a longer payoff period to customers or a high interest rate and a shorter payoff period to customers, and most customers choose the low interest rate and longer payoff period, does that mean that most customers want a lower interest rate? Explain.\nOver the years it has been said that coffee is bad for you. When looking at the studies that have shown that coffee is linked to poor health, you will see that people who tend to drink coffee don’t sleep much, tend to smoke, don’t eat healthy, and tend to not exercise. Can you say that the coffee is the reason for the poor health or is there a lurking variable that is the actual cause? Explain.\nWhen researchers were trying to figure out what caused polio, they saw a connection between ice cream sales and polio. As ice cream sales increased so did the incident of polio. Does that mean that eating ice cream causes polio? Explain your answer.\nThere is a positive correlation between having a discussion of gun control, which usually occur after a mass shooting, and the sale of guns. Does that mean that the discussion of gun control increases the likelihood that people will buy more guns? Explain.\nThere is a study that shows that people who are obese have a vitamin D deficiency. Does that mean that obesity causes a deficiency in vitamin D? Explain.\nA study was conducted that shows that polytetrafluoroethylene (PFOA) (Teflon is made from this chemical) has an increase risk of tumors in lab mice. Does that mean that PFOA’s have an increased risk of tumors in humans? Explain.\nSuppose a telephone poll is conducted by contacting U.S. citizens via landlines about their view of gay marriage. Suppose over 50% of those called do not support gay marriage. Does that mean that you can say over 50% of all people in the U.S. do not support gay marriage? Explain.\nSuppose that it can be shown to be statistically significant that a smaller percentage of the people are satisfied with your business. The percentage before was 87% and is now 85%. Do you change how you conduct business? Explain?\nYou are testing a new drug for weight loss. You find that the drug does in fact statistically show a weight loss. Do you market the new drug? Why or why not?\nThere was an online poll conducted about whether the mayor of Auckland, New Zealand, should resign due to an affair. The majority of people participating said he should. Should the mayor resign due to the results of this poll? Explain.\nAn online poll showed that the majority of Americans believe that the government covered up events of 9/11. Does that really mean that most Americans believe this? Explain.\nA survey was conducted at a college asking all employees if they were satisfied with the level of security provided by the security department. Discuss how the results of this question could be biased.\nAn employee survey says, “Employees at this institution are very satisfied with working here. Please rate your satisfaction with the institution.” Discuss how this question could create bias.\nA survey has a question that says, “Most people are afraid that they will lose their house due to economic collapse. Choose what you think is the biggest issue facing the nation today. a) Economic collapse, b) Foreign policy issues, c) Environmental concerns.” Discuss how this question could create bias.\nA survey says, “Please rate the career of Roberto Clemente, one of the best right field baseball players in the world.” Discuss how this question could create bias.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Statistical Basics</span>"
    ]
  },
  {
    "objectID": "Graphical Description of Data.html",
    "href": "Graphical Description of Data.html",
    "title": "2  Graphical Description of Data",
    "section": "",
    "text": "2.1 Qualitative Data\nRemember, qualitative data are words describing a characteristic of the individual. There are several different graphs that are used for qualitative data. These graphs include bar graphs, Pareto charts, and pie charts. Bar graphs can be created using a statistical program like RStudio.\nBar graphs or charts consist of the frequencies on one axis and the categories on the other axis. Drawing the bar graph using r is performed using the following command.\ngf_bar(~explanatory variable, data=Dataframe)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Description of Data</span>"
    ]
  },
  {
    "objectID": "Graphical Description of Data.html#qualitative-data",
    "href": "Graphical Description of Data.html#qualitative-data",
    "title": "2  Graphical Description of Data",
    "section": "",
    "text": "2.1.1 Example: Drawing a Bar Chart\nData was collected for two semesters in a statistics class. The data frame is in Table 2.1. The command\nhead(data frame)\nshows the variables and the first few lines of the data set. The data sets are usually larger than what is shown. The head command allows one to see the structure of the data frame.\n\nClass&lt;-read.csv( \"https://krkozak.github.io/MAT160/class_survey.csv\") \nknitr::kable(head(Class))\n\n\n\nTable 2.1: Head of Statistics Class Survey\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nvehicle\ngender\ndistance_campus\nice_cream\nrent\nmajor\nheight\nwinter\n\n\n\n\nNone\nFemale\n1.5\nCookie Dough\n724\nEnvironmental and Sustainability Studies\n61\nLiked it\n\n\nMercury\nFemale\n14.7\nSherbet\n200\nAdministrative Justice\n60\nDon’t like it\n\n\nFord\nFemale\n2.4\nChocolate Brownie.\n600\nBio Chem\n68\nLiked it\n\n\nToyota\nFemale\n5.2\ncoffee\n0\n\n66\nLoved it\n\n\nJeep\nMale\n2.0\nCookie Dough\n600\nPre-health Careers\n71\nLoved it\n\n\nSubaru\nMale\n5.0\nnone\n500\nFinance\n72\nNo opinion\n\n\n\n\n\n\n\n\nEvery data frame has a code book that describes the data set, the source of the data set, and a listing and description of the variables in the data frame.\nCode book for data frame class\nDescription Survey results from two semesters of statistics classes at Coconino Community College in the years 2018-2019.\nFormat\nThis data frame contains the following columns:\nvehicle: Type of car a student drives\ngender: Self declared gender of a student\ndistance_campus: how far a student lives from the Lone Tree Campus of Coconino Community College (miles)\nice_cream: favorite ice cream flavor\nrent: How much a student pays in rent\nmajor: Students declared major\nheight: height of the student (inches)\nwinter: Student’s opinion of winter (Love it, Like it, Don’t like, No opinion)\nSource\nKozak K (2019). Survey results form surveys collected in statistics class at Coconino Community College.\nReferences\nKozak, 2019\nCreate a bar graph of vehicle type. To do this in RStudio, use the command\ngf_bar(~variable, data=Data_Frame, …)\nwhere gf_bar is the goal, vehicle is the name of the response variable (there is no explanatory variable), the data frame is Class, and a title was added to the graph.\n\n2.1.1.1 Solution\n\ngf_bar(~vehicle, data=Class, title=\"Bar Chart of Cars driven by students in statistics class\", xlab=\"Vehicle\") \n\n\n\n\n\n\n\nFigure 2.1: Cars driven by students in statistics class\n\n\n\n\n\nDescription of Figure 2.1 is a Bar graph with bars for Audi, Buick, Honda, Hyundai, Mercury, Nissan with height of 1, Dodge and None with height of 2, Jeep, Subaru, Toyota with heights of 3, and Chevrolet and Ford at height of 4.\nNotice from Figure 2.1, you can see that Chevrolet and Ford are the more popular car, with Jeep, Subaru, and Toyota not far behind. Many types seems to be the lesser used, and tied for last place. However, more data would help to figure this out.\nAll graphs should have labels on each axis and a title for the graph.\nThe beauty of data frames with multiple variables is that you can answer many questions from the data. Suppose you want to see if gender makes a difference for the type of car a person drives. If you are a car manufacturer, if you knew that certain genders like certain cars, then you would advertise to the different genders. To create a bar graph that separates based on gender, perform the following command in RStudio.\n\ngf_bar(~vehicle, fill=~gender, data=Class, title=\"Cars driving by students in statistics class\",xlab=\"Vehicle\", position=position_dodge())\n\n\n\n\n\n\n\nFigure 2.2: Bar graph of Cars driven by students in statistics class\n\n\n\n\n\nDescription of Figure 2.2 is a bar graph of number of vehicles separated by female and male. Audi and male has height of 1, Buick and female has a height of 1, Chevrolet and male and Chevrolet and female have heights of 2, Dodge and male and Dodge and female has heights of 1, Ford and female has a height of 4, Honda and female has a height of 1, Hyundai and male has a height of 1, Jeep and male has a height of 2 while Jeep and female has a height of 1, Mercury and female has a height of 1, Nissan and female has a height of 1, no car and female has a height of 2, Subaru and female has a height of 1, Subaru and male has a height of 2, Toyota and female has a height of 1, and Toyota and male has a height of 2.\nNotice a Ford is driven by females more than any other car, while Chevrolet, Mercury, and Subaru cars are equally driven by males. Obviously a larger sample would be needed to make any conclusions from this data.\nThere are other types of graphs that can be created for quantitative variables. Another type is known as a dot plot. The command for this graph is as follows.\n\ngf_dotplot(~vehicle, data=Class, title=\"Cars driven by students in statistics class\", xlab=\"Vehicle\")\n\n\n\n\n\n\n\nFigure 2.3: Cars driven by students in statistics class\n\n\n\n\n\nDescription of Figure 2.8 is a dot plot of number of vehicles with Audi, Buick, Honda, Hyundai, Mercury, Nissan with height of 1, Dodge and None with height of 2, Jeep, Subaru, Toyota with heights of 3, and Chevrolet and Ford at height of 4. Very similar to bar graph.\nNotice a dot plot is like a bar chart. Both give you the same information. You can also divide a dot plot by gender.\nAnother type of graph that is also useful and similar to the dot plot is a point plot (scatter plot). In this plot you can graph the explanatory variable versus the response variable. The command for this in rStudio is as follows.\n\ngf_point(vehicle~gender, data=Class, title=\"Cars driving by students in statistics class\", xlab=\"Gender\", ylab=\"Vehicle\") \n\n\n\n\n\n\n\nFigure 2.4: Cars driven by students in statistics class\n\n\n\n\n\nDescription of Figure 2.4 is a scatter plot of type of vehicles separated by female and male with females owning Toyota, Subaru, none, Nissan, Mercury, Jeep, Honda, Ford, Dodge, Chevrolet, and Buick, while males own Toyota, Subaru, Jeep, Hyundai, Dodge, Chevrolet, and Audi.\nThe problem with Figure 2.4 is that if there are multiple females who drive a Ford, only one dot is shown. So it is best to spread the dots out using a plot known as a jitter plot. In a jitter plot the dots are randomly moved off the center line. The command for a jitter plot is as follows:\n\ngf_jitter(vehicle~gender, data=Class, title=\"Cars driving by students in statistics class\", xlab=\"Gender\", ylab=\"Vehicle\")\n\n\n\n\n\n\n\nFigure 2.5: Cars driven by students in statistics class\n\n\n\n\n\nDescription of Figure 2.5 is a jitter plot of number of vehicles separated by female and male with females owning 1 Toyota, 1 Subaru, 2 with none, 1 Nissan, 1 Mercury, 1 Jeep, 1 Honda, 4 Fords, 1 Dodge, 2 Chevrolets, and 1 Buick, while males own 2 Toyotas, 2 Subarus, 2 Jeeps, 1 Hyundai, 1 Dodge, 1 Chevrolets, and 1 Audi.\nNow you can observe that there are 4 females who drive a Ford. There is one female who drives a Honda. Other information about other cars and genders can be seen better than in the point plot and the bar graph. Jitter plots are useful to see how many data values are for each qualitative data values.\nThere are many other types of graphs that can be used on qualitative data. There are spreadsheet software packages that will create most of them, and it is better to look at them to see how to create then. It depends on your data as to which may be useful, but the bar, dot, and jitter plots are really the most useful.\n\n\n\n2.1.2 Homework for Qualitative Data Section\n\nEyeglassomatic manufactures eyeglasses for different retailers. The number of lenses for different activities is in Table 2.2.\n\n\nEyeglasses&lt;-read.csv( \"https://krkozak.github.io/MAT160/eyglasses.csv\") \nknitr::kable(head(Eyeglasses))\n\n\n\nTable 2.2: Head of Eyeglasses Data frame\n\n\n\n\n\n\nactivity\n\n\n\n\nGrind\n\n\nGrind\n\n\nGrind\n\n\nGrind\n\n\nGrind\n\n\nGrind\n\n\n\n\n\n\n\n\nCode book for Data Frame Eyeglasses\nDescription Activities that an Eyeglass company performs when making eyeglasses, Grind means ground the lenses and put them in frames, multicoat means put tinting or coatings on lenses and then put them in frames, assemble means received frames and lenses from other sources and put them together, make frames means made the frames and put lenses in from other sources, receive finished means received glasses from other source unknown means do not know where the lenses came from.\nFormat\nThis data frame contains the following columns:\nactivity: The activity that is completed to make the eyeglasses by Eyeglassomatic\nSource John Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nReferences John Matic (2013)\nMake a bar chart of this data. State any findings you can see from the graph.\n\nData was collected for two semesters in a statistics class drive. The data frame is in Table 2.1.\n\nCode book for the Data Frame Class is found below Table 2.1.\nCreate a bar graph of the variable ice cream. State any findings you can see from the graphs.\n\nThe number of deaths in the US due to carbon monoxide (CO) poisoning from generators from the years 1999 to 2011 are in Table 2.3 (Hinatov, 2012). Create a bar chart of this data. State any findings you see from the graph.\n\n\nArea&lt;-read.csv( \"https://krkozak.github.io/MAT160/area.csv\") \nknitr::kable(head(Area))\n\n\n\nTable 2.3: Head of Area Data frame\n\n\n\n\n\n\ndeaths\n\n\n\n\nUrban\n\n\nUrban\n\n\nUrban\n\n\nUrban\n\n\nUrban\n\n\nUrban\n\n\n\n\n\n\n\n\n\nData was collected for two semesters in a statistics class drive. The data frame is in Table 2.1. Create a bar graph and dot plot of the variable major. Create a jitter plot of major and gender. State any findings you can see from the graphs.\nCode book for the Data Frame Class is found below Table 2.1.\nEyeglassomatic manufactures eyeglasses for different retailers. They test to see how many defective lenses they made during the time period of January 1 to March 31. The table Table 2.4 gives the defect and the number of defects. Create a bar chart of the data and then describe what this tells you about what causes the most defects.\n\n\nDefects&lt;- read.csv( \"https://krkozak.github.io/MAT160/defects.csv\") \nknitr::kable(head(Defects))\n\n\n\nTable 2.4: Head of Defects Data frame\n\n\n\n\n\n\ntype\n\n\n\n\nsmall\n\n\nsmall\n\n\npd\n\n\nflaked\n\n\nscratch\n\n\nspot\n\n\n\n\n\n\n\n\nCode book for Data Frame Defects\nDescription Types of defects that an Eyeglass company sees in the lenses they make into eyeglasses.\nFormat\nThis data frame contains the following columns:\ntype: The type of defect that is Seen when making eyeglasses by Eyeglassomatic\nSource John Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nReferences John Matic (2013)\n\nAmerican National Health and Nutrition Examination (NHANES) surveys is collected every year by the US National Center for Health Statistics (NCHS). The data frame is in Table 2.5. Create a bar chart of MartialStatus. Create a jitter plot of MaritalStatus versus Education. Describe any findings from the graphs.\n\n\nknitr::kable(head(NHANES))\n\n\n\nTable 2.5: NHANES Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51624\n2009_10\nmale\n34\n30-39\n409\nWhite\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.36\n6\nOwn\nNotWorking\n87.4\nNA\nNA\n164.7\n32.22\nNA\n30.0_plus\n70\n113\n85\n114\n88\n114\n88\n112\n82\nNA\n1.29\n3.49\n352\nNA\nNA\nNA\nNo\nNA\nGood\n0\n15\nMost\nSeveral\nNA\nNA\nNA\n4\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n18\nYes\n17\nNo\nNA\nYes\nYes\n16\n8\n1\nNo\nHeterosexual\nNA\n\n\n51625\n2009_10\nmale\n4\n0-9\n49\nOther\nNA\nNA\nNA\n20000-24999\n22500\n1.07\n9\nOwn\nNA\n17.0\nNA\nNA\n105.4\n15.30\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51630\n2009_10\nfemale\n49\n40-49\n596\nWhite\nNA\nSome College\nLivePartner\n35000-44999\n40000\n1.91\n5\nRent\nNotWorking\n86.7\nNA\nNA\n168.4\n30.57\nNA\n30.0_plus\n86\n112\n75\n118\n82\n108\n74\n116\n76\nNA\n1.16\n6.70\n77\n0.094\nNA\nNA\nNo\nNA\nGood\n0\n10\nSeveral\nSeveral\n2\n2\n27\n8\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n20\nYes\nYes\nSmoker\n38\nYes\n18\nNo\nNA\nYes\nYes\n12\n10\n1\nYes\nHeterosexual\nNA\n\n\n51638\n2009_10\nmale\n9\n0-9\n115\nWhite\nNA\nNA\nNA\n75000-99999\n87500\n1.84\n6\nRent\nNA\n29.8\nNA\nNA\n133.1\n16.82\nNA\n12.0_18.5\n82\n86\n47\n84\n50\n84\n50\n88\n44\nNA\n1.34\n4.86\n123\n1.538\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n5\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nTo view the code book for NHANES, type help(“NHANES”) in rStudio after you load the NHANES packages using library(“NHANES”)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Description of Data</span>"
    ]
  },
  {
    "objectID": "Graphical Description of Data.html#quantitative-data",
    "href": "Graphical Description of Data.html#quantitative-data",
    "title": "2  Graphical Description of Data",
    "section": "2.2 Quantitative Data",
    "text": "2.2 Quantitative Data\nThere are several different graphs for quantitative data. With quantitative data, you can talk about how the data is distributed, called a distribution. The shape of the distribution can be described from the graphs.\nHistogram: a graph of frequencies (counts) on the vertical axis and classes on the horizontal axis. The height of the rectangles is the frequency and the width is the class width. The width depends on how many classes (bins) are in the histogram. The shape of a histogram is dependent on the number of bins. In RStudio the command to create a histogram is\ngf_histogram(~response variable, data=Data_Frame, title=“title of the graph”)\nThe last part of the command puts a title on the graph. You type in what ever you want for the title in the quotes.\nDensity Plot: Similar to a histogram, except smoothing is created to smooth out the graph. The shape is not dependent on the number of bins so the distribution is easier to determine from the density plot. In RStudio the command to create a density plot is\ngf_density(~response variable, data=Data_Frame, title=“title of the graph”, xlab=“Label”, ylab=“Label”)\nThe last part of the command puts a title on the graph and labels on the axes. You type in what every you want for the title and labels in the quotes.\nThe last part of the command puts a title on the graph and labels on the axes. You type in what every you want for the title and labels in the quotes.\n\n2.2.1 Example: Drawing a Histogram and Density plot\nData was collected for two semesters in a statistics class drive. The data frame is in Table 2.1 and the code book is below the data frame\nDraw a histogram, density plot, and a dot plot for the variable the distance a student lives from the Lone Tree Campus of Coconino Community College. Describe the story the graphs tell.\n\n2.2.1.1 Solution\n\ngf_histogram(~distance_campus, data=Class, title=\"Distance in miles from the Lone Tree Campus\", xlab=\"Distance (miles)\")\n\n\n\n\n\n\n\nFigure 2.6: Distance in miles from the Lone Tree Campus\n\n\n\n\n\nDescription of the graph is histogram with high part on left and low part on right with several gaps. The graph contains bars.\n\ngf_density(~distance_campus, data=Class, title=\"Distance in miles from the Lone Tree Campus\", xlab=\"Distance (miles)\")\n\n\n\n\n\n\n\nFigure 2.7: Distance in miles from the Lone Tree Campus\n\n\n\n\n\nDescription of the graph is density graph with high part on left and low part on right with several gaps. The graph is smooth.\n\ngf_dotplot(~distance_campus, data=Class, title=\"Distance in miles from the Lone Tree Campus\", xlab=\"Distance (miles)\") \n\n\n\n\n\n\n\nFigure 2.8: Distance in miles from the Lone Tree Campus\n\n\n\n\n\nDescription of the graph of dot plot with high part on left and low part on right with several gaps. The graph is with dots that represent each data value.\nNotice the histogram, density plot, and dot plot are all very similar, but the density plot is smoother. They all tell you similar ideas of the shape of the distribution. Reviewing the graphs you can see that most of the students live within 10 miles of the Lone Tree Campus, in fact most live within 5 miles from the campus. However, there is a student who lives around 50 miles from the Lone Tree Campus. This is a great deal farther from the rest of the data. This value could be considered an outlier. An outlier is a data value that is far from the rest of the values. It may be an unusual value or a mistake. It is a data value that should be investigated. In this case, the student lived really far from campus, thus the value is not a mistake, and is just very unusual. The density plot is probably the best plot for most data frames.\nThere are other aspects that can be discussed, but first some other concepts need to be introduced.\n\n\n\n2.2.2 Shapes of the distribution:\nWhen you look at a distribution, look at the basic shape. There are some basic shapes that are seen in histograms. Realize though that some distributions have no shape. The common shapes are symmetric, skewed, and uniform. Another interest is how many peaks a graph may have. This is known as modal.\nSymmetric means that you can fold the graph in half down the middle and the two sides will line up. You can think of the two sides as being mirror images of each other. Skewed means one “tail” of the graph is longer than the other. The graph is skewed in the direction of the longer tail (backwards from what you would expect). A uniform graph has all the bars the same height.\nModal refers to the number of peaks. Unimodal has one peak and bimodal has two peaks. Usually if a graph has more than two peaks, the modal information is not longer of interest.\nOther important features to consider are gaps between bars, a repetitive pattern, how spread out is the data, and where the center of the graph is.\n\n\n2.2.3 Examples of graphs:\nThis graph is roughly symmetric and unimodal:\nGraph: Symmetric Distribution\n\n\n\nsymmetric Graph\n\n\nThis graph is symmetric and bimodal:\nGraph: Symmetric and Bimodal Distribution\n\n\n\nBimodal and symmetric graph\n\n\nThis graph is skewed to the right:\nGraph: Skewed Right Distribution\n\n\n\nSkewed right graph\n\n\nThis graph is skewed to the left and has a gap:\nGraph: Skewed Left Distribution\n\n\n\nSkewed Left graph\n\n\nThis graph is uniform since all the bars are the same height:\nGraph: Uniform Distribution\n\n\n\nUniform graph\n\n\n\n\n2.2.4 Example: Drawing a Histogram and Density plot\nData was collected from the Chronicle of Higher Education for tuition from public four year colleges, private four year colleges, and for profit four year colleges. The data frame is in Table 2.6. Draw a density plot of instate tuition levels for all four year institutions, and then separate the density plot for instate tuition based on type of institution. Describe any findings from the graph.\n\nTuition&lt;-read.csv( \"https://krkozak.github.io/MAT160/Tuition_4_year.csv\") \nknitr::kable(head(Tuition))\n\n\n\nTable 2.6: Head of Tuition Data Frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nINSTITUTION\nTYPE\nSTATE\nROOM_BOARD\nINSTATE_TUITION\nINSTATE_TOTAL\nOUTOFSTATE_TUITION\nOUTOFSTATE_TOTAL\n\n\n\n\nUniversity of Alaska AnchoragePublic 4-year\nPublic_4 year\nAK\n12200\n7688\n19888\n23858\n36058\n\n\nUniversity of Alaska FairbanksPublic 4-year\nPublic_4 year\nAK\n8930\n8087\n17017\n24257\n33187\n\n\nUniversity of Alaska SoutheastPublic 4-year\nPublic_4 year\nAK\n9200\n7092\n16292\n19404\n28604\n\n\nAlaska Bible CollegePrivate 4-year\nPrivate_4_year\nAK\n5700\n9300\n15000\n9300\n15000\n\n\nAlaska Pacific UniversityPrivate 4-year\nPrivate_4_year\nAK\n7300\n20830\n28130\n20830\n28130\n\n\nAlabama Agricultural and Mechanical UniversityPublic 4-year\nPublic_4 year\nAL\n8379\n9698\n18077\n17918\n26297\n\n\n\n\n\n\n\n\nCode book for Data Frame Tuition\nDescription Cost of four year institutions.\nFormat\nThis data frame contains the following columns:\nINSTITUTION: Name of four year institution\nTYPE: Type of four year institution, Public_4_year, Private_4_year, For_profit_4_year.\nSTATE: What state the institution resides\nROOM_BOARD: The cost of room and board at the institution (\\$)\nINSTATE_TUTION: The cost of instate tuition (\\$)\nINSTATE_TOTAL: The cost of room and board and instate tuition (\\$ per year)\nOUTOFSTATE_TUTION: The cost of out of state tuition (\\$ per year)\nOUTOFSTATE_TOTAL: The cost of room and board and out of state tuition (\\$ per year)\nSource Tuition and Fees, 1998-99 Through 2018-19. (2018, December 31). Retrieved from https://www.chronicle.com/interactives/tuition-and-fees\nReferences Chronicle of Higher Education *, December 31, 2018.\n\n2.2.4.1 Solution\n\ngf_density(~INSTATE_TUITION, data=Tuition, title=\"Instate Tuition at all Four Year institutions\", xlab=\"Instate Tutition ($ per year)\")\n\n\n\n\nDensity Plot for Instate Tuition Levels at all Four-Year Colleges\n\n\n\n\nDescription of the graph is a density with high part on left, then a dip and up to peak in the middle that is lower than the left peak and then the lowest peak on the right .\n(ref:tuition-instate-type-cap) Density Plot for Instate Tuition Levels at all Four-Year Colleges\n\ngf_density(~INSTATE_TUITION|TYPE, data=Tuition, title=\"Instate Tuition at all Four Year institions\", xlab=\"Instate Tuition ($/year)\") \n\n\n\n\n\n\n\nFigure 2.9: Instate Tuition at all Four Year institions\n\n\n\n\n\nDescription of Figure 2.9 is a density plots separated by for profit 4 year with peak on left, private 4 year with peak in the middle, and public 4 year colleges with peak on the left. Public 4 year has the highest peak, with for profit 4 year is lower, and then private 4 year with the lowest peak.\nThe distribution is skewed right, with no gaps. Most institutions in state is less than \\$ 20,000 per year though some go as high as \\$ 60,000 per year. When separated by public versus private and for profit, most public are much less than \\$ 20,000 per year while private four year cost around \\$ 30,000 per year, and for profit are around \\$ 20,000 per year.\nThere are other types of graphs for quantitative data. They will be explored in the next section.\n\n\n\n2.2.5 Homework for Quantitative Data Section\n\nThe weekly median incomes of males and females for specific occupations, are given in Table 2.7 (CPS News Releases. (n.d.). Retrieved July 8, 2019, from https://www.bls.gov/cps/). Create a density plot for males and females. Discuss any findings from the graph. Note: to put two graphs on the same axis, type the piping symbol |&gt; (base r) or %&gt;% (magrittr package) (Note: |&gt; and %&gt;% are piping symbols that can be thought of as “and then”) at the end of the first command and then type the command for the second graph on the next line. Also, use fill=“pick a color” in the command to plot the graphs with different colors so the two graphs can be easier to distinguish.\n\n\nWages&lt;- read.csv( \"https://krkozak.github.io/MAT160/wages.csv\") \nknitr::kable(head(Wages))\n\n\n\nTable 2.7: Head of Wages Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOccupation\nNumworkers\nmedian_wage\nmale_worker\nmale_wage\nfemale_worker\nfemale_wage\n\n\n\n\nManagement, professional, and related occupations\n48808\n1246\n23685\n1468\n25123\n1078\n\n\nManagement, business, and financial operations occupations\n19863\n1355\n10668\n1537\n9195\n1168\n\n\nManagement occupations\n13477\n1429\n7754\n1585\n5724\n1236\n\n\nChief executives\n1098\n2291\n790\n2488\n307\n1736\n\n\nGeneral and operations managers\n939\n1338\n656\n1427\n283\n1139\n\n\nLegislators\n14\nNA\n10\nNA\n4\nNA\n\n\n\n\n\n\n\n\nCode book for Data Frame Wages\nDescription Median weekly earnings of full-time wage and salary workers by detailed occupation and sex. The Current Population Survey (CPS) is a monthly survey of households conducted by the Bureau of Census for the Bureau of Labor Statistics. It provides a comprehensive body of data on the labor force, employment, unemployment, persons not in the labor force, hours of work, earnings, and other demographic and labor force characteristics.\nFormat\nThis data frame contains the following columns:\nOccupation: Occupations of workers.\nNumworkers: The number of workers in each occupation (in thousands of workers)\nmedian_wage: Median weekly wage (\\$)\nmale_worker: number of male workers (in thousands of workers)\nmale_wage: Median weekly wage of male workers (\\$)\nfemale_worker: number of female workers (in thousands of workers)\nfemale_wage: Median weekly wage of female workers (\\$)\nSource CPS News Releases. (n.d.). Retrieved July 8, 2019, from https://www.bls.gov/cps/\nReferences Current Population Survey (CPS) retrieved July 8, 2019.\n\nThe density of people per square kilometer for certain countries is in Table 2.8 (World Bank, 2019). Create density plot of density in 2018 for just Sub-Saharan Africa. Describe what story the graph tells.\n\n\nDensity&lt;- read.csv( \"https://krkozak.github.io/MAT160/density.csv\") \nknitr::kable(head(Density))\n\n\n\nTable 2.8: Head of Density Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry_Name\nCountry_Code\nRegion\nIncomeGroup\ny1961\ny1962\ny1963\ny1964\ny1965\ny1966\ny1967\ny1968\ny1969\ny1970\ny1971\ny1972\ny1973\ny1974\ny1975\ny1976\ny1977\ny1978\ny1979\ny1980\ny1981\ny1982\ny1983\ny1984\ny1985\ny1986\ny1987\ny1988\ny1989\ny1990\ny1991\ny1992\ny1993\ny1994\ny1995\ny1996\ny1997\ny1998\ny1999\ny2000\ny2001\ny2002\ny2003\ny2004\ny2005\ny2006\ny2007\ny2008\ny2009\ny2010\ny2011\ny2012\ny2013\ny2014\ny2015\ny2016\ny2017\ny2018\n\n\n\n\nAruba\nABW\nLatin America & Caribbean\nHigh income\n307.988889\n312.361111\n314.972222\n316.844444\n318.666667\n320.638889\n322.527778\n324.366667\n326.255556\n328.127778\n330.222222\n332.444444\n334.683333\n336.266667\n336.983333\n336.588889\n335.366667\n333.905556\n333.222222\n333.866667\n336.483333\n340.805556\n345.561111\n349.088889\n350.144444\n348.022222\n343.516667\n339.327778\n339.066667\n345.272222\n359.011111\n379.08333\n402.80000\n426.11111\n446.24444\n462.22222\n474.72778\n484.87222\n494.47222\n504.73889\n516.10000\n527.73333\n538.98333\n548.53889\n555.72778\n560.18889\n562.34444\n563.10000\n563.63889\n564.82778\n566.92222\n569.77778\n573.10556\n576.52222\n579.67222\n582.62222\n585.36667\n588.02778\n\n\nAfghanistan\nAFG\nSouth Asia\nLow income\n14.044987\n14.323808\n14.617537\n14.926295\n15.250314\n15.585020\n15.929795\n16.293023\n16.686236\n17.114913\n17.577191\n18.060863\n18.547565\n19.013188\n19.436265\n19.825220\n20.174779\n20.435006\n20.542009\n20.458461\n20.175341\n19.732451\n19.204316\n18.693582\n18.286015\n17.976563\n17.774920\n17.795553\n18.179820\n19.012205\n20.370396\n22.18783\n24.22664\n26.15527\n27.74049\n28.87822\n29.64973\n30.23277\n30.89612\n31.82911\n33.09590\n34.61810\n36.27251\n37.87440\n39.29522\n40.48808\n41.51049\n42.46282\n43.49296\n44.70408\n46.13150\n47.73056\n49.42804\n51.11478\n52.71207\n54.19711\n55.59599\n56.93776\n\n\nAngola\nAGO\nSub-Saharan Africa\nLower middle income\n4.436891\n4.498708\n4.555593\n4.600180\n4.628676\n4.637213\n4.631622\n4.629544\n4.654892\n4.724765\n4.845414\n5.012073\n5.211328\n5.423422\n5.634074\n5.839022\n6.042941\n6.249063\n6.463517\n6.690695\n6.930654\n7.181319\n7.442124\n7.712163\n7.990693\n8.277943\n8.574036\n8.877878\n9.188078\n9.503799\n9.825059\n10.15270\n10.48773\n10.83159\n11.18570\n11.55107\n11.92875\n12.32021\n12.72709\n13.15110\n13.59249\n14.05263\n14.53556\n15.04624\n15.58803\n16.16259\n16.76856\n17.40245\n18.05910\n18.73446\n19.42782\n20.13951\n20.86771\n21.61047\n22.36655\n23.13506\n23.91654\n24.71305\n\n\nAlbania\nALB\nEurope & Central Asia\nUpper middle income\n60.576642\n62.456898\n64.329234\n66.209307\n68.058066\n69.874927\n71.737153\n73.805548\n75.974270\n77.937190\n79.848650\n81.865912\n83.823066\n85.770949\n87.767555\n89.727226\n91.735255\n93.659343\n95.541314\n97.518139\n99.491095\n101.615985\n103.794161\n106.001058\n108.202993\n110.315146\n112.540329\n114.683796\n117.808139\n119.946788\n119.225912\n118.50507\n117.78420\n117.06336\n116.34248\n115.62164\n114.90077\n114.17993\n113.45905\n112.73821\n111.68515\n111.35073\n110.93489\n110.47223\n109.90828\n109.21704\n108.39478\n107.56620\n106.84376\n106.31463\n106.02901\n105.85405\n105.66029\n105.44175\n105.13515\n104.96719\n104.87069\n104.61226\n\n\nAndorra\nAND\nEurope & Central Asia\nHigh income\n30.585106\n32.702128\n34.919149\n37.168085\n39.465957\n41.802128\n44.165957\n46.574468\n49.059574\n51.651064\n54.380851\n57.217021\n60.068085\n62.808511\n65.329787\n67.610638\n69.725532\n71.780851\n74.080851\n76.738298\n79.787234\n83.221277\n86.951064\n90.863830\n94.893617\n98.972340\n103.095745\n107.306383\n111.591489\n115.976596\n120.576596\n125.29362\n129.72553\n133.35532\n135.85106\n136.93617\n136.86596\n136.47234\n136.95745\n139.12766\n143.27872\n149.04043\n155.70638\n162.22128\n167.80213\n172.32553\n175.92340\n178.42979\n179.70851\n179.67872\n178.18511\n175.37660\n171.85957\n168.53830\n165.98085\n164.46170\n163.83191\n163.84255\n\n\nArab World\nARB\n\n\n8.430860\n8.663154\n8.903441\n9.152526\n9.410965\n9.679951\n9.959490\n10.247580\n10.541383\n10.839409\n11.140162\n11.445801\n11.762925\n12.100336\n12.464221\n12.856964\n13.276051\n13.716559\n14.171137\n14.634158\n15.103942\n15.581254\n16.065812\n16.557944\n17.057705\n17.563945\n18.075438\n18.592082\n19.114029\n19.817110\n20.358106\n20.73408\n21.29364\n21.84602\n22.52760\n23.05216\n23.57027\n24.08237\n24.60020\n25.12980\n25.67166\n26.22642\n26.80081\n27.40153\n28.03371\n28.69994\n29.39751\n30.11889\n30.85858\n31.59402\n32.33012\n33.06767\n33.80379\n34.53398\n35.25690\n35.96876\n36.66980\n37.37237\n\n\n\n\n\n\n\n\nCode book for Data Frame Density\nDescription Population density of all countries in the world\nFormat\nThis data frame contains the following columns:\nCountry_Name: The name of countries or regions around the world\nCountry_Code: The 3 letter code for a country or region\nRegion: World Banks classification of where the country is in the world\nIncomegroup: World Banks classification of what income level the country is considered to be\ny1961-y2018: population density for the years 1961 through 2018, people per sq. km of land area, population density is midyear population divided by land area in square kilometers. Population is based on the de facto definition of population, which counts all residents regardless of legal status or citizenship–except for refugees not permanently settled in the country of asylum, who are generally considered part of the population of their country of origin. Land area is a country’s total area, excluding area under inland water bodies, national claims to continental shelf, and exclusive economic zones. In most cases the definition of inland water bodies includes major rivers and lakes.\nSource Population density (people per sq. km of land area). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/EN.POP.DNST\nReferences Food and Agriculture Organization and World Bank population estimates.\nSince the Density data frame is for all countries, a new data frame must be created with just Sub-Saharan Africa Table 2.9. This is created by using the following command\n\nAfrica &lt;- Density |&gt; \n  filter(Region == \"Sub-Saharan Africa\") \nknitr::kable(head(Africa))\n\n\n\nTable 2.9: Head of Africa Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry_Name\nCountry_Code\nRegion\nIncomeGroup\ny1961\ny1962\ny1963\ny1964\ny1965\ny1966\ny1967\ny1968\ny1969\ny1970\ny1971\ny1972\ny1973\ny1974\ny1975\ny1976\ny1977\ny1978\ny1979\ny1980\ny1981\ny1982\ny1983\ny1984\ny1985\ny1986\ny1987\ny1988\ny1989\ny1990\ny1991\ny1992\ny1993\ny1994\ny1995\ny1996\ny1997\ny1998\ny1999\ny2000\ny2001\ny2002\ny2003\ny2004\ny2005\ny2006\ny2007\ny2008\ny2009\ny2010\ny2011\ny2012\ny2013\ny2014\ny2015\ny2016\ny2017\ny2018\n\n\n\n\nAngola\nAGO\nSub-Saharan Africa\nLower middle income\n4.4368910\n4.4987078\n4.5555932\n4.6001797\n4.6286757\n4.637213\n4.631622\n4.629544\n4.654892\n4.724765\n4.845414\n5.012073\n5.211328\n5.423422\n5.634074\n5.839022\n6.042941\n6.249063\n6.463517\n6.690695\n6.930654\n7.181319\n7.442124\n7.712163\n7.990693\n8.277943\n8.574036\n8.877878\n9.188078\n9.503799\n9.825059\n10.152696\n10.487727\n10.831593\n11.185695\n11.551070\n11.928748\n12.320206\n12.727095\n13.151097\n13.592487\n14.052633\n14.535557\n15.046238\n15.588034\n16.162590\n16.768559\n17.402450\n18.059101\n18.734456\n19.427818\n20.139513\n20.867715\n21.610475\n22.366553\n23.135064\n23.916538\n24.713052\n\n\nBurundi\nBDI\nSub-Saharan Africa\nLow income\n111.0762461\n113.2134346\n115.4371885\n117.8461838\n120.4976246\n123.461449\n126.682944\n129.942640\n132.940187\n135.477959\n137.460942\n139.005685\n140.386527\n141.994977\n144.115265\n146.840771\n150.095210\n153.787617\n157.758333\n161.888551\n166.141744\n170.550000\n175.137578\n179.949494\n185.001441\n190.293731\n195.760826\n201.273287\n206.661565\n211.797391\n216.702726\n221.400506\n225.780880\n229.710553\n233.140304\n235.985631\n238.400701\n240.870794\n244.046885\n248.398403\n254.110008\n261.063590\n269.048053\n277.713902\n286.793692\n296.255802\n306.160981\n316.436994\n327.011994\n337.834969\n348.847586\n360.046262\n371.506581\n383.344899\n395.639797\n408.411137\n421.613084\n435.178271\n\n\nBenin\nBEN\nSub-Saharan Africa\nLow income\n21.8682778\n22.1966655\n22.5510731\n22.9333540\n23.3447677\n23.786440\n24.257778\n24.756917\n25.280782\n25.827776\n26.397410\n26.991548\n27.613294\n28.267222\n28.956767\n29.684046\n30.449087\n31.251667\n32.090511\n32.965280\n33.878397\n34.832512\n35.827856\n36.864305\n37.943429\n39.060890\n40.220495\n41.440688\n42.745796\n44.151259\n45.667781\n47.284525\n48.969165\n50.675949\n52.372810\n54.046284\n55.708044\n57.380853\n59.099840\n60.889952\n62.759250\n64.698421\n66.695238\n68.730082\n70.789509\n72.870672\n74.980428\n77.127714\n79.325186\n81.582645\n83.902359\n86.282795\n88.724619\n91.227758\n93.791699\n96.417763\n99.106101\n101.853920\n\n\nBurkina Faso\nBFA\nSub-Saharan Africa\nLow income\n17.8895468\n18.1298465\n18.3765387\n18.6362939\n18.9139985\n19.211853\n19.528578\n19.861261\n20.205314\n20.557748\n20.918790\n21.290837\n21.675742\n22.076173\n22.494682\n22.931422\n23.387920\n23.869953\n24.384708\n24.937292\n25.530556\n26.163213\n26.830793\n27.526469\n28.245274\n28.986455\n29.751729\n30.542050\n31.359002\n32.204072\n33.077792\n33.980676\n34.914020\n35.879342\n36.878209\n37.912080\n38.982259\n40.090365\n41.237942\n42.426689\n43.657116\n44.930921\n46.252270\n47.626349\n49.056762\n50.545234\n52.090720\n53.690515\n55.340271\n57.036612\n58.778914\n60.567420\n62.400493\n64.276378\n66.193801\n68.151966\n70.150892\n72.191283\n\n\nBotswana\nBWA\nSub-Saharan Africa\nUpper middle income\n0.9046371\n0.9242108\n0.9452208\n0.9667267\n0.9881143\n1.009235\n1.030635\n1.053318\n1.078644\n1.107609\n1.140485\n1.177090\n1.217356\n1.261116\n1.308127\n1.358635\n1.412540\n1.468895\n1.526432\n1.584296\n1.641713\n1.699001\n1.757680\n1.819983\n1.887287\n1.960269\n2.037842\n2.117529\n2.195903\n2.270492\n2.340307\n2.406003\n2.468742\n2.530410\n2.592370\n2.655109\n2.718093\n2.780555\n2.841325\n2.899677\n2.954984\n3.007856\n3.060360\n3.115288\n3.174489\n3.239476\n3.309264\n3.380162\n3.446964\n3.506264\n3.556194\n3.598805\n3.639363\n3.685377\n3.742022\n3.811240\n3.890967\n3.977425\n\n\nCentral African Republic\nCAF\nSub-Saharan Africa\nLow income\n2.4496228\n2.4911073\n2.5351857\n2.5821310\n2.6320363\n2.685510\n2.742146\n2.799759\n2.855406\n2.907227\n2.954377\n2.998141\n3.041595\n3.089005\n3.143547\n3.205583\n3.274453\n3.351091\n3.436349\n3.530380\n3.634855\n3.748648\n3.865801\n3.978269\n4.080659\n4.169895\n4.248676\n4.324333\n4.407419\n4.505336\n4.620548\n4.750130\n4.889642\n5.032288\n5.172969\n5.310336\n5.445497\n5.578818\n5.711281\n5.843570\n5.974539\n6.103130\n6.230025\n6.356344\n6.482362\n6.610275\n6.738595\n6.859556\n6.962703\n7.041587\n7.092741\n7.121280\n7.139783\n7.165840\n7.212382\n7.283841\n7.377489\n7.490412\n\n\n\n\n\n\n\n\n\nThe Affordable Care Act created a market place for individuals to purchase health care plans. In 2014, the premiums for a 27 year old for the different levels health insurance are given in Table 2.10 (\\“Health insurance marketplace,\\” 2013). Create a density plot of bronze_lowest, then silver_lowest, and gold_lowest all on the same aces. Use |&gt; or %&gt;% at the end of each command. Describe the story the graphs tells.\n\n\nInsurance&lt;- read.csv( \"https://krkozak.github.io/MAT160/insurance.csv\") \nknitr::kable(head(Insurance))\n\n\n\nTable 2.10: Head of Insurance Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nstate\naverage_QHP\nbronze_lowest\nsilver_lowest\ngold_lowest\ncatastrophic\nsecond_silver_pretax\nsecond_silver_posttax\nlowest_bronze_posttax\nsilver_family_pretax\nsilver_family_posttax\nbronze_family_posttax\n\n\n\n\nAK\n34\n254\n312\n401\n236\n312\n107\n48\n1131\n205\n0\n\n\nAL\n7\n162\n200\n248\n138\n209\n145\n98\n757\n282\n112\n\n\nAR\n28\n181\n231\n263\n135\n241\n145\n85\n873\n282\n64\n\n\nAZ\n106\n141\n164\n187\n107\n166\n145\n120\n600\n282\n192\n\n\nDE\n19\n203\n234\n282\n137\n237\n145\n111\n859\n282\n158\n\n\nFL\n102\n169\n200\n229\n132\n218\n145\n96\n789\n282\n104\n\n\n\n\n\n\n\n\nCode book for Data Frame Insurance\nDescription The Affordable Care Act created a market place for individuals to purchase health care plans.The data is from 2014.\nFormat\nThis data frame contains the following columns:\nstate: state of insured.\naverage_QHP: The number of qualified health plans\nbronze_lowest: premium for the lowest bronze level of insurance for a single person (\\$)\nsilver_lowest: premium for the lowest silver level of insurance for a single person (\\$)\ngold_lowest: premium for the lowest gold level of insurance for a single person (\\$)\ncatastrophic: premium for the catastrophic level of insurance for a single person (\\$)\nsecond_silver_pretax: premium for the second silver level of insurance for a single person pretax (\\$)\nsecond_silver_posttax: premium for the second silver level of insurance for a single person posttax (\\$)\nsecond_bronze_posttax: premium for the lowest bronze level of insurance for a single person posttax (\\$)\nsilver_family_pretax: premium for the silver level of insurance for a family pretax (\\$)\nsilver_family_posttax: premium for the silver level of insurance for a family posttax (\\$)\nbronze_family_posttax: premium for the bronze level of insurance for a family posttax (\\$)\nSource Health Insurance Market Place Retrieved from website: http://aspe.hhs.gov/health/reports/2013/marketplacepremiums/ib_premiumslandscape.pdf premiums for 2014.\nReferences Department of Health and Human Services, ASPE. (2013). Health insurance marketplace\n\nStudents in a statistics class took their first test. In Table 2.11 are the scores they earned. Create a density plot for grades. Describe the shape of the distribution.\n\n\nFirsttest_1&lt;- read.csv( \"https://krkozak.github.io/MAT160/firsttest_1.csv\") \nknitr::kable(head(Firsttest_1))\n\n\n\nTable 2.11: Head of First Test Data frame\n\n\n\n\n\n\ngrades\n\n\n\n\n80\n\n\n79\n\n\n89\n\n\n74\n\n\n73\n\n\n67\n\n\n\n\n\n\n\n\n\nStudents in a statistics class took their first test. The scores they earned are in Table 2.12. Create a density plot for grades. Describe the shape of the distribution. Compare to the graph in question 4.\n\n\nFirsttest_2&lt;- read.csv( \"https://krkozak.github.io/MAT160/firsttest_2.csv\") \nknitr::kable(head(Firsttest_2))\n\n\n\nTable 2.12: Head of First Test Data frame\n\n\n\n\n\n\ngrades\n\n\n\n\n67\n\n\n67\n\n\n76\n\n\n47\n\n\n85\n\n\n70",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Description of Data</span>"
    ]
  },
  {
    "objectID": "Graphical Description of Data.html#other-graphical-representations-of-data",
    "href": "Graphical Description of Data.html#other-graphical-representations-of-data",
    "title": "2  Graphical Description of Data",
    "section": "2.3 Other Graphical Representations of Data",
    "text": "2.3 Other Graphical Representations of Data\nThere are many other types of graphs. Some of the more common ones are the point plot (scatter plot), and a time-series plot. There are also many different graphs that have emerged lately for qualitative data. Many are found in publications and websites. The following is a description of the point plot (scatter plot), and the time-series plot.\n\n2.3.1 Point Plots or Scatter Plot\nSometimes you have two different variables and you want to see if they are related in any way. A scatter plot helps you to see what the relationship would look like. A scatter plot is just a plotting of the ordered pairs.\n\n\n2.3.2 Example: Scatter Plot\nIs there a relationship between systolic blood pressure and weight? To answer this question some data is needed. The data frame NHANES contains this data, but given the size of the data frame, it may be not be very useful to look at the graph of all the data. It makes sense to take a sample from the data frame. A random sample is the better type of sample to take. Once the sample is taken, then a scatter plot can be created. The rStudio command for a scatter plot is\ngf_point(response_variable ~ explanatory_variable, data= Data_Frame)\nThe sample is Table 2.13.\n\n2.3.2.1 Solution\n\nsample_NHANES &lt;- NHANES |&gt; \n  sample_n(size = 100) \nknitr::kable(head(sample_NHANES))\n\n\n\nTable 2.13: Head of NHANES Sample Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n55893\n2009_10\nfemale\n25\n20-29\n310\nMexican\nNA\n9 - 11th Grade\nNeverMarried\nNA\nNA\nNA\n3\nRent\nWorking\n115.4\nNA\nNA\n159.0\n45.65\nNA\n30.0_plus\n66\n112\n50\n110\n52\n110\n48\n114\n52\nNA\n1.09\n4.14\n36\n0.171\n112\n4\nNo\nNA\nPoor\n3\n10\nSeveral\nSeveral\n1\n1\nNA\n7\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n3\n3\nYes\nYes\nSmoker\n18\nYes\n14\nNo\nNA\nYes\nYes\n15\n3\n1\nNo\nHeterosexual\nNo\n\n\n54520\n2009_10\nmale\n27\n20-29\n326\nWhite\nNA\nHigh School\nMarried\n45000-54999\n50000\n2.62\n5\nRent\nWorking\n90.7\nNA\nNA\n177.4\n28.82\nNA\n25.0_to_29.9\n78\n115\n83\n118\n86\n116\n84\n114\n82\nNA\n0.72\n5.20\n138\n0.414\nNA\nNA\nNo\nNA\nVgood\n4\n0\nNone\nNone\nNA\nNA\nNA\n8\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n3\n104\nNo\nYes\nSmoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n20\n6\n1\nNo\nHeterosexual\nNA\n\n\n61964\n2009_10\nfemale\n80\nNA\nNA\nWhite\nNA\nSome College\nMarried\n15000-19999\n17500\n1.55\n4\nOwn\nNotWorking\n51.7\nNA\nNA\n164.3\n19.15\nNA\n18.5_to_24.9\n52\n152\n68\n158\n68\n152\n68\n152\n68\nNA\n2.22\n5.22\n61\n0.370\nNA\nNA\nNo\nNA\nGood\n14\n30\nSeveral\nSeveral\n4\n4\n23\n5\nYes\nYes\n6\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n69785\n2011_12\nfemale\n80\nNA\nNA\nWhite\nWhite\n9 - 11th Grade\nWidowed\n10000-14999\n12500\n1.08\n5\nOwn\nNotWorking\n52.1\nNA\nNA\n153.7\n22.10\nNA\n18.5_to_24.9\n66\n193\n68\n200\n64\n194\n72\n192\n64\n12.31\n1.86\n6.00\n83\n0.865\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\n3\n3\n21\n8\nNo\nYes\nNA\nMore_4_hr\n0_hrs\nNA\nNA\nNo\nNA\n0\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n66033\n2011_12\nfemale\n37\n30-39\nNA\nWhite\nWhite\n9 - 11th Grade\nMarried\nNA\nNA\nNA\n3\nOwn\nNotWorking\n52.8\nNA\nNA\n151.3\n23.10\nNA\n18.5_to_24.9\n98\n117\n76\nNA\nNA\n118\n78\n116\n74\n280.85\n1.68\n3.47\n195\n2.532\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\n3\nNA\nNA\n7\nNo\nYes\n3\n4_hr\n2_hr\nNA\nNA\nYes\n5\n156\nYes\nYes\nSmoker\n13\nYes\n12\nYes\n14\nYes\nYes\n15\n60\n1\nYes\nHeterosexual\nNo\n\n\n60061\n2009_10\nmale\n50\n50-59\n608\nWhite\nNA\nHigh School\nMarried\n10000-14999\n12500\n0.60\n7\nOwn\nNotWorking\n104.4\nNA\nNA\n175.4\n33.93\nNA\n30.0_plus\n74\n128\n83\n126\n84\n126\n86\n130\n80\nNA\n0.72\n3.23\n279\n2.790\nNA\nNA\nNo\nNA\nVgood\n0\n0\nSeveral\nNone\nNA\nNA\nNA\n6\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n12\n1\nYes\nYes\nSmoker\n16\nYes\n16\nYes\n16\nYes\nYes\n14\n20\n1\nNo\nHeterosexual\nNA\n\n\n\n\n\n\n\n\nPreliminary: State the explanatory variable and the response variable\nLet x=explanatory variable = Weight of a person (Weight)\ny=response variable = Systolic blood pressure (BPSys1)\n\ngf_point(BPSys1~Weight, data=sample_NHANES, xlab=\"Weight (kg)\", ylab=\"Systolic Blood Pressure\", title=\"Blood Pressure versus Weight\")\n\n\n\n\n\n\n\nFigure 2.10: Blood Pressure versus Weight\n\n\n\n\n\nDescription of Figure 2.10 is a scatter plot with dots all over the plot though a line could be thought of fitting the dots with lower on the left and higher on the right.\nLooking at the graph Figure 2.10, it appears that there is a linear relationship between weight and systolic blood pressure though it looks somewhat weak. It also appears to be a positive relationship, thus as weight increases, the systolic blood pressure increases.\n\n\n\n2.3.3 Time-Series\nA time-series plot is a graph showing the data measurements in chronological order, the data being quantitative data. For example, a time-series plot is used to show profits over the last 5 years. To create a time-series plot on RStudio, use the command\ngf_line(response_variable ~ explanatory_variable, data=Data_Frame)\nThe purpose of a time-series graph is to look for trends over time. Caution, you must realize that the trend may not continue. Just because you see an increase, doesn’t mean the increase will continue forever. As an example, prior to 2007, many people noticed that housing prices were increasing. The belief at the time was that housing prices would continue to increase. However, the housing bubble burst in 2007, and many houses lost value, and haven’t recovered.\n\n\n2.3.4 Example: Time-Series Plot\nThe bank assets (in billions of Australia dollars (AUD)) of the Reserve Bank of Australia (RBA) and other financial organizations for the time period of September 1 1969, through March 1 2019, are contained in table Table 2.14 (Reserve Bank of Australia, 2019). Create a time-series plot of the total assets of Authorized Deposit-taking Institutions (ADIs) and interpret any findings.\n\nAustralian&lt;- read.csv( \"https://krkozak.github.io/MAT160/Australian_financial.csv\") \nknitr::kable(head(Australian))\n\n\n\nTable 2.14: Head of Australian Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDate\nDay\nAssets_RBA\nAssets_ADIs_Banks\nAssets_ADIs_Building\nAssets_ADIs_CU\nAssets_ADIs_Total\nAssets_RFCs_MM\nAssets_RFCs_Finance\nAssets_RFCs_Total\nAssets_Life.offices\nAssets_Life_funds\nAssets_Life_Total\nAssets_Other_Public_trusts\nAssets_Other_Cash_trusts\nAssets_Other_Common_funds\nAssets_Others_Friendly\nAssets_Other_General_insurance\nAssets_Other_vehicles\nAssets_Unconsolidated\n\n\n\n\nSep-69\n0\n2.7\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nDec-69\n90\n2.9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMar-70\n180\n3.0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nJun-70\n270\n3.0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nSep-70\n360\n3.0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nDec-70\n450\n3.0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nCode book for Data frame Australian\nDescription The data is a range of economic and financial data produced by the Reserve Bank of Australia and other organizations.\nFormat\nThis data frame contains the following columns:\nDate: quarters from September 1, 1969, to March 1, 2019\nDay: The number of days since September 1, 1969, using 90 days between starts of a quarter. This column is to make it easier to graph in rStudio, and has no other purpose.\nAssets_RBA: The assets for the Royal Bank of Australia\nAssets_ADIs_Banks: The assets for Authorized Deposit-taking Institutions (ADIs), Banks\nAssets_ADIs_Building: The assets for Authorized Deposit-taking Institutions (ADIs), Building societies\nAssets_ADIs_CU: The assets for Authorized Deposit-taking Institutions (ADIs), Credit Unions\nAssets_ADIs_Total: The assets for Authorized Deposit-taking Institutions (ADIs), total\nAssets_RFCs_MM: The assets for Registered Financial Corporations (RFCs), Money Market Corporations\nAssets_RFCs_Finance: The assets for Registered Financial Corporations (RFCs), Finance companies and general financiers\nAssets_RFCs_Total: The assets for Registered Financial Corporations (RFCs) total\nAssets_Life offices: The Assets of Life offices and superannuation funds; Life insurance offices\nAssets_Life_funds: The Assets of Life offices and superannuation funds; Superannuation funds\nAssets_Life_Total: The Assets of Life offices and superannuation; Total\nAssets_Other_Public_trusts: The Assets of Other managed funds; Public unit trusts\nAssets_Other_Cash_trusts: The Assets of Other managed funds; Cash management trusts\nAssets_Other_Common_funds: The Assets of Other managed funds; Common funds\nAssets_Others_Friendly: The Assets of Other managed funds; Friendly societies\nAssets_Other_General_insurance: The Assets of Other financial institutions; General insurance offices\nAssets_Other_vehicles: The Assets Other financial institutions; Securitisation vehicles\nAssets_Unconsolidated: The Assets of Unconsolidated; Statutory funds of life insurance offices; Superannuation\nSource Reserve Bank of Australia. (2019, May 13). Statistical Tables. Retrieved July 10, 2019, from https://www.rba.gov.au/statistics/tables/\nReferences Reserve Bank of Australia and other organizations\n\n2.3.4.1 Solution\nvariable, x=total assets of Authorized Deposit-taking Institutions (ADIs)\nLooking at the code book, one can see that the variable Assets_ADIs_Total is the variable in the data frame that is of interest here. With a time series plot, the other variable is time. In this case the variable in the data frame that represents time is Date. The problem with Date is that the units are every quarter. This is not easily interpreted by rStudio, so a column was created called Day. From the code book, this is the number of days since September 1, 1969, using 90 days between starts of a quarter. Even though this isn’t perfect, it will work for determining trends. So create a time series plot of Assets_ADIs_Total versus Day. The command is:\n\ngf_line(Assets_ADIs_Total~Day, data=Australian, title=\"Total Assets of Authorized Deposit-taking Institutions (ADIs)\", xlab=\"Day since September 1, 1969\", ylab=\"ADI (AUD)\")\n\n\n\n\n\n\n\nFigure 2.11: Total Assets of Authorized Deposit-taking Institutions\n\n\n\n\n\nDescription of Figure 2.11 is an increasing time series Graph of Total Assets of Authorized Deposit-taking Institutions from day 7500 to 17500. The first number starts at 0 and goes up to about 4500.\nFrom the graph, total assets of Authorized Deposit-taking Institutions (ADIs) appear to be increasing with a slight dip around 14000 days since September 1, 1969. That would be around the year 2008 (14000 days /360 days per year + 1969).\nBe careful when making a graph. If the vertical axis doesn’t start at 0, then the change can look much more dramatic than it really is. For a graph to be useful to the reader, it needs to have a title that explains what the graph contains, the axes should be labeled so the reader knows what each axes represents, each axes should have a scale marked, and it is best if the vertical axis contains 0 to show the relationship.\n\n\n\n2.3.5 Homework for Other Graphical Representations of Data Section\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of one of their metacarpal bone (in cm) were collected and are in Table 2.15 (Prediction of height, 2013). Create a scatter plot of length and height and state if there is a relationship between the height of a person and the length of their metacarpal.\n\n\nMetacarpal&lt;- read.csv( \"https://krkozak.github.io/MAT160/metacarpal.csv\") \nknitr::kable(head(Metacarpal))\n\n\n\nTable 2.15: Head of Metacarpal Data frame\n\n\n\n\n\n\nlength\nheight\n\n\n\n\n45\n171\n\n\n51\n178\n\n\n39\n157\n\n\n41\n163\n\n\n48\n172\n\n\n49\n183\n\n\n\n\n\n\n\n\nCode book for Data frame Metacarpal\nDescription When anthropologists analyze human skeletal remains, an important piece of information is living stature. Since skeletons are commonly based on statistical methods that utilize measurements on small bones. The following data was presented in a paper in the American Journal of Physical Anthropology to validate one such method.\nFormat\nThis data frame contains the following columns:\nlength: length of Metacarpal I bone in mm\nheight: stature of skeleton in cm\nSource Prediction of Height from Metacarpal Bone Length. (n.d.). Retrieved July 9, 2019, from http://www.statsci.org/data/general/stature.html\nReferences Musgrave, J., and Harneja, N. (1978). The estimation of adult stature from metacarpal bone length. Amer. J. Phys. Anthropology 48, 113-120.\nDevore, J., and Peck, R. (1986). Statistics. The Exploration and Analysis of Data. West Publishing, St Paul, Minnesota.\n\nThe value of the house and the amount of rental income in a year that the house brings in are in Table 2.16 (Capital and rental 2013). Create a scatter plot and state if there is a relationship between the value of the house and the annual rental income.\n\n\nHouse&lt;- read.csv( \"https://krkozak.github.io/MAT160/house.csv\") \nknitr::kable(head(House))\n\n\n\nTable 2.16: Head of House Data frame\n\n\n\n\n\n\ncapital\nrental\n\n\n\n\n61500\n6656\n\n\n67500\n6864\n\n\n75000\n4992\n\n\n75000\n7280\n\n\n76000\n6656\n\n\n77000\n4576\n\n\n\n\n\n\n\n\nCode book for Data frame House\nDescription The data show the capital value and annual rental value of domestic properties in Auckland in 1991.\nFormat\nThis data frame contains the following columns:\nCapital: Selling price of house in Australian dollar (AUD)\nrental: rental price of a house in Australian dollar (AUD)\nSource Capital and rental values of Auckland properties. (2013, September 26). Retrieved from http://www.statsci.org/data/oz/rentcap.html\nReferences Lee, A. (1994) Data Analysis: An introduction based on R. Auckland: Department of Statistics, University of Auckland. Data courtesy of Sage Consultants Ltd.\n\nThe World Bank collects information on the life expectancy of a person in each country (\\“Life expectancy at,\\” 2013) and the fertility rate per woman in the country (\\“Fertility rate,\\” 2013). The data for countries for the year 2011 are in Table 2.17. Create a scatter plot of the data and state if there appears to be a relationship between life expectancy and the number of births per woman in 2011.\n\n\nFertility&lt;- read.csv( \"https://krkozak.github.io/MAT160/fertility.csv\") \nknitr::kable(head(Fertility))\n\n\n\nTable 2.17: Head of Fertility Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\nlifexp_2011\nfertilrate_2011\nlifexp_2000\nfertilrate_2000\nlifexp_1990\nfertilrate_1990\n\n\n\n\nMacao SAR, China\n79.91\n1.03\n77.62\n0.94\n75.28\n1.69\n\n\nHong Kong SAR, China\n83.42\n1.20\n80.88\n1.04\n77.38\n1.27\n\n\nSingapore\n81.89\n1.20\n78.05\nNA\n76.03\n1.87\n\n\nHungary\n74.86\n1.23\n71.25\n1.32\n69.32\n1.84\n\n\nKorea, Rep.\n80.87\n1.24\n75.86\n1.47\n71.29\n1.59\n\n\nRomania\n74.51\n1.25\n71.16\n1.31\n69.74\n1.84\n\n\n\n\n\n\n\n\nCode book for Data frame Fertility\nDescription Data is from the World Bank on the life expectancy of countries and the fertility rates in those countries.\nFormat\nThis data frame contains the following columns:\nCountry: Countries in the World\nlifexp_2011: Life expectancy of a person born in 2011\nfertilrate_2011: Fertility rate in the country in 2011\nlifexp_2000: Life expectancy of a person born in 2000\nfertilrate_2000: Fertility rate in the country in 2000\nlifexp_1990: Life expectancy of a person born in 1990\nfertilrate_1990: Fertility rate in the country in 1990\nSource Life expectancy at birth. (2013, October 14). Retrieved from http://data.worldbank.org/indicator/SP.DYN.LE00.IN\nReferences Data from World Bank, Life expectancy at birth, total (years)\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of women receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The data for the countries where this information is available in Table 2.18. Create a scatter plot of the health expenditure and percentage of women receiving prenatal care in the year 2000, and state if there appears to be a relationship between percentage spent on health expenditure and the percentage of women receiving prenatal care.\n\n\nFert_prenatal&lt;-read.csv( \"https://krkozak.github.io/MAT160/fertility_prenatal.csv\") \nknitr::kable(head(Fert_prenatal))\n\n\n\nTable 2.18: Head of Fert_prenatal Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry.Name\nCountry.Code\nRegion\nIncomeGroup\nf1960\nf1961\nf1962\nf1963\nf1964\nf1965\nf1966\nf1967\nf1968\nf1969\nf1970\nf1971\nf1972\nf1973\nf1974\nf1975\nf1976\nf1977\nf1978\nf1979\nf1980\nf1981\nf1982\nf1983\nf1984\nf1985\nf1986\nf1987\nf1988\nf1989\nf1990\nf1991\nf1992\nf1993\nf1994\nf1995\nf1996\nf1997\nf1998\nf1999\nf2000\nf2001\nf2002\nf2003\nf2004\nf2005\nf2006\nf2007\nf2008\nf2009\nf2010\nf2011\nf2012\nf2013\nf2014\nf2015\nf2016\nf2017\np1986\np1987\np1988\np1989\np1990\np1991\np1992\np1993\np1994\np1995\np1996\np1997\np1998\np1999\np2000\np2001\np2002\np2003\np2004\np2005\np2006\np2007\np2008\np2009\np2010\np2011\np2012\np2013\np2014\np2015\np2016\np2017\np2018\ne2000\ne2001\ne2002\ne2003\ne2004\ne2005\ne2006\ne2007\ne2008\ne2009\ne2010\ne2011\ne2012\ne2013\ne2014\ne2015\ne2016\n\n\n\n\nAngola\nAGO\nSub-Saharan Africa\nLower middle income\n7.478\n7.524\n7.563\n7.592\n7.611\n7.619\n7.618\n7.613\n7.608\n7.604\n7.601\n7.603\n7.606\n7.611\n7.614\n7.615\n7.609\n7.594\n7.571\n7.540\n7.504\n7.469\n7.438\n7.413\n7.394\n7.380\n7.366\n7.349\n7.324\n7.291\n7.247\n7.193\n7.130\n7.063\n6.992\n6.922\n6.854\n6.791\n6.734\n6.683\n6.639\n6.602\n6.568\n6.536\n6.502\n6.465\n6.420\n6.368\n6.307\n6.238\n6.162\n6.082\n6.000\n5.920\n5.841\n5.766\n5.694\n5.623\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n65.6\nNA\nNA\nNA\nNA\nNA\n79.8\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n81.6\nNA\nNA\n2.334435\n5.483823\n4.072288\n4.454100\n4.757211\n3.734836\n3.366183\n3.211438\n3.495036\n3.578677\n2.736684\n2.840603\n2.692890\n2.990929\n2.798719\n2.950431\n2.877825\n\n\nArmenia\nARM\nEurope & Central Asia\nUpper middle income\n4.786\n4.670\n4.521\n4.345\n4.150\n3.950\n3.758\n3.582\n3.429\n3.302\n3.199\n3.114\n3.035\n2.956\n2.875\n2.792\n2.712\n2.641\n2.582\n2.538\n2.510\n2.499\n2.503\n2.517\n2.538\n2.559\n2.578\n2.591\n2.592\n2.578\n2.544\n2.484\n2.400\n2.297\n2.179\n2.056\n1.938\n1.832\n1.747\n1.685\n1.648\n1.635\n1.637\n1.648\n1.665\n1.681\n1.694\n1.702\n1.706\n1.703\n1.693\n1.680\n1.664\n1.648\n1.634\n1.622\n1.612\n1.604\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n82\nNA\nNA\n92.4\nNA\nNA\nNA\nNA\n93.0\nNA\nNA\nNA\nNA\n99.1\nNA\nNA\nNA\nNA\nNA\n99.6\nNA\nNA\n6.505224\n6.536263\n5.690812\n5.610725\n8.227844\n7.034880\n5.588461\n5.445144\n4.346749\n4.689046\n5.264181\n3.777260\n6.711859\n8.269840\n10.178299\n10.117627\n9.927321\n\n\nBelize\nBLZ\nLatin America & Caribbean\nUpper middle income\n6.500\n6.480\n6.460\n6.440\n6.420\n6.400\n6.379\n6.358\n6.337\n6.316\n6.299\n6.288\n6.284\n6.285\n6.287\n6.278\n6.250\n6.195\n6.109\n5.992\n5.849\n5.684\n5.510\n5.336\n5.170\n5.019\n4.886\n4.771\n4.671\n4.584\n4.508\n4.436\n4.363\n4.286\n4.201\n4.109\n4.010\n3.908\n3.805\n3.703\n3.600\n3.496\n3.390\n3.282\n3.175\n3.072\n2.977\n2.893\n2.821\n2.762\n2.715\n2.676\n2.642\n2.610\n2.578\n2.544\n2.510\n2.475\nNA\nNA\nNA\nNA\nNA\n96\nNA\nNA\nNA\nNA\nNA\nNA\n98\n95.9\n100.0\nNA\n98\nNA\nNA\n94.0\n94.0\n99.2\nNA\nNA\nNA\n96.2\nNA\nNA\nNA\n97.2\n97.2\nNA\nNA\n3.942030\n4.228792\n3.864327\n4.260178\n4.091610\n4.216728\n4.163924\n4.568384\n4.646109\n5.311070\n5.764874\n5.575126\n5.322589\n5.727331\n5.652458\n5.884248\n6.121374\n\n\nCote d’Ivoire\nCIV\nSub-Saharan Africa\nLower middle income\n7.691\n7.720\n7.750\n7.781\n7.811\n7.841\n7.868\n7.893\n7.912\n7.927\n7.936\n7.941\n7.942\n7.939\n7.929\n7.910\n7.877\n7.828\n7.763\n7.682\n7.590\n7.488\n7.383\n7.278\n7.176\n7.078\n6.984\n6.892\n6.801\n6.710\n6.622\n6.536\n6.454\n6.374\n6.298\n6.224\n6.152\n6.079\n6.006\n5.932\n5.859\n5.787\n5.717\n5.651\n5.589\n5.531\n5.476\n5.423\n5.372\n5.321\n5.269\n5.216\n5.160\n5.101\n5.039\n4.976\n4.911\n4.846\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n83.2\nNA\nNA\nNA\nNA\n84.3\n87.6\nNA\nNA\nNA\nNA\n87.3\n84.8\nNA\nNA\nNA\nNA\nNA\n90.6\nNA\nNA\nNA\n93.2\nNA\nNA\n5.672228\n4.850694\n4.476869\n4.645306\n5.213588\n5.353556\n5.808850\n6.259154\n6.121605\n6.223329\n6.146566\n5.978840\n6.019660\n5.074942\n5.043462\n5.262711\n4.403621\n\n\nEthiopia\nETH\nSub-Saharan Africa\nLow income\n6.880\n6.877\n6.875\n6.872\n6.867\n6.864\n6.867\n6.880\n6.903\n6.937\n6.978\n7.020\n7.060\n7.094\n7.121\n7.143\n7.167\n7.195\n7.230\n7.271\n7.316\n7.360\n7.397\n7.424\n7.437\n7.435\n7.418\n7.387\n7.347\n7.298\n7.246\n7.193\n7.143\n7.094\n7.046\n6.995\n6.935\n6.861\n6.769\n6.659\n6.529\n6.380\n6.216\n6.044\n5.867\n5.690\n5.519\n5.355\n5.201\n5.057\n4.924\n4.798\n4.677\n4.556\n4.437\n4.317\n4.198\n4.081\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n26.7\nNA\nNA\nNA\nNA\n27.6\nNA\nNA\nNA\nNA\nNA\n33.9\nNA\nNA\n41.2\nNA\n62.4\nNA\nNA\n4.365290\n4.713670\n4.705820\n4.885341\n4.304562\n4.100981\n4.226696\n4.801925\n4.280639\n4.412473\n5.466372\n4.468978\n4.539596\n4.075065\n4.033651\n3.975932\n3.974016\n\n\nGuinea\nGIN\nSub-Saharan Africa\nLow income\n6.114\n6.127\n6.138\n6.147\n6.154\n6.160\n6.168\n6.177\n6.189\n6.205\n6.225\n6.249\n6.277\n6.306\n6.337\n6.369\n6.402\n6.436\n6.468\n6.500\n6.529\n6.557\n6.581\n6.602\n6.619\n6.631\n6.637\n6.637\n6.631\n6.618\n6.598\n6.570\n6.535\n6.493\n6.444\n6.391\n6.334\n6.273\n6.211\n6.147\n6.082\n6.015\n5.947\n5.877\n5.804\n5.729\n5.653\n5.575\n5.496\n5.417\n5.336\n5.256\n5.175\n5.094\n5.014\n4.934\n4.855\n4.777\nNA\nNA\nNA\nNA\nNA\nNA\n57.6\nNA\nNA\nNA\nNA\nNA\nNA\n70.7\nNA\nNA\nNA\n84.3\nNA\n82.2\nNA\n88.4\nNA\nNA\nNA\nNA\n85.2\nNA\nNA\nNA\n84.3\nNA\nNA\n3.697726\n3.884610\n4.384152\n3.651081\n3.365547\n2.949490\n2.960601\n3.013074\n2.762090\n2.936868\n3.067742\n3.789550\n3.503983\n3.461137\n4.780977\n5.827122\n5.478273\n\n\n\n\n\n\n\n\nCode book for Data frame Fert_prenatal\nDescription Data is from the World Bank on money spent on expenditure of countries and the percentage of women receiving prenatal care in those countries.\nFormat\nThis data frame contains the following columns:\nCountry.Name: Countries around the world\nCountry.Code: Three letter country code for countries around the world\nRegion: Location of a country around the world as classified by the World Bank\nIncomeGroup: The income level of a country as classified by the World Bank\nf1960-f2017: Fertility rate of a country from 1960-2017\np1986-p2018: Percentage of women receiving prenatal care in the country in 1986-2018\ne200-2016: Expenditure amounts of the countries for medical care in 2000-2016 (% of GDP)\nSource Fertility rate, total (births per woman). (n.d.). Retrieved July 8, 2019, from https://data.worldbank.org/indicator/SP.DYN.TFRT.IN Pregnant women receiving prenatal care (%). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/SH.STA.ANVC.ZS Current health expenditure (% of GDP). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/SH.XPD.CHEX.GD.ZS\nReferences Data from World Bank, fertility rate, expenditure on health, and pregnant woman rate of prenatal care.\n\nThe Australian Institute of Criminology gathered data on the number of deaths (per 100,000 people) due to firearms during the period 1983 to 1997 (\\“Deaths from firearms,\\” 2013). The data is in Table 2.19. Create a time-series plot of the data and state any findings you can from the graph.\n\n\nFirearm&lt;- read.csv( \"https://krkozak.github.io/MAT160/rate.csv\") \nknitr::kable(head(Firearm))\n\n\n\nTable 2.19: Head of Firearm Data frame\n\n\n\n\n\n\nyear\nrate\n\n\n\n\n1983\n4.31\n\n\n1984\n4.42\n\n\n1985\n4.52\n\n\n1986\n4.35\n\n\n1987\n4.39\n\n\n1988\n4.21\n\n\n\n\n\n\n\n\nCode book for Data Frame Firearm\nDescription The data give the number of deaths caused by firearms in Australia from 1983 to 1997, expressed as a rate per 100,000 of population.\nFormat\nThis data frame contains the following columns:\nYear: Years from 1983 to 1997\nRate: Rate of deaths caused by firearms in Australia per 100,000 population\nSource Deaths from firearms. (2013, September 26). Retrieved from http://www.statsci.org/data/oz/firearms.html\nReferences Australian Institute of Criminology, 1999.The data was contributed by Rex Boggs, Glenmore State High School, Rockhampton, Queensland, Australia.\n\nThe economic crisis of 2008 affected many countries, though some more than others. Some people in Australia have claimed that Australia wasn’t hurt that badly from the crisis. The bank assets (in billions of Australia dollars (AUD)) of the Reserve Bank of Australia (RBA) for the time period of September 1 1969, through March 1 2019, are contained in @bl-Australian (Reserve Bank of Australia, 2019). Create a time-series plot of the assets of the RBA and interpret any findings.\n\nCode book for Data Frame Australian is below Table 2.14.\n\nThe consumer price index (CPI) is a measure used by the U.S. government to describe the cost of living. The cost of living for the U.S. from the years 1913 through 2019, with the year 1982 being used as the year that all others are compared (Consumer Price Index Data from 1913 to 2019, 2019) is given in Table 2.20. Create a time-series plot of the Average Annual CPI and interpret.\n\n\nCPI&lt;- read.csv( \"https://krkozak.github.io/MAT160/CPI_US.csv\") \nknitr::kable(head(CPI))\n\n\n\nTable 2.20: Head of CPI Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJune\nJuly\nAug\nSep\nOct\nNov\nDec\nAnnual_avg\nPerDec_Dec\nPerc_Avg_Avg\n\n\n\n\n1913\n9.8\n9.8\n9.8\n9.8\n9.7\n9.8\n9.9\n9.9\n10.0\n10.0\n10.1\n10.0\n9.9\n–\n–\n\n\n1914\n10.0\n9.9\n9.9\n9.8\n9.9\n9.9\n10.0\n10.2\n10.2\n10.1\n10.2\n10.1\n10.0\n1\n1\n\n\n1915\n10.1\n10.0\n9.9\n10.0\n10.1\n10.1\n10.1\n10.1\n10.1\n10.2\n10.3\n10.3\n10.1\n2\n1\n\n\n1916\n10.4\n10.4\n10.5\n10.6\n10.7\n10.8\n10.8\n10.9\n11.1\n11.3\n11.5\n11.6\n10.9\n12.6\n7.9\n\n\n1917\n11.7\n12.0\n12.0\n12.6\n12.8\n13.0\n12.8\n13.0\n13.3\n13.5\n13.5\n13.7\n12.8\n18.1\n17.4\n\n\n1918\n14.0\n14.1\n14.0\n14.2\n14.5\n14.7\n15.1\n15.4\n15.7\n16.0\n16.3\n16.5\n15.1\n20.4\n18\n\n\n\n\n\n\n\n\nCode book for Data frame CPI\nDescription This table of Consumer Price Index (CPI) data is based upon a 1982 base of 100.\nFormat\nThis data frame contains the following columns:\nYear: Year from 1913 to 2019\nJan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec: CPI for a particular month\nAverage_Avg: The average CPI for a particular year\nPerDec_Dec: Percent change from December to December\nPer_Avg_Avg: Percent change from Annual Average to Annual Average\nSource Consumer Price Index Data from 1913 to 2019. (2019, June 12). Retrieved July 10, 2019, from https://www.usinflationcalculator.com/inflation/consumer-price-index-and-annual-percent-changes-from-1913-to-2008/\nReferences US Inflation Calculator website, 2019.\n\nThe mean and median incomes income in current dollars is given in Table 2.21. Create a time-series plot and interpret.\n\n\nUS_income&lt;- read.csv( \"https://krkozak.github.io/MAT160/US_income.csv\") \nknitr::kable(head(US_income))\n\n\n\nTable 2.21: Head of US_income Data frame\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nyear\nnumber\nmed_income_current\nmed_income_2017\nmean_income_current\nmean_income_2017\n\n\n\n\n2017\n127586\n61372\n61372\n86220\n86220\n\n\n2016\n126224\n59039\n60309\n83143\n84931\n\n\n2015\n125819\n56516\n58476\n79263\n82012\n\n\n2014\n124587\n53657\n55613\n75738\n78500\n\n\n2013\n122952\n51939\n54744\n72641\n76565\n\n\n2012\n122459\n51017\n54569\n71274\n76237\n\n\n\n\n\n\n\n\nCode book for Data Frame US_income\nDescription This table is of US mean and median incomes in both current dollars and in 2017 dollars.\nFormat\nThis data frame contains the following columns:\nYear: Year from 1975 to 2017\nnumber: Households as of March of the following year. (in thousands)\nmed_income_current: median income of a US household in current dollars\nmed_income_2017: median income of a US household in 2017 CPI-U-RS adjusted dollars\nmean_income_current: mean income of a US household in current dollars\nmean_income_2017: mean income of a US household in 2017 CPI-U-RS adjusted dollars\nSource US Census Bureau. (2018, March 06). Data. Retrieved July 21, 2019, from https://www.census.gov/programs-surveys/cps/data-detail.html\nReferences U.S. Census Bureau, Current Population Survey, Annual Social and Economic Supplements.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Graphical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical-description-of-Data.html",
    "href": "Numerical-description-of-Data.html",
    "title": "3  Numerical Description of Data",
    "section": "",
    "text": "3.1 Measures of Center\nThis section focuses on measures of central tendency. Many times you are asking what to expect on average. Such as when you pick a major, you would probably ask how much you expect to earn in that field. If you are thinking of relocating to a new town, you might ask how much you can expect to pay for housing. If you are planting vegetables in the spring, you might want to know how long it will be until you can harvest. These questions, and many more, can be answered by knowing the center of the data set. There are three measures of the “center” of the data. They are the mode, median, and mean. Any of the values can be referred to as the “average.”\nThe mode is the data value that occurs the most frequently in the data. To find it, you count how often each data value occurs, and then determine which data value occurs most often. The mode is not the most useful measure of center. This is because, a data set can have more than one mode. If there is a tie between two values for the most number of times then both values are the mode and the data is called bimodal (two modes). If every data point occurs the same number of times, there is no mode. If there are more than two numbers that appear the most times, then usually there is no mode.\nThe median is the data value in the middle of a sorted list of data. To find it, you put the data in order, and then determine which data value is in the middle of the data set.\nThe mean is the arithmetic average of the numbers. This is the center that most people call the average, though all three – mean, median, and mode – really are averages.\nThere are no symbols for the mode and the median, but the mean is used a great deal, and statisticians gave it a symbol. There are actually two symbols, one for the population parameter and one for the sample statistic. In most cases you cannot find the population parameter, so you use the sample statistic to estimate the population parameter.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical-description-of-Data.html#measures-of-center",
    "href": "Numerical-description-of-Data.html#measures-of-center",
    "title": "3  Numerical Description of Data",
    "section": "",
    "text": "3.1.1 Population Mean\n\\(\\mu=\\frac{\\sum{x}}{N}\\), pronounced mu\nN is the size of the population.\nx represents a data value.\n\\(\\sum{x}\\) means to add up all of the data values.\n\n\n3.1.2 Sample Mean\n\\(\\bar{x}=\\frac{\\sum{x}}{n}\\), pronounced x bar.\nn is the size of the sample.\nx represents a data value.\n\\(\\sum{x}\\) means to add up all of the data values.\nThe value for \\(\\bar{x}\\) is used to estimate \\(\\mu\\) since \\(\\mu\\) can’t be calculated in most situations.\n\n\n3.1.3 Example: Finding the Mean and Median using Rguroo\nSuppose a vet wants to find the average weight of cats. The weights (in kg) of cats are in Table 3.1.\n\n\n\n\nTable 3.1: Head of Cats\n\n\n\n\n\n\nSex\nBwt\nHwt\n\n\n\n\nF\n2.0\n7.0\n\n\nF\n2.0\n7.4\n\n\nF\n2.0\n9.5\n\n\nF\n2.1\n7.2\n\n\nF\n2.1\n7.3\n\n\nF\n2.1\n7.6\n\n\n\n\n\n\n\n\n\nClick to expand the box below to see instructions to import and view the cats dataset in your Data Toolbox in Rguroo.\n\n\n\n\n\n\n Importing cats to Data Toolbox\n\n\n\n\n\n\nGo to the Data toolbox.\nFrom the Data Import dropdown, select Dataset Repository.\nIn the top search box, type MASS, then select the MASS repository.\nIn the middle search box, type cats, and select the cats dataset that appears in the lower panel.\n\nClick the Import button. The dataset will be imported into your Rguroo account.\nClick the Close button to close the Rguroo dialog.\nTo view the dataset, double-click cats under the Data toolbox list.\n\n\n\n\nFind the mean and median of the weight of a cat.\n\n3.1.3.1 Solution\nBefore starting any mathematics problem, it is always a good idea to define the unknown in the problem. In statistics, you want to define the variable. The symbol for the variable is *x*.\nThe variable is x = weight of a cat\nClick to expand the box below see how to calculate the mean and median in Rguroo.\n\n\n\n\n\n\n Calculating the Mean and Median Weight of Cats\n\n\n\n\n\nBefore you begin: Make sure you have already imported the cats dataset into your Data Toolbox, as was shown here.\n\nOpen the Data toolbox.\n\nClick on the Functions dropdown, and select Summary Statistic. This opens the Basic Summary Statistic dialog.\n\nFrom the Dataset dropdown, select the cats dataset.\n\nFrom the Numerical dropdown, select the Bwt variable.\n\nIn the Statistics section of the dialog, select the checkbox for Mean and Median.\n\nClick the preview icon  to see the summary statistics.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nBasic Summary Statistic dialog in Rguroo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.1: Mean and Median Weight of Cats\n\n\n\n\n\nThe mean weight is 2.72 kg.\n\nThe median weight is 2.7 kg also. It appears the average weight is 2.7 kg of all cats.\n\n\n\n3.1.4 Example: Finding Mean and Median with Factor\nLooking at the data frame for cats weights Table 3.1 you see that there are several variables You may want to know what the other variables are. A Code Book describes the data set, explains what the variables are including the units, and the source of the data frame. The code book for the cats is below.\nImage 3.1.1: Code book for cats data frame\n\n\n\nCode book for cats data frame\n\n\nSuppose you want to know if male cats weigh more than female cats. Looking at the variables, you notice that there is a variable for the sex of the cat. You can look at the weights of males and females separately. This looks like:\n\n3.1.4.1 Solution\nClick to expand the box below see how to calculate the mean and median, separated by sex, in Rguroo.\n\n\n\n\n\n\n Calculating the Mean and Median Weight of Cats by Sex\n\n\n\n\n\nBefore you begin: Make sure you have already imported the cats dataset into your Data Toolbox, as was shown here.\n\nOpen the Data toolbox.\n\nClick on the Functions dropdown, and select Summary Statistic. This opens the Basic Summary Statistic dialog.\n\nFrom the Dataset dropdown, select the cats dataset.\n\nFrom the Numerical dropdown, select the Bwt variable.\nFrom the Factor 1 dropdown, select the Sex factor.\nIn the Statistics section of the dialog, select the checkbox for Mean and Median.\n\nClick the preview icon  to see the summary statistics.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nBasic Summary Statistic dialog in Rguroo\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3.2: Mean and Median Weight of Cats by Sex\n\n\n\n\n\nNotice that the female cats’ mean weight is 2.4 kg and the male cats’ mean weight is 2.9 kg. The median weight of female cats is 2.3 kg and for males it is 2.9 kg. So it does appear that male cats weigh a bit more than the female cats.\nThere are many different summary statistics that can be found. An example is the minimum and maximum value. In this example, you will see how to find the min and max values and then filter them out of a data set to see what effect they have on the mean and median.\n\n\n\n3.1.5 Example: Effect of Extreme Values on Mean and Median\nFind the minimum and maximum values of cats weights.\n\n3.1.5.1 Solution\nThe steps to find the minimum and the maximum in Rguroo are similar to finding the mean and median. In the Statistics section of the dialog, select the checkbox for Minimum and Maximum.\nHere is the Basic Summary Statistic Dialog and Output from Rguroo for the minimum and maximum of cats body weight.\n\n\n\n\n\n\n\n\n\n\n\n(a) Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 3.3: Maximum and Minimum Weight of Cats\n\n\n\n\nThe minimum weight of a cat in this data frame is 2 kg and the maximum weight of a cat is 3.9 kg.\nYou can create two new data sets in Rguroo by subsetting (filtering). One data set will exclude the maximum value. Click to expand the box below to see how to subset a dataset in Rguroo:\n\n\n\n\n\n\n Subsetting the cats Dataset\n\n\n\n\n\nBefore you begin: Make sure you have already imported the cats dataset into your Data Toolbox, as was shown here.\n\nOpen the Data toolbox.\n\nClick on the Functions dropdown, and select Subset. This opens the Data Subset dialog.\n\nFrom the Dataset dropdown, select the cats dataset.\n\nSelect the Logical Expression button.\n\nTo create a new Logical Expression, click the green plus icon.\nFrom the Variable dropdown, select the Bwt. From the Op. (operations) dropwdown, select the less than symbol (&lt;). In the Value column, enter 3.9 (the maximum body weight of cats dataset). Click the Done.\nWhen you are done with your selection, click the preview icon  to see a preview of the scatterplot.\nIn Save As… name the new data set cats_nomax and click the Save As….\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n  \n\n\n\n\n\n\nThen create a data set that excludes the minimum value using the same steps shown here. In the Logical Expression dialog, choose the greater than (&gt;) operator in the Op. dropdown and enter 2 in the Value column. Save this new dataset as cats_nomin.\n\nNow you can find the mean and median of each new data set using the steps described here :\nThe mean without the maximum value is 2.70 kg, and the median is 2.7 kg.\nThe mean without the minimum value is 2.75 kg, and the median is 2.7 kg.\nFrom Example: Effect of Extreme Values on Mean and Median, the mean of the cats data set with all the values is 2.72 kg where the median is 2.7 kg. Notice that when the maximum value was excluded from the data set, the mean decreased a little but the median didn’t change, and when the minimum value was excluded from the data set, the mean increased a little but the median didn’t change. The mean is much higher than the median. Why is this? This is because the mean is affected by extreme values, while the median is not. We say the median is a resistant measure of center because it isn’t affected by extreme values as much.\nAn outlier is a data value that is very different from the rest of the data. It can be really high or really low. Extreme values may be an outlier if the extreme value is far enough from the center. If there are extreme values in the data, the median is a better measure of the center than the mean. If there are no extreme values, the mean and the median will be similar so most people use the mean. The mean is not a resistant measure because it is affected by extreme values. The median is a resistant measure because it not affected by extreme values.\nAs a consumer you need to be aware that people choose the measure of center that best supports their claim. When you read an article in the newspaper and it talks about the “average” it usually means the mean but sometimes it refers to the median. Some articles will use the word “median” instead of “average” to be more specific. If you need to make an important decision and the information says “average”, it would be wise to ask if the “average” is the mean or the median before you decide.\nAs an example, suppose that a company wants to use the mean salary as the average salary for the company. This is because the high salaries of the administrators will pull the mean higher. The company can say that the employees are paid well because the average is high. However, the employees want to use the median since it discounts the extreme values of the administration and will give a lower value of the average. This will make the salaries seem lower and that a raise is in order.\nWhy use the mean instead of the median? The reason is because when multiple samples are taken from the same population, the sample means tend to be more consistent than other measures of the center.\nTo understand how the different measures of center relate to skewed or symmetric distributions, see Figure 3.4. As you can see sometimes the mean is smaller than the median, sometimes the mean is larger than the median, and sometimes they are the same values.\nFigure 3.4: Mean, Median, Mode as Related to a Distribution\n\n\n\n\n\n\n\n\nFigure 3.4: Mean, median, mode as related to distribution\n\n\n\n\n\nOne last type of average is a weighted average. Weighted averages are used quite often in different situations. Some teachers use them in calculating a student’s grade in the course, or a grade on a project. Some employers use them in employee evaluations. The idea is that some activities are more important than others. As an example, a full time teacher at a community college may be evaluated on their service to the college, their service to the community, whether their paperwork is turned in on time, and their teaching. However, teaching is much more important than whether their paperwork is turned in on time. When the evaluation is completed, more weight needs to be given to the teaching and less to the paperwork. This is a weighted average.\n\n\n\n3.1.6 Weighted Average\n\\(\\text{weighted average}=\\frac{\\sum{x*w}}{\\sum{w}}\\)\nwhere w is the weight of the data value, x.\n\n\n3.1.7 Example: Weighted Average\nIn your biology class, your final grade is based on several things: a lab score, scores on two major tests, and your score on the final exam. There are 100 points available for each score. The lab score is worth 15% of the course, the two exams are worth 25% of the course each, and the final exam is worth 35% of the course. Suppose you earned scores of 95 on the labs, 83 and 76 on the two exams, and 84 on the final exam. Compute your weighted average for the course.\n\n3.1.7.1 Solution\nVariable: x = score\nA weighted average can be found using Rguroo. First, we will create a new dataset in Rguroo. Click to expand the box below to see how to create a new dataset in Rguroo:\n\n\n\n\n\n\n Creating a New Dataset\n\n\n\n\n\n\nOpen the Data toolbox.\n\nClick on the Data Import dropdown, and select Create New Dataset. This opens the Create New Dataset dialog. Specify the number of rows and columns for your new dataset.\n\nThe default variable names will be Var1, Var2, etc. To change the variable name, move your cursor over the variable name on the header. You will see the Variable Context Menu icon. Click on the icon to open the variable context menu. From the menu select the option Rename. This will bring up the Rename Variable dialog box. In the dialog box, type in the new name for your variable, and press enter. The new name will now appear on the header. Note that variable names cannot have spaces.\n\nEnter your data.\n\nEnter a name for your dataset in the Save As textbox on the top, and click the Save as button to save your dataset.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog and new dataset\n\n\n\n\n\n \n\n\n\n\n\n\n\nTo calculate the weighted average in Rguroo, you will follow the same steps described here. In the Basic Summary Statistics dialog box select the biology_grade from the dataset dropdown, select score in the Numerical dropdown, and select weight in the frequency dropdown as shown in figure Figure 3.5.\n\n\n\n\n\n\n\n\n\n\n\n(a) Basic Summary Statistics Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 3.5: Weighted Mean of Biology Grade\n\n\n\n\nYour weighted mean in the biology class is 83.4%. Using the traditional grading scale, you have a B in the class.\n\n\n\n3.1.8 Example: Weighted Average\nThe faculty evaluation process at John Jingle University rates a faculty member on the following activities: teaching, publishing, committee service, community service, and submitting paperwork in a timely manner. The process involves reviewing student evaluations, peer evaluations, and supervisor evaluation for each teacher and awarding him/her a score on a scale from 1 to 10 (with 10 being the best). The weights for each activity are 20 for teaching, 18 for publishing, 6 for committee service, 4 for community service, and 2 for paperwork.\n\nOne faculty member had the following ratings: 8 for teaching, 9 for publishing, 2 for committee work, 1 for community service, and 8 for paperwork. Compute the weighted average of the evaluation.\nAnother faculty member had ratings of 6 for teaching, 8 for publishing, 9 for committee work, 10 for community service, and 10 for paperwork. Compute the weighted average of the evaluation.\nWhich faculty member had the higher average evaluation?\n\n\n3.1.8.1 Solution\n\nOne faculty member had the following ratings: 8 for teaching, 9 for publishing, 2 for committee work, 1 for community service, and 8 for paperwork. Compute the weighted average of the evaluation.\n\nVariable: x = rating, w = weight\nTo find the weighted average using Rguroo, start by creating a new dataset as described here. The new dataset is shown in the figure Figure 3.6 below.\n\n\n\n\n\n\n\n\nFigure 3.6: Faculty Ratings Dataset\n\n\n\n\n\nCalculate the weighted average in Rguroo using the same steps described here. In the Basic Summary Statistics dialog box select the faculty_ratings from the dataset dropdown, select faculty_a in the Numerical dropdown, and select weight in the frequency dropdown as shown in figure Figure 3.7.\n\n\n\n\n\n\n\n\n\n\n\n(a) Basic Summary Statistics Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 3.7: Weighted Mean of Faculty Ratings\n\n\n\n\nThe weighted average is 7.08.\n\nAnother faculty member had ratings of 6 for teaching, 8 for publishing, 9 for committee work, 10 for community service, and 10 for paperwork. Compute the weighted average of the evaluation.\n\nThe weighted mean can be calculated in the same fasion as part (a). In the Basic Summary Statistics dialog box select the faculty_ratings from the dataset dropdown, select faculty_b in the Numerical dropdown, and select weight in the frequency dropdown as shown in figure\n\n\n\n\n\n\n\n\n\n\n\n(a) Basic Summary Statistics Dialog box\n\n\n\n\n\n\n\n\n\n\n\n(b) Output results\n\n\n\n\n\n\nFigure 3.8: Weighted Mean of Faculty Ratings\n\n\n\n\nThe weighted average for this employee is 7.56.\n\nWhich faculty member had the higher average evaluation?\nThe second faculty member has a higher average evaluation.\n\nThe last thing to mention is which average is used on which type of data.\nMode can be found on nominal, ordinal, interval, and ratio data, since the mode is just the data value that occurs most often. You are just counting the data values.\nMedian can be found on ordinal, interval, and ratio data, since you need to put the data in order. As long as there is order to the data you can find the median.\nMean can be found on interval and ratio data, since you must have numbers to add together.\n\n\n\n3.1.9 Homework for Measures of Center Section\nUse Rguroo on all problems. State the variable on all problems.\n\nBefore you begin: Make sure you have already imported the cholesterol dataset into your Data Toolbox, as was shown here by selecting the Statistics Using Technology - Kozak as shown in the figure Figure 3.9. Cholesterol levels were collected from patients a certain number of days after they had a heart attack and are in Table 3.2. Find the mean and median for cholesterol levels 2 days after the heart attack.\n\n\n\n\n\n\n\n\n\nFigure 3.9: Repository Search Dialog\n\n\n\n\n\n\n\n\n\nTable 3.2: Head of Cholesterol Levels of Patients After Heart Attack\n\n\n\n\n\n\npatient\nday2\nday4\nday14\n\n\n\n\n1\n270\n218\n156\n\n\n2\n236\n234\nNA\n\n\n3\n210\n214\n242\n\n\n4\n142\n116\nNA\n\n\n5\n280\n200\nNA\n\n\n6\n272\n276\n256\n\n\n\n\n\n\n\n\nCode book for Data Frame Cholesterol\nDescription Cholesterol levels were collected from patients a certain number of days after they had a heart attack\nThis data frame contains the following columns:\nPatient: Patient number\nday2: Cholesterol level of patient 2 days after heart attack. (mg/dL)\nday4: Cholesterol level of patient 4 days after heart attack. (mg/dL)\nday14: Cholesterol level of patient 14 days after heart attack. (mg/dL)\nSource Ryan, B. F., Joiner, B. L., & Ryan, Jr, T. A. (1985). Cholesterol levels after heart attack. Retrieved from here\nReferences Ryan, Joiner & Ryan, Jr, 1985\n\nBefore you begin: Make sure you have already imported the length dataset into your Data Toolbox located in the Statistics Using Technology - Kozak. The lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 3.3 (Lee, 1994). Find the mean and median length of rivers that flow into the Pacific Ocean and the mean and median length of rivers that flow into the Tasman Sea.\n\n\n\n\n\nTable 3.3: Head of Length of New zealand rivers (km)\n\n\n\n\n\n\nriver\nlength\nflowsto\n\n\n\n\nClarence\n209\nPacific\n\n\nConway\n48\nPacific\n\n\nWaiau\n169\nPacific\n\n\nHurunui\n138\nPacific\n\n\nWaipara\n64\nPacific\n\n\nAshley\n97\nPacific\n\n\n\n\n\n\n\n\nCode book for data frame Length\nDescription Rivers in New Zealand, the lengths of river and what body of water the river flows into\nThis data frame contains the following columns:\nRiver: Name of the river\nlength: how long the river is in kilometers\nflowsto: what body of water the river flows into Pacific Ocean is Pacific and the Tasman Sea is Tasman\nSource Lee, A. (1994). Data analysis: An introduction based on r. Auckland. Retrieved from here\nReferences Lee, A. (1994). Data analysis: An introduction based on r. Auckland.\n\nBefore you begin: Make sure you have already imported the pay dataset into your Data Toolbox, located in the Statistics Using Technology - Kozak. Print-O-Matic printing company’s employees have salaries that are contained in Table 3.4.\n\n\n\n\n\nTable 3.4: Head of Salaries of Print-O-Matic Printing Company Employees\n\n\n\n\n\n\nemployee\nsalary\n\n\n\n\nCEO\n272500\n\n\nDriver\n58456\n\n\nCD74\n100702\n\n\nCD65\n57380\n\n\nEmbellisher\n73877\n\n\nFolder\n65270\n\n\n\n\n\n\n\n\nCode book for data frame Pay\nDescription Salaries of Print-O-Matic printing company’s employees\nThis data frame contains the following columns:\nemployee:employees position in the company\nsalary: salary of that employee (Australian dollars (AUD))\nSource John Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nReferences John Matic (2013)\n\nFind the mean and median.\nFind the mean and median with the CEO’s salary removed.\nWhat happened to the mean and median when the CEO’s salary was removed? Why?\nIf you were the CEO, who is answering concerns from the union that employees are underpaid, which average (mean or median) using the complete data set of the complete data set would you prefer? Why?\nIf you were a platen worker, who believes that the employees need a raise, which average (mean or median) using the complete data set would you prefer? Why?\n\n\n\nBefore you begin: Make sure you have already imported the cost dataset into your Data Toolbox, located in the Statistics Using Technology - Kozak. Print-O-Matic printing company spends specific amounts on fixed costs every month. The costs of those fixed costs are in a Table 3.5.\n\n\n\n\n\nTable 3.5: Fixed Costs for Print-O-Matic Printing Company\n\n\n\n\n\n\ncharges\ncost\n\n\n\n\nBank charges\n482\n\n\nCleaning\n2208\n\n\nComputer expensive\n2471\n\n\nLease payments\n2656\n\n\nPostage\n2117\n\n\nUniforms\n2600\n\n\n\n\n\n\n\n\nCode book for data frame Cost\nDescription fixed monthly charges for Print-0-Matic printing company\nThis data frame contains the following columns:\ncharges: Categories of monthly fixed charges\ncost: fixed month costs (AUD)\nSource John Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nReferences John Matic (2013)\n\nFind the mean and median.\nFind the mean and median with the bank charges removed.\nWhat happened to the mean and median when the bank charges was removed? Why?\nIf it is your job to oversee the fixed costs, which average (mean or median) using the complete data set would you prefer to use when submitting a report to administration to show that costs are low? Why?\nIf it is your job to find places in the budget to reduce costs, which average (mean or median) using the complete data set would you prefer to use when submitting a report to administration to show that fixed costs need to be reduced? Why?\n\n\n\nLooking at graph 3.1.2, state if the graph is skewed left, skewed right, or symmetric and then state which is larger, the mean or the median?\n\nGraph 3.1.2: Skewed or Symmetric Graph\n\n\n\nGraph 3.1.2\n\n\n\nLooking at graph 3.1.3, state if the graph is skewed left, skewed right, or symmetric and then state which is larger, the mean or the median?\n\nGraph 3.1.3: Skewed or Symmetric Graph\n\n\n\nGraph 3.1.3\n\n\n\nAn employee at Coconino Community College (CCC) is evaluated based on goal setting and accomplishments toward the goals, job effectiveness, competencies, and CCC core values. Suppose for a specific employee, goal 1 has a weight of 30%, goal 2 has a weight of 20%, job effectiveness has a weight of 25%, competency 1 has a weight of 4%, competency 2 has a weight of 3%, competency 3 has a weight of 3%, competency 4 has a weight of 3%, competency 5 has a weight of 2%, and core values has a weight of 10%. Suppose the employee has scores of 3.0 for goal 1, 3.0 for goal 2, 2.0 for job effectiveness, 3.0 for competency 1, 2.0 for competency 2, 2.0 for competency 3, 3.0 for competency 4, 4.0 for competency 5, and 3.0 for core values. Find the weighted average score for this employee. If an employee has a score less than 2.5, they must have a Performance Enhancement Plan written. Does this employee need a plan?\nAn employee at Coconino Community College (CCC) is evaluated based on goal setting and accomplishments toward goals, job effectiveness, competencies, CCC core values. Suppose for a specific employee, goal 1 has a weight of 20%, goal 2 has a weight of 20%, goal 3 has a weight of 10%, job effectiveness has a weight of 25%, competency 1 has a weight of 4%, competency 2 has a weight of 3%, competency 3 has a weight of 3%, competency 4 has a weight of 5%, and core values has a weight of 10%. Suppose the employee has scores of 2.0 for goal 1, 2.0 for goal 2, 3.0 for goal 3, 2.0 for job effectiveness, 2.0 for competency 1, 3.0 for competency 2, 2.0 for competency 3, 3.0 for competency 4, and 4.0 for core values. Find the weighted average score for this employee. If an employee that has a score less than 2.5, they must have a Performance Enhancement Plan written. Does this employee need a plan?\nA statistics class has the following activities and weights for determining a grade in the course: test 1 worth 15% of the grade, test 2 worth 15% of the grade, test 3 worth 15% of the grade, homework worth 10% of the grade, semester project worth 20% of the grade, and the final exam worth 25% of the grade. If a student receives an 85 on test 1, a 76 on test 2, an 83 on test 3, a 74 on the homework, a 65 on the project, and a 79 on the final, what grade did the student earn in the course?\nA statistics class has the following activities and weights for determining a grade in the course: test 1 worth 15% of the grade, test 2 worth 15% of the grade, test 3 worth 15% of the grade, homework worth 10% of the grade, semester project worth 20% of the grade, and the final exam worth 25% of the grade. If a student receives a 92 on test 1, an 85 on test 2, a 95 on test 3, a 92 on the homework, a 55 on the project, and an 83 on the final, what grade did the student earn in the course?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical-description-of-Data.html#measures-of-spread",
    "href": "Numerical-description-of-Data.html#measures-of-spread",
    "title": "3  Numerical Description of Data",
    "section": "3.2 Measures of Spread",
    "text": "3.2 Measures of Spread\nVariability is an important idea in statistics. If you were to measure the height of everyone in your classroom, every observation gives you a different value. That means not every student has the same height. Thus there is variability in people’s heights. If you were to take a sample of the income level of people in a town, every sample gives you different information. There is variability between samples too. Variability describes how the data are spread out. If the data are very close to each other, then there is low variability. If the data are very spread out, then there is high variability. How do you measure variability? It would be good to have a number that measures it. This section will describe some of the different measures of variability, also known as variation.\nIn [Example: Finding the Mean and Median using Rguroo], the average weight of a cat was calculated to be 2.72 kg. How much does this tell you about the weight of all cats? Can you tell if most of the weights were close to 2.72 kg or were the weights really spread out? The highest weight and the lowest weight are known, but is there more that you can tell? All you know is that the center of the weights is 2.72 kg.\nYou need more information.\nThe range of a set of data is the difference between the highest and the lowest data values (or maximum and minimum values). The interval is the lowest and highest values. The range is one value while the interval is two.\n\n3.2.1 Example: Range\nFrom Example: Effect of Extreme Values on Mean and Median, the maximum is 3.9 kg and the minimum is 2 kg. So the range is \\(3.9-2=1.9 kg\\). But what does that tell you? You don’t know if the weights are really spread out, or if they are close together.\nUnfortunately, range doesn’t really provide a very accurate picture of the variability. A better way to describe how the data is spread out is needed. Instead of looking at the distance the highest value is from the lowest how about looking at the distance each value is from the mean. This distance is called the deviation. You might want to find the average of the deviation. Though the calculation for finding the average deviation is not very straight forward, you end up with a value called the variance. The symbol for the population variance is \\(\\sigma^2\\), and it is the average squared distance from the mean. Statisticians like the variance, but many other people who work with statistics use a descriptive statistic which is the square root of the variance. This gives you the average distance from the mean. This is called the standard deviation, and the population standard deviation is denoted with the letter \\(\\sigma\\).\nThe standard deviation is the average (mean) distance from a data point to the mean. It can be thought of as how much a typical data point differs from the mean.\nThe sample variance formula: \\(s^2=\\frac{\\sum\\left(x-\\bar{x}\\right)^2}{n-1}\\), where \\(\\bar{x}\\) is the sample mean, \\(n\\) is the sample size, and \\(\\sum{}\\) means to find the sum of the values. The \\(n-1\\) on the bottom has to do with a concept called degrees of freedom. Basically, it makes the sample variance a better approximation of the population variance.\nThe sample standard deviation formula: \\(s=\\sqrt{ \\frac{\\sum\\left(x-\\bar{x}\\right)^2}{n-1}}\\).\nThe population variance formula: \\(\\sigma^2 = \\frac{\\sum\\left(x-\\mu \\right)^2}{N}\\), where \\(\\sigma\\) is the Greek letter sigma and \\(\\sigma^2\\) represents the population variance, \\(\\mu\\) is the population mean, and \\(N\\) is the size of the population.\nThe population standard deviation formula: \\(\\sigma =\\sqrt{ \\frac{\\sum\\left(x-\\mu \\right)^2}{N}}\\)\nBoth the sample variance and sample standard deviation can be found using technology. If using Rguroo, you would perform the same steps as described here and select standard deviation and variance as shown in the figure ?fig-sd-dialog. {r,echo=FALSE #| fig-alt: \"Dialog box for standard deviation and variance\" #| warning: FALSE #| label: fig-sd-dialog #| fig-cap: \"Basic Summary Statistic Dialog for Standard Deviation and Variance \" #| out-width: \"70%\" knitr::include_graphics(\"Rguroo_dialogs/Numerical_Description/sd_var_cats_dialog.png\")\n\nThe next example will demonstrate this command.\n\n\n3.2.2 Example: Finding the Standard Deviation\nFor the data frame Cats Table 3.1 find the variance and standard derivation for weight of cats. Then find the variance and standard deviation separated by sex of the cat.\n\n3.2.2.1 Solution\nThe variance and standard deviation for all cats is found by performing the command:\n\n\n\n\n\n\n\n\nFigure 3.10: Standard Deviation and Variance of Weights of Cats\n\n\n\n\n\nThe variance for all cats is 0.24 \\(kg^2\\) and the standard deviation is 0.49 kg.\nTo find out the mean, variance, and standard deviation for each sex of the cats, use the command:\nYou can see that the mean weight of females cats is 2.36 kg, the variance is 0.075 \\(kg^2\\), and the standard deviation is 0.27 kg. For males cats, the mean is 2.9 kg, the variance is 0.22 \\(kg^2\\), and the standard deviation is 0.47 kg. This means that female cats weigh less than males and since the variance and standard deviations are much less for female cats than males cats, female cats’ weights are more consistent than male cats.\nIn general a “small” variance and standard deviation means the data is close together (more consistent) and a “large” variance and standard deviation means the data is spread out (less consistent). Sometimes you want consistent data and sometimes you don’t. As an example if you are making bolts, you want the lengths to be very consistent so you want a small standard deviation. If you are administering a test to see who can be a pilot, you want a large standard deviation so you can tell who are the good pilots and who are the not so good pilots.\nWhat do “small” and “large” standard deviation mean? To a bicyclist whose average speed is 20 mph, \\(s = 20 mph\\) is huge. To an airplane whose average speed is 500 mph, \\(s = 20 mph\\) is nothing. The “size” of the variation depends on the size of the numbers in the problem and the mean. Another situation where you can determine whether a standard deviation is small or large is when you are comparing two different samples such as in Example: Finding the Standard Deviation. A sample with a smaller standard deviation is more consistent than a sample with a larger standard deviation.\nMany other books and authors stress that there is a computational formula for calculating the standard deviation. However, this formula doesn’t give you an idea of what standard deviation is and what you are doing. It is only good for doing the calculations quickly. It goes back to the days when standard deviations were calculated by hand, and the person needed a quick way to calculate the standard deviation. It is an archaic formula that this author is trying to eradicate. It is not necessary anymore, computers will do the calculations for you with as much meaning as this formula gives. It is suggested that you never use it. If you want to understand what the standard deviation is doing, then you should use the definition formula. If you want an answer quickly, use a computer.\n\n\n\n3.2.3 Use of Standard Deviation\nOne of the uses of the standard deviation is to describe how a population is distributed. This describes where much of the data is for most distributions. A general rule is that about 95% of the data is within 2 standard deviations of the mean. This is not perfect, but is works for many distributions. There are rules like the empirical rule and Chebyshev’s theorem that give you more detailed percentages, but 95% in 2 standard deviations is a very good approximation.\n\n\n3.2.4 Example: the general rule\nThe U.S. Weather Service has provided the information in Table 3.6 about the total monthly/annual number of reported tornadoes in Oklahoma for the years 1950 to 2018. (US Department of Commerce & Noaa, 2016)\n\nTornado&lt;-read.csv(\"https://krkozak.github.io/MAT160/Tornado_OK.csv\") \nknitr::kable(head(Tornado))\n\n\n\nTable 3.6: Monthly/Annual Number of tornadoes in Oklahoma\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\nAnnual\n\n\n\n\n1950\n0\n1\n1\n5\n12\n1\n0\n0\n2\n1\n0\n0\n23\n\n\n1951\n0\n2\n0\n11\n11\n11\n4\n2\n1\n1\n0\n0\n43\n\n\n1952\n0\n0\n0\n7\n5\n5\n4\n1\n0\n0\n0\n0\n22\n\n\n1953\n0\n4\n7\n9\n8\n13\n4\n2\n0\n0\n5\n2\n54\n\n\n1954\n0\n0\n7\n13\n19\n4\n4\n2\n3\n1\n0\n0\n53\n\n\n1955\n1\n1\n0\n15\n32\n22\n4\n2\n0\n0\n0\n0\n77\n\n\n\n\n\n\n\n\nCode book for data frame Tornado\nDescription The U.S. Weather Service has collected data on the monthly and annual number of tornadoes in Oklahoma.\nThis data frame contains the following columns:\nYear: Year from 1950-2018\nJan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec: Tornado numbers in each moth of the year\nAnnual: Total number of tornadoes for each year\nSource US Department of Commerce, & Noaa. (2016, November 15). 1950 Oklahoma Tornadoes. Retrieved from https://www.weather.gov/oun/tornadodata-ok-1950\nReferences The data was supplied by The U.S. Weather Service\nFind the general interval that contains about 95% of the data.\n\n3.2.4.1 Solution\nVariable: \\(x\\) = number of annual tornadoes in Oklahoma\nFind the mean and standard deviation:\n\ndf_stats(~Annual, data=Tornado, mean, sd)\n\n  response     mean       sd\n1   Annual 56.02899 27.56061\n\n\nThe mean is \\(\\mu=56\\) tornadoes and the standard deviation is \\(\\sigma=27.6\\) tornadoes. The interval will be \\(\\mu\\pm2*\\sigma=56\\pm2*27.6=(0.8,111.2)\\)\nAbout 95% of the years have between 0.8 or 1 and 111 tornadoes in Oklahoma.\nThe general rule says that about 95% of the data is within two standard deviations of the mean. That percentage is fairly high. There isn’t much data outside two standard deviations. A rule that can be followed is that if a data value is within two standard deviations, then that value is a common data value. If the data value is outside two standard deviations of the mean, either above or below, then the number is uncommon. It could even be called unusual. An easy calculation that you can do to figure it out is to find the difference between the data point and the mean, and then divide that answer by the standard deviation. As a formula this would be\n\\(z=\\frac{x-\\mu}{\\sigma}\\)\nIf you don’t know the population mean, \\(\\mu\\), and the population standard deviation, \\(\\sigma\\), then use the sample mean, \\(\\bar{x}\\), and the sample standard deviation, \\(s\\), to estimate the population parameter values. Realize that using the sample standard deviation may not actually be very accurate.\n\n\n\n3.2.5 Example: Determining If a Value Is Unusual\n\nIn 1974, there were 45 tornadoes in Oklahoma. Is this value unusual? Why or why not?\nIn 1999, there were 145 tornadoes in the Oklahoma. Is this value unusual? Why or why not?\n\n\n3.2.5.1 Solution\n\nIn 1974, there were 45 tornadoes in Oklahoma. Is this value unusual? Why or why not?\nVariable: \\(x\\) = number of tornadoes in Oklahoma\n\nTo answer this question, first find how many standard deviations 45 is from the mean. From 3.2.4 example, we know \\(\\mu=56\\) and \\(\\sigma=27.6\\). For \\(x\\)=45, \\(z=\\frac{45-56}{27.6}=-0.399\\)\nSince this value is between -2 and 2, then it is not unusual to have 45 tornadoes in a year in Oklahoma. The z value is negative, so that means that 45 is less than the mean number of tornadoes.\n\nIn 1999, there were 145 tornadoes in the Oklahoma. Is this value unusual? Why or why not?\nVariable: \\(x\\) = number of tornadoes in Oklahoma\n\nFor this question the \\(x\\) = 145, \\(z=\\frac{145-56}{27.6}=3.22\\)\nSince this value is more than 2, then it is unusual to have only 145 tornadoes in a year in Oklahoma.\n\n\n\n3.2.6 Homework for Measures of Spread Section\nUse Technology on all problems. State the variable on all problems.\n\nCholesterol levels were collected from patients certain days after they had a heart attack and are in Table 3.2. Find the mean, median, range, variance, and standard deviation for cholesterol levels 2 days after the heart attack.\n\nCode book for Data Frame Cholesterol is below Table 3.2.\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 3.3 (Lee, 1994). Find the mean, median, range, variance, and standard deviation of the length of rivers that flow into the Pacific Ocean and the mean, median, range, variance, and standard deviation of the length of rivers that flow into the Tasman Sea. Compare and contrast the length of rivers that flow to the Pacific Ocean versus the ones that flow into the Tasman Sea using both measures of spread and measures of variability.\n\nCode book for data frame Length is below Table 3.3.\n\nPrint-O-Matic printing company’s employees have salaries that are contained in Table 3.4. Find the mean, median, range, variance, and standard deviation for the salaries of all employees.\n\nCode book for data frame Pay below Table 3.4.\n\nPrint-O-Matic printing company spends specific amounts on fixed costs every month. The costs of those fixed costs are in Table 3.5. Find the mean, median, range, variance, and standard deviation for the fixed costs.\n\nCode book for Data frame Cost is below Table 3.5.\n\nThe data frame Pulse Table 3.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute.\n\n\nPulse&lt;-read.csv(\"https://krkozak.github.io/MAT160/pulse.csv\")\nknitr::kable(head(Pulse))\n\n\n\nTable 3.7: Head of Pulse Rates of people Before and After Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\n\n\n\n\n170\n68\n22\nmale\nyes\nyes\nmoderate\nsat\n70\n71\n93\n\n\n182\n75\n26\nmale\nyes\nyes\nmoderate\nsat\n80\n76\n93\n\n\n180\n85\n19\nmale\nyes\nyes\nmoderate\nran\n68\n125\n95\n\n\n182\n85\n20\nmale\nyes\nyes\nlow\nsat\n70\n68\n95\n\n\n167\n70\n22\nmale\nyes\nyes\nlow\nsat\n92\n84\n96\n\n\n178\n86\n21\nmale\nyes\nyes\nlow\nsat\n76\n80\n98\n\n\n\n\n\n\n\n\nCode book for data frame Pulse\nDescription Students in an introductory statistics class (MS212 taught by Professor John Eccleston and Dr Richard Wilson at The University of Queensland) participated in a simple experiment. The students took their own pulse rate. They were then asked to flip a coin. If the coin came up heads, they were to run in place for one minute. Otherwise they sat for one minute. Then everyone took their pulse again. The pulse rates and other physiological and lifestyle data are given in the data.\nFive class groups between 1993 and 1998 participated in the experiment. The lecturer, Richard Wilson, was concerned that some students would choose the less strenuous option of sitting rather than running even if their coin came up heads, In the years 1995-1998 a different method of random assignment was used. In these years, data forms were handed out to the class before the experiment. The forms were pre-assigned to either running or non-running and there were an equal number of each. In 1995 and 1998 not all of the forms were returned so the numbers running and sitting was still not entirely controlled.\nThis data frame contains the following columns:\nheight: height of subject in cm\nweight: weight of subject in kg\nage: age of subject in years\ngender: sex of subject, male, female\nSmokes: whether a subject regularly smokes, yes means does smoke, no means does not smoke\nalcohol: whether a subject regularly drinks alcohol, yes means the person does, no means the person does not\nexercise: whether a subject exercises, low, moderate, high\nran: whether a subject ran one minute between pulse measurements (ran) or sat between pulse measurement (sat)\npulse_before: the pulse rate before a subject either ran or sat (bpm)\npulse_after: the pulse rate after a subject either ran or sat (bpm)\nyear: what year the data was collected (93-98)\nSource Pulse rates before and after exercise. (2013, September 25). Retrieved from http://www.statsci.org/data/oz/ms212.html\nReferences The data was supplied by Dr Richard J. Wilson, Department of Mathematics, University of Queensland.\nCreate a data frame that contains only males, who drink alcohol, but do not smoke. Then compare the pulse before and the pulse after using the mean and standard deviation. Discuss whether pulse before or pulse after has a higher mean and larger spread. The following command creates a new data frame with just males, who drink alcohol, but do not smoke, use the following command, where the new name is Males in Table 3.8.\n\nMales&lt;- Pulse |&gt; \n  filter(gender==\"male\", smokes == \"no\", alcohol == \"yes\")\nknitr::kable(head(Males))\n\n\n\nTable 3.8: Head of Pulse Rates of Nonsmoking Males Before and After Exercise\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\n\n\n\n\n195\n84\n18\nmale\nno\nyes\nhigh\nsat\n71\n73\n93\n\n\n184\n74\n22\nmale\nno\nyes\nlow\nran\n78\n141\n93\n\n\n168\n60\n23\nmale\nno\nyes\nmoderate\nran\n88\n150\n93\n\n\n170\n75\n20\nmale\nno\nyes\nhigh\nran\n76\n88\n93\n\n\n187\n59\n18\nmale\nno\nyes\nhigh\nsat\n78\n82\n93\n\n\n180\n72\n18\nmale\nno\nyes\nmoderate\nsat\n69\n67\n93\n\n\n\n\n\n\n\n\n\nThe data frame Pulse Table 3.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute. Create a data frame that contains females, who do not smoke but do drink alcohol. Compare the pulse rate before and after exercise using the mean and standard deviation. Discuss whether pulse before or pulse after has a higher mean and larger spread.\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997) and the data is in Table 3.9.\n\n\nReiki&lt;- read.csv( \"https://krkozak.github.io/MAT160/reki.csv\") \nknitr::kable(head(Reiki))\n\n\n\nTable 3.9: Head of Pain Measurements Before and After Reiki Treatment\n\n\n\n\n\n\nvas.before\nvas.after\nlikert_before\nlikert_after\n\n\n\n\n6\n3\n2\n1\n\n\n2\n1\n2\n1\n\n\n2\n0\n3\n0\n\n\n9\n1\n3\n1\n\n\n3\n0\n2\n0\n\n\n3\n2\n2\n2\n\n\n\n\n\n\n\n\nCode book for data frame Reiki\nDescription The purpose of this study was to explore the usefulness of Reiki as an adjuvant to opioid therapy in the management of pain. Since no studies in this area could be found, a pilot study was carried out involving 20 volunteers experiencing pain at 55 sites for a variety of reasons, including cancer. All Reiki treatments were provided by a certified second-degree Reiki therapist. Pain was measured using both a visual analogue scale (VAS) and a Likert scale immediately before and after the Reiki treatment. Both instruments showed a highly significant (p &lt; 0.0001) reduction in pain following the Reiki treatment.\nThis data frame contains the following columns:\nvas.before: pain measured using a visual analogue scale (VAS) before Reiki treatment\nvas.after: pain measured using a visual analogue scale (VAS) after Reiki treatment\nlikert_before: pain measured using a likert before Reiki treatment\nlikert_after: pain measured using a likert after Reiki treatment\nSource Olson, K., & Hanson, J. (1997). Using reiki to manage pain: a preliminary report. Cancer Prev Control, 1(2), 108-13. Retrieved from http://www.ncbi.nlm.nih.gov/pubmed/9765732\nReferences** Using Reiki to manage pain: a preliminary report. Olson K1, Hanson J., Cancer Prev Control 1997, Jun; 1(2): 108-13.\nSince the data was collected both before and after the treatment for all of the units of observations, you want to look at the effect size of the treatment. You want to find the difference between before and after for the pain scale. First you must create a new data frame that adds a column for the difference in before and after. This data is known as paired data. To create the new column in a new data frame called Newreiki use the following commands, Table 3.10.\n\nNewreiki&lt;-Reiki |&gt;\n  mutate(vas.diff=vas.before-vas.after) \nknitr::kable(head(Newreiki))\n\n\n\nTable 3.10: Head of Pain Measurements Before and After Reiki Treatment with Difference column\n\n\n\n\n\n\nvas.before\nvas.after\nlikert_before\nlikert_after\nvas.diff\n\n\n\n\n6\n3\n2\n1\n3\n\n\n2\n1\n2\n1\n1\n\n\n2\n0\n3\n0\n2\n\n\n9\n1\n3\n1\n8\n\n\n3\n0\n2\n0\n3\n\n\n3\n2\n2\n2\n1\n\n\n\n\n\n\n\n\nNow find the mean and standard deviation of the vas.diff variable in Newreiki. Perform similar commands to create the likert.diff variable. Then find the mean and standard deviation for likert.diff, and compare and contrast the vas and likert methods for describing pain.\n8.Yearly rainfall amounts (in millimeters) in Sydney, Australia, are in Table 3.11 (Annual maximums of, 2013). a. Calculate the mean and standard deviation. b. Suppose Sydney, Australia received 300 mm of rainfall in a year. Would this be unusual?\n\nRainfall&lt;-read.csv(\"https://krkozak.github.io/MAT160/rainfall.csv\") \nknitr::kable(head(Rainfall))\n\n\n\nTable 3.11: Head of Yearly rainfall amounts in Sydney, Australia\n\n\n\n\n\n\namount\n\n\n\n\n146.8\n\n\n383.0\n\n\n90.9\n\n\n178.1\n\n\n267.5\n\n\n95.5\n\n\n\n\n\n\n\n\nCode book for data frame Rainfall\nDescription Daily rainfall (in millimeters) was recorded over a 47-year period in Turramurra, Sydney, Australia. For each year, the wettest day was identified (that having the greatest rainfall). The data show the rainfall recorded for the 47 annual maxima.\nThis data frame contains the following columns:\namount: daily rainfall (mm)\nSource Annual maximums of daily rainfall in Sydney. (2013, September 25). Retrieved from http://www.statsci.org/data/oz/sydrain.html\nReferences Rayner J.C.W. and Best D.J. (1989) Smooth tests of goodness of fit. Oxford: Oxford University Press. Hand D.J., Daly F., Lunn A.D., McConway K.J., Ostrowski E. (1994). A Handbook of Small Data Sets. London: Chapman & Hall. Data set 157. Thanks to Jim Irish of the University of Technology, Sydney, for assistance in identifying the correct units for this data.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Numerical-description-of-Data.html#ranking",
    "href": "Numerical-description-of-Data.html#ranking",
    "title": "3  Numerical Description of Data",
    "section": "3.3 Ranking",
    "text": "3.3 Ranking\nAlong with the center and the variability, another useful numerical measure is the ranking of a number. A percentile is a measure of ranking. It represents a location measurement of a data value to the rest of the values. Many standardized tests give the results as a percentile. Doctors also use percentiles to track a child’s growth.\nThe \\(k^{th}\\) percentile is the data value that has k% of the data at or below that value.\n\n3.3.1 Example: Interpreting Percentile\n\nWhat does a score of the \\(90^{th}\\) percentile mean?\nWhat does a score of the \\(70^{th}\\) percentile mean?\n\n\n3.3.1.1 Solution\n\nWhat does a score of the \\(90^{th}\\) percentile mean?\nThis means that 90% of the scores were at or below this score. (A person did the same as or better than 90% of the test takers.)\nWhat does a score of the \\(70^{th}\\) percentile mean?\nThis means that 70% of the scores were at or below this score.\n\n\n\n\n3.3.2 Example: Percentile Versus Score\nIf the test was out of 100 points and you scored at the \\(80^{th}\\) percentile, what was your score on the test?\n\n3.3.2.1 Solution\nYou don’t know! All you know is that you scored the same as or better than 80% of the people who took the test. If all the scores were really low, you could have still failed the test. On the other hand, if many of the scores were high you could have gotten a 95% or more.\nThere are special percentiles called quartiles. Quartiles are numbers that divide the data into fourths. One fourth (or a quarter) of the data falls between consecutive quartiles.\n\n\n\n3.3.3 To find the quartiles:\nThe command in rStudio is\ndf_stats(~variable, data=data_frame, summary)\nIf you record the quartiles together with the maximum and minimum you have five numbers. This is known as the five-number summary. The five-number summary consists of the minimum, the first quartile (\\(Q1\\)), the median, the third quartile (\\(Q3\\)), and the maximum (in that order).\nThe interquartile range, \\(IQR\\), is the difference between the first and third quartiles, \\(Q1\\) and $Q3$. Half of the data (50%) falls in the interquartile range. If the \\(IQR\\) is “large” the data is spread out and if the \\(IQR\\) is “small” the data is closer together.\nInterquartile Range (\\(IQR\\))\nDetermining probable outliers from \\(IQR\\): fences\nA value that is less than \\(Q1-1.5*IQR\\) (this value is often referred to as a low fence) is considered an outlier.\nSimilarly, a value that is more than \\(Q3+1.5*IQR\\) (the high fence) is considered an outlier.\nA boxplot (or box-and-whisker plot) is a graphical display of the five-number summary. It can be drawn vertically or horizontally. The basic format is a box from \\(Q1\\) to \\(Q3\\), a vertical line across the box for the median and horizontal lines as whiskers extending out each end to the minimum and maximum. The minimum and maximum can be represented with dots. Don’t forget to label the tick marks on the number line and give the graph a title.\nAn alternate form of a Boxplot, known as a modified box plot, only extends the left line to the smallest value greater than the low fence, and extends the left line to the largest value less than the high fence, and displays markers (dots, circles or asterisks) for each outlier.\nIf the data are symmetrical, then the box plot will be visibly symmetrical. If the data distribution has a left skew or a right skew, the line on that side of the box plot will be visibly long. If the plot is symmetrical, and the four quartiles are all about the same length, then the data are likely a near uniform distribution. If a box plot is symmetrical, and both outside lines are noticeably longer than the \\(Q1\\) to median and median to \\(Q3\\) distance, the distribution is then probably bell-shaped.\n\n\n3.3.4 Example: Five-number Summary and Boxplot\nFind the five-number summary, the interquartile range (*IQR*), and draw a box-and-whiskers plot for the weight of cats Table 3.1.\n\n3.3.4.1 Solution\nVariable: \\(x\\) = weight of cats To compute the five-number summary on RStudio, use the command:\n\ndf_stats(~Bwt, data=cats, summary)\n\n  response Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1      Bwt    2     2.3    2.7 2.723611   3.025  3.9\n\n\nNote rStudio also calculates the mean as part of the summary command, but the five-number summary is just the five numbers:\nMinimum: 2 kg \\(Q1\\): 2.3 kg Median: 2.7 kg \\(Q3\\): 3.025 kg Maximum: 3.9 kg\nTo find the interquartile range, \\(IQR\\) find $Q3-Q1$, so \\(IQR=3.025-2.3=0.725 kg\\)\nTo create a boxplot use the command\ngf_boxplot(~variable, data=data_frame)\nThis is a modified boxplot which shows the outliers in the data.\n\ngf_boxplot(~Bwt, data=cats, title=\"Weight of Cats\", xlab=\"Body Weight (kg)\")\n\n\n\n\n\n\n\nFigure 3.11: Weight of Cats\n\n\n\n\n\nThere are no outliers since there are no dots outside of the fences.\n\n\n\n3.3.5 Example: Separating based on a factor\nFind the five-number summary of the weights of cats separated by the sex of the cat. Then create a box plot of the weights of cats for each sex of the cat.\n\n3.3.5.1 Solution\nVariable: \\(x_1\\) = weight of female cat\nVariable: \\(x_2\\) = weight of male cat\nTo find the five-number summary separated based on gender use the following command:\n\ndf_stats(~Bwt|Sex, data=cats, summary)\n\n  response Sex Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1      Bwt   F    2    2.15    2.3 2.359574     2.5  3.0\n2      Bwt   M    2    2.50    2.9 2.900000     3.2  3.9\n\n\nThe five-number summary for female cats is (in kg)\nMinimum: 2 \\(Q1\\): 2.15 Median: 2.3 \\(Q3\\): 2.5 Maximum: 3.0\nThe five-number summary for male cats is (in kg)\nMinimum: 2 \\(Q1\\): 2.50 Median: 2.9 \\(Q3\\): 3.2 Maximum: 3.9\n\ngf_boxplot(~Bwt|Sex, data=cats, title=\"Weights of Cats\", xlab=\"Body Weight in (kg)\") \n\n\n\n\n\n\n\nFigure 3.12: Weight of Cats Faceted by Sex\n\n\n\n\n\nNotice that the weights of female cats has a median less than male cats, and in fact it can be seen that the \\(Q1\\) to \\(Q3\\) of the female cats is less than the \\(Q1\\) to \\(Q3\\) of the male cats.\n\n\n\n3.3.6 Example: Putting it all together\nThe time (in 1/50 seconds) between successive pulses along a nerve fiber (“Time between nerve,” 2013) are given in Table 3.12.\n\nNerve&lt;-read.csv( \"https://krkozak.github.io/MAT160/Nerve_pulse.csv\") \nknitr::kable(head(Nerve))\n\n\n\nTable 3.12: Head of Successive pulses along a nerve fiber\n\n\n\n\n\n\ntime\n\n\n\n\n10.5\n\n\n1.5\n\n\n2.5\n\n\n5.5\n\n\n29.5\n\n\n3.0\n\n\n\n\n\n\n\n\nCode book for data frame Nerve\nDescription The data gives the time between 800 successive pulses along a nerve fiber. There are 799 observations rounded to the nearest half in units of 1/50 second.\nThis data frame contains the following columns:\ntime: time between successive Pulses along a nerve fiber, 1/50 second.\nSource Time between nerve pulses. (2019, July 3). Retrieved from &lt;http://www.statsci.org/data/general/nerve.html\nReferences Fatt, P., and Katz, B. (1952). Spontaneous subthreshold activity at motor nerve endings. Journal of Physiology 117, 109-128.\nCox, D. R., and Lewis, P. A. W. (1966). The Statistical Analysis of Series of Events. Methuen, London.\nJorgensen, B. (1982). The Generalized Inverse-Gaussian Distribution. Springer-Verlag.\n\n3.3.6.1 Solution\nFirst, it might be useful to look at a visualization of the data, so create a density plot\n\ngf_density(~time, data=Nerve, title=\"Time between Successive Nerve Pulses\", xlab=\"Time (1/50 second)\") \n\n\n\n\n\n\n\nFigure 3.13: Weight of Cats Faceted by Sex\n\n\n\n\n\nFrom the graph Figure 3.13 the data appears to be skewed right. Most of the time between successive nerve pulses appear to be around 5 or 10 1/50 second, but there are some times that are 60 1/50 second.\n\ndf_stats(~time, data=Nerve, mean, median, sd, summary)\n\n  response     mean median       sd Min. 1st Qu. Median     Mean 3rd Qu. Max.\n1     time 10.95119    7.5 10.45956  0.5     3.5    7.5 10.95119      15   69\n\n\nNumerical descriptions might also be useful. Using technology, the mean is 11 1/50 second,the median is 7.5 1/50 second, the standard deviation is 10.5 1/50 second, and the five-number summary is minimum = 3.5, Q1 = 3.5, median = 7.5, Q3 = 15, and maximum = 69 1/50 second.\nTo visualize the five-number summary, create a box plot.\n\ngf_boxplot(~time, data=Nerve, title=\"Nerve Pulses\", xlab=\"Time (1/50 second)\")\n\n\n\n\n\n\n\nFigure 3.14: Boxplot of Nerve Pulses\n\n\n\n\n\nSince there are many dots outside the upper fence the data has many outliers. From all of this information, one could say that nerve pulses between successive pulses is around 11 1/50 second, with a spread of 19.5 1/50 second. Most of the values are round 11 1/50 second, but they are not very consistent. The density plot and boxplot show that there is a great deal of spread of the data and it is skewed to the right. This means mostly the speed is around 11 1/50 second, but there is a great deal of variability in the values.\n\n\n\n3.3.7 Homework for Ranking Section\nUse Technology on all problems. State the variable on all problems.\n\nSuppose you take a standardized test and you are in the \\(10^{th}\\) percentile. What does this percentile mean? Can you say that you failed the test? Explain.\nSuppose your child takes a standardized test in mathematics and scores in the \\(96^{th}\\) percentile. What does this percentile mean? Can you say your child passed the test? Explain.\nSuppose your child is in the \\(83^{rd}\\) percentile in height and \\(24^{th}\\) percentile in weight. Describe what this tells you about your child’s stature.\nSuppose your work evaluates the employees and places them on a percentile ranking. If your evaluation is in the \\(65^{th}\\) percentile, do you think you are working hard enough? Explain.\nCholesterol levels were collected from patients certain days after they had a heart attack and are in table Table 3.2.\n\nCode book for Data Frame Cholesterol below Table 3.2.\nFind the five-number summary and interquartile range (IQR) for the cholesterol level on day 2, and draw a boxplot\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in table Table 3.3 (Lee, 1994).\n\nCode book for data frame Length below Table 3.3.\nFind the five-number summary and interquartile range (IQR) for the lengths of rivers that go to the Pacific Ocean and ones that go to the Tasman Sea, and draw a boxplot of both.\n\nPrint-O-Matic printing company’s employees have salaries that are contained in Table 3.4 Find the five number summary and draw a boxplot for the salaries of all employees.\n\nCode book for data frame Pay below Table 3.4.\n\nThe data frame Pulse Table 3.7 contains various variables about a person including their pulse rates before the subject exercised and after after the subject ran in place for one minute.\n\nCode book for data frame Pulse below Table 3.7.\nCreate a data frame that contains only people who drink alcohol, but do not smoke. Then find the five number summary and draw a boxplot for both males and females separately.\n\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997) and the data is in Table 3.9.\n\nCode book for data frame Reiki below Table 3.9.\nFind the five number summary for both the before and after VAS scores and draw boxplots of before and after VAS scores. To draw two boxplots at the same time, after the command to create the first box plot type the piping symbol |&gt; (base r) or %&gt;% (magrittr package) before pressing enter. (Note: |&gt; and %&gt;% are piping symbols that can be thought of as “and then.”) Then type the command for the second boxplot after the + symbol or on the next line in the r chunk if using an rmd or qmd file. Then press enter. You may want to graph each boxplot as a different color. To do this, the command would be\ngf_boxplot(~variable, data=data_frame, color=“red”, xlab=“type a label”)\nYou can pick any color you want. Just replace the word red with the color you want to use. Now compare and contrast the before and after VAS scores.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Numerical Description of Data</span>"
    ]
  },
  {
    "objectID": "Probability.html",
    "href": "Probability.html",
    "title": "4  Probability",
    "section": "",
    "text": "4.1 Empirical Probability\nEmpirical probabilities are found by actually conducting an experiment many times and counting the number of times the event happens. To understand how this is performed, first some definitions are needed.\nOutcomes: the results of an experiment\nEvent: a set of certain outcomes of an experiment that you want to have happen\nSample Space: collection of all possible outcomes of the experiment. Usually denoted as \\(SS\\).\nEvent space: the set of outcomes that make up an event. The symbol is usually a capital letter.\nFrequency: how often an event happens\nRelative Frequency: the frequency divided by the number of times the experiment is repeated\nStart with an experiment. Suppose that the experiment is rolling a die. The sample space is {1, 2, 3, 4, 5, 6}. The event that you want is to get a 6, and the event space is {6}. To do this, roll a die 10 times. When you do that, you get a 6 two times. Based on this experiment, the probability of getting a 6 is 2 out of 10 or 1/5. To get more accuracy, repeat the experiment more times. It is easiest to put this in a table, where *n* represents the number of times the experiment is repeated. When you put the number of 6s found over the number of times you repeat the experiment, this is the relative frequency.\nTable 4.1: trials for Die Experiment\n\n\n\n\n\n\nn\nnumber_of_6s\nrelative_frequency\n\n\n\n\n10\n2\n0.200\n\n\n50\n6\n0.120\n\n\n100\n18\n0.180\n\n\n500\n81\n0.162\n\n\n1000\n163\n0.163\nNotice that as \\(n\\) increased, the relative frequency seems to approach a number. It looks like it is approaching 0.163. You can say that the probability of getting a 6 is approximately 0.163. If you want more accuracy, then increase \\(n\\) even more.\nThese probabilities are called experimental probabilities since they are found by actually doing the experiment. They come about from the relative frequencies and give an approximation of the true probability. The approximate probability of an event \\(A\\), \\(P(A)\\), is",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Probability.html#empirical-probability",
    "href": "Probability.html#empirical-probability",
    "title": "4  Probability",
    "section": "",
    "text": "4.1.1 Experimental Probabilities\n\\(P(A)=\\frac{\\text{number of times} }{ \\text{number of times experiment is conducted}}\\)\nFor the event of getting a 6, the probability would be\n\\(P(6)=\\frac{163}{1000}=0.163\\).\nYou must do experimental probabilities whenever it is not possible to calculate probabilities using other means. An example is if you want to find the probability that a family has 5 children, you would have to actually look at many families, and count how many have 5 children. Then you could calculate the probability. Another example is if you want to figure out if a die is fair. You would have to roll the die many times and count how often each side comes up. Make sure you repeat an experiment many times, because otherwise you will not be able to estimate the true probability. This is due to the law of large numbers.\nLaw of large numbers: as \\(n\\) increases, the relative frequency tends towards the actual probability value.\nNote: probability, relative frequency, percentage, and proportion are all different words for the same concept. Also, probabilities can be given as percentages, decimals, or fractions.\nTo find probabilities from data, you can take a data frame and count the number of values for each outcome.\n\n\n4.1.2 Example: Statistics class survey\nData was collected for two semesters in a statistics class. The data frame in is in Table 2.1.\nFind the probability (proportion) of people who like Cookie Dough ice cream.\n\n4.1.2.1 Solution\nTo count the number of people who like cookie dough ice cream, use the following command in r Studio:\n\ntally(~ice_cream, data=Class, margins=TRUE)\n\nice_cream\n          Butter Pecan              Chocolate    Chocolate Brownie.  \n                     2                      2                      1 \n                coffee           Cookie Dough      Cookies and Cream \n                     1                      6                      1 \n               Mint CC           Moose Tracks                   none \n                     6                      1                      1 \n            Rocky Road                Sherbet Strawberry and banana  \n                     2                      2                      1 \n               Vanilla                  Total \n                     1                     27 \n\n\nFrom this tally, it can be seen that 6 people like cookie dough ice cream. The probability that someone likes cookie dough is thus 6 divided by the number of people in the data frame, the Total. Instead of dividing, the following command will find the proportions for you. Proportions are just probabilities.\n\ntally(~ice_cream, data=Class, format=\"proportion\")\n\nice_cream\n          Butter Pecan              Chocolate    Chocolate Brownie.  \n            0.07407407             0.07407407             0.03703704 \n                coffee           Cookie Dough      Cookies and Cream \n            0.03703704             0.22222222             0.03703704 \n               Mint CC           Moose Tracks                   none \n            0.22222222             0.03703704             0.03703704 \n            Rocky Road                Sherbet Strawberry and banana  \n            0.07407407             0.07407407             0.03703704 \n               Vanilla \n            0.03703704 \n\n\nSo the probability that a person in the class likes cookie dough ice cream is 0.22.\n\n\n\n4.1.3 Homework for Empirical Probability Section\n\nThe number of M&M’s of each color that were found in a packet is in Table 4.2 (M&M’s Color Distribution Analysis, 2019).\n\n\n\n\n\nTable 4.2: M&M Distribution\n\n\n\n\n\n\ncolor\ntype\npack\n\n\n\n\norange\nplain\n1\n\n\nred\nplain\n1\n\n\ngreen\nplain\n1\n\n\nred\nplain\n1\n\n\nyellow\nplain\n1\n\n\nblue\nplain\n1\n\n\n\n\n\n\n\n\nCode book for Data Frame MaM\nDescription An analysis of the colors in a case of M&M’s to see if they match the published percentages\nUsage MaM\nFormat\nThis data frame contains the following columns:\ncolor: color of M&Ms\ntype: The type of M&M such as plain, peanut, peanut butter\npack: which pack the M&Ms came from.\nSource M&M’s Color Distribution Analysis. (n.d.). Retrieved July 11, 2019, from https://joshmadison.com/2007/12/02/mms-color-distribution-analysis/\nReferences Josh Madison, 2019\nFind the probability of choosing each color based on this data frame.\n\nEyeglassomatic manufactures eyeglasses for different retailers. They test to see how many defective lenses they made the time period of January 1 to March 31. The defect and the number of defects is in Table 2.4.\n\nCode book for Data Frame Defects below Table 2.4.\nFind the probability of each defect type based on this data.\n\nIn Australia in 1995, of the 2907 indigenous people in prison 17 of them died. In that same year, of the 14501 non-indigenous people in prison 42 of them died (\\“Aboriginal deaths in,\\” 2013). Find the probability that an indigenous person dies in prison and the probability that a non-indigenous person dies in prison. Compare these numbers and discuss what the numbers may mean.\nA project conducted by the Australian Federal Office of Road Safety asked people many questions about their cars. One question was the reason that a person chooses a given car, and that data is in Table 4.3 (Car Preferences, 2019).\n\n\n\n\n\nTable 4.3: Reason for Choosing a Car\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nAge\nSex\nLicYr\nLicMth\nActCar\nKids5\nKids6\nPreferCar\nCarsmall_new5K\nReason\nCost\nReliable\nPerform\nFuel\nSafety\nAC.PS\nPark\nRoom\nDoors\nPrestige\nColour\n\n\n\n\n110\n18\nmale\n0\n2\nlarge\nno\nno\nmedium\nlarge_used\nsafety\nimportant\nvery_important\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nlittle_importance\n\n\n111\n25\nfemale\n8\n0\nsmall\nno\nno\nsmall\nsmall_new\nsafety\nvery_important\nvery_important\nvery_important\nvery_important\nvery_important\nimportant\nvery_important\nvery_important\nlittle_importance\nimportant\nimportant\n\n\n112\n63\nmale\n46\n0\nlarge\nno\nno\nlarge\nlarge_used\ncomfort\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\nimportant\n\n\n113\n51\nfemale\n35\n0\nlarge\nno\nno\nmedium\nlarge_used\nsafety\nlittle_importance\nimportant\nimportant\nimportant\nvery_important\nimportant\nimportant\nimportant\nimportant\nnot_important\nimportant\n\n\n114\n19\nfemale\n2\n0\nmedium\nno\nno\nmedium\nsmall_new\nlooks\nimportant\nimportant\nvery_important\nimportant\nimportant\nlittle_importance\nimportant\nimportant\nlittle_importance\nimportant\nimportant\n\n\n115\n51\nfemale\n30\n0\nmedium\nyes\nyes\nmedium\nlarge_used\ncomfort\nimportant\nvery_important\nvery_important\nimportant\nvery_important\nvery_important\nimportant\nimportant\nimportant\nlittle_importance\nimportant\n\n\n\n\n\n\n\n\nCode book for Data Frame Car_pref\nDescription These data were collected as part of a project for the Federal Office for Road Safety conducted by the Research Institute of Gender and Health at the University of Newcastle. There is evidence that women drivers who are involved in motor vehicle accidents are more likely than men to be injured. A possible reason is that women often drive smaller cars that provide less protection in a collision. One of the aims of the project was to examine preferences for cars among men and women and investigate the extent to which safety was a factor in determining preferences. The survey was conducted by research assistants who asked people in car parks to participate and administered a structured questionnaire. They were instructed to obtain data from men and women with small, medium and large cars, with 50 people per group for a total of 300 respondents. (The sample size was based on power requirements for another part of the survey that involved anthropometric measurements.) The research assistants approached people in car parks of the University of Newcastle and nearby shopping centers during December 1997 and January 1998.\nUsage Car_pref\nFormat\nThis data frame contains the following columns:\nID: Identification number of respondent\nAge: Age of respondent (years)\nSex: female, male\nLicYr: Time they have held a full driving licence, in years and months (years)\nLicMth: Time they have held a full driving licence, in years and months (months)\nActCar: Make, model and year of car most often driven, coded to size of car small, medium, large\nKids5: Children under five, yes, no\nKids6: Children 6 to 16, yes, no\nPrefCar: Preferred car, coded to size of car small, medium, large\nCar15k: Preferred type of car if cost \\$15000, small new car; large second-hand car\nReason: safety, reliability, cost, performance, comfort, looks\nCost: How important is cost when buying a car? not important, little importance, important, very important\nReliable: How important is reliability …?\nPerform: How important is performance …?\nFuel: How important is fuel consumption …?\nSafety: How important is safety …?\nAC/PS: How important is air conditioning/power steering …?\nPark: How important is ease of parking …?\nRoom: How important is space/roominess …?\nDoors: How important is the number of doors …?\nPrestige: How important is prestige/style …?\nColour: How important is colour …?\nSource\nCar Preferences. (n.d.). Retrieved July 11, 2019, from http://www.statsci.org/data/oz/carprefs.html\nReferences\nThe data was contributed to OzDASL by Professor Annette Dobson, University of Queensland. Information on the data set was originally provided by Jenny Powers.\nFind the probability a person chooses a car for each of the given reasons.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Probability.html#theoretical-probability",
    "href": "Probability.html#theoretical-probability",
    "title": "4  Probability",
    "section": "4.2 Theoretical Probability",
    "text": "4.2 Theoretical Probability\nIt is not always feasible to conduct an experiment over and over again, so it would be better to be able to find the probabilities without conducting the experiment. These probabilities are called Theoretical Probabilities.\nTo be able to do theoretical probabilities, there is an assumption that you need to consider. It is that all of the outcomes in the sample space need to be **equally likely outcomes**. This means that every outcome of the experiment needs to have the same chance of happening.\n\n4.2.1 Example: Equally Likely Outcomes\nWhich of the following experiments have equally likely outcomes?\n\nRolling a fair die.\nFlip a coin that is weighted so one side comes up more often than the other.\nPull a ball out of a can containing 6 red balls and 8 green balls. All balls are the same size.\nPicking a card from a deck.\nRolling a die to see if it is fair.\n\n\n4.2.1.1 Solution\n\nRolling a fair die.\nSince the die is fair, every side of the die has the same chance of coming up. The outcomes are the different sides, so each outcome is equally likely\nFlip a coin that is weighted so one side comes up more often than the other.\nSince the coin is weighted, one side is more likely to come up than the other side. The outcomes are the different sides, so each outcome is not equally likely\nPull a ball out of a can containing 6 red balls and 8 green balls. All balls are the same size.\nSince each ball is the same size, then each ball has the same chance of being chosen. The outcomes of this experiment are the individual balls, so each outcome is equally likely. Don’t assume that because the chances of pulling a red ball are less than pulling a green ball that the outcomes are not equally likely. The outcomes are the individual balls and they are equally likely.\nPicking a card from a deck.\nIf you assume that the deck is fair, then each card has the same chance of being chosen. Thus the outcomes are equally likely outcomes. You do have to make this assumption. For many of the experiments you will do, you do have to make this kind of assumption.\nRolling a die to see if it is fair.\nIn this case you are not sure the die is fair. The only way to determine if it is fair is to actually conduct the experiment, since you don’t know if the outcomes are equally likely. If the experimental probabilities are fairly close to the theoretical probabilities, then the die is fair.\n\nIf the outcomes are not equally likely, then you must do experimental probabilities. If the outcomes are equally likely, then you can do theoretical probabilities.\nTheoretical Probabilities: If the outcomes of an experiment are equally likely, then the probability of event A happening is\n\\(P(A)=\\frac{\\text{number of outcomes in event space}}{\\text{number of outcomes in sample space}}\\)\n\n\n\n4.2.2 Example: Calculating Theoretical Probabilities\nSuppose you conduct an experiment where you flip a fair coin twice\n\nWhat is the sample space?\nWhat is the probability of getting exactly one head?\nWhat is the probability of getting at least one head?\nWhat is the probability of getting a head and a tail?\nWhat is the probability of getting a head or a tail?\nWhat is the probability of getting a foot?\nWhat is the probability of each outcome? What is the sum of these probabilities?\n\n\n4.2.2.1 Solution\n\nWhat is the sample space?\nThere are several different sample spaces you can do. One is SS={0, 1, 2} where you are counting the number of heads. However, the outcomes are not equally likely since you can get one head by getting a head on the first flip and a tail on the second or a tail on the first flip and a head on the second. There are 2 ways to get that outcome and only one way to get the other outcomes. Instead it might be better to give the sample space as listing what can happen on each flip. Let H = head and T = tail, and list which can happen on each flip.\n\\(SS\\)={HH, HT, TH, TT}\nWhat is the probability of getting exactly one head?\nLet \\(A\\) = getting exactly one head. The event space is \\(A\\) = {HT, TH}. So \\(P(A)=\\frac{2}{4}\\)\nIt may not be advantageous to reduce the fractions to lowest terms, since it is easier to compare fractions if they have the same denominator.\nWhat is the probability of getting at least one head?\nLet \\(B\\) = getting at least one head. At least one head means get one or more. The event space is \\(B\\) = {HT, TH, HH} and \\(P(B)=\\frac{3}{4}\\) Since \\(P(B)\\) is greater than the \\(P(A)\\), then event \\(B\\) is more likely to happen than event \\(A\\).\nWhat is the probability of getting a head and a tail?\nLet \\(C\\) = getting a head and a tail = {HT, TH} and \\(P(C)=\\frac{2}{4}\\) This is the same event space as event \\(A\\), but it is a different event. Sometimes two different events can give the same event space.\nWhat is the probability of getting a head or a tail?\nLet \\(D\\) = getting a head or a tail. Since or means one or the other or both and it doesn’t specify the number of heads or tails, then \\(D\\) = {HH, HT, TH, TT} and \\(P(D)=\\frac{3}{4}\\)\nWhat is the probability of getting a foot?\nLet \\(E\\) = getting a foot. Since you can’t get a foot, \\(E\\) = {} or the empty set and \\(P(E)=\\frac{0}{4}=0\\)\nWhat is the probability of each outcome? What is the sum of these probabilities?\n\\(P(HH)=P(HT)=P(TH)=P(TT)=\\frac{1}{4}\\). If you add all of these probabilities together you get \\(1\\).\n\nThis example had some results in it that are important concepts. They are summarized below:\n\n\n\n4.2.3 Probability Properties\n\n\\(0 \\le P(\\text{event}) \\le 1\\)\nIf the \\(P(\\text{event}) = 1\\), then it will happen and is called the certain event\nIf the \\(P(\\text{event}) = 0\\), then it cannot happen and is called the impossible event\n\\(\\sum{P(\\text{all outcomes})}=1\\)\n\n\n\n4.2.4 Example: Calculating Theoretical Probabilities 2\nSuppose you conduct an experiment where you pull a card from a standard deck.\n\nWhat is the sample space?\nWhat is the probability of getting a Spade?\nWhat is the probability of getting a Jack?\nWhat is the probability of getting an Ace?\nWhat is the probability of not getting an Ace?\nWhat is the probability of not getting an Ace?\nWhat is the probability of getting a Spade or an Ace?\nWhat is the probability of getting a Jack and an Ace?\nWhat is the probability of getting a Jack and an Ace?\n\n\n4.2.4.1 Solution\n\nWhat is the sample space?\n\\(SS\\) = {2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS, KS, AS, 2C, 3C, 4C, 5C, 6C, 7C, 8C, 9C, 10C, JC, QC, KC, AC, 2D, 3D, 4D, 5D, 6D, 7D, 8D, 9D, 10D, JD, QD, KD, AD, 2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH, AH}\nWhat is the probability of getting a Spade?\nGetting a spade = {2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS, KS, AS} so \\(P(spade)=\\frac{13}{52}\\)\nWhat is the probability of getting a Jack?\nGetting a Jack = {JS, JC, JH, JD} so \\(P(jack)=\\frac{4}{52}\\)\nWhat is the probability of getting an Ace?\nGetting an Ace = {AS, AC, AH, AD} so \\(P(ace)=\\frac{4}{52}\\)\nWhat is the probability of not getting an Ace?\nNot getting an Ace = {2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS, KS, 2C, 3C, 4C, 5C, 6C, 7C, 8C, 9C, 10C, JC, QC, KC, 2D, 3D, 4D, 5D, 6D, 7D, 8D, 9D, 10D, JD, QD, KD, 2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH} so \\(P(\\text{not ace})=\\frac{48}{52}\\)\n\nNotice, \\(P(ace)+P(\\text{not ace})=1\\), so you could have found the probability of not ace by doing \\(1\\) minus the probability of ace. \\(P(\\text{not ace})=1-P(ace) = 1-\\frac{4}{52} = \\frac{48}{52}\\)\n\nWhat is the probability of getting a Spade and an Ace?\nGetting a Spade and an Ace = {AS} so \\(P(AS)=\\frac{1}{52}\\)\nWhat is the probability of getting a Spade or an Ace?\nGetting a Spade and an Ace ={2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS, KS, AS, AC, AD, AH} so \\(P(\\text{spade and ace})=\\frac{16}{52}\\)\nWhat is the probability of getting a Jack and an Ace?\nGetting a Jack and an Ace = { } since you can’t do that when picking one card. So \\(P(\\text{Jack and Ace})=\\frac{0}{52}=0\\)\nWhat is the probability of getting a Jack or an Ace?\nGetting a Jack or an Ace = {JS, JC, JD, JH, AS, AC, AD, AH} so \\(P(\\text{Jack or Ace})=\\frac{8}{52}\\)\n\n\n\n\n4.2.5 Example: Calculating Theoretical Probabilities 3\nSuppose you have an iPhone and playing iTunes with the following songs on it: 5 Rolling Stones songs, 7 Beatles songs, 9 Bob Dylan songs, 4 Faith Hill songs, 2 Taylor Swift songs, 7 U2 songs, 4 Mariah Carey songs, 7 Bob Marley songs, 6 Bunny Wailer songs, 7 Elton John songs, 5 Led Zeppelin songs, and 4 Dave Mathews Band songs. The different genre that you have are rock from the 60s which includes Rolling Stones, Beatles, and Bob Dylan; country includes Faith Hill and Taylor Swift; rock of the 90s includes U2 and Mariah Carey; Reggae includes Bob Marley and Bunny Wailer; rock of the 70s includes Elton John and Led Zeppelin; and bluegrass-rock includes Dave Mathews Band.\nSuppose the iTunes is set to shuffle the songs, so it randomly picks the next song so you have no idea what the next song will be. Now you would like to calculate the probability that you will hear the type of music or the artist that you are interested in. The sample set is too difficult to write out, but you can figure it from looking at the number in each set and the total number. The total number of songs you have is 67.\n\nWhat is the probability that you will hear a Faith Hill song?\nWhat is the probability that you will hear a Bunny Wailer song?\nWhat is the probability that you will hear a song from the 60s?\nWhat is the probability that you will hear a Reggae song?\nWhat is the probability that you will hear a song from the 90s or a bluegrass-rock song?\nWhat is the probability that you will hear an Elton John or a Taylor Swift song?\nWhat is the probability that you will hear a country song or a U2 song?\n\n\n4.2.5.1 Solution\n\nWhat is the probability that you will hear a Faith Hill song?\nThere are 4 Faith Hill songs out of the 67 songs, so \\(P(\\text{Faith Hill})=\\frac{4}{67}\\)\nWhat is the probability that you will hear a Bunny Wailer song?\nThere are 6 Bunny Wailer songs, so \\(P(\\text{Bunny Wailer})=\\frac{6}{67}\\)\nWhat is the probability that you will hear a song from the 60s?\nThere are 5, 7, and 9 songs that are classified as rock from the 60s, which is 21 total, so \\(P(\\text{song from 60s})=\\frac{21}{67}\\)\nWhat is the probability that you will hear a Reggae song?\nThere are 6 and 7 songs that are classified as Reggae, which is 13 total, so \\(P(\\text{Reggae})=\\frac{13}{67}\\)\nWhat is the probability that you will hear a song from the 90s or a bluegrass-rock song?\nThere are 7 and 4 songs that are songs from the 90s and 4 songs that are bluegrass-rock, for a total of 15, so \\(P(\\text{song 90s or bluegrass-rock})=\\frac{15}{67}\\)\nWhat is the probability that you will hear an Elton John or a Taylor Swift song?\nThere are 7 Elton John songs and 2 Taylor Swift songs, for a total of 9, so \\(P(\\text{Elton John or Taylor Swift})=\\frac{9}{67}\\)\nWhat is the probability that you will hear a country song or a U2 song?\nThere are 6 country songs and 7 U2 songs, for a total of 13, so \\(P(\\text{country or U2})=\\frac{13}{67}\\)\n\nOf course you can do any other combinations you would like.\nNotice in Example: Calculating Theoretical Probabilities part e, it was mentioned that the probability of getting an ace plus the probability of not getting an ace was 1. This is because these two events have no outcomes in common, and together they make up the entire sample space. Events that have this property are called complementary events.\nIf two events are complementary events then to find the probability of one just subtract the probability of the other from one. Notation used for complement of \\(A\\) is \\(\\text{not }A\\) or \\(A^{c}\\).\n\\(P(A)+P(\\text{not }A)=1\\)\n\n\n\n4.2.6 Example: Complementary Events\n\nSuppose you know that the probability of it raining today is 0.45. What is the probability of it not raining?\nSuppose you know the probability of not getting the flu is 0.24. What is the probability of getting the flu?\nIn an experiment of picking a card from a deck, what is the probability of not getting a card that is a Queen?\n\n\n4.2.6.1 Solution\n\nSuppose you know that the probability of it raining today is 0.45. What is the probability of it not raining?\nSince not raining is the complement of raining, then \\(P(\\text{not raining})=1-P(\\text{raining}) = 1-0.45=0.55\\)\nSuppose you know the probability of not getting the flu is 0.24. What is the probability of getting the flu?\nSince getting the flu is the complement of not getting the flu, then \\(P(\\text{getting flu})=1-P(flu)=1-0.24=0.76\\)\nIn an experiment of picking a card from a deck, what is the probability of not getting a card that is a Queen?\nYou could do this problem by listing all the ways to not get a queen, but that set is fairly large. One advantage of the complement is that it reduces the workload. You use the complement in many situations to make the work shorter and easier. In this case it is easier to list all the ways to get a Queen, find the probability of the Queen, and then subtract from one.\nQueen = {QS, QC, QD, QH} so \\(P(Queen)=\\frac{4}{52}\\) and \\(P(\\text{not Queen})=1-P(Queen)=1-\\frac{4}{52}=\\frac{48}{52}\\)\n\nThe complement is useful when you are trying to find the probability of an event that involves the words at least or an event that involves the words at most. As an example of an at least event is suppose you want to find the probability of making at least \\$50,000 when you graduate from college. That means you want the probability of your salary being greater than or equal to \\$50,000. An example of an at most event is suppose you want to find the probability of rolling a die and getting at most a 4. That means that you want to get less than or equal to a 4 on the die. The reason to use the complement is that sometimes it is easier to find the probability of the complement and then subtract from 1.\n\n\n\n4.2.7 Example: Using the Complement to Find Probabilities\n\nIn an experiment of rolling a fair die one time, find the probability of rolling at most a 4 on the die.\nIn an experiment of pulling a card from a fair deck, find the probability of pulling at least a 5 (ace is a high card in this example).\n\n\n4.2.7.1 Solution\n\nIn an experiment of rolling a fair die one time, find the probability of rolling at most a 4 on the die.\nThe sample space for this experiment is {1, 2, 3, 4, 5, 6}. You want the event of getting at most a 4, which is the same as thinking of getting 4 or less. The event space is {1, 2, 3, 4}. The probability is \\(P(\\text{at most a 4})=\\frac{4}{6}\\)\nOr you could have used the complement. The complement of rolling at most a 4 would be rolling number bigger than 4. The event space for the complement is {5, 6}. The probability of the complement is \\(P(\\text{more than 4})=\\frac{2}{6}\\). The probability of at most 4 would be \\(P(\\text{at most 4})=1-P(\\text{more than 4})=1-\\frac{2}{6}=\\frac{4}{6}\\)\nNotice you have the same answer, but the event space was easier to write out. For this example the complement probability wasn’t that useful, but in the future there will be events where it is much easier to use the complement.\nIn an experiment of pulling a card from a fair deck, find the probability of pulling at least a 5 (ace is a high card in this example).\nThe sample space for this experiment is \\(SS\\) = {2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS, KS, AS, 2C, 3C, 4C, 5C, 6C, 7C, 8C, 9C, 10C, JC, QC, KC, AC, 2D, 3D, 4D, 5D, 6D, 7D, 8D, 9D, 10D, JD, QD, KD, AD, 2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH, AH}\n\nPulling a card that is at least a 5 would involve listing all of the cards that are a 5 or more. It would be much easier to list the outcomes that make up the complement. The complement of at least a 5 is less than a 5. That would be the event of 4 or less. The event space for the complement would be {2S, 3S, 4S, 2C, 3C, 4C, 2D, 3D, 4D, 2H, 3H, 4H}. The probability of the complement would be \\(\\frac{12}{52}\\). The probability of at least a 5 would be \\(P(\\text{at least 5})=1-P(\\text{at most 4})=1-\\frac{12}{52}=\\frac{40}{52}\\)\nAnother concept was shown in Example: Calculating Theoretical Probabilities 2 parts g and i. The problems were looking for the probability of one event or another. In part g, it was looking for the probability of getting a Spade or an Ace. That was equal to \\(\\frac{16}{52}\\). In part i, it was looking for the probability of getting a Jack or an Ace. That was equal to \\(\\frac{8}{52}\\). If you look back at the parts b, c, and d, you might notice the following result: \\(P(\\text{Jack or Ace})=P(Jack)+P(Ace)\\) but \\(P(\\text{Spade or Ace})\\ne P(Spade)+P(Ace)\\).\nWhy does adding two individual probabilities together work in one situation to give the probability of one or another event and not give the correct probability in the other?\nThe reason this is true in the case of the Jack and the Ace is that these two events cannot happen together. There is no overlap between the two events, and in fact the \\(P(\\text{Jack and Ace})=0\\). However, in the case of the Spade and Ace, they can happen together. There is overlap, mainly the ace of spades. The \\(P(\\text{Spade and Ace})\\ne0\\).\nWhen two events cannot happen at the same time, they are called mutually exclusive. In the above situation, the events Jack and Ace are mutually exclusive, while the events Spade and Ace are not mutually exclusive.\n\n\n\n4.2.8 Addition Rules:\nIf two events A and B are mutually exclusive, then \\(P(\\text{A and B})=0\\) and \\(P(\\text{A or B})=P(A)+P(B)\\)\nIf two events A and B are not mutually exclusive, then \\(P(\\text{A or B})=P(A)+P(B)-P(\\text{A and B})\\)\n\n\n4.2.9 Example: Using Addition Rules\nSuppose your experiment is to roll two fair dice.\n\nWhat is the sample space?\nWhat is the probability of getting a sum of 5?\nWhat is the probability of getting the first die a 2?\nWhat is the probability of getting a sum of 7?\nWhat is the probability of getting a sum of 5 and the first die a 2?\nWhat is the probability of getting a sum of 5 or the first die a 2?\nWhat is the probability of getting a sum of 5 and sum of 7?\nWhat is the probability of getting a sum of 5 or sum of 7?\n\n\n4.2.9.1 Solution\n\nWhat is the sample space?\nAs with the other examples you need to come up with a sample space that has equally likely outcomes. One sample space is to list the sums possible on each roll. That sample space would look like: \\(SS\\) = {2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}. However, there are more ways to get a sum of 7 then there are to get a sum of 2, so these outcomes are not equally likely. Another thought is to list the possibilities on each roll. As an example you could roll the dice and on the first die you could get a \\(1\\). The other die could be any number between \\(1\\) and \\(6\\), but say it is a \\(1\\) also. Then this outcome would look like (1,1). Similarly, you could get (1, 2), (1, 3), (1,4), (1, 5), or (1, 6). Also, you could get a 2, 3, 4, 5, or 6 on the first die instead. Putting this all together, you get the sample space:\n\\(SS\\) = {(1,1), (1,2), (1,3), (1,4), (1,5), (1,6),\n(2,1), (2,2), (2,3), (2,4), (2,5), (2,6),\n(3,1), (3,2), (3,3), (3,4), (3,5), (3,6),\n(4,1), (4,2), (4,3), (4,4), (4,5), (4,6),\n(5,1), (5,2), (5,3), (5,4), (5,5), (5,6),\n(6,1), (6,2), (6,3), (6,4), (6,5), (6,6)}\nNotice that a (2,3) is different from a (3,2), since the order that you roll the die is important and you can tell the difference between these two outcomes. You don’t need any of the doubles twice, since these are not distinguishable from each other in either order.\n\nThis will always be the sample space for rolling two dice.\n\nWhat is the probability of getting a sum of 5?\nGetting a sum of 5 = {(4,1), (3,2), (2,3), (1,4)} so \\(P(\\text{sum of 5})=\\frac{4}{36}\\)\nWhat is the probability of getting the first die a 2?\nGetting first die a 2 = {(2,1), (2,2), (2,3), (2,4), (2,5), (2,6)} so \\(P(\\text{1st die 2})=\\frac{6}{36}\\)\nWhat is the probability of getting a sum of 7?\nGetting a sum of 7 = {(6,1), (5,2), (4,3), (3,4), (2,5), (1,6)} so \\(P(\\text{sum of 7})=\\frac{6}{36}\\)\nWhat is the probability of getting a sum of 5 and the first die a 2?\nThis is events A and B which contains the outcome {(2,3)} so \\(P(\\text{sum of 5 and 1st die a 2})=\\frac{1}{36}\\)\nWhat is the probability of getting a sum of 5 or the first die a 2?\nNotice from part e, that these two events are not mutually exclusive, so\n\n\\(P(\\text{sum of 5 or 1st die a 2})\\)\n\\(=P(\\text{sum of 5})+P(\\text{1st die 2})-P(\\text{sum of 5 and 1st die a 2})\\)\n\\(= \\frac{4}{36}+\\frac{6}{36}-\\frac{1}{36}=\\frac{9}{36}\\)\n\nWhat is the probability of getting a sum of 5 and sum of 7?\nThese are the events parts a and c, which have no outcomes in common. Thus sum of 5 and sum of 7 = { } so \\(P(\\text{sum of 5 and sum of 7})=0\\)\nWhat is the probability of getting a sum of 5 or sum of 7?\nFrom part g, these two events are mutually exclusive, so \\(P(\\text{sum of 5 or sum of 7})=P(\\text{sum of 5})+P(\\text{sum of 7})\\)\n\\(=\\frac{4}{36}+\\frac{6}{36}=\\frac{10}{36}\\)\n\n\n\n\n4.2.10 Odds\nMany people like to talk about the odds of something happening or not happening. Mathematicians, statisticians, and scientists prefer to deal with probabilities since odds are difficult to work with, but gamblers prefer to work in odds for figuring out how much they are paid if they win.\nThe actual odds against event \\(A\\) occurring are the ratio \\(\\frac{P(\\text{not }A)}{P(A)}\\), usually expressed in the form \\(a:b\\) or \\(a\\) to \\(b\\), where \\(a\\) and \\(b\\) are integers with no common factors.\nThe actual odds in favor event \\(A\\) occurring are the ratio \\(\\frac{P(A)}{P(\\text{not }A)}\\), which is the reciprocal of the odds against. If the odds against event \\(A\\) are \\(a:b\\), then the odds in favor event \\(A\\) are \\(b:a\\).\nThe payoff odds against event \\(A\\) occurring are the ratio of the net profit (if you win) to the amount bet.\npayoff odds against event \\(A\\) = (net profit) : (amount bet)\n\n\n4.2.11 Example: Odds Against and Payoff Odds\nIn the game of Craps, if a shooter has a come-out roll of a 7 or an 11, it is called a natural and the pass line wins. The payoff odds are given by a casino as \\(1:1\\).\n\nFind the probability of a natural.\nFind the actual odds for a natural.\nFind the actual odds against a natural.\nIf the casino pays 1:1, how much profit does the casino make on a \\$10 bet?\n\n\n4.2.11.1 Solution\n\nFind the probability of a natural.\nA natural is a 7 or 11. The sample space is\nSS = {(1,1), (1,2), (1,3), (1,4), (1,5), (1,6), (2,1), (2,2), (2,3), (2,4), (2,5), (2,6), (3,1), (3,2), (3,3), (3,4), (3,5), (3,6), (4,1), (4,2), (4,3), (4,4), (4,5), (4,6), (5,1), (5,2), (5,3), (5,4), (5,5), (5,6), (6,1), (6,2), (6,3), (6,4), (6,5), (6,6)}\nThe event space is {(1,6), (2,5), (3,4), (4,3), (5,2), (6,1), (5,6), (6,5)}\nSo \\(P(\\text{7 or 11})=\\frac{8}{36}\\)\nFind the actual odds for a natural.\n\\(\\text{odds of natural}=\\frac{P(\\text{7 or 11})}{P(\\text{not 7 or 11})} =\\frac{\\frac{8}{36}}{1-\\frac{8}{36}} =\\frac{\\frac{8}{36}}{\\frac{28}{36}} =\\frac{8}{28}=\\frac{2}{7}\\)\nFind the actual odds against a natural.\n\\(\\text{odds of against a natural}=\\frac{P(\\text{not 7 or 11})}{P(\\text{7 or 11})} =\\frac{1-\\frac{8}{36}}{\\frac{8}{36}} =\\frac{\\frac{28}{36}}{\\frac{8}{36}} =\\frac{28}{8}=\\frac{3.5}{1}\\)\nIf the casino pays 1:1, how much profit does the casino make on a \\$10 bet?\nThe actual odds are 3.5 to 1 while the payoff odds are 1 to 1. The casino pays you \\$10 for your \\$10 bet. If the casino paid you the actual odds, they would pay \\$3.50 on every \\$1 bet, and on \\$10, they pay $3.5*10$ =\\$35. Their profit is \\(35-10\\)= \\$25.\n\n\n\n\n4.2.12 Homework for Theoretical Probability Section\n\nIn Homework for Empirical Probability Section, the probabilities of each color of M&Ms in a packet were found. Use that information to answer the following questions.\n\n\n\nFind the probability of choosing a green or red M&M.\nFind the probability of choosing a blue, red, or yellow M&M.\nFind the probability of not choosing a brown M&M.\nFind the probability of not choosing a green M&M.\n\n\n\nIn Homework for Empirical Probability Section, the probabilities for defects in eyeglasses manufactured by Eyeglassomatic were calculated. Use that information to find the following probabilities.\n\n\n\nFind the probability of picking a lens that is scratched or flaked.\nFind the probability of picking a lens that is the wrong PD or was lost in lab.\nFind the probability of picking a lens that is not scratched.\nFind the probability of picking a lens that is not the wrong shape.\n\n\n\nAn experiment is to flip a fair coin three times.\n\n\n\nState the sample space.\nFind the probability of getting exactly two heads. Make sure you state the event space.\nFind the probability of getting at least two heads. Make sure you state the event space.\nFind the probability of getting an odd number of heads. Make sure you state the event space.\nFind the probability of getting all heads or all tails. Make sure you state the event space.\nFind the probability of getting exactly two heads or exactly two tails.\nFind the probability of not getting an odd number of heads.\n\n\n\nAn experiment is rolling a fair die and then flipping a fair coin.\n\n\n\nState the sample space.\nFind the probability of getting a head. Make sure you state the event space.\nFind the probability of getting a 6. Make sure you state the event space.\nFind the probability of getting a 6 or a head.\nFind the probability of getting a 3 and a tail.\n\n\n\nAn experiment is rolling two fair dice.\n\n\n\nState the sample space.\nFind the probability of getting a sum of 3. Make sure you state the event space.\nFind the probability of getting the first die is a 4. Make sure you state the event space.\nFind the probability of getting a sum of 8. Make sure you state the event space.\nFind the probability of getting a sum of 3 or sum of 8.\nFind the probability of getting a sum of 3 or the first die is a 4.\nFind the probability of getting a sum of 8 or the first die is a 4.\nFind the probability of not getting a sum of 8.\n\n\n\nAn experiment is pulling one card from a fair deck.\n\n\n\nState the sample space.\nFind the probability of getting a Ten. Make sure you state the event space.\nFind the probability of getting a Diamond. Make sure you state the event space.\nFind the probability of getting a Club. Make sure you state the event space.\nFind the probability of getting a Diamond or a Club.\nFind the probability of getting a Ten or a Diamond.\n\n\n\nAn experiment is pulling a ball from an urn that contains 3 blue balls and 5 red balls.\n\n\n\nFind the probability of getting a red ball.\nFind the probability of getting a blue ball.\nFind the odds for getting a red ball.\nFind the odds for getting a blue ball.\n\n\n\nIn the game of roulette, there is a wheel with spaces marked 0 through 36 and a space marked 00.\n\n\n\nFind the probability of winning if you pick the number 7 and it comes up on the wheel.\nFind the odds against winning if you pick the number 7.\nThe casino will pay you \\$20 for every dollar you bet if your number comes up. How much profit is the casino making on the bet?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Probability.html#conditional-probability",
    "href": "Probability.html#conditional-probability",
    "title": "4  Probability",
    "section": "4.3 Conditional Probability",
    "text": "4.3 Conditional Probability\nSuppose you want to figure out if you should buy a new car. When you first go and look, you find two cars that you like the most. In your mind they are equal, and so each has a 50% chance that you will pick it. Then you start to look at the reviews of the cars and realize that the first car has had 40% of them needing to be repaired in the first year, while the second car only has 10% of the cars needing to be repaired in the first year. You could use this information to help you decide which car you want to actually purchase. Both cars no longer have a 50% chance of being the car you choose. You could actually calculate the probability you will buy each car, which is a conditional probability. You probably wouldn’t do this, but it gives you an example of what a conditional probability is.\nConditional probabilities are probabilities calculated after information is given. This is where you want to find the probability of event \\(A\\) happening after you know that event \\(B\\) has happened. If you know that \\(B\\) has happened, then you don’t need to consider the rest of the sample space. You only need the outcomes that make up event \\(B\\). Event \\(B\\) becomes the new sample space, which is called the restricted sample space, \\(R\\). If you always write a restricted sample space when doing conditional probabilities and use this as your sample space, you will have no trouble with conditional probabilities. The notation for conditional probabilities is \\(P(\\text{A, given B})=P(A|B)\\). The event following the vertical line is always the restricted sample space.\n\n4.3.1 Example: Conditional Probabilities\n\nSuppose you roll two dice. What is the probability of getting a sum of 5, given that the first die is a 2?\nSuppose you roll two dice. What is the probability of getting a sum of 7, given the first die is a 4?\nSuppose you roll two dice. What is the probability of getting the second die a 2, given the sum is a 9?\nSuppose you pick a card from a deck. What is the probability of getting a Spade, given that the card is a Jack?\nSuppose you pick a card from a deck. What is the probability of getting an Ace, given the card is a Queen?\n\n\n4.3.1.1 Solution\n\nSuppose you roll two dice. What is the probability of getting a sum of 5, given that the first die is a 2?\nSince you know that the first die is a 2, then this is your restricted sample space, \\(R\\) = {(2,1), (2,2), (2,3), (2,4), (2,5), (2,6)} Out of this restricted sample space, the way to get a sum of 5 is {(2,3)}. Thus\n\\(P(\\text{sum of 5, given first die a 2})=P(\\text{sum of 5}|\\text{first die 2})=\\frac{1}{6}\\)\nSuppose you roll two dice. What is the probability of getting a sum of 7, given the first die is a 4?\nSince you know that the first die is a 4, this is your restricted sample space, \\(R\\) = {(4,1), (4,2), (4,3), (4,4), (4,5), (4,6)} Out of this restricted sample space, the way to get a sum of 7 is {(4,3)}. Thus\n\\(P(\\text{sum of 7, given first die a 4})=P(\\text{sum of 7}|\\text{first die 4})=\\frac{1}{6}\\)\nSuppose you roll two dice. What is the probability of getting the second die a 2, given the sum is a 9?\nSince you know the sum is a 9, this is your restricted sample space, \\(R\\) = {(3,6), (4,5), (5,4), (6,3)}. Out of this restricted sample space there is no way to get the second die a 2. Thus\n\\(P(\\text{first die a 2, given sum is 9})=P(\\text{1st die a 2}|\\text{sum of 9})=\\frac{0}{4}\\)\nSuppose you pick a card from a deck. What is the probability of getting a Spade, given that the card is a Jack?\nSince you know that the card is a Jack, this is your restricted sample space, \\(R\\) = {JS, JC, JD, JH}. Out of this restricted sample space, the way to get a Spade is {JS}. Thus\n\\(P(\\text{spade, given card a Jack})=P(\\text{spade}|\\text{Jack})=\\frac{1}{4}\\)\nSuppose you pick a card from a deck. What is the probability of getting an Ace, given the card is a Queen?\nSince you know that the card is a Queen, then this is your restricted sample space, \\(R\\) = {QS, QC, QD, QH} Out of this restricted sample space, there is no way to get an Ace, thus\n\\(P(\\text{Ace, given Queen})=P(\\text{Ace}|\\text{Queen})=\\frac{0}{4}\\)\n\nIf you look at the results of Example: Calculating Theoretical Probabilities 2 part d and Example: Calculating Theoretical Probabilities part b, you will notice that you get the same answer. This means that knowing that the first die is a 4 did not change the probability that the sum is a 7. This added knowledge did not help you in any way. It is as if that information was not given at all. However, if you compare example Example: Calculating Theoretical Probabilities 2 part b and Example: Calculating Theoretical Probabilities part a, you will notice that they are not the same answer. In this case, knowing that the first die is a 2 did change the probability of getting a sum of 5. In the first case, the events sum of 7 and first die is a 4 are called independent events. In the second case, the events sum of 5 and first die is a 2 are called dependent events.\nEvents \\(A\\) and \\(B\\) are considered independent events if the fact that one event happens does not change the probability of the other event happening. In other words, events \\(A\\) and \\(B\\) are independent if the fact that \\(B\\) has happened does not affect the probability of event \\(A\\) happening and the fact that \\(A\\) has happened does not affect the probability of event \\(B\\) happening. Otherwise, the two events are dependent.\nIn symbols, \\(A\\) and \\(B\\) are independent if \\(P(A|B)=P(A)\\) or \\(P(B|A)=P(B)\\)\n\n\n\n4.3.2 Example: Independent Events\n\nSuppose you roll two dice. Are the events “sum of 7” and “first die is a 3” independent?\nSuppose you roll two dice. Are the events “sum of 6” and “first die is a 4” independent?\nSuppose you pick a card from a deck. Are the events “Jack” and “Spade” independent?\nSuppose you pick a card from a deck. Are the events “Heart” and “Red” card independent?\nSuppose you have two children via separate births. Are the events “the first is a boy” and “the second is a girl” independent?\nSuppose you flip a coin 50 times and get a head every time, what is the probability of getting a head on the next flip?\n\n\n4.3.2.1 Solution\n\nSuppose you roll two dice. Are the events “sum of 7” and “first die is a 3” independent?\nTo determine if they are independent, you need to see if \\(P(\\text{sum of 7| first die a 3}=P(\\text{sum of 7})\\) or the other way around. It doesn’t matter which order these are calculated in, so pick whichever is easier. sum of 7 = {(1,6), (2,5), (3,4), (4,3), (5,2), (6,1)} first die is a 3 = {(3,1), (3,2), (3,3), (3,4), (3,5), (3,6)} \\(P(\\text{sum of 7| first die a 3})\\) means that you assume that first die is a 3 has happened. The restricted sample space is the first die is a 3, \\(R\\) = {(3,1), (3,2), (3,3), (3,4), (3,5), (3,6)} In this restricted sample space, the way for a sum of 7 to happen is {(3,4)}, so \\(P(\\text{sum of 7| first die a 3})=\\frac{1}{6}\\) The \\(P(\\text{sum of 7})=\\frac{6}{36}=\\frac{1}{6}\\). Since \\(P(\\text{sum of 7| first die a 3})=P(\\text{sum of 7})\\), the “sum of 7” and “first die is a 3” are independent events.\nSuppose you roll two dice. Are the events “sum of 6” and “first die is a 4” independent?\nTo determine if they are independent, you need to see if\n\\(P(\\text{sum of 6| first die a 4}) = P(\\text{sum of 6})\\).\nAgain it doesn’t matter what order you do this in. Do which is easier. sum of 6 = {(1,5), (2,4), (3,3), (4,2), (5,1)} and first die is a 4 = {(4,1), (4,2), (4,3), (4,4), (4,5), (4,6)}, if want \\(P(\\text{sum of 6| first die a 4})\\), the restricted sample space is 1st die is a 4, \\(R\\) = {(4,1), (4,2), (4,3), (4,4), (4,5), (4,6)} In this restricted sample space, the way to get a sum of 6 is {(4,2)}, so \\(P(\\text{sum of 6| first die a 4})=\\frac{1}{6}\\). The \\(P(\\text{sum of 6})=\\frac{5}{36}\\) Notice \\(P(\\text{sum of 6| first die a 4})\\ne P(\\text{sum of 6})\\), Thus “sum of 6” and “first die is a 4” are dependent.\nSuppose you pick a card from a deck. Are the events “Jack” and “Spade” independent?\nTo determine if they are independent, you need to see if\n\\(P(Jack| space)=P(Jack)\\).\nRemember, you can do this the other order if you wish. Jack = {JS, JC, JD, JH} and \\(R\\) = Spade {2S, 3S, 4S, 5S, 6S,7S, 8S, 9S, 10S, JS, QS, KS, AS} For \\(P(\\text{Jack| Spade})\\), the restricted sample space is Spade, \\(R\\) = {2S, 3S, 4S, 5S, 6S, 7S, 8S, 9S, 10S, JS, QS, KS, AS}. In this restricted sample space, the way to get a Jack is {JS}, so \\(P(\\text{Jack| Spade})=\\frac{1}{13}\\). The \\(P(Jack)=\\frac{4}{52}=\\frac{1}{13}\\) Since \\(P(\\text{Jack| Spade})=P(\\text{Jack})\\), “Jack” and “Spade” are independent.\nSuppose you pick a card from a deck. Are the events “Heart” and “Red” card independent?\nTo determine if they are independent, you need to see if\n\\(P(\\text{Heart| Red})=P(\\text{Heart})\\).\nHeart = {2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH, AH} and Red card = {2D, 3D, 4D, 5D, 6D, 7D, 8D, 9D, 10D, JD, QD,KD, AD, 2H, 3H, 4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH, AH}. The restricted sample space is, red card, $R$ = {2D, 3D, 4D, 5D, 6D, 7D, 8D, 9D, 10D, JD, QD, KD, AD, 2H, 3H,4H, 5H, 6H, 7H, 8H, 9H, 10H, JH, QH, KH, AH} In this restricted sample space, the way to get a heart is 13, and\n\\(P(\\text{Heart| Red})=\\frac{13}{26}\\).\n\\(P(\\text{Heart})=\\frac{13}{52}\\)\nNote \\(P(\\text{Heart| Red})\\ne P(\\text{Heart})\\), so, “Heart” and “Red” card are dependent.\nSuppose you have two children via separate births. Are the events “the first is a boy” and “the second is a girl” independent?\nIn this case, you actually don’t need to do any calculations. The sex of one child does not affect the sex of the second child. The events are independent.\nSuppose you flip a coin 50 times and get a head every time, what is the probability of getting a head on the next flip?\nSince one flip of the coin does not affect the next flip (the coin does not remember what it did the time before), the probability of getting a head on the next flip is still one-half.\n\n\n\n\n4.3.3 Multiplication Rule:\nTwo more useful formulas:\nIf two events are dependent, then \\(P(\\text{A and B})=P(A)*P(B|A)\\)\nIf two events are independent, then \\(P(\\text{A and B})=P(A)*P(B)\\)\nThese two formulas are useful if the sample space is too large to write out, but it the sample space isn’t too large, it is better to find probabilities of and statements using the sample space techniques.\nIf you solve the first equation for \\(P(B|A)\\), you obtain \\(P(B|A)=\\frac{P(\\text{A and B})}{P(A)}\\), which is a formula to calculate a conditional probability. However, it is easier to find a conditional probability by using the restricted sample space and counting unless the sample space is large.\n\n\n4.3.4 Example: Multiplication Rule\n\nSuppose you pick three cards from a deck, what is the probability that they are all Queens if the cards are not replaced after they are picked?\nSuppose you pick three cards from a deck, what is the probability that they are all Queens if the cards are replaced after they are picked and before the next card is picked?\n\n\n4.3.4.1 Solution\n\nSuppose you pick three cards from a deck, what is the probability that they are all Queens if the cards are not replaced after they are picked?\nThis sample space is too large to write out, so using the multiplication rule makes sense. Since the cards are not replaced, then the probability will change for the second and third cards. They are dependent events. This means that on the second draw there is one less Queen and one less card, and on the third draw there are two less Queens and 2 less cards.\n\\(P(\\text{3 Queens})\\)\\(=P(\\text{Queen on 1st})*P(\\text{Queen on 2nd, given Queen on first})*P(\\text{Queen on third, given Queens on fist 2 draws})\\)\n\\(=\\frac{4}{52}*\\frac{3}{51}*\\frac{2}{50}\\)\nSuppose you pick three cards from a deck, what is the probability that they are all Queens if the cards are replaced after they are picked and before the next card is picked?\nAgain, the sample space is too large to write out, so using the multiplication rule makes sense. Since the cards are put back, one draw has no affect on the next draw and they are all independent. \\(P(\\text{3 Queens})=P(\\text{Queen on 1st})*P(\\text{Queen on 2nd})*P(\\text{Queen on 3rd})\\) \\(=\\frac{4}{52}*\\frac{4}{52}*\\frac{4}{52}\\)\n\n\n\n\n4.3.5 Example: Application Problem\nA project conducted by the Australian Federal Office of Road Safety asked people many questions about their cars. One question was the reason that a person chooses a given car, and that data is in Table 4.3 (Car Preferences, 2019).\nCode book for Data Frame Car_pref is below Table 4.3.\nA contingency table, is a cross tabulation of the data into different categories. As an example, a contingency table of if the person has children under 5 and their current car is below, the following command in R Studio can be used to create this table.\n\ntally(~Kids5+ActCar, data=Car_pref, margins=TRUE)\n\n       ActCar\nKids5   large medium small Total\n  no       87     90    94   271\n  yes      13     10     6    29\n  Total   100    100   100   300\n\n\n\nFind the probability that a person questioned has kids under 5.\nFind the probability that a person questioned actually has a large car.\nFind the probability that a person questioned actually has a large car and has children under 5.\nFind the probability that a person questioned has a large car given that they have children under 5.\nFind the probability that a person has a large car or has children under 5.\nFind the probability that a person questioned has children under 5 given that they have a large car.\nAre the events that a person questioned has a “large car” and “kids under 5” independent events? Why or why not?\n\n\n4.3.5.1 Solution\n\nFind the probability that a person questioned has kids under 5.\nFirst, you need to find the number of people questioned. Add the first row, there are 150 people who did not have kids under 5. Adding the second row, there are 29 people who do have kids under 5. Also, you can add the columns. There are 100 people who have large cars, 100 people who have medium cars, and 100 who have small cars. Adding either the row totals or the columns totals, gives you 300 people in the study. Out of the 300 people, 29 people had kids under 5. So the\n\\(P(\\text{kids under 5})=\\frac{29}{300}\\).\nSo 9.7% of the people questioned had children under 5.\nFind the probability that a person questioned actually has a large car.\nThere are 100 people with large cars out of 300 people. So,\n\\(P(\\text{large car})= \\frac{100}{300}\\). There are 33% of the people who have a large car.\nFind the probability that a person questioned actually has a large car and has children under 5.\nThere are 13 people who have a large car and have children under 5, so the \\(P(\\text{large car and children under 5})=\\frac{13}{300}=0.043\\). 4.3% of all people surveyed have a large car and children under 5.\nFind the probability that a person questioned has a large car given that they have children under 5.\nIn this case you know that the person had children under 5. You don’t need to consider the people who don’t. You only need to look at the row with people who have have children under 5. In that row, look to see how many people have a large car. There are 13 people with a large car out of the 29 people with kids under 5. So,\\(P(\\text{large car|kids under 5})=\\frac{13}{29}=0.45\\)\nThere is 45% chance that a person with a large car have children under 5.\nFind the probability that a person has a large car or has children under 5.\nThis problem can be done two ways. One is to use the addition formula, but a better way is to realize that there are 29 people who have kids under 5, and there are 100 people who have a large car. That is 34 people. But the 13 people who have large cars and kids under 5 were just counted twice. So subtract the 13 people from the 34. That give 21 people who have either kids under 5 or a large car. So\n\\(P(\\text{large car or kids under 5})=\\frac{21}{300}=0.07\\).\nThat means 70% of the people questioned has a large car or has children under 5.\nFind the probability that a person questioned has children under 5 given that they have a large car.\nIn this case you know that the person has a large car. You don’t need to include the people who have medium or small cars. You only need to consider the column headed by large. In that column, there are 100 people who have large cars and out of those 100, 13 have children under 5. So, \\(P(\\text{kids under 5|large})=\\frac{13}{100}=0.13\\). Thus 13% of people have children under 5 given that they have a large car.\nAre the events that a person questioned has a “large car” and “kids under 5” independent events? Why or why not?\nIn order for these events to be independent, either \\(P(\\text{kids under 5|large car})=P(\\text{kids under 5})\\) or \\(P(\\text{large car|kids under 5})=P(\\text{large car})\\) have to be true. Part (d) showed \\(P(\\text{kids under 5|large car})=0.44\\) and part (b) showed \\(P(\\text{kids under 5})==0.33\\). Since these are not equal, then these two events are dependent.\n\nA big deal has been made about the difference between dependent and independent events while calculating the probability of *and* compound events. You must multiply the probability of the first event with the conditional probability of the second event.\nWhy do you care? Calculating probabilities when performing sampling is important, as this will be seen later. But here is a simplification that can make the calculations a lot easier: when the sample size is very small compared to the population size, you can assume that the conditional probabilities just don’t change very much over the sample.\nFor example, consider acceptance sampling. Suppose there is a big population of parts delivered to you factory, say 12,000 parts. Suppose there are 85 defective parts in the population. You decide to randomly select ten parts, and see if you should reject the shipment. What is the probability of rejecting the shipment?\nThere are many different ways you could reject the shipment. For example, maybe the first three parts are good, one is bad, and the rest are good. Or all ten parts could be bad, or maybe the first five. So many ways to reject! But there is only one way that you’d accept the shipment: if all ten parts are good. That would happen if the first part is good, and the second part is good, and the third part is good, and so on. Since the probability of the second part being good is (slightly) dependent on whether the first part was good, technically you should take this into consideration when you calculate the probability that all ten are good.\nThe probability of getting the first sampled part good is $\\frac{1200-85}{1200}=\\frac{11915}{1200}$. So the probability that all ten being good is\n\\(\\frac{11915}{1200}*\\frac{11914}{1200}*\\frac{11913}{1200}*\\cdots*\\frac{11906}{1200}=0.931357\\).\nIf instead you assume that the probability doesn’t change much, you get \\((\\frac{11915}{12000})^{10}=0.931382\\). So as you can see, there is not much difference. So here is the rule: if the sample is very small compared to the size of the population, then you can assume that the probabilities are independent, even though they aren’t technically. By the way, the probability of rejecting the shipment is \\(1-0.9314=0.0686\\).\n\n\n\n4.3.6 Homework for Conditional Probability Section\n\nAre owning a refrigerator and owning a car independent events? Why or why not?\nAre owning a computer, tablet, or smart phone and paying for Internet service independent events? Why or why not?\nAre passing your statistics class and passing your biology class independent events? Why or why not?\nAre owning a bike and owning a car independent events? Why or why not?\nAn experiment is picking a card from a fair deck.\n\n\n\nWhat is the probability of picking a Jack given that the card is a face card?\nWhat is the probability of picking a heart given that the card is a three?\nWhat is the probability of picking a red card given that the card is an ace?\nAre the events Jack and face card independent events? Why or why not?\nAre the events red card and ace independent events? Why or why not?\n\n\n\nAn experiment is rolling two dice.\n\n\n\nWhat is the probability that the sum is 6 given that the first die is a 5?\nWhat is the probability that the first die is a 3 given that the sum is 11?\nWhat is the probability that the sum is 7 given that the fist die is a 2?\nAre the two events sum of 6 and first die is a 5 independent events? Why or why not?\nAre the two events sum of 7 and first die is a 2 independent events? Why or why not?\n\n\n\nYou flip a coin four times. What is the probability that all four of them are heads?\nYou flip a coin six times. What is the probability that all six of them are heads?\nYou pick three cards from a deck with replacing the card each time before picking the next card. What is the probability that all three cards are kings?\nYou pick three cards from a deck without replacing a card before picking the next card. What is the probability that all three cards are kings?\nA project conducted by the Australian Federal Office of Road Safety asked people many questions about their cars. One question was the reason that a person chooses a given car, and that data is in Table 4.3 (Car Preferences, 2019).\n\nCode book for Data Frame Car_pref is below Table 4.3.\nThe contingency table for the sex of a person and the size car the person prefers is in table below\n\ntally(~PreferCar+Sex, data=Car_pref, margins=TRUE)\n\n         Sex\nPreferCar female male Total\n   4           6   17    23\n   large      26   47    73\n   medium     75   61   136\n   small      43   25    68\n   Total     150  150   300\n\n\n\nWhat is the probability that a person questioned was female?\nWhat is the probability that a person questioned prefers a medium car?\nWhat is the probability that a person questioned prefers a medium car given that the person was female?\nWhat is the probability that a person questioned was a female and prefers a medium car?\nWhat is the probability that a person questioned was a female or prefers a medium car?\nAre the events person questioned is a female and person questioned prefers a medium car mutually exclusive? Why or why not?\nAre the events person questioned is a female and person questioned prefers a medium car independent? Why or why not?\n\n\n\nResearchers watched groups of dolphins off the coast of Ireland in 1998 to determine what activities the dolphins partake in at certain times of the day (Activities of Dolphin Groups, 2019). The numbers in table \\#4.3.5 represent the number of groups of dolphins that were partaking in an activity at certain times of days.\n\n\nDolphin&lt;- read.csv( \"https://krkozak.github.io/MAT160/dolphins.csv\") \nknitr::kable(head(Dolphin))\n\n\n\nTable 4.4: Dolphin Activity\n\n\n\n\n\n\nactivity\nperiod\n\n\n\n\nTravel\nMorning\n\n\nTravel\nMorning\n\n\nTravel\nMorning\n\n\nTravel\nMorning\n\n\nTravel\nMorning\n\n\nTravel\nMorning\n\n\n\n\n\n\n\n\nCode book for Data Frame Dolphin\nDescription Groups of dolphins were observed off the coast of Iceland near Keflavik in 1998. The data here give the time of the day and the main activity of the group, whether travelling quickly, feeding or socializing. The dolphin groups varied in size - usually feeding or socializing groups were larger than travelling groups.\nUsage Dolphin\nFormat\nThis data frame contains the following columns:\nActivity: Main activity of group: travelling (Travel), feeding (Feed) or socializing (Social)\nPeriod: Time of the day: Morning, Noon, Afternoon or Evening\nSource Activities of Dolphin Groups. (n.d.). Retrieved July 12, 2019, from http://www.statsci.org/data/general/dolpacti.html\nReferences Marianne Rasmussen, Department of Biology, University of Southern Denmark, Odense, Denmark.\n\nWhat is the probability that a dolphin group is partaking in travel?\nWhat is the probability that a dolphin group is around in the morning?\nWhat is the probability that a dolphin group is partaking in travel given that it is morning?\nWhat is the probability that a dolphin group is around in the morning given that it is partaking in socializing?\nWhat is the probability that a dolphin group is around in the afternoon given that it is partaking in feeding?\nWhat is the probability that a dolphin group is around in the afternoon and is partaking in feeding?\nWhat is the probability that a dolphin group is around in the afternoon or is partaking in feeding?\nAre the events dolphin group around in the afternoon and dolphin group feeding mutually exclusive events? Why or why not?\nAre the events dolphin group around in the morning and dolphin group partaking in travel independent events? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Probability.html#counting-techniques",
    "href": "Probability.html#counting-techniques",
    "title": "4  Probability",
    "section": "4.4 Counting Techniques",
    "text": "4.4 Counting Techniques\nThere are times when the sample space or event space are very large, that it isn’t feasible to write it out. In that case, it helps to have mathematical tools for counting the size of the sample space and event space. These tools are known as counting techniques.\n\n4.4.1 Multiplication Rule in Counting Techniques\nIf task 1 can be done ways, task 2 can be done ways, and so forth to task \\(n\\) being done ways. Then the number of ways to do task 1, 2,…, \\(n\\) together would be \\(m_{1}*m_{2}*\\cdots *m_{n}\\).\n\n\n4.4.2 Example: Multiplication Rule in Counting\nA menu offers a choice of 3 salads, 8 main dishes, and 5 desserts. How many different meals consisting of one salad, one main dish, and one dessert are possible?\n\n4.4.2.1 Solution\nThere are three tasks, picking a salad, a main dish, and a dessert. The salad task can be done 3 ways, the main dish task can be done 8 ways, and the dessert task can be done 5 ways. The ways to pick a salad, main dish, and dessert are \\(3*8*5=120\\).\n\n\n\n4.4.3 Example: Multiplication Rule in Counting\nHow many three letter “words” can be made from the letters a, b, and c with no letters repeating? A “word” is just an ordered group of letters. It doesn’t have to be a real word in a dictionary.\n\n4.4.3.1 Solution\nThere are three tasks that must be done in this case. The tasks are to pick the first letter, then the second letter, and then the third letter. The first task can be done 3 ways since there are 3 letters. The second task can be done 2 ways, since the first task took one of the letters. The third task can be done 1 way, since the first and second task took two of the letters. There are \\(3*2*1=6\\)\nIn Example: Multiplication Rule, the solution was found by find \\(3*2*1\\). Many counting problems involve multiplying a list of decreasing numbers. This is called a factorial. There is a special symbol for this.\n\n\n\n4.4.4 Factorial\n\\(n!=n(n-1)(n-2)*\\cdots*2*1\\)\nAs an example: \\(5!=5*4*3*2*1=120\\) \\(8!=8*7*6*5*4*3*2*1=40320\\)\n0 factorial is defined to be \\(0!=1\\) and 1 factorial is defined to be \\(1!=1\\). In rStudio, the command for factorial is factorial(number). As an example \\(7!\\) using r Studio would be\n\nfactorial(7)\n\n[1] 5040\n\n\nSometimes you are trying to select \\(r\\) objects from \\(n\\) total objects. The number of ways to do this depends on if the order you choose the \\(r\\) objects matters or if it doesn’t. As an example if you are trying to call a person on the phone, you have to have their number in the right order. Otherwise, you call someone you didn’t mean to. In this case, the order of the numbers matters. If however you were picking random numbers for the lottery, it doesn’t matter which number you pick first. As long as you have the same numbers that the lottery people pick, you win. In this case the order doesn’t matter. A permutation is an arrangement of items with a specific order. You use permutations to count items when the order matters. When the order doesn’t matter you use combinations. A combination is an arrangement of items when order is not important. When you do a counting problem, the first thing you should ask yourself is “does order matter?”\n\n\n4.4.5 Permutation Formula\nPicking \\(r\\) objects from \\(n\\) total objects when order matters\n\\(_{n}P_{r}=\\frac{n!}{(n-r)!}\\).\n\n\n4.4.6 Combination Formula\nPicking \\(r\\) objects from \\(n\\) total objects when order doesn’t matter\n\\(_{n}C_{r}=\\frac{n!}{r!(n-r)!}\\)\nMost calculators have a factorial button on them, and many have the combination and permutation functions also.\n\n\n4.4.7 Homework for Counting Techniques Sections\n\nYou are going to a benefit dinner, and need to decide before the dinner what you want for salad, main dish, and dessert. You have 2 different salads to choose from, 3 main dishes, and 5 desserts. How many different meals are available?\nHow many different phone numbers are possible in the area code 928?\nYou are opening a T-shirt store. You can have long sleeves or short sleeves, three different colors, five different designs, and four different sizes. How many different shirts can you make?\nThe California license plate has one number followed by three letters followed by three numbers. How many different license plates are there?\nFind \\(_9P_4\\)\nFind \\(_{10}P_6\\)\nFind \\(_{10}C_5\\)\nFind \\(_{20}P_4\\)\nYou have a group of twelve people. You need to pick a president, treasurer, and secretary from the twelve. How many different ways can you do this?\nA baseball team has a 25-person roster. A batting order has nine people. How many different batting orders are there?\nAn urn contains five red balls, seven yellow balls, and eight white balls. How many different ways can you pick two red balls?\nHow many ways can you choose seven people from a group of twenty?",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Probability</span>"
    ]
  },
  {
    "objectID": "Discrete Probability Distribution.html",
    "href": "Discrete Probability Distribution.html",
    "title": "5  Discrete Probability Distribution",
    "section": "",
    "text": "5.0.1 Examples of each:\nHow tall is a plant given a new fertilizer? Continuous. This is something you measure.\nHow many fleas are on prairie dogs in a colony? Discrete. This is something you count.\nNow suppose you put all the values of the random variable together with the probability that the random variable would occur. You could then have a distribution like before, but now it is called a probability distribution since it involves probabilities. A probability distribution is an assignment of probabilities to the values of the random variable.\nWith the idea of a probability distribution, the next thing is to look at the basics of a probability distribution.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Discrete Probability Distribution.html#basics-of-probability-distributions",
    "href": "Discrete Probability Distribution.html#basics-of-probability-distributions",
    "title": "5  Discrete Probability Distribution",
    "section": "5.1 Basics of Probability Distributions",
    "text": "5.1 Basics of Probability Distributions\nAs a reminder, a variable or what will be called the random variable from now on, is represented by the letter \\(x\\) and it represents a quantitative (numerical) variable that is measured or observed in an experiment.\nAs with probabilities, probability distributions, have the properties, \\(0 \\le P(outcome)\\le1\\) and \\(\\sum{P(outcomes)}=1\\)\n\n5.1.1 Example: Probability Distribution\nThe 2010 U.S. Census found the chance of a household being a certain size. The data is in Table 5.1 (\\“Households by age,\\” 2013). Note, the category 7 is really 7 or more people in the household. Draw the probability distribution and find the mean, variance, and standard deviation.\n\n\n\n\nTable 5.1: Household Size from U.S. Census of 2010\n\n\n\n\n\n\nsize\nprob\n\n\n\n\n1\n0.267\n\n\n2\n0.336\n\n\n3\n0.158\n\n\n4\n0.137\n\n\n5\n0.063\n\n\n6\n0.024\n\n\n7\n0.015\n\n\n\n\n\n\n\n\n\n5.1.1.1 Solution\nIn this case, the random variable is \\(x\\) = size of household. This is a discrete random variable, since you are counting the number of people in a household.\nIt is a probability distribution since you have the \\(x\\) value and the probabilities that go with it, all of the probabilities are between zero and one, and the sum of all of the probabilities is one.\nYou can give a probability distribution in table form (as in Table 5.1) or as a graph. The graph looks like a histogram. To graph the histogram, use the following commands and process in rStudio.\nFirst you need to load a few packages using the following commands. These packages are “arm” and “Weighted.Desc.Stat”. If these packages have not been installed, they need to be installed before you can load them using library. Once you have installed them, they will always be available in /r Studio to be loaded. To load a package, use the command\nlibrary(“name of package”)\nIn this case the packages you need are arm and Weighted.Desc.Stat.\n\n\nLoading required package: MASS\n\n\nLoading required package: Matrix\n\n\nLoading required package: lme4\n\n\n\narm (Version 1.14-4, built: 2024-4-1)\n\n\nWorking directory is /Users/mori/CSU Fullerton Dropbox/Mortaza Jamshidian/Statistics Text Using Rguroo Positron/Statistics-Using-Technology-book\n\n\nTo draw the probability distribution, use the following command. First you need to create variables for \\(x\\), size, and the probability, \\(prob\\), in r Studio. Then you can draw the distribution.\n(ref:discrete-histogram-cap) Histogram of Size of Family\n\ndiscrete.histogram(Household$size,Household$prob, bar.width = 1, main=\"Size of family\", xlab=\"Size\") \n\n\n\n\n\n\n\nFigure 5.1: Histogram of Household Size from U.S. Census of 2010\n\n\n\n\n\nThis command is different than the commands used in the past, but is needed for discrete probability distributions. So putting a title on the graph uses the command main=“title you want” instead of title= as before.\nNotice this graph Figure 5.1 is skewed right, which means that most families have around 2 people in them and larger families become more and more rare.\nTo find the mean, variance, and standard deviation using r Studio, make sure that the package Weighted.Desc.Stat is loaded, then use the following commands.\n\nw.mean(Household$size, Household$prob) \n\n[1] 2.525\n\nw.var(Household$size, Household$prob) \n\n[1] 2.023375\n\nw.sd(Household$size, Household$prob)\n\n[1] 1.422454\n\n\nThe mean is 2.525 people, the variance is 2.02 \\(people^2\\), and the standard deviation is 1.42 people.\nWhen calculating the mean and standard deviation of a probability distribution, you can consider the population distribution the population even though it was most likely created from a large sample. Since a probability distribution is basically a population, the mean and standard deviation that are calculated are actually the population parameters and not the sample statistics. The notation used is the same as the notation for population mean, \\(\\mu\\), and population standard deviation, $\\sigma$, that was used in chapter 3. Note: the mean can also be thought of as the expected value. It is the value you expect to get if the trials were repeated infinite number of times. The mean or expected value does not need to be a whole number, even if the possible values of \\(x\\) are whole numbers. This means one can find what value they can expect to get in the long run for gambling or insurance including extended warranties using the mean of a probability distribution. First one needs to figure out the probability distribution, and then follow the process in example 5.1.1.\n\n\n\n5.1.2 Example: Calculating the Expected Value\nIn the Arizona lottery game called Pick 3, a player pays \\$1 and then picks a three-digit number. If those three numbers are picked in that specific order the person wins \\$500. What is the expected value in this game?\n\n5.1.2.1 Solution\nTo find the expected value, you need to first create the probability distribution. In this case, the random variable \\(x\\) = winnings. If you pick the right numbers in the right order, then you win \\$500, but you paid \\$1 to play, so you actually win \\$499. If you didn’t pick the right numbers, you lose the \\$1, the \\(x\\) value is $-1$. You also need the probability of winning and losing. Since you are picking a three-digit number, and for each digit there are 10 numbers you can pick with each independent of the others, you can use the multiplication rule. To win, you have to pick the right numbers in the right order. The first digit, you pick 1 number out of 10, the second digit you pick 1 number out of 10, and the third digit you pick 1 number out of 10. The probability of picking the right number in the right order is $\\frac{1}{1000}$. The probability of losing (not winning) would be\n\\(1-\\frac{1}{1000}=\\frac{999}{1000}\\).\nPutting this information into a table will help to organize the information and find the expected value.\n\nProbability Distribution of Lottery\n\n\noutcome\namount\nprobability\n\n\n\n\nwin\n499\n0.001\n\n\nlose\n-1\n0.999\n\n\n\nNow type the values into r using the following command:\nNow to find the expected value, it is the same as finding the mean, though the command is a little different since you don’t have a data frame for this data.\n\nweighted.mean(amount, probability)\n\n[1] -0.5\n\n\nThe expected value (or mean) is -0.5. That is -\\$0.50. Since it is negative, that means you lose \\$0.50 every time you play the Pick 3. It seems you would be better off putting the \\$1 every week into a savings account then playing the Pick 3 lottery.\nThe reason probability is studied in statistics is to help in making decisions in inferential statistics. To understand how that is done the concept of a rare event is needed.\n\n\n\n5.1.3 Rare Event Rule for Inferential Statistics\nIf, under a given assumption, the probability of a particular observed event is extremely small, then you can conclude that the assumption is probably not correct.\nAn example of this is suppose you roll an assumed fair die 1000 times and get a six 600 times, when you should have only rolled a six around 160 times, then you should believe that your assumption about it being a fair die is untrue.\n\n\n5.1.4 Determining if an event is unusual\nIf you are looking at a value of \\(x\\) for a discrete variable, and the P(the variable has a value of \\(x\\) or more) is less than 0.05, then you can consider the \\(x\\) an unusually high value. Another way to think of this is if the probability of getting such a high value is less than 0.05, then the event of getting the value x is unusual.\nSimilarly, if the P(the variable has a value of \\(x\\) or less) is less than 0.05, then you can consider this an unusually low value. Another way to think of this is if the probability of getting a value as small as \\(x\\) is less than 0.05, then the event \\(x\\) is considered unusual.\nWhy is it “\\(x\\) or more” or “\\(x\\) or less” instead of just “\\(x\\)” when you are determining if an event is unusual? Consider this example: you and your friend go out to lunch every day. Instead of Going Dutch (each paying for their own lunch), you decide to flip a coin, and the loser pays for both. Your friend seems to be winning more often than you’d expect, so you want to determine if this is unusual before you decide to change how you pay for lunch (or accuse your friend of cheating). The process for how to calculate these probabilities will be presented in the next section on the binomial distribution. If your friend won 6 out of 10 lunches, the probability of that happening turns out to be about 20.5%, not unusual. The probability of winning 6 or more is about 37.7%. But what happens if your friend won 501 out of 1,000 lunches? That doesn’t seem so unlikely! The probability of winning 501 or more lunches is about 47.8%, and that is consistent with your hunch that this isn’t so unusual. But the probability of winning exactly 501 lunches is much less, only about 2.5%. That is why the probability of getting exactly that value is not the right question to ask: you should ask the probability of getting that value or more (or that value or less on the other side).\nThe value 0.05 will be explained later, and it is not the only value you can use for unusual events.\n\n\n5.1.5 Example: Is the Event Unusual\nThe 2010 U.S. Census found the chance of a household being a certain size. The data is in the table (\\“Households by age,\\” 2013).\nThe 2010 U.S. Census found the chance of a household being a certain size. The data is in Table 5.1 (\\“Households by age,\\” 2013). Note, the category 7 is really 7 or more people in the household.\nState random variable:\nSolution\nState random variable\n\nIs it unusual for a household to have six people in the family?\n\nsize = number of people in a household\n\nIs it unusual for a household to have six people in the family?\nIf you did come upon many families that had six people in the family, what would you think?\nIs it unusual for a household to have four people in the family?\nIf you did come upon a family that has four people in it, what would you think?\n\n\n5.1.5.1 Solution\nTo determine this, you need to look at probabilities. However, you cannot just look at the probability of six people. You need to look at the probability of \\(x\\) being six or less people or the probability of \\(x\\) being six or more people. The\n\\(P(x \\le 6)=P(1)+P(2)+P(3)+P(4)+P(5)+P(6)\\) \\(=0.267+0.336+0.158+0.137+0.063+0.024=0.985\\)\nSince this probability is more than 5%, then six is not an unusually low value.\nThe \\(P(x \\ge 6)=P(6)+P(7)=0.024+0.015=0.039\\)\nSince this probability is less than 5%, then six is an unusually high value. It is unusual for a household to have six people in the family.\n\nIf you did come upon many families that had six people in the family, what would you think?\n\nSince it is unusual for a family to have six people in it, then you may think that either the size of families is increasing from what it was or that you are in a location where families are larger than in other locations.\n\nIs it unusual for a household to have four people in the family?\n\nTo determine this, you need to look at probabilities. Again, look at the probability of \\(x\\) being four or less or the probability of \\(x\\) being four or more. The\n\\(P(x \\le 4)=P(0)+P(1)+P(2)+P(3)+P(4)\\) \\(=0.267+0.336+0.158+0.137=0.898\\)\nSince this probability is more than 5%, four is not an unusually low value.\nThe\n\\(P(\\ge4)=P(4)+P(5)+P(5)+P(7)\\) \\(=0.137+0.063+0.024+0.015=0.239\\)\nSince this probability is more than 5%, four is not an unusually low value. Thus, four is not an unusual size of a family.\n\nIf you did come upon a family that has four people in it, what would you think?\n\nSince it is not unusual for a family to have four members, then you would not think anything is amiss.\n\n\n\n5.1.6 Homework for Basics of Probability Distributions Section\n\nEyeglassomatic manufactures eyeglasses for different retailers. The number of days it takes to fix defects in an eyeglass and the probability that it will take that number of days are in Table 5.2.\n\n\n\nDays&lt;- read.csv(\n  \"https://krkozak.github.io/MAT160/table_5_1_3.csv\") \nknitr::kable(Days)\n\n\n\nTable 5.2: Nuumber of Days to fix Eyeglasses\n\n\n\n\n\n\ndays\nprob\n\n\n\n\n1\n0.249\n\n\n2\n0.108\n\n\n3\n0.091\n\n\n4\n0.123\n\n\n5\n0.133\n\n\n6\n0.114\n\n\n7\n0.070\n\n\n8\n0.046\n\n\n9\n0.019\n\n\n10\n0.013\n\n\n11\n0.010\n\n\n12\n0.008\n\n\n13\n0.006\n\n\n14\n0.004\n\n\n15\n0.002\n\n\n16\n0.002\n\n\n17\n0.001\n\n\n18\n0.001\n\n\n\n\n\n\n\n\n\nState the random variable.\nDraw a histogram of the number of days to fix defects\nFind the mean number of days to fix defects.\nFind the variance for the number of days to fix defects.\nFind the standard deviation for the number of days to fix defects.\nFind probability that a lens will take at least 16 days to make a fix the defect.\nIs it unusual for a lens to take 16 days to fix a defect?\nIf it does take 16 days for eyeglasses to be repaired, what would you think?\n\n\n\nSuppose you have an experiment where you flip a coin three times. You then count the number of heads.\n\n\n\nState the random variable.\nWrite the probability distribution for the number of heads.\nDraw a histogram for the number of heads.\nFind the mean number of heads.\nFind the variance for the number of heads.\nFind the standard deviation for the number of heads.\nFind the probability of having two or more number of heads.\nIs it unusual for to flip two heads?\n\n\n\nThe Ohio lottery has a game called Pick 4 where a player pays \\$1 and picks a four-digit number. If the four numbers come up in the order you picked, then you win \\$2,500. What is your expected value?\nAn LG Dishwasher, which costs \\$800, has a 20% chance of needing to be replaced in the first 2 years of purchase. A two-year extended warranty costs \\$112.10 on a dishwasher. What is the expected value of the extended warranty assuming it is replaced in the first 2 years?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Discrete Probability Distribution.html#binomial-probability-distribution",
    "href": "Discrete Probability Distribution.html#binomial-probability-distribution",
    "title": "5  Discrete Probability Distribution",
    "section": "5.2 Binomial Probability Distribution",
    "text": "5.2 Binomial Probability Distribution\nSection 5.1 introduced the concept of a probability distribution. The focus of the section was on discrete probability distributions. To find the probability distribution for a situation, you usually needed to actually conduct the experiment and collect data. Then you can calculate the experimental probabilities. Normally you cannot calculate the theoretical probabilities. However, there are certain types of experiment that allow you to calculate the theoretical probability. One of those types is called a Binomial Experiment.\nProperties of a binomial experiment (or Bernoulli trial):\n\nFixed number of trials, \\(n\\), which means that the experiment is repeated a specific number of times.\nThe \\(n\\) trials are independent, which means that what happens on one trial does not influence the outcomes of other trials.\nThere are only two outcomes, which are called a success and a failure.\nThe probability of a success doesn’t change from trial to trial, where \\(p\\) = probability of success and \\(q = 1-p\\) = probability of failure.\n\nIf you know you have a binomial experiment, then you can calculate binomial probabilities. This is important because binomial probabilities come up often in real life. Examples of binomial experiments are:\nToss a fair coin ten times, and find the probability of getting two heads.\nQuestion twenty people in class, and look for the probability of more than half being women?\nShoot five arrows at a target, and find the probability of hitting it five times?\n\n5.2.1 Formula for the probabilities for a Binomial experiment\nFirst, the random variable in a binomial experiment is \\(x\\) = number of successes.\nBe careful, a success is not always a good thing. Sometimes a success is something that is bad, like finding a defect. A success just means you observed the outcome you wanted to see happen.\nBinomial Formula for the probability of \\(r\\) successes in \\(n\\) trials is \\(P(X=r)=_nC_r*p^r*q^{n-r}\\)\nwhere \\(_nC_r\\) is the number of combinations of \\(n\\) things taking \\(r\\) at a time. It is read “\\(n\\) choose \\(r\\)”.\nWhen solving problems, make sure you define your random variable and state what \\(n, p\\), and \\(r\\) are. Without doing this, the problems are a great deal harder.\nThe command to find a binomial probability in r Studio is\nP\\((X=r)=\\)\ndbinom(r, n, p)\n\\(P(x \\le r)=\\)\npbinom(r, n, p, lower.tail=TRUE)\n\\(P(x \\ge r)=\\)\npbinom(r-1, n, p, lower.tail = FALSE)\n\n\n5.2.2 Example: Calculating Binomial Probabilities\nWhen looking at a person’s eye color, it turns out that 1% of people in the world has green eyes (“What percentage of,” 2013). Consider a group of 20 people.\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that none of the 20 people have green eyes.\nFind the probability that nine have green eyes.\nFind the probability that at most three have green eyes.\nFind the probability that at most two have green eyes.\nFind the probability that at least four have green eyes.\nIn Europe, four people out of twenty have green eyes. Is this unusual? What does that tell you?\n\n\n5.2.2.1 Solution\n\nState the random variable.\n\n\\(x\\) = number of people with green eyes\n\nArgue that this is a binomial experiment.\n\n\n\nThere are 20 people, and each person is a trial, so there are a fixed number of trials. In this case, \\(n\\) = 20.\nIf you assume that each person in the group is chosen at random the eye color of one person doesn’t affect the eye color of the next person, thus the trials are independent.\nEither a person has green eyes or they do not have green eyes, so there are only two outcomes. In this case, the success is a person has green eyes.\nThe probability of a person having green eyes is 0.01. This is the same for every trial since each person has the same chance of having green eyes.\n\n\n\nFind the probability that none of the 20 people have green eyes.\n\nIf none have green eyes, then \\(r=0\\).\nProbability that none have green eyes is \\(P(X=0)=0.818\\), using the command:\n\ndbinom(0,20,0.01) \n\n[1] 0.8179069\n\n\n\nFind the probability that nine have green eyes.\n\nIf nine have green eyes, then \\(r=9\\).\nProbability that 9 have green eyes is\n\\(P(X=9)=1.50X10^{-13}\\). Notice that r gives the answer as 1.50391e-13. This is the way many computer programs write a number in scientific notation. It isn’t possible for a computer to write it as \\(1.50381X10^{-13}\\), but it is possible for humans to write it correctly. So make sure the answer is written in the correct scientific notation.\n\ndbinom(9,20,0.01)\n\n[1] 1.50381e-13\n\n\n\nFind the probability that at most three have green eyes.\n\nAt most three means that three is the highest value you will have. Find the probability of \\(x\\) is less than or equal to three.\nSince this is less than, then the lower tail of the probability distribution is being used, so \\(P(X \\le 3)=0.99996\\) using the command in r Studio of\n\npbinom(3,20,0.01, lower.tail=TRUE)\n\n[1] 0.9999574\n\n\nThe reason the answer is written to more decimal places is because when it is rounded to three decimal places the rounding makes the answer 1. But 1 means that the event will happen, when in reality there is a slight chance that it won’t happen. It is best to write the answer to more decimal places or it can be written as \\(&gt;0.999\\) to represent that the number is very close to 1, but isn’t 1.\n\nFind the probability that at most two have green eyes.\n\nAt most 2 means 2 or less. So find the probability that there are less than or equal to 2. \\(P(X \\le 2)=0.999\\), and again, this is the lower tail of the probability distribution, so use lower.tail=TRUE in the r command:\n\npbinom(2,20,0.01, lower.tail=TRUE)\n\n[1] 0.9989964\n\n\n\nFind the probability that at least four have green eyes.\n\nAt least four means four or more. Find the probability of \\(x\\) being greater than or equal to four. Since it is greater than or equal to, this is the right tail of the probability distribution. However, if you just use lower.tail=FALSE, then the 4 is not included in r calculations. You want all numbers from 4 on up, so you need to use\n\\(r=4-1=3\\) in the r command. This will include 4 in the calculation. \\(P(X \\ge 4)=4.26X10^{-5}\\)\n\npbinom(4-1,20,0.01, lower.tail=FALSE) \n\n[1] 4.262093e-05\n\n\n\nIn Europe, four people out of twenty have green eyes. Is this unusual? What does that tell you?\n\nSince the probability of finding four or more people with green eyes is much less than 0.05, it is unusual to find four people out of twenty with green eyes. That should make you wonder if the proportion of people in Europe with green eyes is more than the 1% for the general population. If this is true, then you may want to ask why Europeans have a higher proportion of green-eyed people. That of course could lead to more questions.\n\n\n\n5.2.3 Example: Calculating Binomial Probabilities\nAccording to the Center for Disease Control (CDC), about 1 in 88 children in the U.S. have been diagnosed with autism (“CDC-data and statistics,” 2013). Suppose you consider a group of 10 children.\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that none have autism.\nFind the probability that seven have autism.\nFind the probability that at least five have autism.\nFind the probability that at most two have autism.\nSuppose five children out of ten have autism. Is this unusual? What does that tell you?\n\n\n5.2.3.1 Solution\n\nState the random variable.\n\n\\(x\\) = number of children with autism.\n\nArgue that this is a binomial experiment\n\n\n\nThere are 10 children, and each child is a trial, so there are a fixed number of trials. In this case, \\(n\\) = 10.\nIf you assume that each child in the group is chosen at random, then whether a child has autism does not affect the chance that the next child has autism. Thus the trials are independent.\nEither a child has autism or they do not have autism, so there are two outcomes. In this case, the success is a child has autism.\nThe probability of a child having autism is \\(\\frac{1}{88}\\). This is the same for every trial since each child has the same chance of having autism.\n\n\n\nFind the probability that none have autism.\n\n\\(P(X=0)=0.892\\)\n\ndbinom(0,10, 1/88)\n\n[1] 0.892002\n\n\n\nFind the probability that seven have autism.\n\n\\(P(X=7)=2.84X10^{-12}\\)\n\ndbinom(7,10, 1/88)\n\n[1] 2.837346e-12\n\n\n\nFind the probability that at least five have autism.\n\n\\(P(X \\ge 5)=4.553X10^{-8}\\). Again, this is the upper tail of the probability distribution, so use lower=tail=FALSE and\n\\(r=5-1=4\\) to make sure that r calculates for 5 and on up.\n\npbinom(5-1, 10, 1/88, lower.tail=FALSE)\n\n[1] 4.553416e-08\n\n\n\nFind the probability that at most two have autism.\n\n\\(P(X \\le 2)=0.9998\\). This is using the lower tail of the probability distribution.\n\npbinom(2, 10, 1/88, lower.tail=TRUE)\n\n[1] 0.9998341\n\n\n\nSuppose five children out of ten have autism. Is this unusual? What does that tell you?\n\nSince the probability of five or more children in a group of ten having autism is much less than 5%, it is unusual to happen. If this does happen, then one may think that the proportion of children diagnosed with autism is actually more than \\(\\frac{1}{88}\\).\n\n\n\n5.2.4 Homework for Binomial Probability Distribution Section\n\nApproximately 10% of all people are left-handed (\\“11 little-known facts,\\” 2013). Consider a grouping of fifteen people.\n\n\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that none are left-handed.\nFind the probability that seven are left-handed.\nFind the probability that at least two are left-handed.\nFind the probability that at most three are left-handed.\nFind the probability that at least seven are left-handed.\nSeven of the last 15 U.S. Presidents were left-handed. Is this unusual? What does that tell you?\n\n\n\nAccording to an article in the American Heart Association’s publication *Circulation*, 24% of patients who had been hospitalized for an acute myocardial infarction did not fill their cardiac medication by the seventh day of being discharged (Ho, Bryson & Rumsfeld, 2009). Suppose there are twelve people who have been hospitalized for an acute myocardial infarction.\n\n\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that all filled their cardiac medication.\nFind the probability that seven did not fill their cardiac medication.\nFind the probability that none filled their cardiac medication.\nFind the probability that at most two did not fill their cardiac medication.\nFind the probability that at least three did not fill their cardiac medication.\nFind the probability that at least ten did not fill their cardiac medication.\nSuppose of the next twelve patients discharged, ten did not fill their cardiac medication, would this be unusual? What does this tell you?\n\n\n\nEyeglassomatic manufactures eyeglasses for different retailers. In March 2010, they tested to see how many defective lenses they made, and there were 16.9% defective lenses due to scratches. Suppose Eyeglassomatic examined twenty eyeglasses.\n\n\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that none are scratched.\nFind the probability that all are scratched.\nFind the probability that at least three are scratched.\nFind the probability that at most five are scratched.\nFind the probability that at least ten are scratched.\nIs it unusual for ten lenses to be scratched? If it turns out that ten lenses out of twenty are scratched, what might that tell you about the manufacturing process?\n\n\n\nThe proportion of brown M&M’s in a milk chocolate packet is approximately 14% (Madison, 2013). Suppose a package of M&M’s typically contains 52 M&M’s.\n\n\n\nState the random variable.\nArgue that this is a binomial experiment\nFind the probability that six M&M’s are brown.\nFind the probability that twenty-five M&M’s are brown.\nFind the probability that all of the M&M’s are brown.\nWould it be unusual for a package to have only brown M&M’s? If this were to happen, what would you think is the reason?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Discrete Probability Distribution.html#mean-and-standard-deviation-of-binomial-distribution",
    "href": "Discrete Probability Distribution.html#mean-and-standard-deviation-of-binomial-distribution",
    "title": "5  Discrete Probability Distribution",
    "section": "5.3 Mean and Standard Deviation of Binomial Distribution",
    "text": "5.3 Mean and Standard Deviation of Binomial Distribution\nIf you list all possible values of \\(x\\) in a Binomial distribution, you get the Binomial Probability Distribution. You can draw a histogram of the probability distribution and find the mean (expected value), variance, and standard deviation of it. To have r Studio calculate the binomial values and save them to a variable, use the command\nx&lt;-c(0:n) p&lt;-dbinom(0:n, n, p)\n\n5.3.1 Example: Finding the Probability Distribution, Mean, Variance and Standard Deviation of a Binomial Distribution\nWhen looking at a person’s eye color, it turns out that 1% of people in the world has green eyes (“What percentage of,” 2013). Consider a group of 20 people.\n\nState the random variable.\nWrite the probability distribution.\nDraw a histogram.\nFind the mean, variance, and standard deviation.\n\n\n5.3.1.1 Solution\n\nState the random variable.\n\n\\(x\\) = number of people who have green eyes\n\nWrite the probability distribution.\n\nIn this case you need to write each value of \\(x\\) and its corresponding probability. It is easiest to do this by using the r Command:\n\ngreen&lt;-c(0:20) \nprobability_green&lt;-dbinom(0:20,20, 0.01) \n\nIt looks like nothing happened, but r save the values as variables. To see what is in each of those values, type\n\ngreen\n\n [1]  0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20\n\nprobability_green\n\n [1] 8.179069e-01 1.652337e-01 1.585576e-02 9.609552e-04 4.125313e-05\n [6] 1.333434e-06 3.367259e-08 6.802543e-10 1.116579e-11 1.503810e-13\n[11] 1.670900e-15 1.534344e-17 1.162381e-19 7.225371e-22 3.649177e-24\n[16] 1.474415e-26 4.654088e-29 1.106141e-31 1.862190e-34 1.980000e-37\n[21] 1.000000e-40\n\n\nThese can now be typed into a table if desired.\n\nDraw a histogram.\n\nOn r, this is like what was done in Section 5.1. Makes sure that the packages “arm” and “Weighted.Desc.Stat” are loaded. Then perform the command to get:\n\ndiscrete.histogram(green, probability_green, bar.width = 1, main=\"Number of People with Green Eyes\", xlab=\"Numbr of People with Green Eyes\")\n\n\n\n\n\n\n\nFigure 5.2: Histogram of Number of People with Green Eyes\n\n\n\n\n\nNotice this graph Figure 5.2 is skewed right.\n\nFind the mean, variance, and standard deviation\n\nUsing r Studio command such as those in Section 5.1:\n\nw.mean(green, probability_green) \n\n[1] 0.2\n\nw.var(green, probability_green) \n\n[1] 0.198\n\nw.sd(green, probability_green)\n\n[1] 0.4449719\n\n\nYou expect on average that out of 20 people, less than 1 person would have green eyes, with are variance of 0.198 \\(people^2\\) and a standard deviation of 0.44 people.\n\n\n\n5.3.2 Homework for Mean and Standard Deviation of Binomial Distribution Section\n\nSuppose a random variable, \\(x\\), arises from a binomial experiment. Suppose \\(n = 6\\), and \\(p = 0.13\\).\n\n\n\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nSuppose a random variable, \\(x\\), arises from a binomial experiment. Suppose \\(n = 10\\), and \\(p = 0.81\\).\n\n\n\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nSuppose a random variable, \\(x\\), arises from a binomial experiment. Suppose \\(n = 7\\), and \\(p = 0.50\\).\n\n\n\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nApproximately 10% of all people are left-handed. Consider a grouping of fifteen people.\n\n\n\nState the random variable.\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nAccording to an article in the American Heart Association’s publication *Circulation*, 24% of patients who had been hospitalized for an acute myocardial infarction did not fill their cardiac medication by the seventh day of being discharged (Ho, Bryson & Rumsfeld, 2009). Suppose there are twelve people who have been hospitalized for an acute myocardial infarction.\n\n\n\nState the random variable.\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nEyeglassomatic manufactures eyeglasses for different retailers. In March 2010, they tested to see how many defective lenses they made, and there were 16.9% defective lenses due to scratches. Suppose Eyeglassomatic examined twenty eyeglasses.\n\n\n\nState the random variable.\nWrite the probability distribution.\nDraw a histogram.\nDescribe the shape of the histogram.\nFind the mean.\nFind the variance.\nFind the standard deviation.\n\n\n\nThe proportion of brown M&M’s in a milk chocolate packet is approximately 14% (Madison, 2013). Suppose a package of M&M’s typically contains 52 M&M’s.\n\n\n\nState the random variable.\nFind the mean.\nFind the variance.\nFind the standard deviation.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Discrete Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html",
    "href": "Continuous Probability Distribution.html",
    "title": "6  Continuous Probability Distribution",
    "section": "",
    "text": "6.1 Normal Distribution\nMany populations have a distribution that is a symmetric, unimodal, and bell-shaped. For example: height, blood pressure, and cholesterol level. However, not every bell shaped curve is a normal curve. In a normal curve, there is a specific relationship between its “height” and its “width.” Normal curves can be tall and skinny or they can be short and fat. They are all symmetric, unimodal, and centered at \\(\\mu\\), the population mean.\nFigure 6.6 and Figure 6.7 show two different normal curves drawn on the same scale. Both have \\(\\mu=2\\) but the one in Figure 6.6 has a standard deviation of 1 and the one in Figure 6.7 has a standard deviation of 4. Notice that the larger standard deviation makes the graph wider (more spread out) and shorter.\nFigure 6.6: Normal curve with mean 2 and standard deviation 1\nFigure 6.7: Normal curve with mean 2 and standard deviation 4\nEvery normal curve has common features.\nJust as in a discrete probability distribution, the object is to find the probability of an event occurring. However, unlike in a discrete probability distribution where the event can be a single value, in a continuous probability distribution the event must be a range. You are interested in finding the probability of \\(x\\) occurring in the range between \\(a\\) and \\(b\\), or \\(P(a \\le x \\le b) = P(a&lt;x&lt;b)\\). Calculus tells us this probability is the area under the curve in the interval from \\(a\\) to \\(b\\).\nBefore looking at the process for finding the probabilities under the normal curve, it is somewhat useful to look at the Empirical Rule that gives approximate values for these areas. The Empirical Rule is just an approximation and it will only be used in this section to give you an idea of what the size of the probabilities is for different shadings. A more precise method for finding probabilities for the normal curve will be demonstrated in the next section. Please do not use the empirical rule except for real rough estimates.\nThe Empirical Rule for any normal distribution: Approximately 68% of the data is within one standard deviation of the mean. Approximately 95% of the data is within two standard deviations of the mean. Approximately 99.7% of the data is within three standard deviations of the mean.\nBe careful, there is still some area left over in each end. Remember, the maximum a probability can be is 100%, so if you calculate you will see that for both ends together there is 0.3% of the curve. Because of symmetry, you can divide this equally between both ends and find that there is 0.15% in each tail beyond the 3rd standard deviations.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html#normal-distribution",
    "href": "Continuous Probability Distribution.html#normal-distribution",
    "title": "6  Continuous Probability Distribution",
    "section": "",
    "text": "The center, or the highest point, is at the population mean, \\(\\mu\\).\nThe transition points are the places where the curve changes from a “hill” to a “valley”. The distance from the mean to the transition point is one standard deviation.\nThe area under the whole curve is exactly 1. Therefore, the area under the half below or above the mean is 0.5.\n\n\n\n\n\n\n\nEmpirical Rule Graph",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html#finding-probabilities-for-the-normal-distribution",
    "href": "Continuous Probability Distribution.html#finding-probabilities-for-the-normal-distribution",
    "title": "6  Continuous Probability Distribution",
    "section": "6.2 Finding Probabilities for the Normal Distribution",
    "text": "6.2 Finding Probabilities for the Normal Distribution\nThe Empirical Rule is just an approximation and only works for certain values. What if you want to find the probability for \\(x\\) values that are not integer multiples of the standard deviation? The probability is the area under the curve. To find areas under the curve, you need calculus. Before technology, you needed to convert every \\(x\\) value to a standardized number, called the \\(z\\)-score or \\(z\\)-value or simply just \\(z\\). The \\(z\\)-score is a measure of how many standard deviations an \\(x\\) value is from the mean. To convert from a normally distributed \\(x\\) value to a \\(z\\)-score, you use the following formula.\n\\(z-score=\\frac{x-\\mu}{\\sigma}\\)\nwhere \\(\\mu\\) = mean of the population of the \\(x\\) value and \\(\\sigma\\) = standard deviation for the population of the \\(x\\) value\nThe \\(z\\)-score is normally distributed, with a mean of 0 and a standard deviation of 1. It is known as the standard normal curve. The \\(z\\)-score is a measure of how many standard deviations a data value is from its mean. If the \\(z\\) - score is positive, the data value is above the mean. If the \\(z\\)-score is negative, the data value is below the mean. The farther the \\(z\\)-value is from 0, the farther the data value is from the mean.\nThese days technology can find probabilities without converting to the \\(z\\)-score and looking the probabilities up in a table. There are many programs available that will calculate the probability for a normal curve. The command on r to find the area to the left \\(P(x&lt;value)\\) is\npnorm(value, mean, standard_deviation, lower.tail=TRUE)\nThe command on r to find the area to the right, \\(P(x&gt;value)\\) is\npnorm(value, mean, standard_deviation, lower.tail=FALSE)\n\n6.2.1 Example: General Normal Distribution\nThe length of a human pregnancy is normally distributed with a mean of 272 days with a standard deviation of 9 days (Bhat & Kushtagi, 2006).\n\nState the random variable\nFind the probability of a pregnancy lasting more than 280 days.\nFind the probability of a pregnancy lasting less than 250 days.\nFind the probability that a pregnancy lasts between 265 and 280 days.\nFind the length of pregnancy that 10% of all pregnancies last less than.\nSuppose you meet a woman who says that she was pregnant for less than 250 days. Would this be unusual and what might you think?\n\n\n6.2.1.1 Solution\n\nState the random variable.\n\n\\(x\\) = length of a human pregnancy\n\nFind the probability of a pregnancy lasting more than 280 days.\n\nFirst translate the statement into a mathematical statement. \\(P(x&gt;280)\\)\nNow, draw a picture Figure 6.8.\n\n\n\n\n\n\n\n\nFigure 6.8: Normally distributed with mean 272 and standard deviation 9, and P(x&gt;280)\n\n\n\n\n\nThe probability of a pregnancy lasting longer than 280 days is \\(P(x&gt;280)=0.187\\). The command in rStudio is\n\npnorm(280, 272, 9, lower.tail=FALSE)\n\n[1] 0.1870314\n\n\nThus 18.7% of all pregnancies last more than 280 days. This is not unusual since the probability is greater than 5%.\n\nFind the probability of a pregnancy lasting less than 250 days.\n\nFirst translate the statement into a mathematical statement. \\(P(x&lt;250)\\)\nNow, draw a picture Figure 6.9.\n\n\n\n\n\n\n\n\nFigure 6.9: Density plot of pregnancy length. Normally distributed with mean 272 and standard deviation 9, and P(x&lt;250)\n\n\n\n\n\nThe probability of a pregnancy lasting longer than 250 days is \\(P(x&lt;250)=0.0073\\). The command in r Studio is\n\npnorm(250, 272, 9, lower.tail=TRUE)\n\n[1] 0.007253771\n\n\nThus 0.73% of all pregnancies last less than 250 days. This is unusual since the probability is less than 5%.\n\nFind the probability that a pregnancy lasts between 265 and 280 days.\n\nFirst translate the statement into a mathematical statement. \\(P(265&lt;x&lt;280)\\)\nNow draw a picture Figure 6.10.\n\n\n\n\n\n\n\n\nFigure 6.10: Density plot of pregnancy length. Normally distributed with mean 272 and standard deviation 9, and P(265&lt;x&lt;280)\n\n\n\n\n\nThe probability of a pregnancy lasting between 265 days and 280 days is \\(P(265&lt;x&lt;280)=0.187\\). To find the area between two values on the normal distribution, first, find the area to the left of the lower value, Graphically, this looks like Figure 6.11\n\n\n\n\n\n\n\n\nFigure 6.11: Density plot of pregnancy length. Normally distributed with mean 272 and standard deviation 9, and P(x&lt;265)\n\n\n\n\n\nNow find the area less than 280. Graphically this looks like Figure 6.12\n\n\n\n\n\n\n\n\nFigure 6.12: Density plot of pregnancy length. Normally distributed with mean 272 and standard deviation 9, and P(x&lt;280)\n\n\n\n\n\nLooking at the three figures, if you take the area in Figure 6.12 and subtract the area in Figure 6.11 you get the area in Figure 6.10. In rStudio, the way to find the probability the probability of a pregnancy lasting between 265 days and 280 days, \\(P(265&lt;x&lt;280)=0.595\\) use the following command\n\npnorm(280, 272, 9, lower.tail=TRUE)-pnorm(265, 272, 9, lower.tail=TRUE) \n\n[1] 0.5946186\n\n\nThus 59.5% of all pregnancies last between 265 and 280 days.\n\nFind the length of pregnancy that 10% of all pregnancies last less than.\n\nThis problem is asking you to find an \\(x\\) value from a probability. You want to find the \\(x\\) value that has 10% of the length of pregnancies to the left of it. In this case, you are given the probability. In r, the command is\nqnorm(area, mean, standard_deviation, lower.tail=TRUE or FALSE)\nFor this example since you know the area in the lower tail, then use lower.tail=TRUE. So the command is\n\nqnorm(0.1, 272, 9, lower.tail = TRUE)\n\n[1] 260.466\n\n\nThus 10% of all pregnancies last less than approximately 260 days.\n\nSuppose you meet a woman who says that she was pregnant for less than 250 days. Would this be unusual and what might you think?\n\nFrom part (c) you found the probability that a pregnancy lasts less than 250 days is 0.73%. Since this is less than 5%, it is very unusual. You would think that either the woman had a premature baby, or that she may be wrong about when she actually became pregnant.\n\n\n\n6.2.2 Example: General Normal Distribution\nThe mean mathematics SAT score in 2012 was 514 with a standard deviation of 117 (“Total group profile,” 2012). Assume the mathematics SAT score is normally distributed.\n\nState the random variable.\nFind the probability that a person has a mathematics SAT score over 700.\nFind the probability that a person has a mathematics SAT score of less than 400.\nFind the probability that a person has a mathematics SAT score between a 500 and a 650.\nFind the mathematics SAT score that represents the top 1% of all scores.\n\n\n6.2.2.1 Solution\n\nState the random variable.\n\n\\(x\\) = mathematics SAT score\n\nFind the probability that a person has a mathematics SAT score over 700.\n\nFirst translate the statement into a mathematical statement. \\(P(x&gt;700)\\)\nNow, draw a picture Figure 6.13\n\n\n\n\n\n\n\n\nFigure 6.13: Density plot of SAT mathematics score. Normally distributed with mean 514 and standard deviation 117 and P(x&gt;700)\n\n\n\n\n\nTo find \\(P(x&gt;700)=0.0559\\), the command in r would be\n\npnorm(700, 514,117, lower.tail = FALSE)\n\n[1] 0.05594631\n\n\nThere is a 5.6% chance that a person scored above a 700 on the mathematics SAT test. This is not unusual.\n\nFind the probability that a person has a mathematics SAT score of less than 400.\n\nFirst translate the statement into a mathematical statement. \\(P(x&lt;400)\\)\nNow, draw a picture Figure 6.14\n\n\n\n\n\n\n\n\nFigure 6.14: Density plot of SAT mathematics score. Normally distributed with mean 514 and standard deviation 117 and P(x&lt;400)\n\n\n\n\n\nTo find \\(P(x&lt;400)=0.165\\), the command in r would be\n\npnorm(400, 514, 117, lower.tail = TRUE) \n\n[1] 0.1649392\n\n\nSo, there is a 16.5% chance that a person scores less than a 400 on the mathematics part of the SAT.\n\nFind the probability that a person has a mathematics SAT score between a 500 and a 650.\n\nFirst translate the statement into a mathematical statement \\(P(500&lt;x&lt;650)\\)\nNow, draw a picture Figure 6.15\n\n\n\n\n\n\n\n\nFigure 6.15: Density plot of SAT mathematics score. Normally distributed with mean 514 and standard deviation 117 and P(514&lt;x&lt;650)\n\n\n\n\n\nTo find \\(P(500&lt;x&lt;650)=0.425\\), the command in r would be\n\npnorm(650, 514, 117, lower.tail = TRUE)-pnorm(500, 514, 117, lower.tail=TRUE)\n\n[1] 0.4250851\n\n\nSo, there is a 42.5% chance that a person has a mathematical SAT score between 500 and 650.\n\nFind the mathematics SAT score that represents the top 1% of all scores.\n\nThis problem is asking you to find an \\(x\\) value from a probability. You want to find the \\(x\\) value that has 1% of the mathematics SAT scores to the right of it. In this case you are using the upper tail of the curve. To find this \\(x\\) value on rStudio, use the command\n\nqnorm(0.01, 514, 117, lower.tail=FALSE) \n\n[1] 786.1827\n\n\nSo, 1% of all people who took the SAT scored over about 786 points on the mathematics SAT.\n\n\n\n6.2.3 Homework for Normal Distribution Section\n\nFind each of the probabilities, where \\(z\\) is a \\(z\\)-score from the standard normal distribution with mean of \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\). It helps to draw a picture for each problem.\n\n\n\n\\(P(z&lt;2.36)\\)\n\\(P(z&gt;0.67)\\)\n\\(P(0&lt;x&lt;2.11)\\)\n\\(P(-2.78&lt;z&lt;1.97)\\)\n\n\n\nFind the z-score corresponding to the given area. Remember, z is distributed as the standard normal distribution with mean of \\(\\mu=0\\) and standard deviation \\(\\sigma=1\\).\n\n\n\nThe area to the left of \\(z\\) is 15%.\nThe area to the right of \\(z\\) is 65%.\nThe area to the left of \\(z\\) is 10%.\nThe area to the right of \\(z\\) is 5%.\nThe area between \\(-z\\) and \\(z\\) is 95%. (Hint draw a picture and figure out the area to the left of \\(-z\\).)\nThe area between \\(-z\\) and \\(z\\) is 99%.\n\n\n\nIf a random variable that is normally distributed has a mean of 25 and a standard deviation of 3, convert the given value to a \\(z\\)-score.\n\n\n\n\\(x\\) = 23\n\\(x\\) = 33\n\\(x\\) = 19\n\\(x\\) = 45\n\n\n\nAccording to the WHO MONICA Project the mean blood pressure for people in China is 128 mmHg with a standard deviation of 23 mmHg (Kuulasmaa, Hense & Tolonen, 1998). Assume that blood pressure is normally distributed.\n\n\n\nState the random variable.\nFind the probability that a person in China has blood pressure of 135 mmHg or more.\nFind the probability that a person in China has blood pressure of 141 mmHg or less.\nFind the probability that a person in China has blood pressure between 120 and 125 mmHg.\nIs it unusual for a person in China to have a blood pressure of 135 mmHg? Why or why not?\nWhat blood pressure do 90% of all people in China have less than?\n\n\n\nThe size of fish is very important to commercial fishing. A study conducted in 2012 found the length of Atlantic cod caught in nets in Karlskrona to have a mean of 49.9 cm and a standard deviation of 3.74 cm (Ovegard, Berndt & Lunneryd, 2012). Assume the length of fish is normally distributed.\n\n\n\nState the random variable.\nFind the probability that an Atlantic cod has a length less than 52 cm.\nFind the probability that an Atlantic cod has a length of more than 74 cm.\nFind the probability that an Atlantic cod has a length between 40.5 and 57.5 cm.\nIf you found an Atlantic cod to have a length of more than 74 cm, what could you conclude?\nWhat length are 15% of all Atlantic cod longer than?\n\n\n\nThe mean cholesterol levels of women age 45-59 in Ghana, Nigeria, and Seychelles is 5.1 mmol/l and the standard deviation is 1.0 mmol/l (Lawes, Hoorn, Law & Rodgers, 2004). Assume that cholesterol levels are normally distributed.\n\n\n\nState the random variable.\nFind the probability that a woman age 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level above 6.2 mmol/l (considered a high level).\nFind the probability that a woman age 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level below 5.2 mmol/l (considered a normal level).\nFind the probability that a woman age 45-59 in Ghana, Nigeria, or Seychelles has a cholesterol level between 5.2 and 6.2 mmol/l (considered borderline high).\nIf you found a woman age 45-59 in Ghana, Nigeria, or Seychelles having a cholesterol level above 6.2 mmol/l, what could you conclude?\nWhat value do 5% of all woman ages 45-59 in Ghana, Nigeria, or Seychelles have a cholesterol level less than?\n\n\n\nIn the United States, males between the ages of 40 and 49 eat on average 103.1 g of fat every day with a standard deviation of 4.32 g (“What we eat,” 2012). Assume that the amount of fat a person eats is normally distributed.\n\n\n\nState the random variable.\nFind the probability that a man age 40-49 in the U.S. eats more than 110 g of fat every day.\nFind the probability that a man age 40-49 in the U.S. eats less than 93 g of fat every day.\nFind the probability that a man age 40-49 in the U.S. eats less than 65 g of fat every day.\nIf you found a man age 40-49 in the U.S. who says he eats less than 65 g of fat every day, would you believe him? Why or why not?\nWhat daily fat level do 5% of all men age 40-49 in the U.S. eat more than?\n\n\n\nA dishwasher has a mean life of 12 years with an estimated standard deviation of 1.25 years (“Appliance life expectancy,” 2013). Assume the life of a dishwasher is normally distributed.\n\n\n\nState the random variable.\nFind the probability that a dishwasher will last more than 15 years.\nFind the probability that a dishwasher will last less than 6 years.\nFind the probability that a dishwasher will last between 8 and 10 years.\nIf you found a dishwasher that lasted less than 6 years, would you think that you have a problem with the manufacturing process? Why or why not?\nA manufacturer of dishwashers only wants to replace free of charge 5% of all dishwashers. How long should the manufacturer make the warranty period?\n\n\n\nThe mean starting salary for nurses is \\$67,694 nationally (“Staff nurse -,” 2013). The standard deviation is approximately $10,333. Assume that the starting salary is normally distributed.\n\n\n\nState the random variable.\nFind the probability that a starting nurse will make more than \\$80,000.\nFind the probability that a starting nurse will make less than \\$60,000.\nFind the probability that a starting nurse will make between \\$55,000 and \\$72,000.\nIf a nurse made less than \\$50,000, would you think the nurse was under paid? Why or why not?\nWhat salary do 30% of all nurses make more than?\n\n\n\nThe mean yearly rainfall in Sydney, Australia, is about 137 mm and the standard deviation is about 69 mm (“Annual maximums of,”2013). Assume rainfall is normally distributed.\n\n\n\nState the random variable.\nFind the probability that the yearly rainfall is less than 100 mm.\nFind the probability that the yearly rainfall is more than 240 mm.\nFind the probability that the yearly rainfall is between 140 and 250 mm.\nIf a year has a rainfall less than 100mm, does that mean it is an unusually dry year? Why or why not?\nWhat rainfall amount are 90% of all yearly rainfalls more than?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html#assessing-normality",
    "href": "Continuous Probability Distribution.html#assessing-normality",
    "title": "6  Continuous Probability Distribution",
    "section": "6.3 Assessing Normality",
    "text": "6.3 Assessing Normality\nThe distributions you have seen up to this point have been assumed to be normally distributed, but how do you determine if it is normally distributed. One way is to take a sample and look at the sample to determine if it appears normal. If the sample looks normal, then most likely the population is also. Here are some guidelines that are use to help make that determination.\n\nDensity Plot: Make a density plot. For a normal distribution, the density plot should be roughly bell-shaped. For small samples, this is not very accurate, and another method is needed. A distribution may not look normally distributed from the density plot, but it still may be normally distributed.\nNormal quantile plot (or normal probability plot): This plot is provided through statistical software on a computer. If the points lie close to a line, the data comes from a distribution that is approximately normally distributed. If the points do not lie close to a line or they show a pattern that is not a line, the data are likely to come from a distribution that is not normally distributed.\n\n\n6.3.1 To create a density plot on rStudio:\nRead the Data Frame into r Studio. The command for density is\ngf_density(~variable, data=Data_Frame)\nSee chapter 2 for more examples of this.\n\n\n6.3.2 To create a normal quantile plot on rStudio\nRead the Data Frame into rStudio. The command for normal quantile plot is\ngf_qqnorm(~variable, data=Data_Frame)\nRealize that your random variable may be normally distributed, even if the sample fails the two tests. However, if the density plot definitely doesn’t look symmetric and bell shaped, and the normal probability plot doesn’t look linear, then you can be fairly confident that the data set does not come from a population that is normally distributed.\n\n\n6.3.3 Example: Is It Normal?\nIn Kiama, NSW, Australia, there is a blowhole. The data in Table 6.1 are times in seconds between eruptions (“Kiama blowhole eruptions,” 2013). Do the data come from a population that is normally distributed?\n\nEruption&lt;-read.csv( \"https://krkozak.github.io/MAT160/Blowhole_eruptions.csv\") \nknitr::kable(head(Eruption))\n\n\n\nTable 6.1: Time (in Seconds) Between Kiama Blowhole Eruptions\n\n\n\n\n\n\nInterval\n\n\n\n\n83\n\n\n51\n\n\n87\n\n\n60\n\n\n28\n\n\n95\n\n\n\n\n\n\n\n\nCode book for Data Frame Eruption\nDescription The ocean swell produces spectacular eruptions of water through a hole in the cliff at Kiama, about 120km south of Sydney, known as the Blowhole. The times at which 65 successive eruptions occurred from 1340 hours on 12 July 1998 were observed using a digital watch.\nFormat This data frame contains the following columns:\nInterval: Waiting time between eruptions (seconds)\nSource Kiama Blowhole Eruptions. (n.d.). Retrieved from http://www.statsci.org/data/oz/kiama.html\nReferences The data was collected and contributed by Jim Irish, Faculty of Engineering, University of Technology, Sydney.\n\nState the random variable\nDraw a Density plot\nDraw the normal quantile plot.\nDo the data come from a population that is normally distributed?\n\n\n6.3.3.1 Solution\n\nState the random variable\n\n\\(x\\) = time in seconds between eruptions of Kiama Blowhole\n\nDraw a Density plot\n\nThe density plot produced is in Figure 6.16\n\ngf_density(~Interval, data=Eruption, title=\"Eruption times for Kiama Blowhole\", xlab=\"Time (seconds)\") \n\n\n\n\n\n\n\nFigure 6.16: Density Plot of Eruption Times for Kiama Blowhole\n\n\n\n\n\nThis looks skewed right and not symmetric.\n\nDraw the normal quantile plot.\n\nThe normal quantile plot is in Figure 6.17\n\ngf_qq(~Interval, data=Eruption, title=\"Eruption times for Kiama Blowhole\")\n\n\n\n\n\n\n\nFigure 6.17: Normal Quantile Plot of Eruption Times for Kiama blowhole.\n\n\n\n\n\nFigure 6.17 looks more like an exponential growth than linear.\n\nDo the data come from a population that is normally distributed?\n\nConsidering the density plot is skewed right, and the normal probability plot does not look linear, then the conclusion is that this sample is not from a population that is normally distributed.\n\n\n\n6.3.4 Example: Is It Normal?\nThe US National Center for Health Statistics (NCHS) conducted a series of health and nutrition surveys called NHANES. One of the many variables in NHANES is pulse. Determine if pulse is a normally distributed variable. The NHANES data frame is Table 2.5.\n\nState the random variable\nDraw a density plot\nDraw the normal quantile plot.\nDo the data come from a population that is normally distributed?\n\n\n6.3.4.1 Solution\n\nState the random variable\n\n\\(x\\) = pulse\n\nDraw a density plot\n\nThe density plot is in Figure 6.18\n\ngf_density(~Pulse, data=NHANES, title=\"Pulse Rate\", xlab=\"Pulse Rate (bpm)\")\n\n\n\n\n\n\n\nFigure 6.18: Density plot of Pulse Rate (bpm)\n\n\n\n\n\nThis looks somewhat symmetric and bell shaped.\n\nDraw the normal quantile plot.\n\nThe normal quantile plot is in Figure 6.19\n\ngf_qq(~Pulse, data=NHANES, title=\"Pulse Rate\") \n\n\n\n\n\n\n\nFigure 6.19: Normal Quantile Plot of Pulse Rate (bmp)\n\n\n\n\n\nFigure 6.19 looks fairly linear.\n\nDo the data come from a population that is normally distributed?\n\nConsidering the density plot is bell shaped and the normal probability plot looks linear. The conclusion is that this sample is from a population that is normally distributed.\n\n\n\n6.3.5 Homework for Assessing Normality Section\n\nCholesterol data was collected on patients four days after having a heart attack. The data is in Table 3.2. Assess if the data is from a population that is normally distributed.\n\nCode Book for Cholesterol See is below Table 3.2.\n\nThe size of fish is very important to commercial fishing. A study conducted in 2012 collected the lengths of Atlantic cod caught in nets in Karlskrona (Ovegard, Berndt & Lunneryd, 2012). Data based on information from the study is in Table 6.2. Determine if the data is from a population that is normally distributed.\n\n\nCod&lt;-read.csv( \"https://krkozak.github.io/MAT160/cod.csv\") \nknitr::kable(head(Cod))\n\n\n\nTable 6.2: Atlantic Cod Lengths\n\n\n\n\n\n\nlength\n\n\n\n\n48\n\n\n50\n\n\n50\n\n\n55\n\n\n53\n\n\n50\n\n\n\n\n\n\n\n\n\nThe WHO MONICA Project collected blood pressure data for people in China (Kuulasmaa, Hense & Tolonen, 1998). Data based on information from the study is in Table 6.3. Determine if the data is from a population that is normally distributed.\n\n\nBP&lt;-read.csv( \"https://krkozak.github.io/MAT160/bp.csv\") \nknitr::kable(head(BP))\n\n\n\nTable 6.3: Blood Pressure Values for People in China\n\n\n\n\n\n\npressure\n\n\n\n\n114\n\n\n141\n\n\n154\n\n\n137\n\n\n131\n\n\n132\n\n\n\n\n\n\n\n\n\nAnnual rainfalls for Sydney, Australia are given in Table 6.4 (“Annual maximums of,” 2013). Can you assume rainfall is normally distributed?\n\n\nAnnual&lt;-read.csv( \"https://krkozak.github.io/MAT160/annual.csv\") \nknitr::kable(head(Annual))\n\n\n\nTable 6.4: Annual Rainfall in Sydney, Australia\n\n\n\n\n\n\namount\n\n\n\n\n146.8\n\n\n383.0\n\n\n90.9\n\n\n178.1\n\n\n267.5\n\n\n95.5",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "Continuous Probability Distribution.html#sampling-distribution-and-the-central-limit-theorem",
    "href": "Continuous Probability Distribution.html#sampling-distribution-and-the-central-limit-theorem",
    "title": "6  Continuous Probability Distribution",
    "section": "6.4 Sampling Distribution and the Central Limit Theorem",
    "text": "6.4 Sampling Distribution and the Central Limit Theorem\nYou now have most of the skills to start statistical inference, but you need one more concept.\nFirst, it would be helpful to state what statistical inference is in more accurate terms.\nStatistical Inference: to make accurate decisions about parameters from statistics\nWhen it says “accurate decision,” you want to be able to measure how accurate. You measure how accurate using probability. In both binomial and normal distributions, you needed to know that the random variable followed either distribution. You need to know how the statistic is distributed and then you can find probabilities. In other words, you need to know the shape of the sample mean or whatever statistic you want to make a decision about.\nHow is the statistic distributed? This is answered with a sampling distribution.\nSampling Distribution: how a sample statistic is distributed when repeated trials of size \\(n\\) are taken.\n\n6.4.1 Example: Sampling Distribution\nThe NHANES data frame has the pulse rates for approximately 50,000 individuals. The random variable is \\(x\\) = pulse rate. The probability distribution of this random variable is presented in Figure 6.20. Although pulse rates from 50,000 individuals isn’t the entire population, the sample is most likely a good representation of the population. Thus, it is safe to assume the population is normally distributed. An estimate for the population mean is 73.6 pbm, and the population standard deviation estimate is 12.2 bpm.\n\ngf_density(~Pulse, data=NHANES, title = \"Pulse Rate\", xlab=\"Pulse (bpm)\") \ndf_stats(~Pulse, data=NHANES, mean, sd)\n\n  response     mean       sd\n1    Pulse 73.55973 12.15542\n\n\n\n\n\n\n\n\nFigure 6.20: Distribution of Pulse Rate\n\n\n\n\n\nSuppose you take a random sample of 10 pulse rates from those 50,000 individuals. A random sample of data from 10 individuals is:\n\nNHANES|&gt; \n  sample_n(size=10) \n\n# A tibble: 10 × 76\n      ID SurveyYr Gender   Age AgeDecade AgeMonths Race1    Race3    Education  \n   &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;  &lt;int&gt; &lt;fct&gt;         &lt;int&gt; &lt;fct&gt;    &lt;fct&gt;    &lt;fct&gt;      \n 1 65565 2011_12  male      27 \" 20-29\"         NA White    White    High School\n 2 56067 2009_10  male      80  &lt;NA&gt;            NA Mexican  &lt;NA&gt;     8th Grade  \n 3 56670 2009_10  male      16 \" 10-19\"        192 White    &lt;NA&gt;     &lt;NA&gt;       \n 4 60059 2009_10  female     7 \" 0-9\"           86 Black    &lt;NA&gt;     &lt;NA&gt;       \n 5 56159 2009_10  female    49 \" 40-49\"        588 White    &lt;NA&gt;     College Gr…\n 6 65889 2011_12  male      30 \" 30-39\"         NA Hispanic Hispanic Some Colle…\n 7 66763 2011_12  female    80  &lt;NA&gt;            NA White    White    High School\n 8 52965 2009_10  female    36 \" 30-39\"        443 White    &lt;NA&gt;     Some Colle…\n 9 61988 2009_10  male       8 \" 0-9\"          107 White    &lt;NA&gt;     &lt;NA&gt;       \n10 53098 2009_10  female    73 \" 70+\"          876 White    &lt;NA&gt;     College Gr…\n# ℹ 67 more variables: MaritalStatus &lt;fct&gt;, HHIncome &lt;fct&gt;, HHIncomeMid &lt;int&gt;,\n#   Poverty &lt;dbl&gt;, HomeRooms &lt;int&gt;, HomeOwn &lt;fct&gt;, Work &lt;fct&gt;, Weight &lt;dbl&gt;,\n#   Length &lt;dbl&gt;, HeadCirc &lt;dbl&gt;, Height &lt;dbl&gt;, BMI &lt;dbl&gt;,\n#   BMICatUnder20yrs &lt;fct&gt;, BMI_WHO &lt;fct&gt;, Pulse &lt;int&gt;, BPSysAve &lt;int&gt;,\n#   BPDiaAve &lt;int&gt;, BPSys1 &lt;int&gt;, BPDia1 &lt;int&gt;, BPSys2 &lt;int&gt;, BPDia2 &lt;int&gt;,\n#   BPSys3 &lt;int&gt;, BPDia3 &lt;int&gt;, Testosterone &lt;dbl&gt;, DirectChol &lt;dbl&gt;,\n#   TotChol &lt;dbl&gt;, UrineVol1 &lt;int&gt;, UrineFlow1 &lt;dbl&gt;, UrineVol2 &lt;int&gt;, …\n\n\nIt might be useful to find the mean pulse rate from a random sample of size 10.\n\n\n  response mean\n1    Pulse 77.4\n\n\nNow suppose you took another random sample of size 10 and found the mean pulse rate for that sample. Repeat this process 100 times. At this point you would basically have a new sample of 100 mean pulse rates. You could assess how this sample is distributed by creating a density plot Figure 6.21\n\nTrials &lt;- do(100) * { NHANES |&gt; \n    sample_n(size = 10) |&gt;\n    df_stats( ~Pulse, means = mean)}\ngf_density( ~means, data = Trials, title = \"Density plot of sample mean when n=10\")\n\ndf_stats(~means, data=Trials, mean, sd)\n\n  response     mean       sd\n1    means 73.82821 4.239885\n\n\n\n\n\n\n\n\nFigure 6.21: Density Plot of Sample Means When n = 10\n\n\n\n\n\nThis distribution is a sampling distribution. That is all a sampling distribution is. It is a distribution created from statistics.\nNotice the distribution does look a great deal like the distribution of the original random variable. Notice the mean of the sample means \\(\\mu_{\\bar{x}} = 73.8\\) bpm which is almost the same of as the mean of the population. The standard deviation of the sample means, \\(\\sigma_{\\bar{x}}=4.35\\) pbm is about \\(\\frac{1}{3}\\) of the population standard deviation.\nWhat does this distribution look like if instead of repeating the experiment 10 times you repeat it 50 times instead?\nThis density plot of the sampling distribution is displayed in Figure 6.22\n\nTrials &lt;- do(100) * { NHANES |&gt;\n    sample_n(size = 50) |&gt;\n    df_stats( ~Pulse, means = mean) }\ngf_density( ~means, data = Trials, title=\"Sample means when n=50\")|&gt;\n  gf_lims(x=c(68,79))\ndf_stats(~means, data=Trials, mean, sd) \n\n  response     mean       sd\n1    means 73.58535 1.820259\n\n\n\n\n\n\n\n\nFigure 6.22: Density Plot of Sample Means When n = 50\n\n\n\n\n\nNotice this density plot of the sample mean looks approximately symmetrical and could almost be called normal. Notice, the mean of the sample means is 73.6 bpm which is approximately what the population mean is. The standard deviation of the sample means is 1.77 bpm which is around \\(\\frac{1}{7}\\) of the population standard deviation. What if you keep increasing \\(n\\)? What will the sampling distribution of the sample mean look like? In other words, what does the sampling distribution of \\(\\bar{x}\\) look like as \\(n\\) gets even larger?\nThis depends on how the original distribution is distributed. In Example: Sampling Distribution, the random variable was approximately normally distributed. When \\(n\\) was 10, the distribution of the mean looked approximately normal. What if the original distribution wasn’t normal? How big would \\(n\\) have to be? Consider a different variable in the NHANES data frame that isn’t normally distributed such as age when a participant started to smoke cigarettes (SmokeAge). The density plot for the large sample is in Figure 6.23. The mean for the large sample is 17.8 years and the standard deviation is 5.3 years, so \\(\\mu=17.8\\) dollars and \\(\\sigma=5.3\\) dollars approximately.\n\ngf_density(~SmokeAge, data=NHANES, title = \"Density Plot of Age when Person Started Smoking\", xlab=\"Age\") \ndf_stats(~SmokeAge, data=NHANES, mean, sd) \n\n  response     mean      sd\n1 SmokeAge 17.82662 5.32666\n\n\n\n\n\n\n\n\nFigure 6.23: Density Plot of Age When Person Started Smoking.\n\n\n\n\n\nNow take 100 samples of size 50 individuals from the NHANES Data Frame. Then graph a density plot of SmokeAge, the age when someone started to smoke. Notice the the sampling distribution of the sample means looks fairly normally distributed even though the original random variable was not normally distributed. The mean of the sample mean. \\(\\mu_{ \\bar{x}}=\\) 17.8 years and the standard deviation of the sample mean, \\(\\sigma_{\\bar{x}}=\\) 1.56 years. The mean of the sample mean is the same as the mean of the population, but the standard deviation of the sample mean is much less than the standard deviation of the original data.\n\n\n  response     mean       sd\n1    means 17.67773 1.172155\n\n\n\n\n\n\n\n\nFigure 6.24: Density Plot of Age When Person Started Smoking when sample size is 50.\n\n\n\n\n\nOne question is, why is the mean of the sample means the same as the mean of the population? Suppose you have a random variable that has a population mean, \\(\\mu\\), and a population standard deviation, \\(\\sigma\\). If a sample of size \\(n\\) is taken, then the sample mean, has a mean \\(\\mu_{ \\bar{x}}=\\mu\\) and standard deviation of \\(\\sigma_{ \\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}\\) . The standard deviation of the sample mean is lower because by taking the mean you are averaging out the extreme values, which makes the distribution of the sample mean less spread out.\nYou now know the center and the variability of \\(\\bar{x}\\). You also want to know the shape of the distribution of \\(\\bar{x}\\). You hope it is normal, since you know how to find probabilities using the normal curve. The following theorem tells you the requirement to have \\(\\bar{x}\\) be normally distributed.\n\n\n6.4.2 Central Limit Theorem\nSuppose a random variable is from any distribution. If a sample of size \\(n\\) is taken, the the sample mean, \\(\\bar{x}\\), becomes normally distributed as \\(n\\) increases.\nWhat this says is that no matter what \\(x\\) looks like, \\(\\bar{x}\\) would look normal if \\(n\\) is large enough. Now, what size of \\(n\\) is large enough? That depends on how \\(x\\) is distributed in the first place. If the original random variable is normally distributed, then \\(n\\) just needs to be 2 or more data points. If the original random variable is somewhat mound shaped and symmetrical, then \\(n\\) needs to be greater than or equal to 30. Sometimes the sample size can be smaller, but this is a good general rule to use. The sample size may have to be much larger if the original random variable is really skewed one way or another.\nNow that you know when the sample mean will look like a normal distribution, then you can find the probability related to the sample mean. Remember that the mean of the sample mean is just the mean of the original data (\\(\\mu_{\\bar{x}}=\\mu\\)), but the standard deviation of the sample mean, \\(\\sigma_{\\bar{x}}\\), also known as the standard error of the mean, is actually \\(\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}\\). Make sure you use this in all calculations. If you are using the \\(z\\)-score, the formula when working with \\(\\bar{x}\\) is \\(z=\\frac{x-\\mu_{\\bar{x}}}{\\sigma_{\\bar{x}}}=\\frac{x-\\mu}{\\frac{\\sigma}{\\sqrt{n}}}\\). To use rStudio to calculate probabilities use \\(P(\\bar{x}&lt;a)= pnorm(a, \\mu_{\\bar{x}}, \\sigma_{\\bar{x}}, lower.tail=TRUE)\\) \\(P(\\bar{x}&gt;a)= pnorm(a, \\mu_{\\bar{x}}, \\sigma_{\\bar{x}}, lower.tail=FALSE)\\).\n\n\n6.4.3 Example: Finding Probabilities for Sample Means\nThe birth weight of boy babies of European descent who were delivered at 40 weeks is normally distributed with a mean of 3687.6 g with a standard deviation of 410.5 g (Janssen, Thiessen, Klein, Whitfield, MacNab & Cullis-Kuhl, 2007). Suppose there were nine European descent boy babies born on a given day and the mean birth weight is calculated.\n\nState the random variable.\nWhat is the mean of the sample mean?\nWhat is the standard deviation of the sample mean?\nWhat distribution is the sample mean distributed as?\nFind the probability that the mean weight of the nine boy babies born was less than 3500.4 g.\nFind the probability that the mean weight of the nine babies born was less than 3452.5 g.\n\n\n6.4.3.1 Solution\n\nState the random variable.\n\n\\(x\\) = birth weight of boy babies (Note: the random variable is something you measure, and it is not the mean birth weight. Mean weight is calculated.)\n\nWhat is the mean of the sample mean?\n\n\\(\\mu_{\\bar{x}}=\\mu=3687.4g\\)\n\nWhat is the standard deviation of the sample mean?\n\n\\(\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}=\\frac{410.5g}{\\sqrt{9}}=136.8g\\)\n\nWhat distribution is the sample mean distributed as?\n\nSince the original random variable is distributed normally, then the sample mean is distributed normally.\n\nFind the probability that the mean weight of the nine boy babies born was less than 3500.4 g.\n\nTo find \\(P(\\bar{x}&lt;3500.4)=0.086\\). use the rStudio command\n\npnorm(3500.4,3687.6, 410.5/sqrt(9), lower.tail = TRUE)\n\n[1] 0.08564231\n\n\nThere is an 8.6% chance that the mean birth weight of the nine boy babies born would be less than 3500.4 g. Since this is more than 5%, this is not unusual.\n\nFind the probability that the mean weight of the nine babies born was less than 3452.5 g.\n\nYou are looking for the \\(P(\\bar{x}&lt;3452.5)\\).\nTo find in rStudio, \\(P(\\bar{x}&lt;3452.5)=0.043\\) use the command\n\npnorm(3452.5, 3687.4, 410.5/sqrt(9), lower.tail = TRUE)\n\n[1] 0.04301819\n\n\nThere is a 4.3% chance that the mean birth weight of the nine boy babies born would be less than 3452.5 g. Since this is less than 5% this would be an unusual event. If it actually happened, then you may think there is something unusual about this sample. Maybe some of the nine babies were born as multiples, which brings the mean weight down, or some or all of the babies were not of European descent (in fact the mean weight of South Asian boy babies is 3452.5 g), or some were born before 40 weeks, or the babies were born at high altitudes.\n\n\n\n6.4.4 Example: Finding Probabilities for Sample Means\nFor Americans that smoke, the average age that they started smoking is 17.8 years, with a standard deviation of approximately 1.56 years from the NHANES data. This random variable is not normally distributed, though it is somewhat mound shaped.\n\nState the random variable.\nSuppose a sample of 35 smoking American’s is taken. Find the probability that the mean age that these 35 smoking Americans started to smoke is more than 21 years.\n\n\n6.4.4.1 Solution\n\nState the random variable.\n\n\\(x\\) = age that smoking Americans started to smoke\n\nSuppose a sample of 35 smoking American’s is taken. Find the probability that the mean age that these 35 smoking Americans started to smoke is more than 21 years.\n\nEven though the original random variable is not normally distributed, the sample size is over 30, by the central limit theorem the sample mean will be normally distributed. The mean of the sample mean is \\(\\mu_{\\bar{x}}=17.8\\). The standard deviation of the sample mean is \\(\\sigma_{\\bar{x}}=\\frac{\\sigma}{\\sqrt{n}}=\\frac{1.56}{\\sqrt{35}}\\). You have all the information you need to use the normal command using rStudio. Without the central limit theorem, you couldn’t use the normal command, and you would not be able to answer this question.\nThe probability that the mean age that 35 smoking Americans start to smoke is more than 21 years, is the mathematical statement \\(P(\\bar{x}&gt;21)\\)\nTo find \\(P(\\bar{x}&gt;21)= 3.42X10^{-34}\\) using r studio, use the command:\n\npnorm(21, 17.8, 1.56/sqrt(35), lower.tail=FALSE) \n\n[1] 3.422499e-34\n\n\nThe probability of a sample mean of 35 smoking Americans being more than 21 years when they smoked for the first time is very small. This is extremely unlikely to happen. If it does, it may make you wonder about the sample. Could the population mean have increased from the 17.8 years as was stated? Could the sample not have been random, and instead have been a group of smoking Americans who had started to smoke much later? These questions, and more, are ones that you would want to ask as a researcher\n\n\n\n6.4.5 Homework for Sampling Distribution and the Central Limit Theorem Section\n\nA random variable is not normally distributed, but it is mound shaped. It has a mean of 14 and a standard deviation of 3.\n\n\n\nIf you take a sample of size 10, can you say what the shape of the sampling distribution for the sample mean is? Why?\nFor a sample of size 10, state the mean of the sample mean and the standard deviation of the sample mean.\nIf you take a sample of size 35, can you say what the shape of the distribution of the sample mean is? Why?\nFor a sample of size 35, state the mean of the sample mean and the standard deviation of the sample mean.\n\n\n\nA random variable is normally distributed. It has a mean of 245 and a standard deviation of 21.\n\n\n\nIf you take a sample of size 10, can you say what the shape of the distribution for the sample mean is? Why?\nFor a sample of size 10, state the mean of the sample mean and the standard deviation of the sample mean.\nFor a sample of size 10, find the probability that the sample mean is more than 241.\nIf you take a sample of size 35, can you say what the shape of the distribution of the sample mean is? Why?\nFor a sample of size 35, state the mean of the sample mean and the standard deviation of the sample mean.\nFor a sample of size 35, find the probability that the sample mean is more than 241.\nCompare your answers in part c and f. Why is one smaller than the other?\n\n\n\nThe mean starting salary for nurses is \\$67,694 nationally (“Staff nurse -,” 2013). The standard deviation is approximately $10,333. The starting salary is not normally distributed but it is mound shaped. A sample of 42 starting salaries for nurses is taken.\n\n\n\nState the random variable.\nWhat is the mean of the sample mean?\nWhat is the standard deviation of the sample mean?\nWhat is the shape of the sampling distribution of the sample mean? Why?\nFind the probability that the sample mean is more than \\$75,000.\nFind the probability that the sample mean is less than \\$60,000.\nIf you did find a sample mean of more than \\$75,000 would you find that unusual? What could you conclude?\n\n\n\nAccording to the WHO MONICA Project the mean blood pressure for people in China is 128 mmHg with a standard deviation of 23 mmHg (Kuulasmaa, Hense & Tolonen, 1998). Blood pressure is normally distributed.\n\n\n\nState the random variable.\nSuppose a sample of size 15 is taken. State the shape of the distribution of the sample mean.\nSuppose a sample of size 15 is taken. State the mean of the sample mean.\nSuppose a sample of size 15 is taken. State the standard deviation of the sample mean.\nSuppose a sample of size 15 is taken. Find the probability that the sample mean blood pressure is more than 135 mmHg.\nWould it be unusual to find a sample mean of 15 people in China of more than 135 mmHg? Why or why not?\nIf you did find a sample mean for 15 people in China to be more than 135 mmHg, what might you conclude?\n\n\n\nThe size of fish is very important to commercial fishing. A study conducted in 2012 found the length of Atlantic cod caught in nets in Karlskrona to have a mean of 49.9 cm and a standard deviation of 3.74 cm (Ovegard, Berndt & Lunneryd, 2012). The length of fish is normally distributed. A sample of 15 fish is taken.\n\n\n\nState the random variable.\nFind the mean of the sample mean.\nFind the standard deviation of the sample mean\nWhat is the shape of the distribution of the sample mean? Why?\nFind the probability that the sample mean length of the Atlantic cod is less than 52 cm.\nFind the probability that the sample mean length of the Atlantic cod is more than 74 cm.\nIf you found sample mean length for Atlantic cod to be more than 74 cm, what could you conclude?\n\n\n\nThe mean cholesterol levels of women age 45-59 in Ghana, Nigeria, and Seychelles is 5.1 mmol/l and the standard deviation is 1.0 mmol/l (Lawes, Hoorn, Law & Rodgers, 2004). Assume that cholesterol levels are normally distributed.\n\n\n\nState the random variable.\nFind the probability that a woman age 45-59 in Ghana has a cholesterol level above 6.2 mmol/l (considered a high level).\nSuppose doctors decide to test the woman’s cholesterol level again and average the two values. Find the probability that this woman’s mean cholesterol level for the two tests is above 6.2 mmol/l.\nSuppose doctors being very conservative decide to test the woman’s cholesterol level a third time and average the three values. Find the probability that this woman’s mean cholesterol level for the three tests is above 6.2 mmol/l.\nIf the sample mean cholesterol level for this woman after three tests is above 6.2 mmol/l, what could you conclude?\n\n\n\nIn the United States, males between the ages of 40 and 49 eat on average 103.1 g of fat every day with a standard deviation of 4.32 g (“What we eat,” 2012). The amount of fat a person eats is not normally distributed but it is relatively mound shaped.\n\n\n\nState the random variable.\nFind the probability that a sample mean amount of daily fat intake for 35 men age 40-59 in the U.S. is more than 100 g.\nFind the probability that a sample mean amount of daily fat intake for 35 men age 40-59 in the U.S. is less than 93 g.\nIf you found a sample mean amount of daily fat intake for 35 men age 40-59 in the U.S. less than 93 g, what would you conclude?\n\n\n\nA dishwasher has a mean life of 12 years with an estimated standard deviation of 1.25 years (“Appliance life expectancy,” 2013). The life of a dishwasher is normally distributed. Suppose you are a manufacturer and you take a sample of 10 dishwashers that you made.\n\n\n\nState the random variable.\nFind the mean of the sample mean.\nFind the standard deviation of the sample mean.\nWhat is the shape of the sampling distribution of the sample mean? Why?\nFind the probability that the sample mean of the dishwashers is less than 6 years.\nIf you found the sample mean life of the 10 dishwashers to be less than 6 years, would you think that you have a problem with the manufacturing process? Why or why not?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Continuous Probability Distribution</span>"
    ]
  },
  {
    "objectID": "One Sample Inference.html",
    "href": "One Sample Inference.html",
    "title": "7  One Sample Inference",
    "section": "",
    "text": "7.1 Basics of Hypothesis Testing\nTo understand the process of a hypothesis tests, you need to first have an understanding of what a hypothesis is, which is an educated guess about a parameter. Once you have the hypothesis, you collect data and use the data to make a determination to see if there is enough evidence to show that the hypothesis is true. However, in hypothesis testing you actually assume something else is true, and then you look at your data to see how likely it is to get an event that your data demonstrates with that assumption. If the event is very unusual, then you might think that your assumption is actually false. If you are able to say this assumption is false, then your hypothesis must be true. This is known as a proof by contradiction. You assume the opposite of your hypothesis is true and show that it can’t be true. If this happens, then your hypothesis must be true. All hypothesis tests go through the same process. Once you have the process down, then the concept is much easier. It is easier to see the process by looking at an example. Concepts that are needed will be detailed in this example.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>One Sample Inference</span>"
    ]
  },
  {
    "objectID": "One Sample Inference.html#basics-of-hypothesis-testing",
    "href": "One Sample Inference.html#basics-of-hypothesis-testing",
    "title": "7  One Sample Inference",
    "section": "",
    "text": "7.1.1 Example: Basics of Hypothesis Testing\nSuppose a manufacturer of the XJ35 battery claims the mean life of the battery is 500 days with a standard deviation of 25 days. You are the buyer of this battery and you think this claim is incorrect. You would like to test your belief because without a good reason you can’t get out of your contract.\n\n7.1.1.1 Solution\nWhat do you do?\nWell first, you should know what you are trying to measure. Define the random variable.\nLet \\(x\\) = life of a XJ35 battery\nNow you are not just trying to find different $x$ values. You are trying to find what the true mean is. Since you are trying to find it, it must be unknown. You don’t think it is 500 days. If you did, you wouldn’t be doing any testing. The true mean, \\(\\mu\\), is unknown. That means you should define that too.\nLet \\(\\mu\\) = mean life of a XJ35 battery\nNow what?\nYou may want to collect a sample. What kind of sample?\nYou could ask the manufacturers to give you batteries, but there is a chance that there could be some bias in the batteries they pick. To reduce the chance of bias, it is best to take a random sample.\nHow big should the sample be?\nA sample of size 30 or more means that you can use the central limit theorem. Pick a sample of size 50.\nTable 7.1 contains the data for the sample you collected:\n\nBattery&lt;- read.csv( \"https://krkozak.github.io/MAT160/battery.csv\") \nknitr::kable(head(Battery))\n\n\n\nTable 7.1: Data on Battery Life\n\n\n\n\n\n\nlife\n\n\n\n\n491\n\n\n485\n\n\n503\n\n\n492\n\n\n482\n\n\n490\n\n\n\n\n\n\n\n\nNow what should you do? Looking at the data set, you see some of the times are above 500 and some are below. But looking at all of the numbers is too difficult. It might be helpful to calculate the mean for this sample.\n\ndf_stats(~life, data=Battery, mean)\n\n  response mean\n1     life  490\n\n\nThe sample mean is 491.42 days. Looking at the sample mean, one might think that you are right. However, the standard deviation and the sample size also plays a role, so maybe you are wrong.\nBefore going any farther, it is time to formalize a few definitions.\nYou have a guess that the mean life of a battery is not 500 days. This is opposed to what the manufacturer claims. There really are two hypotheses, which are just guesses here — the one that the manufacturer claims and the one that you believe. It is helpful to have names for them.\nNull Hypothesis: historical value, claim, or product specification. The symbol used is \\(H_o\\).\nAlternate Hypothesis: what you want to prove. This is what you want to accept as true when you reject the null hypothesis. There are two symbols that are commonly used for the alternative hypothesis: \\(H_a\\) or \\(H_1\\). The symbol \\(H_a\\) will be used in this book.\nIn general, the hypotheses look something like this:\n\\(H_0:\\mu=\\mu_o\\)\n\\(H_a:\\mu\\ne \\mu_o\\)\nwhere \\(\\mu_o\\) just represents the value that the claim says the population mean is actually equal to.\nAlso, \\(H_a\\) can be less than, greater than, or not equal to, though not equal to is more common these days.\nFor this problem:\n\\(H_o:\\mu=500\\text{ days}\\), since the manufacturer says the mean life of a battery is 500 days.\n\\(H_a:\\mu\\ne 500\\text{ days}\\), since you believe that the mean life of the battery is not 500 days.\nNow back to the mean. You have a sample mean of 491.42 days. Is this different enough to believe that you are right and the manufacturer is wrong? How different does it have to be?\nIf you calculated a sample mean of 235 or 690, you would definitely believe the population mean is not 500. But even if you had a sample mean of 435 or 575 you would probably believe that the true mean was not 500. What about 475? or 535? Or 483? or 514? There is some point where you would stop being so sure that the population mean is not 500. That point separates the values of where you are sure or pretty sure that the mean is not 500 from the area where you are not so sure. How do you find that point?\nWell it depends on how much error you want to make. Of course you don’t want to make any errors, but unfortunately that is unavoidable in statistics. You need to figure out how much error you made with your sample. Take the sample mean, and find the probability of getting another sample mean less than it, assuming for the moment that the manufacturer is right. The idea behind this is that you want to know what is the chance that you could have come up with your sample mean even if the population mean really is 500 days.\nChances are probabilities. So you want to find the probability that the sample mean of 491.42 is unusual given that the population mean is really 500 days. To compute this probability, you need to know how the sample mean is distributed. Since the sample size is at least 30, then you know the sample mean is approximately normally distributed. Now, you want to find the \\(z\\)-value. The \\(z\\)-value is \\(z=\\frac{491.42-500}{\\frac{25}{\\sqrt{50}}}=-2.43\\).\nThis is more than 2 standard deviations below the mean, so that seems that the sample mean is usual. It might be helpful to find the probability though. Since you are saying that the sample mean is different from 500 days, then you are asking if it is greater than or less than. This means that you are in the tails of the normal curve. So the probability you want to find is the probability being more than 2.43 or less than \\(-2.43\\). This is \\(P(-2.43&lt;z)+P(z&gt;2.43)=0.015\\)\n\npnorm(-2.43, 0, 1, lower.tail=TRUE)+pnorm(2.43, 0, 1, lower.tail=FALSE) \n\n[1] 0.01509882\n\n\nSo the probability of being in the tails is 0.015. This probability is known as a p-value for probability-value. This is unusual, so it is unlikely to get a sample mean of 491.42 if the population mean is 500 days.\nSo it appears the assumption that the population mean is 500 days is wrong, and you can reject the manufacturer’s claim.\nBut how do you quantify really small? Is 5% or 10% or 15% really small? How do you decide?\nBefore you answer that question, a couple more definitions are needed.\nTest statistic: \\(z=\\frac{\\bar{x}-\\mu_o}{\\frac{\\sigma}{\\sqrt{n}}}\\) since it is calculated as part of the testing of the hypothesis\np - value: probability that the test statistic will take on more extreme values than the observed test statistic, given that the null hypothesis is true. It is the probability that was calculated above.\nNow, how small is small enough? To answer that, you really want to know the types of errors you can make.\nThere are actually only two errors that can be made. The first error is if you say that is false, when in fact it is true. This means you reject when was true. The second error is if you say that is true, when in fact it is false. This means you fail to reject when is false. The following table organizes this for you:\n\n\n\n7.1.2 Type of errors:\n\nType of errors\n\n\n\nHo true\nHo false\n\n\n\n\nReject Ho\nType I error\nno error\n\n\nFail to reject Ho\nno error\nType II error\n\n\n\nThus\nType I Error is rejecting \\(H_o\\) when \\(H_o\\) is true, and\nType II Error is failing to reject \\(H_o\\) when is \\(H_o\\) false.\nSince these are the errors, then one can define the probabilities attached to each error.\n\\(\\alpha\\)= P(type I error) = P(rejecting$H_o$ given it is true)\n\\(\\beta\\)= P(type II error) = P(failing to reject$H_o$ given it is false)\n\\(\\alpha\\) is also called the level of significance.\nAnother common concept that is used is Power = \\(1-\\beta\\)\nNow there is a relationship between \\(\\alpha\\) and \\(\\beta\\). They are not complements of each other. How are they related?\nIf \\(\\alpha\\) increases that means the chances of making a type I error will increase. It is more likely that a type I error will occur. It makes sense that you are less likely to make type II errors, only because you will be rejecting more often. You will be failing to reject less, and therefore, the chance of making a type II error will decrease. Thus, as \\(\\alpha\\) increases, \\(\\beta\\) will decrease, and vice versa. That makes them seem like complements, but they aren’t complements. What gives? Consider one more factor -- sample size.\nConsider if you have a larger sample that is representative of the population, then it makes sense that you have more accuracy then with a smaller sample. Think of it this way, which would you trust more, a sample mean of 490 if you had a sample size of 35 or sample size of 350 (assuming a representative sample)? Of course the 350 because there are more data points and so more accuracy. If you are more accurate, then there is less chance that you will make any error. By increasing the sample size of a representative sample, you decrease both \\(\\alpha\\) and \\(\\beta\\).\nSummary of all of this:\n\nFor a certain sample size, \\(\\alpha\\) increases, \\(\\beta\\) decreases.\nFor a certain level of significance, \\(\\alpha\\), if \\(n\\) increases, \\(\\beta\\) decreases.\n\nNow how do you find \\(\\alpha\\) and \\(\\beta\\)? Well \\(\\alpha\\) is actually chosen. There are only two values that are usually picked for \\(\\alpha\\): 0.01 and 0.05. is very difficult to find \\(\\beta\\), so usually it isn’t found. If you want to make sure it is small you take as large of a sample as you can afford provided it is a representative sample. This is one use of the Power. You want to be small and the Power of the test is large. The Power word sounds good.\nWhich pick of \\(\\alpha\\) do you pick? Well that depends on what you are working on. Remember in this example you are the buyer who is trying to get out of a contract to buy these batteries. If you create a type I error, you said that the batteries are bad when they aren’t, most likely the manufacturer will sue you. You want to avoid this. You might pick \\(\\alpha\\) to be 0.01. This way you have a small chance of making a type I error. Of course this means you have more of a chance of making a type II error. No big deal right? What if the batteries are used in pacemakers and you tell the person that their pacemaker’s batteries are good for 500 days when they actually last less, that might be bad. If you make a type II error, you say that the batteries do last 500 days when they last less, then you have the possibility of killing someone. You certainly do not want to do this. In this case you might want to pick \\(\\alpha\\) as 0.05. If both errors are equally bad, then pick \\(\\alpha\\) as 0.05.\nThe above discussion is why the choice of depends on what you are researching. As the researcher, you are the one that needs to decide what level to use based on your analysis of the consequences of making each error is.\nIf a type I error is really bad, then pick \\(\\alpha\\)= 0.01.\nIf a type II error is really bad, then pick \\(\\alpha\\)= 0.05\nIf neither error is bad, or both are equally bad, then pick \\(\\alpha\\) = 0.05\nUsually \\(\\alpha\\) is picked to be 0.05 in most cases.\nThe main thing is to always pick the \\(\\alpha\\) before you collect the data and start the test.\nThe above discussion was long, but it is really important information. If you don’t know what the errors of the test are about, then there really is no point in making conclusions with the tests. Make sure you understand what the two errors are and what the probabilities are for them.\nNow it is time to go back to the example and put this all together. This is the basic structure of testing a hypothesis, usually called a hypothesis test. Since this one has a test statistic involving \\(z\\), it is also called a \\(z\\)-test. And since there is only one sample, it is usually called a one-sample \\(z\\)-test.\n\n\n7.1.3 Example: Battery Example Revisited.\nSteps of a hypothesis test:\n\nState the random variable and the parameter in words\nState the null and alternative hypothesis and the level of significance\nState and check the conditions for a hypothesis test\nFind the sample statistic, test statistic, and p-value\nConclusion:\nInterpretation:\n\n\n7.1.3.1 Solution\n\nState the random variable and the parameter in words\n\n\\(x\\) = life of battery\n\\(\\mu\\) = mean life of a XJ35 battery\n\nState the null and alternative hypothesis and the level of significance\n\n\\(H_o:\\mu=500\\)\n\\(H_a:\\mu\\ne500\\)\n\\(\\alpha\\) = 0.05 (from above discussion about consequences)\n\nState and check the conditions for a hypothesis test\n\nEvery hypothesis has some conditions that be met to make sure that the results of the hypothesis are valid. The conditions are different for each test. This test has the following conditions.\n\nA random sample of size \\(n\\) is taken.\n\nThis occurred in this example, since it was stated that a random sample of 50 battery lives were taken.\n\nThe population standard deviation is known.\n\nThis is true, since it was given in the problem.\n\nThe sample size is at least 30 or the population of the random variable is normally distributed.\n\nThe sample size was 30, so this condition is met.\n\nFind the sample statistic, test statistic, and p-value\n\nThe test statistic depends on how many samples there are, what parameter you are testing, and conditions that need to be checked. In this case, there is one sample and you are testing the mean. The conditions were checked above.\nSample statistic:\n\ndf_stats(~life, data=Battery, mean)\n\n  response mean\n1     life  490\n\n\nTest statistic: The z-value is \\(z=\\frac{491.42-400}{\\frac{25}{\\sqrt{n}}}=-2.43\\).\np-value: \\(P(-2.43&lt;z)+P(z&gt;2.43)=0.015\\)\n\nConclusion:\n\nNow what? Well, this p-value is 0.015. This is a lot smaller than the amount of error you would accept in the problem \\(\\alpha\\) = 0.05. That means that finding a sample mean less than 490 days is unusual to happen if is true. This should make you think that is not true. You should reject \\(H_o\\).\nIn fact, in general:\nReject \\(H_o\\) if the p-value \\(&lt;\\alpha\\)\nFail to reject \\(H_o\\) if the p-value \\(\\ge\\alpha\\).\n\nInterpretation:\n\nSince you rejected \\(H_o\\), what does this mean in the real world? That it what goes in the interpretation. Since you rejected the claim by the manufacturer that the mean life of the batteries is 500 days, then you now can believe that your hypothesis was correct. In other words, there is enough evidence to support that the mean life of the battery is less than 500 days.\nNow that you know that the batteries last less than 500 days, should you cancel the contract? Statistically, there is evidence that the batteries do not last as long as the manufacturer says they should. However, based on this sample there are only ten days less on average that the batteries last. There may not be practical significance in this case. Ten days do not seem like a large difference. In reality, if the batteries are used in pacemakers, then you would probably tell the patient to have the batteries replaced every year. You have a large buffer whether the batteries last 490 days or 500 days. It seems that it might not be worth it to break the contract over ten days. What if the 10 days was practically significant? Are there any other things you should consider? You might look at the business relationship with the manufacturer. You might also look at how much it would cost to find a new manufacturer. These are also questions to consider before making any changes. What this discussion should show you is that just because a hypothesis has statistical significance does not mean it has practical significance. The hypothesis test is just one part of a research process. There are other pieces that you need to consider.\nThat’s it. That is what a hypothesis test looks like. All hypothesis tests are done with the same six steps. Those general six steps are outlined below.\n\n\n\n7.1.4 Steps for hypothesis test\n\nState the random variable and the parameter in words. This is where you are defining what the unknowns are in this problem.\n\n\\(x\\) = random variable\n\\(\\mu\\) = mean of random variable, if the parameter of interest is the mean. There are other parameters you can test, and you would use the appropriate symbol for that parameter.\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu=\\mu_o\\), where \\(\\mu_o\\) is the known mean\n\\(H_a:\\mu\\ne\\mu_o\\), You can replace \\(\\ne\\) with \\(&lt;\\) or \\(&gt;\\) but usually you use \\(\\ne\\)\nAlso, state your level here.\n\nState and check the conditions for a hypothesis test\n\nEach hypothesis test has its own conditions. They will be stated when the different hypothesis tests are discussed.\n\nFind the sample statistic, test statistic, and p-value\n\nThis depends on what parameter you are working with, how many samples, and the conditions of the test. Technology will be used to find the sample statistic, test statistic, and p-value.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge\\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\nSorry, one more concept about the conclusion and interpretation. First, the conclusion is that you reject or you fail to reject $H_o$. Why was it said like this? It is because you never accept the null hypothesis. If you wanted to accept the null hypothesis, then why do the test in the first place? In the interpretation, you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\). You wouldn’t want to go to all this work and then find out you wanted to accept the claim. Why go through the trouble? You always want to have enough evidence to support the alternative hypothesis. Sometimes you can do that and sometimes you can’t. If you don’t have enough evidence to support \\(H_a\\), it doesn’t mean you support the null hypothesis; it just means you can’t support the alternative hypothesis. Here is an example to demonstrate this.\n\n\n7.1.5 Example: Conclusions in Hypothesis Tests\nIn the U.S. court system a jury trial could be set up as a hypothesis test. To really help you see how this works, let’s use OJ Simpson as an example. In the court system, a person is presumed innocent until he/she is proven guilty, and this is your null hypothesis. OJ Simpson was a football player in the 1970s. In 1994 his ex-wife and her friend were killed. OJ Simpson was accused of the crime, and in 1995 the case was tried. The prosecutors wanted to prove OJ was guilty of killing his wife and her friend, and that is the alternative hypothesis. In this case, a verdict of not guilty was given. That does not mean that he is innocent of this crime. It means there was not enough evidence to prove he was guilty. Many people believe that OJ was guilty of this crime, but the jury did not feel that the evidence presented was enough to show there was guilt. The verdict in a jury trial is always guilty or not guilty!\nThe same is true in a hypothesis test. There is either enough or not enough evidence to support the alternative hypothesis. It is not that you proved the null hypothesis true.\nWhen identifying hypothesis, it is important to state your random variable and the appropriate parameter you want to make a decision about. If you count something, then the random variable is the number of whatever you counted. The parameter is the proportion of what you counted. If the random variable is something you measured, then the parameter is the mean of what you measured. (Note: there are other parameters you can calculate, and some analysis of those will be presented in later chapters.)\n\n\n7.1.6 Example: Stating Hypotheses\nIdentify the hypotheses necessary to test the following statements:\n\nThe average salary of a teacher is different from \\$30,000.\nThe proportion of students who like math is not 10%.\nThe average age of students in this class differs from 21.\n\n\n7.1.6.1 Solution\n\nThe average salary of a teacher is different from \\$30,000.\n\n\\(x\\) = salary of teacher\n\\(\\mu=\\) mean salary of teacher\nThe guess is that \\(\\mu\\ne30000\\) and that is the alternative hypothesis.\nThe null hypothesis has the same parameter and number with an equal sign.\n\\(H_o:\\mu=30000\\) \\(H_a:\\mu\\ne30000\\)\n\nThe proportion of students who like math is not 10%.\n\n\\(x\\) = number of students who like math\n\\(p\\) = proportion of students who like math\nThe guess is that \\(p\\) is not 0.10 and that is the alternative hypothesis. \\(H_a:p\\ne0.10\\) and the null hypothesis would be \\(H_o:p=0.10\\)\n\nThe average age of students in this class differs from 21.\n\n\\(x\\) = age of students in this class\n\\(\\mu\\)=mean age of students in this class\nThe guess is that \\(\\mu\\ne21\\) and that is the alternative hypothesis. \\(H_a:\\mu\\ne21\\) and the null hypothesis would be \\(H_o: \\mu=21\\)\n\n\n\n7.1.7 Example: Stating Type I and II Errors and Picking Level of Significance\n\nThe plant-breeding department at a major university developed a new hybrid raspberry plant called YumYum Berry. Based on research data, the claim is made that from the time shoots are planted 90 days on average are required to obtain the first berry with a standard deviation of 9.2 days. A corporation that is interested in marketing the product tests 60 shoots by planting them and recording the number of days before each plant produces its first berry. The sample mean is 92.3 days. The corporation wants to know if the mean number of days is more than the 90 days claimed. State the type I and type II errors in terms of this problem, consequences of each error, and state which level of significance to use.\nA concern was raised in Australia that the percentage of deaths of Aboriginal prisoners was higher than the percent of deaths of non-indigenous prisoners, which is 0.27%. State the type I and type II errors in terms of this problem, consequences of each error, and state which level of significance to use.\n\n\n7.1.7.1 Solution\n\nThe plant-breeding department at a major university developed a new hybrid raspberry plant called YumYum Berry. Based on research data, the claim is made that from the time shoots are planted 90 days on average are required to obtain the first berry with a standard deviation of 9.2 days. A corporation that is interested in marketing the product tests 60 shoots by planting them and recording the number of days before each plant produces its first berry. The sample mean is 92.3 days. The corporation wants to know if the mean number of days is more than the 90 days claimed. State the type I and type II errors in terms of this problem, consequences of each error, and state which level of significance to use.\n\n\\(x\\) = time to first berry for YumYum Berry plant\n\\(\\mu\\)= mean time to first berry for YumYum Berry plant\nType I Error: If the corporation does a type I error, then they will say that the plants take longer to produce than 90 days when they don’t. They probably will not want to market the plants if they think they will take longer. They will not market them even though in reality the plants do produce in 90 days. They may have loss of future earnings, but that is all.\nType II error: The corporation do not say that the plants take longer then 90 days to produce when they do take longer. Most likely they will market the plants. The plants will take longer, and so customers might get upset and then the company would get a bad reputation. This would be really bad for the company.\nLevel of significance: It appears that the corporation would not want to make a type II error. Pick a 5% level of significance, \\(\\alpha=0.05\\).\n\nA concern was raised in Australia that the percentage of deaths of Aboriginal prisoners was higher than the percent of deaths of non-indigenous prisoners, which is 0.27%. State the type I and type II errors in terms of this problem, consequences of each error, and state which level of significance to use.\n\n\\(x\\) = number of Aboriginal prisoners who have died\n\\(p\\) = proportion of Aboriginal prisoners who have died\nType I error: Rejecting that the proportion of Aboriginal prisoners who died was 0.27%, when in fact it was 0.27%. This would mean you would say there is a problem when there isn’t one. You could anger the Aboriginal community, and spend time and energy researching something that isn’t a problem.\nType II error: Failing to reject that the proportion of Aboriginal prisoners who died was 0.27%, when in fact it is higher than 0.27%. This would mean that you wouldn’t think there was a problem with Aboriginal prisoners dying when there really is a problem. You risk causing deaths when there could be a way to avoid them.\nLevel of significance: It appears that both errors may be issues in this case. You wouldn’t want to anger the Aboriginal community when there isn’t an issue, and you wouldn’t want people to die when there may be a way to stop it. It may be best to pick a 5% level of significance, \\(\\alpha=0.05\\).\nHint -- hypothesis testing is really easy if you follow the same recipe every time. The only differences in the various problems are the conditions of the test and the test statistic you calculate so you can find the p-value. Do the same steps, in the same order, with the same words, every time and these problems become very easy.\n\n\n\n7.1.8 Homework for Basics of Hypothesis Testing Section\nFor the problems in this section, a question is being asked. This is to help you understand what the hypotheses are. You are not to run any hypothesis tests nor come up with any conclusions in this section.\n\nThe Arizona Republic/Morrison/Cronkite News poll published on Monday, October 20, 2016, found 390 of the registered voters surveyed favor Proposition 205, which would legalize marijuana for adults. The statewide telephone poll surveyed 779 registered voters between Oct. 10 and Oct. 15. (Sanchez, 2016) Fifty-five percent of Colorado residents supported the legalization of marijuana. Does the data provide evidence that the percentage of Arizona residents who support legalization of marijuana is different from the proportion of Colorado residents who support it? State the random variable, population parameter, and hypotheses.\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, 23% of all complaints in 2007 were for identity theft. In that year, Alaska had 321 complaints of identity theft out of 1,432 consumer complaints (\\“Consumer fraud and,\\” 2008). Does this data provide enough evidence to show that Alaska had a different proportion of identity theft than 23%? State the random variable, population parameter, and hypotheses.\nThe Kyoto Protocol was signed in 1997, and required countries to start reducing their carbon emissions. The protocol became enforceable in February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per capita. Is there enough evidence to show that the mean CO2 emission is different in 2010 than in 2004? State the random variable, population parameter, and hypotheses.\nThe FDA regulates that fish that is consumed is allowed to contain 1.0 mg/kg of mercury. In Florida, bass fish were collected in 53 different lakes to measure the amount of mercury in the fish. Do the data provide enough evidence to show that the fish in Florida lakes has a different amount of mercury than the allowable amount? State the random variable, population parameter, and hypotheses.\nThe Arizona Republic/Morrison/Cronkite News poll published on Monday, October 20, 2016, found 390 of the registered voters surveyed favor Proposition 205, which would legalize marijuana for adults. The statewide telephone poll surveyed 779 registered voters between Oct. 10 and Oct. 15. (Sanchez, 2016) Fifty-five percent of Colorado residents supported the legalization of marijuana. Does the data provide evidence that the percentage of Arizona residents who support legalization of marijuana is different from the proportion of Colorado residents who support it. State the type I and type II errors in this case, consequences of each error type for this situation from the perspective of the manufacturer, and the appropriate alpha level to use. State why you picked this alpha level.\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, 23% of all complaints in 2007 were for identity theft. In that year, Alaska had 321 complaints of identity theft out of 1,432 consumer complaints (\\“Consumer fraud and,\\” 2008). Does this data provide enough evidence to show that Alaska had a different proportion of identity theft than 23%? State the type I and type II errors in this case, consequences of each error type for this situation from the perspective of the state of Alaska, and the appropriate alpha level to use. State why you picked this alpha level.\nThe Kyoto Protocol was signed in 1997, and required countries to start reducing their carbon emissions. The protocol became enforceable in February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per capita. Is there enough evidence to show that the mean CO2 emission is lower in 2010 than in 2004? State the type I and type II errors in this case, consequences of each error type for this situation from the perspective of the agency overseeing the protocol, and the appropriate alpha level to use. State why you picked this alpha level.\nThe FDA regulates that fish that is consumed is allowed to contain 1.0 mg/kg of mercury. In Florida, bass fish were collected in 53 different lakes to measure the amount of mercury in the fish. Do the data provide enough evidence to show that the fish in Florida lakes has different amount of mercury than the allowable amount? State the type I and type II errors in this case, consequences of each error type for this situation from the perspective of the FDA, and the appropriate alpha level to use. State why you picked this alpha level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>One Sample Inference</span>"
    ]
  },
  {
    "objectID": "One Sample Inference.html#one-sample-proportion-test",
    "href": "One Sample Inference.html#one-sample-proportion-test",
    "title": "7  One Sample Inference",
    "section": "7.2 One-Sample Proportion Test",
    "text": "7.2 One-Sample Proportion Test\nThere are many different parameters that you can test. There is a test for the mean, such as was introduced with the $z$-test. There is also a test for the population proportion, \\(p\\). This is where you might be curious if the proportion of students who smoke at your school is lower than the proportion in your area. Or you could question if the proportion of accidents caused by teenage drivers who do not have a drivers’ education class is more than the national proportion.\nTo test a population proportion, there are a few things that need to be defined first. Usually, Greek letters are used for parameters and Latin letters for statistics. When talking about proportions, it makes sense to use \\(p\\) for proportion. The Greek letter for \\(p\\) is \\(\\pi\\), but that is too confusing to use. Instead, it is best to use \\(p\\) for the population proportion. That means that a different symbol is needed for the sample proportion. The convention is to use, \\(\\hat{p}\\), known as p-hat. This way you know that \\(p\\) is the population proportion, and that \\(\\hat{p}\\) is the sample proportion related to it.\nNow proportion tests are about looking for the percentage of individuals who have a particular attribute. You are really looking for the number of successes that happen. Thus, a proportion test involves a binomial distribution.\n\n7.2.1 Hypothesis Test for One Population Proportion (1-Prop Test)\n\nState the random variable and the parameter in words.\n\n\\(x\\) = number of successes\n\\(p\\) = proportion of successes\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:p=p_o\\), where \\(p_o\\) is the known proportion\n\\(H_a:p\\ne p_o\\), you can also use &lt; or &gt;, but \\(\\ne\\) is the more common one to use.\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A simple random sample of size \\(n\\) is taken. Check: describe how the sample was collected\nState: The conditions for the binomial experiment are satisfied. Check: Show all four properties are true.\nState: The sampling distribution of \\(\\hat{p}\\) is normally distributed. Check: you need to show that \\(p*n\\ge5\\) and \\(q*n\\ge5\\), where \\(q=1-p\\). If this requirement is true, then the sampling distribution of \\(\\hat{p}\\) is well approximated by a normal curve.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nThis will be computed on r Studio using the command\nprop.test(r, n, p=what_Ho_says)\nwhere \\(r\\)=observed number of successes and \\(n\\) = number of trials.\n\nConclusion\n\nThis is where you write reject or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_0\\). If the p-value \\(\\ge\\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n7.2.2 Example: Hypothesis Test for One Proportion\nA concern was raised in Australia that the percentage of deaths of Aboriginal prisoners was different than the percent of deaths of non-Aboriginal prisoners, which is 0.27%. A sample of six years (1990-1995) of data was collected, and it was found that out of 14,495 Aboriginal prisoners, 51 died (\\“Indigenous deaths in,\\” 1996). Do the data provide enough evidence to show that the proportion of deaths of Aboriginal prisoners is different from 0.27%?\n\n7.2.2.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = number of Aboriginal prisoners who die\n\\(p\\) = proportion of Aboriginal prisoners who die\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:p=0.0027\\)\n\\(H_a:p\\ne0.0027\\)\nFrom Example: Stating Type I and II Errors and Picking Level of Significance part b, the argument was made to pick 5% for the level of significance. So \\(\\alpha=0.05\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nA simple random sample of 14,495 Aboriginal prisoners was taken. Check: The sample was not a random sample, since it was data from six years. It is the numbers for all prisoners in these six years, but the six years were not picked at random. Unless there was something special about the six years that were chosen, the sample is probably a representative sample. This condition is probably met.\nThe properties of a binomial experiment are met. There are 14,495 prisoners in this case. Check: The prisoners are all Aboriginals, so you are not mixing Aboriginal with non-Aboriginal prisoners. There are only two outcomes, either the prisoner dies or doesn’t. The chance that one prisoner dies over another may not be constant, but if you consider all prisoners the same, then it may be close to the same probability. Thus the conditions for the binomial distribution are satisfied\nThe sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distributed. Check: In this case \\(p = 0.0027\\) and \\(n = 14,495\\). \\(n*p=39.1365\\ge5\\) and \\(n*q=14455.86\\ge5\\). So, the sampling distribution for \\(\\hat{p}\\) is normally distributed.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nUse the following command in rStudio:\n\nprop.test(51, 14495, p=0.0027)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  51 out of 14495\nX-squared = 3.3084, df = 1, p-value = 0.06893\nalternative hypothesis: true p is not equal to 0.0027\n95 percent confidence interval:\n 0.002647440 0.004661881\nsample estimates:\n          p \n0.003518455 \n\n\nSample Proportion: \\(\\hat{p}=0.0035\\)\nTest Statistic: \\(\\chi^2=3.3085\\)\np-value: \\(p-value=0.06893\\)\n\nConclusion\n\nSince the \\(p-value\\ge0.05\\), then fail to reject \\(H_o\\).\n\nInterpretation\n\nThere is not enough evidence to support that the proportion of deaths of Aboriginal prisoners is different from non-Aboriginal prisoners.\n\n\n\n7.2.3 Example: Hypothesis Test for One Proportion\nA researcher who is studying the effects of income levels on breastfeeding of infants hypothesizes that countries with a low income level have a different rate of infant breastfeeding than higher income countries. It is known that in Germany, considered a high-income country by the World Bank, 22% of all babies are breastfeed. In Tajikistan, considered a low-income country by the World Bank, researchers found that in a random sample of 500 new mothers that 125 were breastfeeding their infant. At the 5% level of significance, does this show that low-income countries have a different incident of breastfeeding?\n\n7.2.3.1 Solution\n\nState you random variable and the parameter in words.\n\n\\(x\\) = number of woman who breastfeed in a low-income country\n\\(p\\) = proportion of woman who breastfeed in a low-income country\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:p=0.22\\)\n\\(H_a:p\\ne0.22\\)\n\\(\\alpha=0.05\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nA simple random sample of 500 breastfeeding habits of woman in a low-income country was taken. Check: This was stated in the problem.\nThe properties of a Binomial Experiment have been met. Check: There were 500 women in the study. The women are considered identical, though they probably have some differences. There are only two outcomes, either the woman breastfeeds or she doesn’t. The probability of a woman breastfeeding is probably not the same for each woman, but it is probably not very different for each woman. The conditions for the binomial distribution are satisfied\nThe sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distributed. Check: In this case, \\(n = 500\\) and \\(p = 0.22\\). \\(n*p= 110\\ge5\\) and \\(n*q=390\\ge5\\), so the sampling distribution of \\(\\hat{p}\\) is well approximated by a normal curve.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nOn r studio, use the following command\n\nprop_test(125, 500, p=0.22) \n\n\n    1-sample proportions test with continuity correction\n\ndata:  125 out of 500\nX-squared = 2.4505, df = 1, p-value = 0.1175\nalternative hypothesis: true p is not equal to 0.22\n95 percent confidence interval:\n 0.2131062 0.2908059\nsample estimates:\n   p \n0.25 \n\n\nSample Statistic: \\(\\hat{p}=0.25\\)\ntest Statistic: \\(\\chi^2=2.4505\\)\np-value: \\(p-value=0.1175\\)\n\nConclusion\n\nSince the p-value is more than 0.05, you fail to reject \\(H_o\\).\n\nInterpretation\n\nThere is not enough evidence to support that the proportion of women who breastfeed in low-income countries is different from the proportion of women in high-income countries who breastfeed.\nNotice, the conclusion is that there wasn’t enough evidence to support \\(H_a\\). The conclusion was not that you support \\(H_o\\). There are many reasons why you can’t say that \\(H_o\\) is true. It could be that the countries you chose were not very representative of what truly happens. If you instead looked at all high-income countries and compared them to low-income countries, you might have different results. It could also be that the sample you collected in the low-income country was not representative. It could also be that income level is not an indication of breastfeeding habits. It could be that the sample that was taken didn’t show evidence but another sample would show evidence. There could be other factors involved. This is why you can’t say that you support \\(H_o\\). There are too many other factors that could be the reason that you failed to reject \\(H_0\\).\n\n\n\n7.2.4 Homework for One-Sample Proportion Test Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nThe Arizona Republic/Morrison/Cronkite News poll published on Monday, October 20, 2016, found 390 of the registered voters surveyed favor Proposition 205, which would legalize marijuana for adults. The statewide telephone poll surveyed 779 registered voters between Oct. 10 and Oct. 15. (Sanchez, 2016) Fifty-five percent of Colorado residents supported the legalization of marijuana. Does the data provide evidence that the percentage of Arizona residents who support legalization of marijuana is different from the proportion of Colorado residents who support it. Test at the 1% level.\nIn July of 1997, Australians were asked if they thought unemployment would increase, and 47% thought that it would increase. In November of 1997, they were asked again. At that time 284 out of 631 said that they thought unemployment would increase (\\“Morgan Gallup poll,\\” 2013). At the 5% level, is there enough evidence to show that the proportion of Australians in November 1997 who believe unemployment would increase is different from the proportion who felt it would increase in July 1997?\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, 23% of all complaints in 2007 were for identity theft. In that year, Arkansas had 1,601 complaints of identity theft out of 3,482 consumer complaints (\\“Consumer fraud and,\\” 2008). Does this data provide enough evidence to show that Arkansas had a different percentage of identity theft than 23%? Test at the 5% level.\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, 23% of all complaints in 2007 were for identity theft. In that year, Alaska had 321 complaints of identity theft out of 1,432 consumer complaints (\\“Consumer fraud and,\\” 2008). Does this data provide enough evidence to show that Alaska had a different proportion of identity theft than 23%? Test at the 5% level.\nIn 2001, the Gallup poll found that 81% of American adults believed that there was a conspiracy in the death of President Kennedy. In 2013, the Gallup poll asked 1,039 American adults if they believe there was a conspiracy in the assassination, and found that 634 believe there was a conspiracy (\\“Gallup news service,\\” 2013). Do the data show that the proportion of Americans who believe in this conspiracy has changed? Test at the 1% level.\nIn 2008, there were 507 children in Arizona out of 32,601 who were diagnosed with Autism Spectrum Disorder (ASD) (\\“Autism and developmental,\\” 2008). Nationally 1 in 88 children are diagnosed with ASD (\\“CDC features -,\\” 2013). Is there sufficient data to show that the incident of ASD is different in Arizona than nationally? Test at the 1% level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>One Sample Inference</span>"
    ]
  },
  {
    "objectID": "One Sample Inference.html#one-sample-test-for-the-mean",
    "href": "One Sample Inference.html#one-sample-test-for-the-mean",
    "title": "7  One Sample Inference",
    "section": "7.3 One-Sample Test for the Mean",
    "text": "7.3 One-Sample Test for the Mean\nIt is time to go back to look at the test for the mean that was introduced in section 7.1 called the \\(z\\)-test. In the example, you knew what the population standard deviation, \\(\\sigma\\), was. What if you don’t know \\(\\sigma\\)?\nIf you don’t know \\(\\sigma\\), then you don’t know the sampling distribution of the mean. Can it be found another way? The answer is of course, yes. One way is to use a method called resampling. The following example explains how resampling is performed.\n\n7.3.1 Example: Resampling\nA random sample of 10 body mass index (BMI) were taken from the NHANES Data frame The mean BMI of Australians is 27.2 \\(kg/m^2\\). Is there evidence that Americans have a different BMI from people in Australia. Test at the 5% level.\n\n7.3.1.1 Solution\nThe standard deviation of BMI is not known for Australians. To answer this questions, first look at the sample from NHANES Table 7.2.\n\nsample_NHANES_10&lt;- \n  NHANES |&gt;\n  slice_sample(n=10)\nknitr::kable(head(sample_NHANES_10))\n\n\n\nTable 7.2: Sample of size 10 from NHANES\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n58424\n2009_10\nmale\n46\n40-49\n561\nWhite\nNA\nSome College\nDivorced\nmore 99999\n1e+05\n5.00\n9\nOwn\nWorking\n76.2\nNA\nNA\n171.7\n25.85\nNA\n25.0_to_29.9\n68\n120\n74\n118\n74\n124\n70\n116\n78\nNA\n1.32\n8.79\n155\n2.719\nNA\nNA\nNo\nNA\nExcellent\n0\n0\nNone\nNone\nNA\nNA\nNA\n7\nNo\nYes\n5\nNA\nNA\nNA\nNA\nYes\n2\n260\nNo\nYes\nSmoker\n22\nYes\n17\nYes\n18\nYes\nYes\n17\n6\n1\nNo\nHeterosexual\nNA\n\n\n56758\n2009_10\nmale\n3\n0-9\n42\nMexican\nNA\nNA\nNA\nNA\nNA\nNA\n6\nRent\nNA\n28.4\n102.8\nNA\n99.3\n28.80\nNA\n25.0_to_29.9\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n6\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n54310\n2009_10\nmale\n50\n50-59\n601\nWhite\nNA\nSome College\nMarried\n65000-74999\n7e+04\n2.95\n9\nOwn\nWorking\n88.6\nNA\nNA\n176.5\n28.44\nNA\n25.0_to_29.9\n54\n131\n21\n132\n54\n128\n42\n134\n0\nNA\n1.68\n6.62\n84\n0.800\nNA\nNA\nNo\nNA\nGood\n5\n0\nNone\nNone\nNA\nNA\nNA\n7\nYes\nYes\n3\nNA\nNA\nNA\nNA\nYes\n3\n208\nNo\nYes\nSmoker\n19\nYes\n18\nNo\nNA\nNo\nYes\n17\n15\n1\nNo\nHeterosexual\nNA\n\n\n63771\n2011_12\nmale\n43\n40-49\nNA\nWhite\nWhite\nSome College\nMarried\n35000-44999\n4e+04\n2.71\n7\nOther\nNotWorking\n119.1\nNA\nNA\n172.7\n39.90\nNA\n30.0_plus\n82\n134\n86\nNA\nNA\n132\n88\n136\n84\n102.95\n1.11\n4.73\n187\nNA\nNA\nNA\nNo\nNA\nGood\n30\n10\nSeveral\nSeveral\nNA\nNA\nNA\n7\nYes\nNo\n4\nMore_4_hr\n4_hr\nNA\nNA\nYes\nNA\n0\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n17\n1\n1\nNo\nHeterosexual\nNA\n\n\n52637\n2009_10\nfemale\n7\n0-9\n84\nWhite\nNA\nNA\nNA\nmore 99999\n1e+05\n5.00\n7\nOwn\nNA\n24.5\nNA\nNA\n125.7\n15.51\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n0.83\n3.65\n30\n0.811\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n56700\n2009_10\nmale\n19\n10-19\n233\nWhite\nNA\nNA\nNA\n25000-34999\n3e+04\n0.55\n6\nRent\nWorking\n65.3\nNA\nNA\n173.9\n21.59\nNA\n18.5_to_24.9\n54\n110\n71\n116\n64\n112\n78\n108\n64\nNA\n0.98\n3.96\n204\n0.322\nNA\nNA\nNo\nNA\nVgood\n0\n3\nNone\nNone\nNA\nNA\nNA\n8\nNo\nYes\n7\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n14\nYes\n15\nNo\nYes\n13\n9\n3\nNo\nHeterosexual\nNA\n\n\n\n\n\n\n\n\nThe mean BMI from this sample is\n\ndf_stats(~BMI, data=sample_NHANES_10, mean)\n\n  response   mean\n1      BMI 23.966\n\n\nThe sample mean for Americans is different from the mean BMI for Australians, but could it just be by chance. Suppose you take another sample of size 10, but you only have these 10 BMIs to work with. So how could you do this. One way is to assume that the sample you took is representative of the entire population, and so you create a population by copying this sample over and over again. So you could have over 1000 copies of this sample of 10 BMIs. Then take a sample of size 10 from this created population. When doing this, you could conceivably choose the same number several times that was in the original sample and not choose some of the numbers that were in the original sample. Instead of physically creating this new population, you could just take samples from your original sample but with replacement. This means that you randomly pick the first number, record it, and then put it back that value back before collecting the next number. This kind a sampling is called randomization sampling. A sample using randomization could be Table 7.3.\n\nknitr::kable(resample(sample_NHANES_10))\n\n\n\nTable 7.3: Resample from NHANES Sample\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\norig.id\n\n\n\n\n54310\n2009_10\nmale\n50\n50-59\n601\nWhite\nNA\nSome College\nMarried\n65000-74999\n70000\n2.95\n9\nOwn\nWorking\n88.6\nNA\nNA\n176.5\n28.44\nNA\n25.0_to_29.9\n54\n131\n21\n132\n54\n128\n42\n134\n0\nNA\n1.68\n6.62\n84\n0.800\nNA\nNA\nNo\nNA\nGood\n5\n0\nNone\nNone\nNA\nNA\nNA\n7\nYes\nYes\n3\nNA\nNA\nNA\nNA\nYes\n3\n208\nNo\nYes\nSmoker\n19\nYes\n18\nNo\nNA\nNo\nYes\n17\n15\n1\nNo\nHeterosexual\nNA\n3\n\n\n56700\n2009_10\nmale\n19\n10-19\n233\nWhite\nNA\nNA\nNA\n25000-34999\n30000\n0.55\n6\nRent\nWorking\n65.3\nNA\nNA\n173.9\n21.59\nNA\n18.5_to_24.9\n54\n110\n71\n116\n64\n112\n78\n108\n64\nNA\n0.98\n3.96\n204\n0.322\nNA\nNA\nNo\nNA\nVgood\n0\n3\nNone\nNone\nNA\nNA\nNA\n8\nNo\nYes\n7\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n14\nYes\n15\nNo\nYes\n13\n9\n3\nNo\nHeterosexual\nNA\n6\n\n\n67151\n2011_12\nmale\n5\n0-9\nNA\nBlack\nBlack\nNA\nNA\n0-4999\n2500\n0.26\n4\nRent\nNA\n22.4\nNA\nNA\n119.6\n15.70\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n3_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n8\n\n\n56700\n2009_10\nmale\n19\n10-19\n233\nWhite\nNA\nNA\nNA\n25000-34999\n30000\n0.55\n6\nRent\nWorking\n65.3\nNA\nNA\n173.9\n21.59\nNA\n18.5_to_24.9\n54\n110\n71\n116\n64\n112\n78\n108\n64\nNA\n0.98\n3.96\n204\n0.322\nNA\nNA\nNo\nNA\nVgood\n0\n3\nNone\nNone\nNA\nNA\nNA\n8\nNo\nYes\n7\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nYes\n14\nYes\n15\nNo\nYes\n13\n9\n3\nNo\nHeterosexual\nNA\n6\n\n\n63361\n2011_12\nmale\n67\n60-69\nNA\nBlack\nBlack\nCollege Grad\nWidowed\n25000-34999\n30000\n2.75\n4\nOwn\nNotWorking\n84.9\nNA\nNA\n182.1\n25.60\nNA\n25.0_to_29.9\n88\n138\n88\n144\n86\n144\n90\n132\n86\n326.00\n1.03\n5.92\n373\n2.234\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\nNA\nNA\nNA\n6\nYes\nYes\n3\n4_hr\n1_hr\nNA\nNA\nYes\nNA\n0\nNo\nYes\nSmoker\n16\nNA\nNA\nNA\nNA\nNo\nYes\n9\n8\nNA\nNo\nNA\nNA\n9\n\n\n63771\n2011_12\nmale\n43\n40-49\nNA\nWhite\nWhite\nSome College\nMarried\n35000-44999\n40000\n2.71\n7\nOther\nNotWorking\n119.1\nNA\nNA\n172.7\n39.90\nNA\n30.0_plus\n82\n134\n86\nNA\nNA\n132\n88\n136\n84\n102.95\n1.11\n4.73\n187\nNA\nNA\nNA\nNo\nNA\nGood\n30\n10\nSeveral\nSeveral\nNA\nNA\nNA\n7\nYes\nNo\n4\nMore_4_hr\n4_hr\nNA\nNA\nYes\nNA\n0\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n17\n1\n1\nNo\nHeterosexual\nNA\n4\n\n\n54310\n2009_10\nmale\n50\n50-59\n601\nWhite\nNA\nSome College\nMarried\n65000-74999\n70000\n2.95\n9\nOwn\nWorking\n88.6\nNA\nNA\n176.5\n28.44\nNA\n25.0_to_29.9\n54\n131\n21\n132\n54\n128\n42\n134\n0\nNA\n1.68\n6.62\n84\n0.800\nNA\nNA\nNo\nNA\nGood\n5\n0\nNone\nNone\nNA\nNA\nNA\n7\nYes\nYes\n3\nNA\nNA\nNA\nNA\nYes\n3\n208\nNo\nYes\nSmoker\n19\nYes\n18\nNo\nNA\nNo\nYes\n17\n15\n1\nNo\nHeterosexual\nNA\n3\n\n\n67151\n2011_12\nmale\n5\n0-9\nNA\nBlack\nBlack\nNA\nNA\n0-4999\n2500\n0.26\n4\nRent\nNA\n22.4\nNA\nNA\n119.6\n15.70\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n4\n3_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n8\n\n\n54310\n2009_10\nmale\n50\n50-59\n601\nWhite\nNA\nSome College\nMarried\n65000-74999\n70000\n2.95\n9\nOwn\nWorking\n88.6\nNA\nNA\n176.5\n28.44\nNA\n25.0_to_29.9\n54\n131\n21\n132\n54\n128\n42\n134\n0\nNA\n1.68\n6.62\n84\n0.800\nNA\nNA\nNo\nNA\nGood\n5\n0\nNone\nNone\nNA\nNA\nNA\n7\nYes\nYes\n3\nNA\nNA\nNA\nNA\nYes\n3\n208\nNo\nYes\nSmoker\n19\nYes\n18\nNo\nNA\nNo\nYes\n17\n15\n1\nNo\nHeterosexual\nNA\n3\n\n\n54310\n2009_10\nmale\n50\n50-59\n601\nWhite\nNA\nSome College\nMarried\n65000-74999\n70000\n2.95\n9\nOwn\nWorking\n88.6\nNA\nNA\n176.5\n28.44\nNA\n25.0_to_29.9\n54\n131\n21\n132\n54\n128\n42\n134\n0\nNA\n1.68\n6.62\n84\n0.800\nNA\nNA\nNo\nNA\nGood\n5\n0\nNone\nNone\nNA\nNA\nNA\n7\nYes\nYes\n3\nNA\nNA\nNA\nNA\nYes\n3\n208\nNo\nYes\nSmoker\n19\nYes\n18\nNo\nNA\nNo\nYes\n17\n15\n1\nNo\nHeterosexual\nNA\n3\n\n\n\n\n\n\n\n\nNotice that some of the unit of observations are repeated. That is what happens when you resample. Now one resampling isn’t enough. So you want to resample many times so you can create a resampling distribution Figure 7.1\n\n# mutate NHANES to subtract 27.2 (Australia's BMI) from US BMI measurements \nmutate_NHANES &lt;- NHANES |&gt;\n  mutate(NewBMI=BMI-27.2)\n# Generate the single sample\nSingle_sample&lt;- mutate_NHANES |&gt;\n  sample_n(size = 10)\n#Calculate the mean age of the single sample \nSingle_sample_mean &lt;- \n  Single_sample |&gt;\n  df_stats( ~ NewBMI, means = mean)\n#Take 200 resamples from the single sample\nTrials_resample &lt;- \n  do(200) * { Single_sample |&gt;\n      resample() |&gt;\n      df_stats( ~ NewBMI, means = mean) }\n# Plot the resample distribution of means\ngf_density( ~ means, data = Trials_resample, bins = 10) |&gt;\n  gf_lims(x = c(-5, 10)) |&gt;\n  gf_labs(title = \"Resampling Distribution\") |&gt;\n  gf_vline(data = Single_sample_mean, xintercept = ~ means, color=\"red\")\ndf_stats( ~ means, data = Trials_resample, mean, sd)\n\n  response    mean       sd\n1    means 0.34217 2.848246\n\n\n\n\n\n\n\n\nFigure 7.1: Resampling distribution of mean BMI with sample size 10\n\n\n\n\n\nNotice the sample mean from the resampling is very close to 0, so that means that the US BMI are not that different from the Australian BMI. There doesn’t seem to be enough evidence to show that the US BMI is different from the Australian BMI. One note, the sample size used here was 10 so you could see the sample, but really the sample size should be more than 100 for this method to be valid.\nSo this is one way to answer the question about if there is evidence to show a population mean is different from a value. This is actually the method that Ronald Fisher developed when he create all the foundation work that he did in statistics in the early 1900s. However, at the time, computers didn’t exist, so taking 100 resampling samples was not possible at that time. So other methods had to be developed that could be computed during that time. One method was developed by William (W.S) Gossett, a Chemist who worked for Guinness as their head brewer. Gossett developed a distribution called the Student’s T-distribution. His process was to use the sample standard deviation, \\(s\\), as an approximation of \\(\\sigma\\). This means the test statistic is now \\(t=\\frac{x-\\mu}{\\frac{s}{\\sqrt{n}}}\\). This new test statistic is actually distributed as a Student’s t-distribution, developed by W.S. Gossett. There are some conditions that must be made for this formula to be a Student’s t-distribution. These are outlined in the following theorem. Note: the t-distribution is called the Student’s t-distribution because that is the name he published under because he couldn’t publish under his own name due to his employer not wanting him to publish under his own name. His employer by the way was Guinness and they didn’t want competitors knowing they had a chemist/statistician working for them. It is not called the Student’s t-distribution because it is only used by students.\nTheorem: If the following conditions are met\n\nA random sample of size \\(n\\) is taken.\nThe distribution of the random variable is normal.\n\nThen the distribution of is a Student’s t-distribution with \\(n-1\\) degrees of freedom.\nExplanation of degrees of freedom: Recall the formula for sample standard deviation is \\(\\sqrt{{\\frac{\\sum{x-\\bar{x}}}{n-1}}}\\). Notice the denominator is \\(n-1\\). This is the same as the degrees of freedom. This is no accident. The reason the denominator and the degrees of freedom are both comes from how the standard deviation is calculated. First you take each data value and subtract \\(\\bar{x}\\). If you add up all of these new values, you will get 0. This must happen. Since it must happen, the first \\(n-1\\) data values you have “freedom of choice”, but the nth data value, you have no freedom to choose. Hence, you have \\(n-1\\) degrees of freedom. Another way to think about it is that if you five people and five chairs, the first four people have a choice of where they are sitting, but the last person does not. They have no freedom of where to sit. Only \\(n-1\\) people have freedom of choice.\nThe Student’s t-distribution is bell-shape that is more spread out than the normal distribution. There are many \\(t\\)-distributions, one for each different degree of freedom.\nFigure 7.2 is of the normal distribution and the Student’s t-distribution for df = 1, df = 3, df=8, df=30.\n\n\n\n\n\n\n\n\nFigure 7.2: Typical Student t-Distributions\n\n\n\n\n\nAs the degrees of freedom increases, the student’s t-distribution looks more like the normal distribution.\nTo find probabilities for the t-distribution, again technology can do this for you. There are many technologies out there that you can use.\n\n\n\n7.3.2 Hypothesis Test for One Population Mean (t-Test)\n\nState the random variable and the parameter in words.\n\n\\(x\\) = random variable\n\\(\\mu\\) = mean of random variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu=\\mu_o\\) , where \\(\\mu_o\\) is the known mean\n\\(H_a:\\mu\\ne\\mu_o\\), you can also use &lt; or &gt;, but \\(\\ne\\) is the more modern one to use.\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A random sample of size \\(n\\) is taken. Check: Describe the process taken to collect the sample.\nState: The population of the random variable is normally distributed. Check: examine density graph and normal quantile plot. Note: The t-test is fairly robust to the condition if the sample size is large. This means that if this condition isn’t met, but your sample size is quite large, then the results of the t-test are valid.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nOn rStudio, the command is\nt.test(~variable, data=data_frame, mu=what_Ho_says)\n\nConclusion\n\nThis is where you write reject or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\nNote: if the conditions behind this test are not valid, then the conclusions you make from the test are not valid. If you do not have a random sample, that is your fault. Make sure the sample you take is as random as you can make it following sampling techniques from chapter 1. If the population of the random variable is not normal, then take a larger sample. If you cannot afford to do that, or if it is not logistically possible, then you do different tests called non-parametric tests or you can try resampling. The advantage fo resampling is that you don’t need to know the under laying distribution of the random variable.\n\n\n7.3.3 Example: Test of the Mean Using One Sample T-test\nA random sample of 50 body mass index (BMI) were taken from the NHANES Data frame Table 7.4. The mean BMI of Australians is 27.2 \\(kg/m^2\\). Is there evidence that Americans have a different BMI from people in Australia. Test at the 5% level.\n\nsample_NHANES_50&lt;- sample_n(NHANES, size=50) \nknitr::kable(head(sample_NHANES_50))\n\n\n\nTable 7.4: BMI of Americans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n56351\n2009_10\nmale\n1\n0-9\n16\nWhite\nNA\nNA\nNA\n25000-34999\n30000\n1.45\n7\nOwn\nNA\n9.8\n78.1\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n63128\n2011_12\nfemale\n34\n30-39\nNA\nBlack\nBlack\nSome College\nSeparated\n25000-34999\n30000\n1.84\n4\nRent\nWorking\n46.4\nNA\nNA\n160.7\n18.00\nNA\n12.0_18.5\n64\n110\n73\n108\n66\n110\n70\n110\n76\n45.10\n1.81\n3.49\n68\n0.618\nNA\nNA\nNo\nNA\nVgood\n0\n30\nNone\nNone\n3\n1\nNA\n4\nNo\nYes\n3\n2_hr\n2_hr\nNA\nNA\nYes\n2\n156\nNA\nNo\nNon-Smoker\nNA\nYes\n18\nYes\n18\nNo\nYes\n15\n30\n4\nNo\nHeterosexual\nUnknown\n\n\n62163\n2011_12\nmale\n14\n10-19\nNA\nOther\nAsian\nNA\nNA\nmore 99999\n100000\n4.07\n6\nRent\nNA\n49.4\nNA\nNA\n168.9\n17.30\nNormWeight\n12.0_18.5\n72\n107\n37\n112\n38\n108\n36\n106\n38\n274.95\n1.14\n3.98\n116\nNA\nNA\nNA\nNo\nNA\nGood\n0\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n1\n2_hr\n3_hr\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n61570\n2009_10\nfemale\n34\n30-39\n415\nMexican\nNA\nSome College\nDivorced\n35000-44999\n40000\n2.75\n6\nRent\nWorking\n60.7\nNA\nNA\n159.8\n23.77\nNA\n18.5_to_24.9\n70\n116\n78\n120\n84\n120\n82\n112\n74\nNA\nNA\nNA\n13\n0.448\n29\n0.326\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n6\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\n\n\n59522\n2009_10\nmale\n37\n30-39\n446\nBlack\nNA\n9 - 11th Grade\nLivePartner\n35000-44999\n40000\n1.81\n4\nRent\nNotWorking\n223.0\nNA\nNA\n186.8\n63.91\nNA\n30.0_plus\n104\n92\n64\n106\n72\n94\n66\n90\n62\nNA\n1.37\n3.96\n116\n0.312\nNA\nNA\nYes\n37\nFair\n14\n30\nSeveral\nSeveral\nNA\nNA\nNA\n3\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n6\n104\nNo\nYes\nSmoker\n19\nYes\n21\nYes\n24\nNo\nYes\n15\n10\n1\nNo\nHeterosexual\nNA\n\n\n56447\n2009_10\nmale\n17\n10-19\n207\nHispanic\nNA\nNA\nNA\n75000-99999\n87500\n3.30\n7\nOwn\nWorking\n85.4\nNA\nNA\n180.6\n26.18\nNA\n25.0_to_29.9\n66\n111\n18\n114\n40\n112\n36\n110\n0\nNA\n1.03\n5.15\n54\n0.831\nNA\nNA\nNo\nNA\nGood\n0\n0\nNA\nNA\nNA\nNA\nNA\n7\nNo\nYes\n4\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n\n7.3.3.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = BMI of an American\n\\(\\mu\\) = mean BMI of Americans\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu=27.2\\)\n\\(H_a:\\mu\\ne 27.2\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nA random sample of 50 BMI levels was taken. Check: A random sample was taken from the NHANES data frame using r Studio\nThe population of BMI levels is normally distributed. Check:\n\n(ref:sample-NHANES-50-density-cap) Density Plot of BMI from NHANES sample\n\ngf_density(~BMI, data=sample_NHANES_50, title=\"Body Mass Index\", xlab=\"Body Mass Index\")\ngf_qq(~BMI, data=sample_NHANES_50, title=\"Body Mass Index\", xlab=\"Body Mass Index\")\n\n\n\n\n\n\n\nFigure 7.3: Density Plot of BMI from NHANES sample\n\n\n\n\n\n\n\n\n\n\n\nFigure 7.4: Density Plot of BMI from NHANES sample\n\n\n\n\n\nThe density plot looks somewhat skewed right and the normal quantile plot looks somewhat linear. However, there doesn’t seem to be strong evidence that the sample comes from a population that is normally distributed. However, since the sample is moderate to large, the \\(t\\)-test is robust to this condition not being met. So the results of the test are probably valid.\n\nFind the sample statistic, test statistic, and \\(p\\)-value\n\nOn rStudio, the command would be\n\nt.test(~BMI, data= sample_NHANES_50, mu=27.2) \n\n\n    One Sample t-test\n\ndata:  BMI\nt = -0.80366, df = 47, p-value = 0.4256\nalternative hypothesis: true mean is not equal to 27.2\n95 percent confidence interval:\n 23.19027 28.92056\nsample estimates:\nmean of x \n 26.05542 \n\n\nThe test statistic is the \\(t\\) in the output, the sample statistic is the mean of \\(x\\) in the output, and the \\(p\\)-value is the \\(p\\)-value is the output.\n\nConclusion\n\nSince the \\(p\\)-value is not less than 5%, then fail to reject \\(H_o\\).\n\nInterpretation\n\nThere is not enough evidence to support that Americans have a different BMI from Australians.\nNote: this is the same conclusion that was found when using resampling. So the two method could give similar conclusions.\n\n\n\n7.3.4 Example: Test of the Mean Using One Sample T-test\nIn 2011, the average life expectancy for a woman in Europe was 79.8 years. The data in Table 7.5 are the life expectancies for all people in European countries (\\“WHO life expectancy,” 2013). The Table 7.6 filtered the data frame for just males and just year 2000. The year 2000 was randomly chosen as the year to use. Do the data indicate that men’s life expectancy is different from women’s? Test at the 1% level.\n\nExpectancy&lt;-read.csv( \"https://krkozak.github.io/MAT160/Life_expectancy_Europe.csv\") \nknitr::kable(head(Expectancy))\n\n\n\nTable 7.5: Life Expectancies for European Countries\n\n\n\n\n\n\nyear\nWHO_region\ncountry\nsex\nexpect\n\n\n\n\n1990\nEurope\nAlbania\nMale\n67\n\n\n1990\nEurope\nAlbania\nFemale\n71\n\n\n1990\nEurope\nAlbania\nBoth sexes\n69\n\n\n2000\nEurope\nAlbania\nMale\n68\n\n\n2000\nEurope\nAlbania\nFemale\n73\n\n\n2000\nEurope\nAlbania\nBoth sexes\n71\n\n\n\n\n\n\n\n\n\nExpectancy_male&lt;- \n  Expectancy |&gt;\n  filter(sex==\"Male\", year==\"2000\") \nknitr::kable(head(Expectancy_male))\n\n\n\nTable 7.6: Life Expectancies of males in European Countries in 2000\n\n\n\n\n\n\nyear\nWHO_region\ncountry\nsex\nexpect\n\n\n\n\n2000\nEurope\nAlbania\nMale\n68\n\n\n2000\nEurope\nAndorra\nMale\n76\n\n\n2000\nEurope\nArmenia\nMale\n68\n\n\n2000\nEurope\nAustria\nMale\n75\n\n\n2000\nEurope\nAzerbaijan\nMale\n64\n\n\n2000\nEurope\nBelarus\nMale\n63\n\n\n\n\n\n\n\n\nCode book for data frame Expectancy\nDescription This data extract has been generated by the Global Health Observatory of the World Health Organization. The data was extracted on 2013-09-19 13:10:20.0.\nThis data frame contains the following columns:\nyear: year for life expectancies\nWHO_region: World Health Organizations designation for the location of the country\ncountry: country where the epectancies are from\nsex: sex of the group that expectancies are calculated for\nexpect: average life expectancies of the different groups of the different countries.\nSource http://apps.who.int/gho/athena/data/download.xsl?format=xml&target=GHO/WHOSIS_000001&profile=excel&filter=COUNTRY:*;SEX:*;REGION:EUR\nReferences World Health Organization (WHO).\n\n7.3.4.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = life expectancy for a European man\n\\(\\mu\\) = mean life expectancy for European men\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu=79.8\\)\n\\(H_a:\\mu\\ne79.8\\)\n\\(\\alpha=0.01\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A random sample of 53 life expectancies of European men in 2000 was taken.\nCheck: The data is actually all of the life expectancies for every country that is considered part of Europe by the World Health Organization in the year 2000. Since the year 2000 was picked at random, then the sample is a random sample.\nState: The distribution of life expectancies of European men in 2000 is normally distributed.\nCheck:\n\n\ngf_density(~expect, data=Expectancy_male, title=\"Life Expectancies of Males in Europe in 2000\", xlab=\"Life expectancy\")\n\n\n\n\n\n\n\nFigure 7.5: Density Plot of Life Expectancy of Males in Europe in 2000\n\n\n\n\n\n\ngf_qq(~expect, data=Expectancy_male, title=\"Life Expectancies of Males in Europe in 2000\")\n\n\n\n\n\n\n\nFigure 7.6: Quantile Plot of Life Expectancy of Males in Europe in 2000\n\n\n\n\n\nThis sample does not appear to come from a population that is normally distributed. This sample is moderate to large, so it is good that the t-test is robust.\n\nFind the sample statistic, test statistic, and \\(p\\)-value\n\nOn rStudio, the command is\n\nt.test(~expect, data=Expectancy_male, mu=79.8)\n\n\n    One Sample t-test\n\ndata:  expect\nt = -11.733, df = 52, p-value = 3.145e-16\nalternative hypothesis: true mean is not equal to 79.8\n95 percent confidence interval:\n 69.11930 72.23919\nsample estimates:\nmean of x \n 70.67925 \n\n\nSample statistic is 70.68 years, test statistic is \\(t = -11.733\\), and \\(p-value =3.14X10^{-16}\\).\n\nConclusion\n\nSince the p-value is less than 1%, then reject \\(H_o\\).\n\nInterpretation\n\nThere is enough evidence to support that the mean life expectancy for European men is different than the mean life expectancy for European women of 79.8 years.\nNote: if you want to conduct a hypothesis test with \\(H_a:\\mu&gt;\\mu_o\\), then the rStudio command would be\nt.test(~variable, data=Data_Frame, mu=number \\(H_0\\) equals, alternative=“greater”)\nIf you want to conduct a hypothesis test with \\(H_a:\\mu&lt;\\mu_o\\), then the r Studio command would be\nt.test(~variable, data=Data_Frame, mu=number \\(H_0\\) equals, alternative=“less”)\n\n\n\n7.3.5 Homework for One-Sample Test for the Mean Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nThe Kyoto Protocol was signed in 1997, and required countries to start reducing their carbon emissions. The protocol became enforceable in February 2005. In 2004, the mean CO2 emission was 4.87 metric tons per capita. The Table 7.7 contains a random sample of CO2 emissions in 2010 (CO2 emissions (metric tons per capita), 2018). Is there enough evidence to show that the mean CO2 emission is different in 2010 than in 2004? Test at the 1% level.\n\n\nEmission &lt;- read.csv( \"https://krkozak.github.io/MAT160/CO2_emission.csv\") \nknitr::kable(head(Emission))\n\n\n\nTable 7.7: CO2 Emissions (in metric tons per capita) in 2010\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncountry\ny1960\ny1961\ny1962\ny1963\ny1964\ny1965\ny1966\ny1967\ny1968\ny1969\ny1970\ny1971\ny1972\ny1973\ny1974\ny1975\ny1976\ny1977\ny1978\ny1979\ny1980\ny1981\ny1982\ny1983\ny1984\ny1985\ny1986\ny1987\ny1988\ny1989\ny1990\ny1991\ny1992\ny1993\ny1994\ny1995\ny1996\ny1997\ny1998\ny1999\ny2000\ny2001\ny2002\ny2003\ny2004\ny2005\ny2006\ny2007\ny2008\ny2009\ny2010\ny2011\ny2012\ny2013\ny2014\ny2015\ny2016\ny2017\ny2018\n\n\n\n\nAruba\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n2.8683194\n7.2351980\n10.0261792\n10.6347326\n26.3745032\n26.0461298\n21.4425588\n22.0007862\n21.0362451\n20.7719362\n20.3183534\n20.4268177\n20.5876692\n20.3115668\n26.1948752\n25.9340244\n25.6711618\n26.4204521\n26.517293\n27.2007078\n26.9477260\n27.8950228\n26.2295527\n25.9153221\n24.6705289\n24.5075162\n13.1577223\n8.353561\n8.4100642\nNA\nNA\nNA\nNA\n\n\nAfghanistan\n0.0460567\n0.0535888\n0.0737208\n0.0741607\n0.0861736\n0.1012849\n0.1073989\n0.1234095\n0.1151425\n0.0865099\n0.1496515\n0.1652083\n0.1299956\n0.1353666\n0.1545032\n0.1676124\n0.1535579\n0.1815222\n0.1618942\n0.1670664\n0.1317829\n0.1506147\n0.1631039\n0.2012243\n0.2319613\n0.2939569\n0.2677719\n0.2692296\n0.2468233\n0.2338822\n0.2106434\n0.1833636\n0.0961966\n0.0850871\n0.0758065\n0.0686399\n0.0624346\n0.0566423\n0.0527632\n0.0407225\n0.0372348\n0.0378461\n0.0473773\n0.0504813\n0.038410\n0.0517440\n0.0624275\n0.0838928\n0.1517209\n0.2383985\n0.2899876\n0.4064242\n0.3451488\n0.310341\n0.2939464\nNA\nNA\nNA\nNA\n\n\nAngola\n0.1008353\n0.0822038\n0.2105315\n0.2027373\n0.2135603\n0.2058909\n0.2689414\n0.1721017\n0.2897181\n0.4802340\n0.6082236\n0.5645482\n0.7212460\n0.7512399\n0.7207764\n0.6285689\n0.4513535\n0.4692212\n0.6947369\n0.6830629\n0.6409664\n0.6111351\n0.5193546\n0.5513486\n0.5209829\n0.4719028\n0.4516189\n0.5440851\n0.4635083\n0.4372955\n0.4317436\n0.4155308\n0.4105229\n0.4417211\n0.2881191\n0.7870325\n0.7262335\n0.4963612\n0.4758152\n0.5770829\n0.5819615\n0.5743161\n0.7229589\n0.5002254\n1.001878\n0.9857364\n1.1050190\n1.2031340\n1.1850005\n1.2344251\n1.2440915\n1.2526808\n1.3302186\n1.253776\n1.2903068\nNA\nNA\nNA\nNA\n\n\nAlbania\n1.2581949\n1.3741860\n1.4399560\n1.1816811\n1.1117420\n1.1660990\n1.3330555\n1.3637463\n1.5195513\n1.5589676\n1.7532399\n1.9894979\n2.5159144\n2.3038974\n1.8490067\n1.9106336\n2.0135846\n2.2758764\n2.5306250\n2.8982085\n1.9350583\n2.6930239\n2.6248568\n2.6832399\n2.6942914\n2.6580154\n2.6653562\n2.4140608\n2.3315985\n2.7832431\n1.6781067\n1.3122126\n0.7747249\n0.7237903\n0.6002037\n0.6545371\n0.6366253\n0.4903651\n0.5602714\n0.9601644\n0.9781747\n1.0533042\n1.2295407\n1.4126972\n1.376213\n1.4124982\n1.3025764\n1.3223349\n1.4843111\n1.4956002\n1.5785736\n1.8037147\n1.6929083\n1.749211\n1.9787633\nNA\nNA\nNA\nNA\n\n\nAndorra\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n7.4673357\n7.1824566\n6.9120534\n6.7360548\n6.4942004\n6.6620517\n7.0650715\n7.2397127\n7.6607839\n7.9754544\n8.0192843\n7.7869500\n7.5906151\n7.3157607\n7.358625\n7.2998719\n6.7460521\n6.5193871\n6.4278100\n6.1215799\n6.1225947\n5.8674102\n5.9168840\n5.901775\n5.8329062\nNA\nNA\nNA\nNA\n\n\nArab World\n0.6457359\n0.6874654\n0.7635736\n0.8782377\n1.0030533\n1.1705403\n1.2781736\n1.3374436\n1.5522420\n1.7986689\n1.8103078\n2.0037220\n2.1208746\n2.4095329\n2.2858907\n2.1967827\n2.5843424\n2.6487624\n2.7623331\n2.8636143\n3.0928915\n2.9302350\n2.7231544\n2.8165670\n2.9813539\n3.0618504\n3.2844996\n3.1978064\n3.2950428\n3.2566742\n3.0169588\n3.2366449\n3.4154849\n3.6694456\n3.6743582\n3.4240095\n3.3283037\n3.1455322\n3.3499672\n3.3283411\n3.7038571\n3.6079561\n3.6046128\n3.7964674\n4.068562\n4.1856773\n4.2857192\n4.1171475\n4.4089483\n4.5620151\n4.6368134\n4.5594617\n4.8377796\n4.674925\n4.8869875\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\nCode book for data frame Emission\nDescription Carbon dioxide emissions are those stemming from the burning of fossil fuels and the manufacture of cement. They include carbon dioxide produced during consumption of solid, liquid, and gas fuels and gas flaring.\nThis data frame contains the following columns:\ncountry: country around the world\ny1960-y2018: weighted averages of CO2 emission for the years 1960 through 2018 in metric tons per capita\nSource CO2 emissions (metric tons per capita). (n.d.). Retrieved July 18, 2019, from https://data.worldbank.org/indicator/EN.ATM.CO2E.PC\nReferences Carbon Dioxide Information Analysis Center, Environmental Sciences Division, Oak Ridge National Laboratory, Tennessee, United States.\n\nThe amount of sugar in a Krispy Kream glazed donut is 10 g. Many people feel that cereal is a healthier alternative for children over glazed donuts. The Table 7.8 contains the amount of sugar in a sample of cereal (breakfast cereal, 2019). Is there enough evidence to show that the mean amount of sugar in children’s cereal is different than in a glazed donut? Test at the 5% level.\n\n\nSugar &lt;- read.csv( \"https://krkozak.github.io/MAT160/cereal.csv\") \nknitr::kable(head(Sugar))\n\n\n\nTable 7.8: Nutrition Amounts in Cereal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nmanf\nage\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarb\nsugar\nshelf\npotassium\nvit\nweight\nserving\n\n\n\n\n100%_Bran\nNabisco\nadult\ncold\n70\n4\n1\n130\n10.0\n5.0\n6\n3\n280\n25\n1\n0.33\n\n\n100%_Natural_Bran\nQuaker_Oats\nadult\ncold\n120\n3\n5\n15\n2.0\n8.0\n8\n3\n135\n0\n1\n-1.00\n\n\nAll-Bran\nKelloggs\nadult\ncold\n70\n4\n1\n260\n9.0\n7.0\n5\n3\n320\n25\n1\n0.33\n\n\nAll-Bran_with_Extra_Fiber\nKelloggs\nadult\ncold\n50\n4\n0\n140\n14.0\n8.0\n0\n3\n330\n25\n1\n0.50\n\n\nAlmond_Delight\nRalston_Purina\nadult\ncold\n110\n2\n2\n200\n1.0\n14.0\n8\n3\n-1\n25\n1\n0.75\n\n\nApple_Cinnamon_Cheerios\nGeneral_Mills\nchild\ncold\n110\n2\n2\n180\n1.5\n10.5\n10\n1\n70\n25\n1\n0.75\n\n\n\n\n\n\n\n\nCode book for data frame Sugar\nDescription Nutritional information about cereals.\nThis data frame contains the following columns:\nname: the cereal brand\nmanf: manufacturer\nage: whether the cereal is geared towards children or adults\ntype: whether the cereal is considered a hot or cold cereal\ncalories: the number of calories in the cereal (number)\nprotein: the amount of protein in a serving of the cereal (g)\nfat: the amount of fat a serving of the cereal (g)\nsodium: the amount of sodium in a serving of the cereal (mg)\nfiber: the amount of fiber in a serving of the cereal (g)\ncarb: the amount of complex carbohydrates in a serving of the cereal (g)\nsugars: the amount of sugar in a serving of the cereal (g)\ndisplay shelf: what shelf the cereal is on counting from the floor\npotassium: the amount of potassium in a serving of the cereal (mg)\nvit: the amount of vitamins and minerals in a serving of the cereal (0, 25, or 100)\nweight: weight in ounces of one serving\nserving: cups per serving\nSource (n.d.). Retrieved July 18, 2019, from https://www.idvbook.com/teaching-aid/data-sets/the-breakfast-cereal-data-set/ The Best Kids’ Cereal. (n.d.). Retrieved July 18, 2019, from https://www.ranker.com/list/best-kids-cereal/ranker-food\nReferences Interactive Data Visualization Foundations, Techniques, Applications (Matthew Ward | Georges Grinstein | Daniel Keim)\nA new data frame Table 7.9 will need to be created of just cereal for children. To create that use the following command in rStudio\n\nSugar_children&lt;- \n  Sugar%&gt;% \n  filter(age==\"child\") \nknitr::kable(head(Sugar_children))\n\n\n\nTable 7.9: Nutrition Amounts in Children’s Cereal\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nname\nmanf\nage\ntype\ncalories\nprotein\nfat\nsodium\nfiber\ncarb\nsugar\nshelf\npotassium\nvit\nweight\nserving\n\n\n\n\nApple_Cinnamon_Cheerios\nGeneral_Mills\nchild\ncold\n110\n2\n2\n180\n1.5\n10.5\n10\n1\n70\n25\n1\n0.75\n\n\nApple_Jacks\nKelloggs\nchild\ncold\n110\n2\n0\n125\n1.0\n11.0\n14\n2\n30\n25\n1\n1.00\n\n\nBran_Chex\nRalston_Purina\nchild\ncold\n90\n2\n1\n200\n4.0\n15.0\n6\n1\n125\n25\n1\n0.67\n\n\nCap’n’Crunch\nQuaker_Oats\nchild\ncold\n120\n1\n2\n220\n0.0\n12.0\n12\n2\n35\n25\n1\n0.75\n\n\nCheerios\nGeneral_Mills\nchild\ncold\n110\n6\n2\n290\n2.0\n17.0\n1\n1\n105\n25\n1\n1.25\n\n\nCinnamon_Toast_Crunch\nGeneral_Mills\nchild\ncold\n120\n1\n3\n210\n0.0\n13.0\n9\n2\n45\n25\n1\n0.75\n\n\n\n\n\n\n\n\n\nThe FDA regulates that fish that is consumed is allowed to contain 1.0 mg/kg of mercury. In Florida, bass fish were collected in 53 different lakes to measure the health of the lakes. The data frame of measurements from Florida lakes is in Table 7.10 (NISER 081107 ID Data, 2019). Do the data provide enough evidence to show that the fish in Florida lakes has different amounts of mercury than the allowable amount? Test at the 10% level.\n\n\nMercury&lt;- read.csv( \"https://krkozak.github.io/MAT160/mercury.csv\") \nknitr::kable(head(Mercury))\n\n\n\nTable 7.10: Health of Florida lake Fish\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nlake\nalkalinity\nph\ncalcium\nchlorophyll\nmercury\nno.samples\nmin\nmax\nX3_yr_standmercury\nage_data\n\n\n\n\n1\nAlligator\n5.9\n6.1\n3.0\n0.7\n1.23\n5\n0.85\n1.43\n1.53\n1\n\n\n2\nAnnie\n3.5\n5.1\n1.9\n3.2\n1.33\n7\n0.92\n1.90\n1.33\n0\n\n\n3\nApopka\n116.0\n9.1\n44.1\n128.3\n0.04\n6\n0.04\n0.06\n0.04\n0\n\n\n4\nBlue_Cypress\n39.4\n6.9\n16.4\n3.5\n0.44\n12\n0.13\n0.84\n0.44\n0\n\n\n5\nBrick\n2.5\n4.6\n2.9\n1.8\n1.20\n12\n0.69\n1.50\n1.33\n1\n\n\n6\nBryant\n19.6\n7.3\n4.5\n44.1\n0.27\n14\n0.04\n0.48\n0.25\n1\n\n\n\n\n\n\n\n\nCode book for data frame Mercury\nDescription Largemouth bass were studied in 53 different Florida lakes to examine the factors that influence the level of mercury contamination. Water samples were collected from the surface of the middle of each lake in August 1990 and then again in March 1991. The pH level, the amount of chlorophyll, calcium, and alkalinity were measured in each sample. The average of the August and March values were used in the analysis. Next, a sample of fish was taken from each lake with sample sizes ranging from 4 to 44 fish. The age of each fish and mercury concentration in the muscle tissue was measured. (Note: Since fish absorb mercury over time, older fish will tend to have higher concentrations). Thus, to make a fair comparison of the fish in different lakes, the investigators used a regression estimate of the expected mercury concentration in a three year old fish as the standardized value for each lake. Finally, in 10 of the 53 lakes, the age of the individual fish could not be determined and the average mercury concentration of the sampled fish was used instead of the standardized value. ( Reference: Lange, Royals, & Connor. (1993))\nThis data frame contains the following columns:\nID: ID number\nLake: Name of lake\nalkalinity: Alkalinity (mg/L as Calcium Carbonate)\npH: pH\ncalcium: calcium (mg/l)\nchlorophyll: chlorophyll (mg/l)\nmercury: Average mercury concentration (parts per million) in the muscle tissue of the fish sampled from that lake\nno.samples: How many fish were sampled from the lake\nmin: Minimum mercury concentration among the sampled fish\nmax: Maximum mercury concentration among the sampled fish\nX3_yr_Standard_mercury: Regression estimate of the mercury concentration in a 3 year old fish from the lake (or = Avg Mercury when age data was not available)\nage_data: Indicator of the availability of age data on fish sampled\nSource Lange TL, Royals HE, Connor LL (1993) Influence of water chemistry on mercury concentration in largemouth bass from Florida lakes. Trans Am Fish Soc 122:74-84. Michael K. Saiki, Darell G. Slotton, Thomas W. May, Shaun M. Ayers, and Charles N. Alpers (2000) Summary of Total Mercury Concentrations in Fillets of Selected Sport Fishes Collected during 2000–2003 from Lake Natoma, Sacramento County, California (Raw data is included in appendix), U.S. Geological Survey Data Series 103, 1-21. NISER 081107 ID Data. (n.d.). Retrieved July 18, 2019, from http://wiki.stat.ucla.edu/socr/index.php/NISER_081107_ID_Data\nReferences NISER 081107 ID Data\n\nThe data frame Table 3.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute. The mean pulse rate after running for 1 minute of females who do not drink is 97 beats per minute. Do the data show that the mean pulse rate of females who do drink alcohol is higher than the mean pulse rate of females who do not drink? Test at the 5% level.\n\nCode book for data frame Pulse is below Table 3.7.\nCreate a data frame Table 9.2 that contains only females who drink alcohol. Then test the pulse after for woman who do drink alcohol to the known value for females who do not drink alcohol. To create a new data frame with just females who drink alcohol use the following command, where the new name is Females:\n\nFemales&lt;- Pulse%&gt;% filter(gender==\"female\", alcohol==\"yes\") \nknitr::kable(head(Females))\n\n\n\nTable 7.11: Pulse Rates Before and After Exercise of Females who do drink Alcohol\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\n\n\n\n\n165\n60\n19\nfemale\nyes\nyes\nlow\nran\n88\n120\n98\n\n\n163\n47\n23\nfemale\nyes\nyes\nlow\nran\n71\n125\n98\n\n\n173\n57\n18\nfemale\nno\nyes\nmoderate\nsat\n86\n88\n93\n\n\n179\n58\n19\nfemale\nno\nyes\nmoderate\nran\n82\n150\n93\n\n\n167\n62\n18\nfemale\nno\nyes\nhigh\nran\n96\n176\n93\n\n\n173\n64\n18\nfemale\nno\nyes\nlow\nsat\n90\n88\n93\n\n\n\n\n\n\n\n\n\nThe economic dynamism is an index of productive growth in dollars. Economic data for many countries are in Table 7.12 (SOCR Data 2008 World CountriesRankings, 2019). Countries that are considered high-income have a mean economic dynamism of 60.29.\n\n\nEconomics &lt;- read.csv( \"https://krkozak.github.io/MAT160/Economics_country.csv\") \nknitr::kable(head(Economics))\n\n\n\nTable 7.12: Economic Data for Countries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nId\nincGroup\nkey\nname\npopGroup\nregion\nkey2\nED\nEdu\nHI\nQOL\nPE\nOA\nRelig\n\n\n\n\n0\nLow\nal\nAlbania\nSmall\nSouthern_Europe\npopS\n34.0862\n81.0164\n71.0244\n67.9240\n58.6742\n57\n39\n\n\n1\nMiddle\ndz\nAlgeria\nMedium\nNorth_Africa\npopM\n25.8057\n74.8027\n66.1951\n60.9347\n32.6054\n85\n95\n\n\n2\nMiddle\nar\nArgentina\nMedium\nSouth_America\npopM\n37.4511\n69.8825\n78.2683\n68.1559\n68.6647\n46\n66\n\n\n3\nHigh\nau\nAustralia\nMedium\nAustralia\npopM\n71.4888\n91.4802\n95.1707\n90.5729\n90.9629\n4\n65\n\n\n4\nHigh\nat\nAustria\nSmall\nCentral_Europe\npopS\n53.9431\n90.4578\n90.3415\n87.5630\n91.2073\n18\n20\n\n\n5\nLow\naz\nAzerbaijan\nSmall\ncentral_Asia\npopS\n53.6457\n68.9880\n58.9512\n68.9572\n40.0390\n69\n50\n\n\n\n\n\n\n\n\nCode book for data frame Economics\nDescription These data represent commonly accepted measures for raking Countries on variety of factors which affect the country’s internal and external international perception of the country’s rank relative the to rest of the World.\nThis data frame contains the following columns:\nid: Unique country identifier\nincGroup: Income group: Low: GNI per capita &lt; \\$3,946, Middle: \\$3,946 &lt; GNI per capita &lt; \\$12,195, High: GNI per capita &gt; \\$12,196\nkey: unique 2-letter country code\nname: Country Name\npopGroup: Population Group: Small: Population &lt; 20 million, Medium: 20 million &lt; Population &lt; 50 million, Large: Population &gt; 50 million\nregion: Relative geographic position of the Country\nkey2: Country Group Classification Label: world: All countries, g7: G7, g20: G20, latin: Latin America & Caribbean, eu: European Union, centasia: Europe & Central Asia, pacasia: East Asia & Pacific, asean: Asean, sasia: South Asia, mideast: Middle East & North Africa, africa: Sub-Saharan Africa, bric: Brazil, Russia, India and China (BRIC)\nED: Economic Dynamism: Index of Productive growth in dollars (GDP/capita at PPP, Avg of GDP/capita growth rate over last ten years, GDP/capita growth rate over next ten years, Economic Dynamism: Manufacturing percent of GDP, Services percent of GDP percent (100=best, 0=worst).\nEdu: Education/Literacy Rate (percent of population able to read and write at a specified age)\nHI: Health Index: The average number of years a person lives in full health, taking into account years lived in less than full health\nQOL: Quality of Life: Population percent living on &lt; \\$2/day\nPE: Political Environment: Freedom house rating of political participation (qualitative assessment of voter participation/turn-out for national elections, citizens engagement with politics)\nOA: Overall country ranking taking all measures into account.\nRelig: Religiosity of the Country as a percent (%) of the population.\nSource SOCR Data 2008 World CountriesRankings. (n.d.). Retrieved July 19, 2019, from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_2008_World_CountriesRankings#SOCR_Data_-_Ranking_of_the_top_100_Countries_based_on_Political.2C_Economic.2C_Health.2C_and_Quality-of-Life_Factors\nReferences SOCR Data 2008 World CountriesRankings, Amazon Web-Services World’s Best Countries.\nCreate a data frame that contains only middle income countries. Do the data show that the mean economic dynamism of middle-income countries is less than the mean for high-income countries? Test at the 5% level. To create a new data frame Table 7.13 with just middle income countries use the following command, where the new name is Middle_economics:\n\nMiddle_economics&lt;- \n  Economics |&gt; \n  filter(incGroup==\"Middle\") \nknitr::kable(head(Middle_economics))\n\n\n\nTable 7.13: Economic Data for Middle income Countries\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nId\nincGroup\nkey\nname\npopGroup\nregion\nkey2\nED\nEdu\nHI\nQOL\nPE\nOA\nRelig\n\n\n\n\n1\nMiddle\ndz\nAlgeria\nMedium\nNorth_Africa\npopM\n25.8057\n74.8027\n66.1951\n60.9347\n32.6054\n85\n95\n\n\n2\nMiddle\nar\nArgentina\nMedium\nSouth_America\npopM\n37.4511\n69.8825\n78.2683\n68.1559\n68.6647\n46\n66\n\n\n7\nMiddle\nby\nBelarus\nSmall\ncentral_Asia\npopS\n51.9150\n86.6155\n66.1951\n74.1467\n34.0501\n56\n34\n\n\n10\nMiddle\nbw\nBotswana\nSmall\nAfrica\npopS\n43.6952\n73.4608\n34.8049\n50.0875\n72.6833\n80\n80\n\n\n11\nMiddle\nbr\nBrazil\nLarge\nSouth_America\npopL\n47.8506\n71.3735\n71.0244\n62.4238\n67.4131\n48\n87\n\n\n12\nMiddle\nbg\nBulgaria\nSmall\nSouthern_Europe\npopS\n43.7178\n82.2277\n75.8537\n73.1197\n73.1686\n38\n50\n\n\n\n\n\n\n\n\n\nIn 1999, the average percentage of women who received prenatal care per country is 80.1%. Table 2.18 contains the percentage of woman receiving prenatal care in a sample of countries over several years. (births per woman), 2019). Do the data show that the average percentage of women receiving prenatal care in 2009 (p2009) is different than in 1999? Test at the 5% level.\n\nCode book for Data frame Fert_prenatal is below Table 2.18.\n\nMaintaining your balance may get harder as you grow older. A study was conducted to see how steady the elderly is on their feet. They had the subjects stand on a force platform and have them react to a noise. The force platform then measured how much they swayed forward and backward, and the data is in Table 7.14 (Maintaining Balance while Concentrating, 2019). Do the data show that the elderly sway more than the mean forward sway of younger people, which is 18.125 mm? Test at the 5% level. Follow the filtering methods in other homework problems to create a data frame for only Elderly.\n\n\nSway &lt;- read.csv( \"https://krkozak.github.io/MAT160/sway.csv\") \nknitr::kable(head(Sway))\n\n\n\nTable 7.14: Sway (in mm) of Elderly Subjects\n\n\n\n\n\n\nage\nfbsway\nsidesway\n\n\n\n\nElderly\n19\n14\n\n\nElderly\n30\n41\n\n\nElderly\n20\n18\n\n\nElderly\n19\n11\n\n\nElderly\n29\n16\n\n\nElderly\n25\n24\n\n\n\n\n\n\n\n\nCode book for data frame Sway\nDescription How difficult is it to maintain your balance while concentrating? It is more difficult when you are older? Nine elderly (6 men and 3 women) and eight young men were subjects in an experiment. Each subject stood barefoot on a “force platform” and was asked to maintain a stable upright position and to react as quickly as possible to an unpredictable noise by pressing a hand held button. The noise came randomly and the subject concentrated on reacting as quickly as possible. The platform automatically measured how much each subject swayed in millimeters in both the forward/backward and the side-to-side directions.\nThis data frame contains the following columns:\nAge: Elderly or Young\nFBSway: Sway in forward/backward direction\nSideSwayy: Sway in side to side direction\nSource Maintaining Balance while Concentrating. (n.d.). Retrieved July 19, 2019, from http://www.statsci.org/data/general/balaconc.html\nReferences Teasdale, N., Bard, C., La Rue, J., and Fleury, M. (1993). On the cognitive penetrability of posture control. Experimental Aging Research 19, 1-13. The data was obtained from the DASL Data and Story Line online database.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>One Sample Inference</span>"
    ]
  },
  {
    "objectID": "Estimation.html",
    "href": "Estimation.html",
    "title": "8  Estimation",
    "section": "",
    "text": "8.1 Basics of Confidence Intervals\nA point estimator is just the statistic that you have calculated previously. As an example, when you wanted to estimate the population mean, \\(\\mu\\), the point estimator is the sample mean, \\(\\bar{x}\\). To estimate the population proportion, \\(p\\), you use the sample proportion, \\(\\hat{p}\\). In general, if you want to estimate any population parameter, we will call it \\(\\theta\\), you use the sample statistic, \\(\\hat{\\theta}\\).\nPoint estimators are really easy to find, but they have some drawbacks. First, if you have a large sample size, then the estimate is better. But with a point estimator, you don’t know what the sample size is. Also, you don’t know how accurate the estimate is. Both of these problems are solved with a confidence interval.\nConfidence interval: This is where you have an interval surrounding your parameter, and the interval has a chance of being a true statement. In general, a confidence interval looks like: \\(\\hat{\\theta}\\pm E\\), where \\(\\hat{\\theta}\\) is the point estimator and \\(E\\) is the margin of error term that is added and subtracted from the point estimator. Thus making an interval.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "Estimation.html#basics-of-confidence-intervals",
    "href": "Estimation.html#basics-of-confidence-intervals",
    "title": "8  Estimation",
    "section": "",
    "text": "8.1.1 Interpreting a confidence interval:\nThe statistical interpretation is that the confidence interval has a probability \\(C=(1-\\alpha)\\) (where \\(\\alpha\\) is the complement of the confidence level) of containing the population parameter. As an example, if you have a 95% confidence interval of $0.65 &lt; p &lt; 0.73$, then you would say, “you are 95% confident that the interval 0.65 to 0.73 contains the true population proportion.” This means that if you have 100 intervals, 95 of them will contain the true proportion, and 5 will not. The wrong interpretation is that there is a 95% confidence that the true value of \\(p\\) will fall between 0.65 and 0.73. The reason that this interpretation is wrong is that the true value is fixed out there somewhere. You are trying to capture it with this interval. So this is the chance that your interval captures it, and not that the true value falls in the interval.\nThere is also a real world interpretation that depends on the situation. It is where you are telling people what numbers you found the parameter to lie between. So your real world is where you tell what values your parameter is between. There is no probability attached to this statement. That probability is in the statistical interpretation.\nThe common probabilities used for confidence intervals are 90%, 95%, and 99%. These are known as the confidence level. The confidence level and the alpha level are related. If you are conducting a hypothesis test with \\(H_a:\\mu\\ne \\mu_o\\), then the confidence level is \\(C=1-\\alpha\\). This is because the \\(\\alpha\\) is both tails and the confidence level is area between the two tails. As an example, for a hypothesis test \\(H_a:\\mu\\ne \\mu_o\\) with \\(\\alpha\\) equal to 0.05, the confidence level would be 0.95 or 95%. If you have a hypothesis test with \\(H_a:\\mu&lt;\\mu_o\\), then your \\(\\alpha\\) is only one tail of the curve. Because of symmetry the other tail is also \\(\\alpha\\). You have \\(2\\alpha\\) with both tails. So the confidence level, which is the area between the two tails, is \\(C-2\\alpha\\).\n\n\n8.1.2 Example: Stating the Statistical and Real World Interpretations for a Confidence Interval\n\nSuppose you have a 95% confidence interval for the mean age a woman gets married in 2013 is $26&lt;\\mu&lt;28$. State the statistical and real world interpretations of this statement.\nSuppose a 99% confidence interval for the proportion of Americans who have tried marijuana as of 2013 is $0.35&lt;p&lt;0.41$. State the statistical and real world interpretations of this statement.\n\n\n8.1.2.1 Solution\n\nSuppose you have a 95% confidence interval for the mean age a woman gets married in 2013 is $26&lt;\\mu&lt;28$. State the statistical and real world interpretations of this statement.\n\nStatistical Interpretation: You are 95% confident that the interval contains the mean age in 2013 that a woman gets married.\nReal World Interpretation: The mean age that a woman married in 2013 is between 26 and 28 years of age.\n\nSuppose a 99% confidence interval for the proportion of Americans who have tried marijuana as of 2013 is $0.35&lt;p&lt;0.41$. State the statistical and real world interpretations of this statement.\n\nStatistical Interpretation: You are 99% confident that the interval contains the proportion of Americans who have tried marijuana as of 2013.\nReal World Interpretation: The proportion of Americans who have tried marijuana as of 2013 is between 0.35 and 0.41.\nOne last thing to know about confidence is how the sample size and confidence level affect how wide the interval is. The following discussion demonstrates what happens to the width of the interval as you get more confident.\nThink about shooting an arrow into the target. Suppose you are really good at that and that you have a 90% chance of hitting the bull’s eye. Now the bull’s eye is very small. Since you hit the bull’s eye approximately 90% of the time, then you probably hit inside the next ring out 95% of the time. You have a better chance of doing this, but the circle is bigger. You probably have a 99% chance of hitting the target, but that is a much bigger circle to hit. You can see, as your confidence in hitting the target increases, the circle you hit gets bigger. The same is true for confidence intervals. This is demonstrated in Image \\#8.1.1.\n\n\n\nImage #8.1.1 Confidence Level Effect\n\n\nThe higher level of confidence makes a wider interval. There’s a trade off between width and confidence level. You can be really confident about your answer but your answer will not be very precise. Or you can have a precise answer (small margin of error) but not be very confident about your answer.\nNow look at how the sample size affects the size of the interval. Suppose Image \\#8.1.2 represents confidence intervals calculated on a 95% interval. A larger sample size from a representative sample makes the width of the interval narrower. This makes sense. Large samples are closer to the true population so the point estimate is pretty close to the true value.\n\n\n\nImage #8.1.2 Effect of Sample size\n\n\nNow you know everything you need to know about confidence intervals except for the actual formula. The formula depends on which parameter you are trying to estimate. With different situations you will be given the confidence interval for that parameter.\n\n\n\n8.1.3 Homework for Basics of Confidence Intervals Section\n\nSuppose you compute a confidence interval with a sample size of 25. What will happen to the confidence interval if the sample size increases to 50?\nSuppose you compute a 95% confidence interval. What will happen to the confidence interval if you increase the confidence level to 99%?\nSuppose you compute a 95% confidence interval. What will happen to the confidence interval if you decrease the confidence level to 90%?\nSuppose you compute a confidence interval with a sample size of 100. What will happen to the confidence interval if the sample size decreases to 80?\nA 95% confidence interval is \\(6353km&lt; \\mu&lt;6384km\\), where \\(\\mu\\) is the mean diameter of the Earth. State the statistical interpretation.\nA 95% confidence interval is \\(6353 km &lt; \\mu &lt; 6384 km\\), where \\(\\mu\\) is the mean diameter of the Earth. State the real world interpretation.\nIn 2013, Gallup conducted a poll and found a 95% confidence interval of $0.52 &lt; p &lt; 0.60$, where p is the proportion of Americans who believe it is the government’s responsibility for health care. Give the real world interpretation.\nIn 2013, Gallup conducted a poll and found a 95% confidence interval of $0.52 &lt; p &lt; 0.60$, where p is the proportion of Americans who believe it is the government’s responsibility for health care. Give the statistical interpretation.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "Estimation.html#one-sample-interval-for-the-proportion",
    "href": "Estimation.html#one-sample-interval-for-the-proportion",
    "title": "8  Estimation",
    "section": "8.2 One-Sample Interval for the Proportion",
    "text": "8.2 One-Sample Interval for the Proportion\nSuppose you want to estimate the population proportion, p. As an example you may be curious what proportion of students at your school smoke. Or you could wonder what is the proportion of accidents caused by teenage drivers who do not have a drivers’ education class.\n\n8.2.1 Confidence Interval for One Population Proportion (1-Prop Interval)\n\nState the random variable and the parameter in words.\n\n\\(x\\) = number of successes\n\\(p\\) = proportion of successes\n\nState and check the conditions for the confidence interval\n\n\n\nState: A simple random sample of size \\(n\\) is taken. Check: describe how sample was taken.\nState: The condition for the binomial distribution are satisfied. Check: argue that each condition has been met.\nState: The sampling distribution of \\(\\hat{p}\\) can be approximated by a normal distributed. check: To determine the sampling distribution of \\(\\hat{p}\\) is normally distributed, you need to show that \\(n*\\hat{p}\\ge5\\) and , \\(n*\\hat{q}\\ge5\\) where \\(\\hat{q}=1-\\hat{p}\\). If this requirement is true, then the sampling distribution of \\(\\hat{p}\\) is well approximated by a normal curve. (In reality this is not really true, since the correct condition deals with \\(p\\). However, in a confidence interval you do not know \\(p\\), so you must use \\(\\hat{p}\\).)\n\n\n\nFind the sample statistic and the confidence interval\n\nThis will be conducted using rStudio. The command is\nprop.test(r, n, conf.level=C) #type C as a decimal\n\nStatistical Interpretation: In general this looks like, “you are C% confident that \\(\\hat{p}\\pm E\\) contains the true proportion.”\nReal World Interpretation: This is where you state what interval contains the true proportion.\n\n\n\n8.2.2 Example: Confidence Interval for the Population Proportion\nA concern was raised in Australia that the percentage of deaths of Aboriginal prisoners was higher than the percent of deaths of non-Aboriginal prisoners, which is 0.27%. A sample of six years (1990-1995) of data was collected, and it was found that out of 14,495 Aboriginal prisoners, 51 died (\\“Indigenous deaths in,\\” 1996). Find a 95% confidence interval for the proportion of Aboriginal prisoners who died.\n\n8.2.2.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = number of Aboriginal prisoners who die\n\\(p\\) = proportion of Aboriginal prisoners who die\n\nState and check the conditions for the confidence interval\n\n\n\nState: A simple random sample of 14,495 Aboriginal prisoners was taken. Check: The sample was not a random sample, since it was data from six years. It is the numbers for all prisoners in these six years, but the six years were not picked at random. Unless there was something special about the six years that were chosen, the sample is probably a representative sample. This condition is probably met.\nState: The properties of the binomial experiment have been met. Check: There are 14,495 prisoners in this case. The prisoners are all Aboriginals, so you are not mixing Aboriginal with non-Aboriginal prisoners. There are only two outcomes, either the prisoner dies or doesn’t. The chance that one prisoner dies over another may not be constant, but if you consider all prisoners the same, then it may be close to the same probability. Thus the properties of the binomial experiment are satisfied\nState: The sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distribution. Check: \\(\\hat{p}*n=\\frac{51}{14495}*14495=51\\ge5\\) and \\(\\hat{q}*n=\\frac{14495-51}{14495}*14495=14444\\ge5\\). The sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distribution.\n\n\n\nFind the sample statistic and the confidence interval\n\nThe command in r Studio for a confidence interval for a proportion is\n\nprop.test(51,14495, conf.level = 0.95)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  51 out of 14495\nX-squared = 14290, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n95 percent confidence interval:\n 0.002647440 0.004661881\nsample estimates:\n          p \n0.003518455 \n\n\nthe 95% confidence level is \\(0.002647440&lt;p&lt;0.004661881\\).\n\nStatistical Interpretation: You are 95% confident that the interval \\(0.0026&lt;p&lt;0.0047\\) contains the proportion of Aboriginal prisoners who have died in prison.\nReal World Interpretation: The proportion of Aboriginal prisoners who died in prison is between 0.26% and 0.47%.\n\n\n\n\n8.2.3 Example: Confidence Interval for the Population Proportion\nA researcher who is studying the effects of income levels on breastfeeding of infants hypothesizes that countries with a low income level have a different rate of infant breastfeeding than higher income countries. It is known that in Germany, considered a high-income country by the World Bank, 22% of all babies are breastfeed. In Tajikistan, considered a low-income country by the World Bank, researchers found that in a random sample of 500 new mothers that 125 were breastfeeding their infant. Find a 90% confidence interval of the proportion of mothers in low-income countries who breastfeed their infants?\n\n8.2.3.1 Solution\n\nState you random variable and the parameter in words.\n\n\\(x\\) = number of woman who breastfeed in a low-income country\n\\(p\\) = proportion of woman who breastfeed in a low-income country\n\nState and check the conditions for the confidence interval\n\n\n\nState: A simple random sample of 500 breastfeeding habits of woman in a low-income country was taken. Check: This was stated in the problem.\nState: The properties of a Binomial Experiment have been met. Check: There were 500 women in the study. The women are considered identical, though they probably have some differences. There are only two outcomes, either the woman breastfeeds or she doesn’t. The probability of a woman breastfeeding is probably not the same for each woman, but it is probably not very different for each woman. The conditions for the binomial distribution are satisfied\nState: The sampling distribution of \\(\\hat{p}\\) can be approximated with a normal distributed. Check:\\(n*\\hat{p}= 500*\\frac{125}{500}=125\\ge5\\) and \\(n*\\hat{q}=500*\\frac{500-125}{500}=375\\ge5\\), so the sampling distribution of \\(\\hat{p}\\) is well approximated by a normal distribution.\n\n\n\nFind the sample statistic and confidence interval\n\nOn rstudio, use the following command\n\nprop.test(125, 500, conf.level = .90)\n\n\n    1-sample proportions test with continuity correction\n\ndata:  125 out of 500\nX-squared = 124, df = 1, p-value &lt; 2.2e-16\nalternative hypothesis: true p is not equal to 0.5\n90 percent confidence interval:\n 0.2185980 0.2841772\nsample estimates:\n   p \n0.25 \n\n\n90% confidence interval for \\(p\\) is \\(0.2185980&lt;p&lt;0.2841772\\).\n\nStatistical Interpretation: You are 90% confident that \\(0.2185980&lt;p&lt;0.2841772\\) contains the proportion of women in low-income countries who breastfeed their infants.\nReal World Interpretation: The proportion of women in low-income countries who breastfeed their infants is between 0.219 and 0.284.\n\n\n\n\n8.2.4 Homework for One-Sample Interval for the Proportion Section\nIn each problem show all steps of the confidence interval. If some of the conditions are not met, note that the results of the interval may not be correct and then continue the process of the confidence interval.\n\nThe Arizona Republic/Morrison/Cronkite News poll published on Monday, October 20, 2016, found 390 of the registered voters surveyed favor Proposition 205, which would legalize marijuana for adults. The statewide telephone poll surveyed 779 registered voters between Oct. 10 and Oct. 15. (Sanchez, 2016) Find a 99% confidence interval for the proportion of Arizona’s who supported legalizing marijuana for adults.\nIn November of 1997, Australians were asked if they thought unemployment would increase. At that time 284 out of 631 said that they thought unemployment would increase (\\“Morgan gallup poll,\\” 2013). Estimate the proportion of Australians in November 1997 who believed unemployment would increase using a 95% confidence interval?\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, Arkansas had 1,601 complaints of identity theft out of 3,482 consumer complaints (\\“Consumer fraud and,\\” 2008). Calculate a 90% confidence interval for the proportion of identity theft in Arkansas.\nAccording to the February 2008 Federal Trade Commission report on consumer fraud and identity theft, Alaska had 321 complaints of identity theft out of 1,432 consumer complaints (\\“Consumer fraud and,\\” 2008). Calculate a 90% confidence interval for the proportion of identity theft in Alaska.\nIn 2013, the Gallup poll asked 1,039 American adults if they believe there was a conspiracy in the assassination of President Kennedy, and found that 634 believe there was a conspiracy (\\“Gallup news service,\\” 2013). Estimate the proportion of American’s who believe in this conspiracy using a 98% confidence interval.\nIn 2008, there were 507 children in Arizona out of 32,601 who were diagnosed with Autism Spectrum Disorder (ASD) (\\“Autism and developmental,\\” 2008). Find the proportion of ASD in Arizona with a confidence level of 99%.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "Estimation.html#one-sample-interval-for-the-mean",
    "href": "Estimation.html#one-sample-interval-for-the-mean",
    "title": "8  Estimation",
    "section": "8.3 One-Sample Interval for the Mean",
    "text": "8.3 One-Sample Interval for the Mean\nSuppose you want to estimate the mean height of Americans, or you want to estimate the mean salary of college graduates. A confidence interval for the mean would be the way to estimate these means.\n\n8.3.1 Confidence Interval for One Population Mean (t-Interval)\n\nState the random variable and the parameter in words.\n\n\\(x\\) = random variable\n\\(\\mu\\) = mean of random variable\n\nState and check the conditions for the confidence interval\n\n\n\nState: A random sample of size \\(n\\) is taken. Check: describe how the sample was collected.\nState: The population of the random variable is normally distributed. Check: look at density plot and normal quantile plot. Note: though the t-test is fairly robust to the condition if the sample size is large. This means that if this condition isn’t met, but your sample size is quite large, then the results of the t-test are valid.\n\n\n\nFind the sample statistic and confidence interval\n\nUse rStudio to find the confidence interval. The command is\nt.test(~variable, data= Data_Frame, conf.level=C) #type C as a decimal\n\nStatistical Interpretation: In general this looks like, “You are C% confident that the interval contains the true mean.”\nReal World Interpretation: This is where you state what interval contains the true mean.\n\n\n\n8.3.2 Example: Confidence Interval for the Population Mean\nA random sample of 50 body mass index (BMI) were taken from the NHANES Data frame Table 8.1. Estimate the mean BMI of Americans at the 95% level.\n\nsample_NHANES_50&lt;- sample_n(NHANES, size=50) \nknitr::kable(head(sample_NHANES_50))\n\n\n\nTable 8.1: BMI of Americans\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nID\nSurveyYr\nGender\nAge\nAgeDecade\nAgeMonths\nRace1\nRace3\nEducation\nMaritalStatus\nHHIncome\nHHIncomeMid\nPoverty\nHomeRooms\nHomeOwn\nWork\nWeight\nLength\nHeadCirc\nHeight\nBMI\nBMICatUnder20yrs\nBMI_WHO\nPulse\nBPSysAve\nBPDiaAve\nBPSys1\nBPDia1\nBPSys2\nBPDia2\nBPSys3\nBPDia3\nTestosterone\nDirectChol\nTotChol\nUrineVol1\nUrineFlow1\nUrineVol2\nUrineFlow2\nDiabetes\nDiabetesAge\nHealthGen\nDaysPhysHlthBad\nDaysMentHlthBad\nLittleInterest\nDepressed\nnPregnancies\nnBabies\nAge1stBaby\nSleepHrsNight\nSleepTrouble\nPhysActive\nPhysActiveDays\nTVHrsDay\nCompHrsDay\nTVHrsDayChild\nCompHrsDayChild\nAlcohol12PlusYr\nAlcoholDay\nAlcoholYear\nSmokeNow\nSmoke100\nSmoke100n\nSmokeAge\nMarijuana\nAgeFirstMarij\nRegularMarij\nAgeRegMarij\nHardDrugs\nSexEver\nSexAge\nSexNumPartnLife\nSexNumPartYear\nSameSex\nSexOrientation\nPregnantNow\n\n\n\n\n71046\n2011_12\nmale\n52\n50-59\nNA\nWhite\nWhite\nSome College\nMarried\n75000-99999\n87500\n4.03\n9\nOwn\nWorking\n113.7\nNA\nNA\n183.5\n33.80\nNA\n30.0_plus\n74\n116\n78\n114\n76\n120\n78\n112\n78\n179.25\n1.16\n5.20\n137\n1.505\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\nNA\nNA\nNA\n7\nYes\nYes\n5\n0_to_1_hr\n0_to_1_hr\nNA\nNA\nYes\n2\n260\nNo\nYes\nSmoker\n22\nYes\n14\nYes\n18\nYes\nYes\n16\n10\n1\nNo\nHeterosexual\nNA\n\n\n58977\n2009_10\nfemale\n28\n20-29\n344\nMexican\nNA\nHigh School\nMarried\n25000-34999\n30000\n1.13\n4\nRent\nNotWorking\n60.1\nNA\nNA\n161.3\n23.10\nNA\n18.5_to_24.9\n80\n112\n80\n106\n74\nNA\nNA\n112\n80\nNA\nNA\nNA\n156\n1.147\nNA\nNA\nNo\nNA\nGood\n0\n0\nNone\nNone\n2\n2\n23\n6\nNo\nNo\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNo\nNon-Smoker\nNA\nNo\nNA\nNo\nNA\nNo\nYes\n22\n2\n2\nNo\nHeterosexual\nNo\n\n\n60054\n2009_10\nmale\n6\n0-9\n76\nWhite\nNA\nNA\nNA\nmore 99999\n100000\n5.00\n9\nOwn\nNA\n26.1\nNA\nNA\n126.3\n16.36\nNA\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n0\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n63557\n2011_12\nmale\n3\n0-9\nNA\nOther\nOther\nNA\nNA\nmore 99999\n100000\n5.00\n6\nOwn\nNA\n13.4\n95.6\nNA\n95.5\n14.70\nNormWeight\n12.0_18.5\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNo\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n1\n2_hr\n0_hrs\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n53715\n2009_10\nfemale\n74\n70+\n895\nWhite\nNA\nSome College\nWidowed\n25000-34999\n30000\n2.77\n6\nOwn\nNotWorking\n76.7\nNA\nNA\n160.5\n29.77\nNA\n25.0_to_29.9\n62\n101\n55\n102\n60\n100\n56\n102\n54\nNA\nNA\nNA\n76\n0.466\nNA\nNA\nNo\nNA\nFair\n15\n30\nSeveral\nSeveral\n8\n6\n21\n5\nYes\nNo\nNA\nNA\nNA\nNA\nNA\nYes\n2\n12\nNA\nNo\nNon-Smoker\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n51829\n2009_10\nfemale\n34\n30-39\n418\nWhite\nNA\nCollege Grad\nMarried\nmore 99999\n100000\n5.00\n7\nOwn\nWorking\n76.6\nNA\nNA\n173.8\n25.36\nNA\n25.0_to_29.9\n76\n121\n74\n116\n74\n120\n74\n122\n74\nNA\n2.02\n3.72\n300\n3.093\nNA\nNA\nNo\nNA\nVgood\n0\n0\nNone\nNone\n3\n3\n21\n6\nNo\nYes\n3\nNA\nNA\nNA\nNA\nYes\n2\n156\nNA\nNo\nNon-Smoker\nNA\nYes\n25\nNo\nNA\nNo\nYes\n16\n6\n1\nNo\nHeterosexual\nNo\n\n\n\n\n\n\n\n\n\n8.3.2.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = BMI of an American\n\\(\\mu\\) = mean BMI of Americans\n\nState and check the conditions for the confidence interval\n\n\n\nA random sample of 50 BMI levels was taken. Check: A random sample was taken from the NHANES data frame using r Studio\nThe population of BMI levels is normally distributed. Check:\n\n\ngf_density(~BMI, data=sample_NHANES_50, title=\"BMI of an American\", xlab=\"Body Mass Index\")\n\n\n\n\n\n\n\nFigure 8.1: Density Plot of BMI from NHANES sample\n\n\n\n\n\n\ngf_qq(~BMI, data=sample_NHANES_50, title=\"BMI of an American\") \n\n\n\n\n\n\n\nFigure 8.2: Normal quantile Plot of BMI from NHANES sample\n\n\n\n\n\nThe density plot looks somewhat skewed right and the normal quantile plot looks somewhat linear. There doesn’t seem to be strong evidence that the sample comes from a population that is normally distributed. However, since the sample is moderate to large, the t-test is robust to this condition not being met. So the results of the test are probably valid.\n\nFind the sample statistic and confidence interval\n\nOn r Studio, the command would be\n\nt.test(~BMI, data= sample_NHANES_50, conf.level=0.95)\n\n\n    One Sample t-test\n\ndata:  BMI\nt = 22.874, df = 49, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 25.31318 30.18922\nsample estimates:\nmean of x \n  27.7512 \n\n\nThe sample statistic is the mean of \\(x\\) in the output, and confidence interval is under the words 95 percent confidence interval.\n\nStatistical Interpretation: You are 95% confident that \\(24.87190&lt;\\mu&lt;28.71422\\) contains the mean BMI of Americans.\nReal World Interpretation: The mean BMI of Americans is between 24.87 and 28.71 \\(kg/m^2\\).\n\nNotice that in example the chapter 7, you were asked if the mean BMI of Americans was different from Australians’ mean BMI of 27.2 \\(kg/m^2\\). The interval that Example: Confidence Interval for the Population Mean calculated does contain the value of 27.2. So you can’t say that Americans’ mean BMI and Australians’ mean BMI are different.This means that you can just use confidence intervals and not conduct hypothesis tests at all if you prefer.\nNote: When creating this book, the random samples may change. So the answers may be different from what is said in the interpretations. This shows sampling variability, so it was not adjusted to show that this could happen.\n\n\n\n8.3.3 Example: Confidence Interval for the Population Mean\nThe data in Table 7.5 are the life expectancies for all people in European countries (\\“WHO life expectancy,\\” 2013). The data in Table 7.6 filtered the data frame for just males and just year 2000. The year 2000 was randomly chosen as the year to use. Estimate the mean life expectancy for a man in Europe at the 99% level.\nCode book for data frame Expectancy is below Table 7.5.\n\n8.3.3.1 Solution\n\nState the random variable and the parameter in words.\n\n\\(x\\) = life expectancy for a European man\n\\(\\mu\\) = mean life expectancy for European men\n\nState and check the conditions for the confidence interval\n\n\n\nState: A random sample of 53 life expectancies of European men in 2000 was taken.\nCheck: The data is actually all of the life expectancies for every country that is considered part of Europe by the World Health Organization in the year 2000. Since the year 2000 was picked at random, then the sample is a random sample.\nState: The distribution of life expectancies of European men in 2000 is normally distributed.\nCheck:\n\n\ngf_density(~expect, data=Expectancy_male, title=\"Life Expectancy of a male\", xlab=\"Life Expectancy of a Male\")\n\n\n\n\n\n\n\nFigure 8.3: Density Plot of Life Expectancy of Males in Europe in 2000\n\n\n\n\n\n\ngf_qq(~expect, data=Expectancy_male, title=\"Male Life Expectancy\")\n\n\n\n\n\n\n\nFigure 8.4: Quantile Plot of Life Expectancy of Males in Europe in 2000\n\n\n\n\n\nThis sample does not appear to come from a population that is normally distributed. This sample is moderate to large, so it is good that the t-test is robust.\n\nFind the sample statistic and confidence interval\n\nOn rStudio, the command would be\n\nt.test(~expect, data=Expectancy_male, conf.level=0.99) \n\n\n    One Sample t-test\n\ndata:  expect\nt = 90.919, df = 52, p-value &lt; 2.2e-16\nalternative hypothesis: true mean is not equal to 0\n99 percent confidence interval:\n 68.60071 72.75778\nsample estimates:\nmean of x \n 70.67925 \n\n\nSample statistic is 70.68 years, and the confidence interval is \\(68.60071&lt;\\mu&lt;72.75778\\).\n\nStatistical Interpretation: You are 99% confident that \\(68.60071&lt;\\mu&lt;72.75778\\) contains the mean life expectancy of European men.\nReal World Interpretation: The mean life expectancy of European men is between 68.60 and 72.76 years.\n\n\n\n\n8.3.4 Homework for One-Sample Interval for the Mean Section\nIn each problem show all steps of the confidence interval. If some of the conditions are not met, note that the results of the interval may not be correct and then continue the process of the confidence interval.\n\nThe Kyoto Protocol was signed in 1997, and required countries to start reducing their carbon emissions. The protocol became enforceable in February 2005. Table 7.7 contains a random sample of CO2 emissions in 2010 (CO2 emissions (metric tons per capita), 2018). Find a 99% confidence interval for the mean CO-2 emissions in 2010.\n\nCode book for data frame Emission is below Table 7.7.\n\nThe amount of sugar in a Krispy Kream glazed donut is 10 g. Many people feel that cereal is a healthier alternative for children over glazed donuts. Table 7.8 contains the amount of sugar in a sample of cereal that is geared towards children (breakfast cereal, 2019). Estimate the mean amount of sugar in children’s cereal at the 95% confidence level.\n\nCode book for data frame Sugar is below Table 7.8.\nA new data frame will need to be created of just cereal for children. It is Table 7.9.\n\nThe FDA regulates that fish that is consumed is allowed to contain 1.0 mg/kg of mercury. In Florida, bass fish were collected in 53 different lakes to measure the health of the lakes. The data frame of measurements from Florida lakes is in Table 7.10 (NISER 081107 ID Data, 2019). Calculate with 90% confidence the mean amount of mercury in fish in Florida lakes. Is there too much mercury in the fish in Florida?\n\nCode book for data frame Mercury is below Table 7.10.\n\nThe data frame Table 3.7 contains various variables about a person including their pulse rates before the subject exercised and after the subject ran in place for one minute. Estimate the mean pulse rate before exercise of females who do drink alcohol with a 95% level of confidence?\n\nCode book for data frame Pulse below Table 3.7.\nA new data frame with just females who drink alcohol is Table 9.2 from chapter 7.\n\nThe economic dynamism is an index of productive growth in dollars. Economic data for many countries are in Table 7.12 (SOCR Data 2008 World CountriesRankings, 2019).\n\nCode book for data frame Economics is below Table 7.12.\nA data frame that contains only middle income countries was created in chapter 7 and is Table 7.13. Find a 95% confidence interval for the mean economic dynamism for middle income countries.\n\nTable 2.18 contains the percentage of woman receiving prenatal care in a sample of countries over several years. (births per woman), 2019). Estimate the average percentage of women receiving prenatal care in 2009 (p2009) with a 95% confidence interval?\n\nCode book for Data frame Fert_prenatal is below Table 2.18.\n\nMaintaining your balance may get harder as you grow older. A study was conducted to see how steady the elderly is on their feet. They had the subjects stand on a force platform and have them react to a noise. The force platform then measured how much they swayed forward and backward, and the data is in Table 7.14 (Maintaining Balance while Concentrating, 2019). Find the mean forward/backward sway of elderly person? Use a 95% confidence level. Follow the filtering methods in other homework problems to create a data frame for only Elderly.\n\nCode book for data frame Sway is below Table 7.14.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Estimation</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html",
    "href": "Two Sample Inference.html",
    "title": "9  Two Sample Inference",
    "section": "",
    "text": "9.1 Two Proportions\nThere are times you want to test a claim about two population proportions or construct a confidence interval estimate of the difference between two population proportions. As with all other hypothesis tests and confidence intervals, the process is the same though the formulas and conditions are different.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html#two-proportions",
    "href": "Two Sample Inference.html#two-proportions",
    "title": "9  Two Sample Inference",
    "section": "",
    "text": "9.1.1 Hypothesis Test for Two Population Proportion (2-Prop Test)\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = number of successes from group 1\n\\(x_2\\) = number of successes from group 2\n\\(p_1\\) = proportion of successes in group 1\n\\(p_2\\) = proportion of successes in group 2\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:p_1=p_2\\)\n\\(H_a: p_1\\ne p_2\\). the \\(\\ne\\) can be replaced with \\(&lt;\\) or \\(&gt;\\) depending on the question.\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A simple random sample of size \\(n_1\\) is taken from population 1, and a simple random sample of size \\(n_2\\) is taken from population 2. Check: describe how each sample was collected.\nState: The samples are independent. Check: describe why the two samples are independent.\nState: The properties for the binomial distribution are satisfied for both populations. Check: describe how each population meets all the properties.\nState: The sampling distribution of \\(\\hat{p_1}\\) can be approximated as a normal distribution. Check: To determine the sampling distribution of \\(\\hat{p_1}\\), you need to show that \\(p_1*n_1\\ge5\\) and \\(q_1*n_1\\ge5\\) where \\(q_1=1-p_1\\). If this requirement is true, then the sampling distribution of \\(\\hat{p_1}\\) is well approximated by a normal curve. State: The sampling distribution of \\(\\hat{p_2}\\) can be approximated as a normal distribution. Check: To determine the sampling distribution of \\(\\hat{p_2}\\), you need to show that \\(p_2*n_2\\ge 5\\) and \\(q_2*n_2\\ge 5\\) where \\(q_2=1-p_2\\). If this requirement is true, then the sampling distribution of \\(\\hat{p_2}\\) is well approximated by a normal curve. However, if you do not know \\(p_1\\) and \\(p_2\\), you will need to use \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) instead. This is not perfect, but it is the best you can do.\n\n\n\nFind the sample statistics, test statistic, and p-value\n\nOn rStudio, use the command\nprop.test(c(x1,x2), c(n1, n2)\n\nConclusion\n\nThis is where you write reject or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\).\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n9.1.2 Confidence Interval for the Difference Between Two Population Proportion (2-Prop Interval)\nThe confidence interval for the difference in proportions has the same random variables and proportions and the same conditions as the hypothesis test for two proportions. If you have already completed the hypothesis test, then you do not need to state them again. If you haven’t completed the hypothesis test, then state the random variables and proportions and state and check the conditions before completing the confidence interval step.\n\nFind the sample statistics and the confidence interval\n\nThe confidence interval estimate of the difference is found using the following command in r Studio:\nprop.test(c(x1,x2), c(n1, n2), conf.level=C) Type C as a decimal\n\nStatistical Interpretation: In general this looks like, “You are C% confident that the confidence interval contains the true difference in proportions.”\nReal World Interpretation: This is where you state how much more (or less) the first proportion is from the second proportion.\n\n\n\n9.1.3 Example: Hypothesis Test for Two Population Proportions\nDo husbands cheat on their wives in a different proportion from the proportion of wives cheat on their husbands (“Statistics brain,” 2013)? Suppose you take a group of 1000 randomly selected husbands and find that 231 had cheated on their wives. Suppose in a group of 1200 randomly selected wives, 176 cheated on their husbands. Do the data show that the proportion of husbands who cheat on their wives is different from the proportion of wives who cheat on their husbands. Test at the 5% level.\n\n9.1.3.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = number of husbands who cheat on his wife\n\\(x_2\\) = number of wives who cheat on her husband\n\\(p_1\\) = proportion of husbands who cheat on his wife\n\\(p_2\\) = proportion of wives who cheat on her husband\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: p_1=p_2\\)\n\\(H_a: p_1\\ne p_2\\)\nlevel of significance is \\(\\alpha=0.05\\)\n\nState and check the conditions for a hypothesis test\n\n\n\nState: A simple random sample of 1000 responses about cheating from husbands is taken. Check: This was stated in the problem. State: A simple random sample of 1200 responses about cheating from wives is taken. Check: This was stated in the problem.\nState: The samples are independent. Check: The samples are independent. This is true since the samples involved different genders.\nState: The properties of the binomial distribution are satisfied in both populations. Check: This is true since there are only two responses, there are a fixed number of trials, the probability of a success is the same, and the trials are independent.\nState: The sampling distributions of \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) can be approximated with a normal distribution. Check: \\(n_1*p_1\\), \\(n_2*p_2\\), \\(n_1*q_1\\), and \\(n_2*q_2\\) are all greater than or equal to 5. So both sampling distributions of \\(\\hat{p_1}\\) and \\(\\hat{p_2}\\) can be approximated with a normal distribution.\n\n\n\nFind the sample statistics, test statistic, and p-value\n\nOn r use the command:\n\nprop.test(c(231,176), c(1000, 1200))\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c out of c231 out of 1000176 out of 1200\nX-squared = 25.173, df = 1, p-value = 5.241e-07\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.05050705 0.11815962\nsample estimates:\n   prop 1    prop 2 \n0.2310000 0.1466667 \n\n\n\nConclusion\n\nReject \\(H_o\\), since the p-value is less than 5%.\n\nInterpretation\n\nThis is enough evidence to support that the proportion of husbands having affairs is different from the proportion of wives having affairs.\n\n\n\n9.1.4 Example: Confidence Interval for Two Population Proportions\nWhat is the difference in proportion that husbands cheat on their wives than wives cheat on the husbands (“Statistics brain,” 2013)? Suppose you take a group of 1000 randomly selected husbands and find that 231 had cheated on their wives. Suppose in a group of 1200 randomly selected wives, 176 cheated on their husbands. Estimate the difference in the proportion of husbands and wives who cheat on their spouses using a 95% confidence level.\n\n9.1.4.1 Solution\n\nState the random variables and the parameters in words.\n\nThese were stated in Example: Hypothesis Test for Two Population Proportions.\n\nState and check the conditions for the confidence interval\n\nThe conditions were stated and checked in Example: Hypothesis Test for Two Population Proportions.\n\nFind the sample statistics and the confidence interval\n\nOn r use the command:\n\nprop.test(c(231,176), c(1000, 1200), conf.level = .95)\n\n\n    2-sample test for equality of proportions with continuity correction\n\ndata:  c out of c231 out of 1000176 out of 1200\nX-squared = 25.173, df = 1, p-value = 5.241e-07\nalternative hypothesis: two.sided\n95 percent confidence interval:\n 0.05050705 0.11815962\nsample estimates:\n   prop 1    prop 2 \n0.2310000 0.1466667 \n\n\n\nStatistical Interpretation: You are 95% confident that \\(0.05050705&lt;p_1-p_2&lt;0.11815962\\) contains the true difference in proportions.\nReal World Interpretation: The proportion of husbands who cheat on their wives is anywhere from 5.05% to 11.82% higher than the proportion of wives who cheat on their husband.\n\n\n\n\n9.1.5 Homework for Two Proportions Section\nIn each problem show all steps of the hypothesis test or confidence interval. If some of the conditions are not met, note that the results of the test or interval may not be correct and then continue the process of the hypothesis test or confidence interval.\n\nMany high school students take the AP tests in different subject areas. In 2007, of the 144,796 students who took the biology exam 84,199 of them were female. In that same year, of the 211,693 students who took the calculus AB exam 102,598 of them were female (“AP exam scores,” 2013). Is there enough evidence to show that the proportion of female students taking the biology exam is different than the proportion of female students taking the calculus AB exam? Test at the 5% level.\nMany high school students take the AP tests in different subject areas. In 2007, of the 144,796 students who took the biology exam 84,199 of them were female. In that same year, of the 211,693 students who took the calculus AB exam 102,598 of them were female (“AP exam scores,” 2013). Estimate the difference in the proportion of female students taking the biology exam and female students taking the calculus AB exam using a 90% confidence level.\nMany high school students take the AP tests in different subject areas. In 2007, of the 211,693 students who took the calculus AB exam 102,598 of them were female and 109,095 of them were male (“AP exam scores,” 2013). Is there enough evidence to show that the proportion of female students taking the calculus AB exam is different from the proportion of male students taking the calculus AB exam? Test at the 5% level.\nMany high school students take the AP tests in different subject areas. In 2007, of the 211,693 students who took the calculus AB exam 102,598 of them were female and 109,095 of them were male (“AP exam scores,” 2013). Estimate using a 90% level the difference in proportion of female students taking the calculus AB exam versus male students taking the calculus AB exam.\nAre there more children diagnosed with Autism Spectrum Disorder (ASD) in states that have larger urban areas over states that are mostly rural? In the state of Pennsylvania, a fairly urban state, there are 245 eight year old diagnosed with ASD out of 18,440 eight year old evaluated. In the state of Utah, a fairly rural state, there are 45 eight year old diagnosed with ASD out of 2,123 eight year old evaluated (“Autism and developmental,” 2008). Is there enough evidence to show that the proportion of children diagnosed with ASD in Pennsylvania is different than the proportion in Utah? Test at the 1% level.\nAre there more children diagnosed with Autism Spectrum Disorder (ASD) in states that have larger urban areas over states that are mostly rural? In the state of Pennsylvania, a fairly urban state, there are 245 eight year old diagnosed with ASD out of 18,440 eight year old evaluated. In the state of Utah, a fairly rural state, there are 45 eight year old diagnosed with ASD out of 2,123 eight year old evaluated (“Autism and developmental,” 2008). Estimate the difference in proportion of children diagnosed with ASD between Pennsylvania and Utah. Use a 98% confidence level.\nA child dying from an accidental poisoning is a terrible incident. Is it more likely that a male child will get into poison than a female child? To find this out, data was collected that showed that out of 1830 children between the ages one and four who pass away from poisoning, 1031 were males and 799 were females (Flanagan, Rooney & Griffiths, 2005). Do the data show that there is different proportion of male children dying of poisoning than female children? Test at the 1% level.\nA child dying from an accidental poisoning is a terrible incident. Is it more likely that a male child will get into poison than a female child? To find this out, data was collected that showed that out of 1830 children between the ages one and four who pass away from poisoning, 1031 were males and 799 were females (Flanagan, Rooney & Griffiths, 2005). Compute a 99% confidence interval for the difference in proportions of poisoning deaths of male and female children ages one to four.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html#paired-samples-for-two-means",
    "href": "Two Sample Inference.html#paired-samples-for-two-means",
    "title": "9  Two Sample Inference",
    "section": "9.2 Paired Samples for Two Means",
    "text": "9.2 Paired Samples for Two Means\nAre two populations the same? Is the average height of men taller than the average height of women? Is the mean weight less after a diet than before?\nYou can compare populations by comparing their means. You take a sample from each population and compare the statistics.\nAnytime you compare two populations you need to know if the samples are independent or dependent. The formulas you use are different for different types of samples.\nIf how you choose one sample has no effect on the way you choose the other sample, the two samples are independent. The way to think about it is that in independent samples, the observations from one sample are overall different from the observations from the other sample. This will mean that sample one has no affect on sample two. The sample values from one sample are not related or paired with values from the other sample.\nIf you choose the samples so that a measurement in one sample is paired with a measurement from the other sample, the samples are dependent or matched or paired. (Often a before and after situation.) You want to make sure the there is a meaning for pairing data values from one sample with a specific data value from the other sample. One way to think about it is that in dependent samples, the observations from one sample are the same observations from the other sample, though there can be other reasons to pair values. This makes the sample values from each sample paired.\nIn tidy data, remember each row is a unit of observation, and each column is a variable. In paired samples, you would have two variables that you are working with. In independent samples, you would have a variable that distinguishes an observation from another observation. As an example, in the Pulse data frame, consider the variables pulse_before and pulse_after. Since they are measured off the same observation, then comparing the two variables would be a paired samples analysis. However, consider the pulse_after and whether a person smokes would be comparing the variable pulse_after against the variable smokes to see if smoking effects a person’s pulse rate after exercise. In this case, the observations would be different based on smoking yes or smoking no. Consider the variable smoking to be the factor that one is interested in seeing how it effects pulse rate in the data frame Table 3.7.\n\n9.2.1 Example: Independent or Dependent Samples\nDetermine if the following are dependent or independent samples.\n\nRandomly choose 5 men and 6 women and compare their heights\nChoose 10 men and weigh them. Give them a new diet drug and later weigh them again.\nTake 10 people and measure the strength of their dominant arm and their non-dominant arm.\n\n\n9.2.1.1 Solution\n\nRandomly choose 5 men and 6 women and compare their heights\nIndependent, since there is no reason that one value belongs to another. The units of observations are not the same for both samples. The units of observations are definitely different. A way to think about this is that the knowledge that a man is chosen in one sample does not give any information about any of the woman chosen in the other sample.\n\n\n\nChoose 10 men and weigh them. Give them a new diet drug and later weigh them again.\nDependent, since each person’s before weight can be matched with their after weight. The units of observations are the same for both samples. A way to think about this is that the knowledge that a person weighs 400 pounds at the beginning will tell you something about their weight after the diet drug.\nTake 10 people and measure the strength of their dominant arm and their non-dominant arm.\nDependent, since you can match the two arm strengths. The units of observations are the same for both samples. So the knowledge of one person’s dominant arm strength will tell you something about the strength of their non-dominant arm.\n\nTo analyze data when there are matched or paired samples, called dependent samples, you conduct a paired t-test. Since the samples are matched, you can find the difference between the values of the two random variables.\n\n\n\n9.2.2 Hypothesis Test for Two Sample Paired t-Test\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = random variable 1\n\\(x_2\\) = random variable 2\n\\(\\mu_1\\) = mean of random variable 1\n\\(\\mu_2\\) = mean of random variable 2\n\nState the null and alternative hypotheses and the level of significance\n\nThe hypotheses would be\n\\(H_o:\\mu_1=\\mu_2\\) or\\(H_o:\\mu_1-\\mu_2=0\\)\n\\(H_a:\\mu_1\\ne \\mu_2\\) or \\(H_a:\\mu_1-\\mu_2\\ne0\\)\nHowever, since you are finding the differences, then you can actually think of \\(\\mu_1-\\mu_2=\\mu_d\\).\nSo the hypotheses could become\n\\(H_o:\\mu_d=0\\)\n\\(H_a:\\mu_d\\ne 0\\) Remember, you can replace \\(\\ne\\) with \\(&lt;\\) or \\(&gt;\\).\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of \\(n\\) pairs is taken. Check: state how the sample was collected.\nCheck: The population of the difference between random variables is normally distributed. Check: In this case the population you are interested in has to do with the differences that you find. It does not matter if each random variable is normally distributed. It is only important if the differences you find are normally distributed. Just as before, the t-test is fairly robust to the condition if the sample size is large. This means that if this condition isn’t met, but your sample size is quite large, then the results of the t-test are valid.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nRealize that a paired test is a one sample t-test on the difference between two variables. So you are running a one-sample t-test on a new variable known as the difference variable. You need to create this difference variable by creating a new data frame. This is done on rStudio by doing the following command (The following shows how to create the variable difference for pulse_after-pulse_before on the data frame Pulse. Change the variables used and data frame used to your data frame and variables):\n\nPulse&lt;-\n  Pulse |&gt;\n  mutate(difference=pulse_after-pulse_before) \nknitr::kable(head(Pulse))\n\n\n\nTable 9.1: Pulse Data frame with Difference Column Added\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\ndifference\n\n\n\n\n170\n68\n22\nmale\nyes\nyes\nmoderate\nsat\n70\n71\n93\n1\n\n\n182\n75\n26\nmale\nyes\nyes\nmoderate\nsat\n80\n76\n93\n-4\n\n\n180\n85\n19\nmale\nyes\nyes\nmoderate\nran\n68\n125\n95\n57\n\n\n182\n85\n20\nmale\nyes\nyes\nlow\nsat\n70\n68\n95\n-2\n\n\n167\n70\n22\nmale\nyes\nyes\nlow\nsat\n92\n84\n96\n-8\n\n\n178\n86\n21\nmale\nyes\nyes\nlow\nsat\n76\n80\n98\n4\n\n\n\n\n\n\n\n\nNotice rStudio added a new variable called difference to the data frame Table 9.1. Now to conduct a paired t-test use the rStudio command\nt.test(~difference_variable, data=Data_Frame)\nNote: if the \\(H_a\\) is &lt;, then the command becomes\nt.test(~difference_variable, data=Data_Frame, alternative=“less”)\nSimilarly for &gt; put alternative=“greater”\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge\\alpha\\), then fail to reject \\(H_o\\).\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n9.2.3 Confidence Interval for Difference in Means from Paired Samples (t-Interval)\nThe confidence interval for the difference in means has the same random variables and means and the same conditions as the hypothesis test for two paired samples. If you have already completed the hypothesis test, then you do not need to state them again. If you haven’t completed the hypothesis test, then state the random variables and means, and state and check the conditions before completing the confidence interval step.\n\nFind the sample statistic and confidence interval. Again, you will need to create a new data frame with a difference variable. Then on rStudio the command is\nt.test(~difference_variable, data=Data_Frame, conf.level=C) Type C as a decimal\n\n\n\nStatistical Interpretation: In general this looks like, “You are C% confident that the statement contains the true mean difference.”\nReal World Interpretation: This is where you state what interval contains the true mean difference.\n\n\n\n9.2.4 Example: Hypothesis Test for Paired Samples\nIs the pulse rate after exercise different from the pulse rate before exercise for a woman who drinks alcohol? Use the data frame Table 3.7. Test at the 5% level.\nCode book for data frame Pulse below Table 3.7.\n\n9.2.4.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = pulse of a smoking woman who drinks alcohol after exercise\n\\(x_2\\) = pulse of a smoking woman who drinks alcohol before exercise\n\\(\\mu_1\\) = mean pulse of a smoking woman who drinks alcohol after exercise\n\\(\\mu_2\\) = mean pulse of a smoking woman who drinks alcohol after exercise\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\mu_1=\\mu_2\\)\n\\(H_a: \\mu_1\\ne \\mu_2\\)\nlevel of significance, \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of 110 pairs of pulse rates after and before exercise was taken. Check: The data frame says that the data was collected from students in classes for several years. Though this was not a random sample, it is probably a representative sample.\nState: The population of the difference in after and before pulse rates is normally distributed. Check: To see if this is true, look at the density plot and the normal quantile plot for the difference between after and before. This variable must be created before the density plot and normal quantile plot can be created. The data frame Table 9.2 is females who drink alcohol.\n\n\nPulse_female&lt;- \n  Pulse |&gt; \n  filter(gender==\"female\", alcohol==\"yes\") \nknitr::kable(head(Pulse_female))\n\n\n\nTable 9.2: Pulse Rates Before and After Exercise of Females who do drink Alcohol with Difference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\ndifference\n\n\n\n\n165\n60\n19\nfemale\nyes\nyes\nlow\nran\n88\n120\n98\n32\n\n\n163\n47\n23\nfemale\nyes\nyes\nlow\nran\n71\n125\n98\n54\n\n\n173\n57\n18\nfemale\nno\nyes\nmoderate\nsat\n86\n88\n93\n2\n\n\n179\n58\n19\nfemale\nno\nyes\nmoderate\nran\n82\n150\n93\n68\n\n\n167\n62\n18\nfemale\nno\nyes\nhigh\nran\n96\n176\n93\n80\n\n\n173\n64\n18\nfemale\nno\nyes\nlow\nsat\n90\n88\n93\n-2\n\n\n\n\n\n\n\n\nNow mutate Table 9.2 data frame to include a difference variable.\n\nPulse_female&lt;-\n  Pulse_female |&gt; \n  mutate(difference=pulse_after-pulse_before) \nknitr::kable(head(Pulse_female))\n\n\n\nTable 9.3: Pulse Rates Before and After Exercise of Females who do drink Alcohol with Difference\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nheight\nweight\nage\ngender\nsmokes\nalcohol\nexercise\nran\npulse_before\npulse_after\nyear\ndifference\n\n\n\n\n165\n60\n19\nfemale\nyes\nyes\nlow\nran\n88\n120\n98\n32\n\n\n163\n47\n23\nfemale\nyes\nyes\nlow\nran\n71\n125\n98\n54\n\n\n173\n57\n18\nfemale\nno\nyes\nmoderate\nsat\n86\n88\n93\n2\n\n\n179\n58\n19\nfemale\nno\nyes\nmoderate\nran\n82\n150\n93\n68\n\n\n167\n62\n18\nfemale\nno\nyes\nhigh\nran\n96\n176\n93\n80\n\n\n173\n64\n18\nfemale\nno\nyes\nlow\nsat\n90\n88\n93\n-2\n\n\n\n\n\n\n\n\nUsing Table 9.3 create a density plot and normal quantile plot on the difference variable.\n\ngf_density(~difference, data=Pulse_female, title = \"Difference in Pulse Rates for Females who drink Alcohol\", xlab=\"Difference Between Before and After\")\n\n\n\n\n\n\n\nFigure 9.1: Density plot of differences in pulse rates\n\n\n\n\n\n\ngf_qq(~difference, data=Pulse_female, title = \"Difference in Pulse Rates for Females who drink Alcohol\") \n\n\n\n\n\n\n\nFigure 9.2: Normal Quantile Plot of Differences in Pulse Rates\n\n\n\n\n\nThe density plot is not symmetrical and the normal quantile plot on the differences is not linear. So you cannot assume that the distribution of the difference in pulse rates is normal. It is good that the t-test is robust if there is a large sample. The sample is of size 110, so that should be adequate to assume the conclusion is valid.\n\nFind the sample statistic, test statistic, and p-value On r Studio, use the command:\n\n\nt.test(~difference, data=Pulse_female)\n\n\n    One Sample t-test\n\ndata:  difference\nt = 4.1353, df = 26, p-value = 0.0003283\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 11.51152 34.26625\nsample estimates:\nmean of x \n 22.88889 \n\n\n\nConclusion\n\nSince the p-value &lt; 0.05, reject \\(H_o\\).\n\nInterpretation\n\nThere is enough evidence to support that there is a difference in pulse rate before and after exercise of females who smoke.\n\n\n\n9.2.5 Example: Hypothesis Test for Paired Samples\nThe New Zealand Air Force purchased a batch of flight helmets. They then found out that the helmets didn’t fit. In order to make sure that they order the correct size helmets, they measured the head size of recruits. To save money, they wanted to use cardboard calipers, but were not sure if they will be accurate enough. So they took 18 recruits and measured their heads with the cardboard calipers and also with metal calipers. The data frame is in Table 9.4 (Helmet Sizes for New Zealand Airforce, 2019). Do the data provide enough evidence to show that there is a difference in measurements between the cardboard and metal calipers? Use a 5% level of significance.\n\nHelmet&lt;-read.csv( \"https://krkozak.github.io/MAT160/helmet.csv\") \nknitr::kable(head(Helmet))\n\n\n\nTable 9.4: Helmet Head Measurments\n\n\n\n\n\n\nCardboard\nMetal\n\n\n\n\n146\n145\n\n\n151\n153\n\n\n163\n161\n\n\n152\n151\n\n\n151\n145\n\n\n151\n150\n\n\n\n\n\n\n\n\nCode book for data frame Helmet\nDescription After purchasing a batch of flight helmets that did not fit the heads of many pilots, the NZ Airforce decided to measure the head sizes of all recruits. Before this was carried out, information was collected to determine the feasibility of using cheap cardboard calipers to make the measurements, instead of metal ones which were expensive and uncomfortable. The data lists the head diameters of 18 recruits measured once using cardboard calipers and again using metal calipers. One question is whether there is any systematic difference between the two sets of calipers. One might also ask whether there is more variability in the cardboard calipers measurement than that of the metal calipers.\nThis data frame contains the following columns:\nCardboard: measurement using cardboard calipers (cm)\nMetal: measurement using metal calipers (cm)\nSource Helmet Sizes for New Zealand Airforce. (n.d.). Retrieved July 20, 2019, from http://www.statsci.org/data/oz/nzhelmet.html\nReferences Data courtesy of Dr Stephen Legg. Seber and Lee (1998). Page 545.\n\n9.2.5.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = head measurement of recruit using cardboard caliper\n\\(x_2\\) = head measurement of recruit using metal caliper\n\\(\\mu_1\\)= mean head measurement of recruit using cardboard caliper\n\\(\\mu_2\\) = mean head measurement of recruit using metal caliper\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\mu_1=\\mu_2\\)\n\\(H_a:\\mu_1\\ne \\mu_2\\)\nlevel of significance, \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of 18 pairs of head measures of recruits with cardboard and metal caliper was taken. Check: This was not stated, but probably could be safely assumed.\nState: The population of the difference in head measurements between cardboard and metal calipers is normally distributed. Check: First create the difference variable, then the density plot and normal quantile plot.\n\n\nHelmet&lt;-\n  Helmet |&gt; \n  mutate(difference=Cardboard-Metal) \nknitr::kable(head(Helmet))\n\n\n\nTable 9.5: Helmet Head Measurments\n\n\n\n\n\n\nCardboard\nMetal\ndifference\n\n\n\n\n146\n145\n1\n\n\n151\n153\n-2\n\n\n163\n161\n2\n\n\n152\n151\n1\n\n\n151\n145\n6\n\n\n151\n150\n1\n\n\n\n\n\n\n\n\n\ngf_density(~difference, data=Helmet, title=\"Differences in Head Measurements\", xlab=\"Difference Between Cardboard and Metal\") \n\n\n\n\n\n\n\nFigure 9.3: Density plot of differences in head measurements\n\n\n\n\n\n\ngf_qq(~difference, data=Helmet, title=\"Differences in Head Measurements\") \n\n\n\n\n\n\n\nFigure 9.4: Normal Quantile Plot of Differences in Head Measurements\n\n\n\n\n\nThis density plot Figure 9.3 looks somewhat bell shaped. The normal quantile plot Figure 9.4 on the differences looks somewhat linear. So you can assume that the distribution of the difference in weights is normal.\n\nFind the sample statistic, test statistic, and p-value\n\nUsing rStudio the command is\n\nt.test(~difference, data=Helmet)\n\n\n    One Sample t-test\n\ndata:  difference\nt = 3.1854, df = 17, p-value = 0.005415\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.5440163 2.6782060\nsample estimates:\nmean of x \n 1.611111 \n\n\nThe sample statistic is 1.6111, the test statistic is 3.1854, and the p-value is 0.005415.\n\nConclusion\n\nSince the p-value \\(&lt;\\) 0.05, reject \\(H_o\\).\n\nInterpretation\n\nThere is enough evidence to support that the mean head measurements using the cardboard calipers are not the same as when using the metal calipers. So it looks like the New Zealand Air Force shouldn’t use the cardboard calipers.\n\n\n\n9.2.6 Example: Confidence Interval for Paired Samples\nThe New Zealand Air Force purchased a batch of flight helmets. They then found out that the helmets didn’t fit. In order to make sure that they order the correct size helmets, they measured the head size of recruits. To save money, they wanted to use cardboard calipers, but were not sure if they will be accurate enough. So they took 18 recruits and measured their heads with the cardboard calipers and also with metal calipers. The data frame is in Table 9.4 (Helmet Sizes for New Zealand Airforce, 2019). Estimate the difference in measurements between the cardboard and metal calipers using a 95% confidence interval.\n\n9.2.6.1 Solution\n\nState the random variables and the parameters in words.\n\nThese were stated in Example: Hypothesis Test for Paired Samples.\n\nState and check the conditions for the confidence interval\n\nThe conditions were stated and checked in Example: Hypothesis Test for Paired Samples.\n\nFind the sample statistic and confidence interval\n\nUsing the data frame Table 9.5 the rStudio the command is\n\nt.test(~difference, data=Helmet, conf.leve=0.95)\n\n\n    One Sample t-test\n\ndata:  difference\nt = 3.1854, df = 17, p-value = 0.005415\nalternative hypothesis: true mean is not equal to 0\n95 percent confidence interval:\n 0.5440163 2.6782060\nsample estimates:\nmean of x \n 1.611111 \n\n\n\nStatistical Interpretation: You are 95% confidence that \\(0.5440163&lt;\\mu_1-\\mu_2&lt;2.6782060\\) contains the true mean difference in head measurement between using the cardboard and metal calibers.\nReal World Interpretation: The mean head measurement using the cardboard calibers is anywhere from 0.54 cm to 2.68 cm more than the head measurement using the metal calibers.\n\nExamples 9.2.6 and 9.2.7 use the same data set, but one is conducting a hypothesis test and the other is conducting a confidence interval. Notice that the hypothesis test’s conclusion was to reject and say that there was a difference in the means, and the confidence interval does not contain the number 0. If the confidence interval did contain the number 0, then that would mean that the two means could be the same. Since the interval did not contain 0, then you could say that the means are different just as in the hypothesis test. This means that the hypothesis test and the confidence interval can produce the same interpretation. Do be careful though, you can run a hypothesis test with a particular significance level and a confidence interval with a confidence level that is not compatible with your significance level. This will mean that the conclusion from the confidence interval would not be the same as with a hypothesis test. So if you want to estimate the mean difference, then conduct a confidence interval. If you want to show that the means are different, then conduct a hypothesis test. As a reminder, the American Statistical Association (ASA) suggests not conducting hypothesis tests and just create confidence intervals.\n\n\n\n9.2.7 Homework for Paired Samples for Two Means Section\nIn each problem show all steps of the hypothesis test or confidence interval. If some of the conditions are not met, note that the results of the test or interval may not be correct and then continue the process of the hypothesis test or confidence interval.\n\nThe cholesterol level of patients who had heart attacks was measured multiple times after the heart attack. The researchers want to see if the cholesterol level of patients who have heart attacks changes as the time since their heart attack increases. The data is in Table 3.2. Do the data show that the mean cholesterol level of patients that have had a heart attack changes as the time increases since their heart attack? Use day2 and day4 variables to answer the question. Test at the 1% level.\n\nCode book for Data Frame Cholesterol is below Table 9.5.\n\nThe cholesterol level of patients who had heart attacks was measured multiple times after the heart attack. The researchers want to see if the cholesterol level of patients who have heart attacks changes as the time since their heart attack increases. The data is in Table 9.5. Calculate a 98% confidence interval for the mean difference in cholesterol levels from day two to day four.\nAll Fresh Seafood is a wholesale fish company based on the east coast of the U.S. Catalina Offshore Products is a wholesale fish company based on the west coast of the U.S. Table 9.6 contains prices from both companies for specific fish types (\\“Seafood online,\\” 2013) (\\“Buy sushi grade,\\” 2013). Do the data provide enough evidence to show that fish cost different from west coast fish wholesaler and east coast wholesaler? Test at the 5% level.\n\n\nPrice &lt;- read.csv( \"https://krkozak.github.io/MAT160/price.csv\") \nknitr::kable(head(Price))\n\n\n\nTable 9.6: Wholesale Prices of Fish in Dollars\n\n\n\n\n\n\nfish\neast\nwest\n\n\n\n\nCod\n19.99\n17.99\n\n\nTilapi\n6.00\n13.99\n\n\nFarmed Salmon\n19.99\n22.99\n\n\nOrganic Salmon\n24.99\n24.99\n\n\nGrouper Fillet\n29.99\n19.99\n\n\nTuna\n28.99\n31.99\n\n\n\n\n\n\n\n\nCode book for data frame Price\nDescription Price of fish was collected from two websites. One for Catalina Offshore Products (west coast) and the other for All Fresh Seafood (east coast) in 2013.\nThis data frame contains the following columns:\nfish: type of fish for sale\neast: price of fish from east coast supplier ($)\nwest: price of fish from west coast supplier ($)\nSource Seafood online. (2013, November 20). Retrieved from http://www.allfreshseafood.com/\nBuy sushi grade fish online. (2013, November 20). Retrieved from http://www.catalinaop.com/\nReferences Websites of Catalina Offshore Products and All Fresh Seafood\n\nAll Fresh Seafood is a wholesale fish company based on the east coast of the U.S. Catalina Offshore Products is a wholesale fish company based on the west coast of the U.S. Table 9.6 contains prices from both companies for specific fish types (\\“Seafood online,\\” 2013) (\\“Buy sushi grade,\\” 2013). Find a 95% confidence interval for the mean difference in wholesale price between the east coast and west coast suppliers.\nThe British Department of Transportation studied to see if people avoid driving or shopping, or have more accidents on Friday the 13th. They collected data from different locations (Friday the 13th, 2019). The data for each location on the two different dates is in Table 9.7. Do the data show that on average different number of people are engaged in activities on Friday the 13th? Test at the 5% level.\n\n\nTraffic &lt;- read.csv( \"https://krkozak.github.io/MAT160/traffic.csv\") \nknitr::kable(head(Traffic))\n\n\n\nTable 9.7: Traffic Count\n\n\n\n\n\n\nsource\nyear\nmonth\nX6th\nX13th\nlocation\n\n\n\n\ntraffic\n1990,\nJuly\n139246\n138548\n7 to 8\n\n\ntraffic\n1990,\nJuly\n134012\n132908\n9 to 10\n\n\ntraffic\n1991,\nSeptember\n137055\n136018\n7 to 8\n\n\ntraffic\n1991,\nSeptember\n133732\n131843\n9 to 10\n\n\ntraffic\n1991,\nDecember\n123552\n121641\n7 to 8\n\n\ntraffic\n1991,\nDecember\n121139\n118723\n9 to 10\n\n\n\n\n\n\n\n\nCode book for data frame Traffic\nDescription This file consists of three separate data sets, all of which address the issues of how superstitions regarding Friday the 13th affect human behavior, and whether Friday the 13th is an unlucky day. Scanlon, et al. collected data on traffic and shopping patterns and accident frequency for Fridays the 6th and 13th between October of 1989 and November of 1992.\nFor the first data set, the researchers obtained information from the British Department of Transport regarding the traffic flows between junctions 7 to 8 and junctions 9 to 10 of the M25 motorway. They collected the numbers of shoppers in nine different supermarkets in southeast England for the second data set. The third data set contains numbers of emergency admissions to hospitals due to transport accidents.\nWe present the three data sets in a combined format, with the variable “Data set” as an identifier that may be used to separate them.\nThis data frame contains the following columns:\nsource: which data set the data were obtained from\nyear: which year the data was collected from\nMonth: the month that the Friday was in\nx6th: Number of cars passing through junction (traffic data set), shoppers for each supermarket (shopping data set), or admissions due to transport accidents (accident data set) on Friday the 6th\nx13th: Number of cars passing through junction (traffic data set), shoppers for each supermarket (shopping data set), or admissions due to transport accidents (accident data set) on Friday the 13th\nlocation: Motorway junction (traffic data set), supermarket location (shopping data set) or hospital (accident data set) to which the data correspond\nSource (n.d.). Retrieved from https://www3.nd.edu/~busiforc/handouts/Data and Stories/t test/Friday The Thirteenth/Friday The Thirteenth Data.html\nReferences Scanlon, T.J., Luben, R.N., Scanlon, F.L., Singleton, N. (1993), “Is Friday the 13th Bad For Your Health?,” BMJ, 307, 1584-1586.\n\nThe British Department of Transportation studied to see if people avoid driving or shopping, or have more accidents on Friday the 13th. They collected data from different locations (Friday the 13th, 2019). The data for each location on the two different dates is in Table 9.7. Do the data show that on average different number of people are engaged in activities on Friday the 13th? Estimate the mean difference in activity count between the 6th and the 13th using a 95% level.\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997). The data is in Table 3.9. Do the data show that Reiki treatment reduces pain? Test at the 5% level.\n\nCode book for data frame Reiki is below Table 3.9.\n\nTo determine if Reiki is an effective method for treating pain, a pilot study was carried out where a certified second-degree Reiki therapist provided treatment on volunteers. Pain was measured using a visual analogue scale (VAS) and a likert scale immediately before and after the Reiki treatment (Olson & Hanson, 1997). The data is in Table 3.9. Compute a 90% confidence level for the mean difference in VAS score from before and after Reiki treatment.\nThe female labor force participation rates (FLFPR) of women in countries from 1990 to 2018 are in table 9.2.8.5 (Labor force participation rate, female (% of female population ages 15+) (modeled ILO estimate), 2019). Do the data show that the mean female labor force participation rate in 1990 is different from that in the 2018 using a 5% level of significance?\n\n\nLabor &lt;- read.csv( \"https://krkozak.github.io/MAT160/labor.csv\")\nknitr::kable(head(Labor))\n\n\n\nTable 9.8: Female Labor Force Participation Rates\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCountry.Name\nCountry.Code\nRegion\nIncomeGroup\ny1990\ny1991\ny1992\ny1993\ny1994\ny1995\ny1996\ny1997\ny1998\ny1999\ny2000\ny2001\ny2002\ny2003\ny2004\ny2005\ny2006\ny2007\ny2008\ny2009\ny2010\ny2011\ny2012\ny2013\ny2014\ny2015\ny2016\ny2017\ny2018\n\n\n\n\nAruba\nABW\nLatin America & Caribbean\nHigh income\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nAfghanistan\nAFG\nSouth Asia\nLow income\n43.11500\n43.12400\n43.12900\n43.07200\n43.00300\n43.01700\n42.77000\n42.55400\n42.41300\n42.3340\n42.27400\n42.53900\n42.89900\n43.28600\n43.66100\n44.02500\n43.59700\n43.19200\n42.8730\n42.70900\n42.73500\n43.32800\n44.11700\n45.03900\n46.01700\n47.00100\n47.76600\n48.47400\n48.66000\n\n\nAngola\nAGO\nSub-Saharan Africa\nLower middle income\n74.94500\n74.87900\n74.82600\n74.78200\n74.77000\n74.78400\n74.78300\n74.80600\n74.84600\n74.8940\n74.94100\n74.96200\n74.98400\n75.01100\n75.04800\n75.09400\n75.12600\n75.16500\n75.2090\n75.25600\n75.30700\n75.34400\n75.38900\n75.43300\n75.46500\n75.47900\n75.47000\n75.45100\n75.41200\n\n\nAlbania\nALB\nEurope & Central Asia\nUpper middle income\n53.77100\n56.29600\n56.68700\n55.74700\n54.90400\n53.74600\n53.07500\n53.81200\n53.15400\n52.2540\n51.76900\n51.11000\n50.67900\n49.75900\n48.87800\n48.05100\n47.38900\n46.80300\n46.2690\n44.94500\n45.69300\n47.10400\n48.80600\n44.65000\n44.78900\n47.67600\n47.45900\n47.31200\n47.19100\n\n\nAndorra\nAND\nEurope & Central Asia\nHigh income\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nArab World\nARB\n\n\n19.18997\n19.24094\n19.13159\n19.29515\n19.64479\n19.66156\n19.51602\n19.27293\n19.07511\n19.5351\n19.59284\n19.52237\n19.08892\n19.32403\n19.44488\n19.53444\n19.68183\n20.17107\n19.8473\n20.05784\n20.17166\n20.27703\n20.46453\n20.76731\n20.70378\n20.51515\n20.61605\n20.56842\n20.58152\n\n\n\n\n\n\n\n\nCode book for data frame Labor\nDescription Labor force participation rate, female (% of female population ages 15+)\nThis data frame contains the following columns:\nCountry Name: The name of a country around the world\nCountry Code: The 3 letter country code\nRegion: The location of the country in the world\nIncomeGroup: The World Bank’s income classification\ny1990-y2018: Labor force participation rate, female (% of female population ages 15+) for the years 100–2018\nSource Labor force participation rate, female (% of female population ages 15 ) (modeled ILO estimate). (n.d.). Retrieved July 20, 2019, from https://data.worldbank.org/indicator/SL.TLF.CACT.FE.ZS\nReferences International Labour Organization, ILOSTAT database. Data retrieved in April 2019.\n\nThe female labor force participation rates (FLFPR) of women in countries from 1990 to 2018 are in Table 9.8 (Labor force participation rate, female (% of female population ages 15+) (modeled ILO estimate), 2019). Estimate the mean difference in the female labor force participation rate in 1990 to 2018 using a 95% confidence level?\nIs the pulse rate after exercise different from the pulse rate before exercise for a man who drinks alcohol but doesn’t smoke? Use the data frame Pulse Table 3.7. Test at the 5% level.\n\nCode book for data frame Pulse is below Table 3.7.\n\nTable 3.7 contains pulse rates Compute a 95% confidence interval for the mean difference in pulse rates from before and after exercise for males who drink but do not smoke.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html#independent-samples-for-two-means",
    "href": "Two Sample Inference.html#independent-samples-for-two-means",
    "title": "9  Two Sample Inference",
    "section": "9.3 Independent Samples for Two Means",
    "text": "9.3 Independent Samples for Two Means\nThis section will look at how to analyze when two samples are collected that are independent. As with all other hypothesis tests and confidence intervals, the process is the same though the formulas and conditions are different.\n\n9.3.1 Hypothesis Test for the Difference in Means from Two Independent Samples\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = random variable 1\n\\(x_2\\)= random variable 2\n\\(\\mu_1\\)= mean of random variable 1\n\\(\\mu_2\\)= mean of random variable 2\n\nState the null and alternative hypotheses and the level of significance\n\nThe hypotheses would be\n\\(H_o:\\mu_1=\\mu_2\\)\n\\(H_a:\\mu_1\\ne \\mu_2\\), the \\(\\ne\\) can be replaced with \\(&lt;\\) or \\(&gt;\\)\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of size \\(n_1\\) is taken from population 1. A random sample of size \\(n_2\\) is taken from population 2. Check: describe how both samples are collected. Note: the samples do not need to be the same size, but the test is more robust if they are.\nState: The two samples are independent. Check: describe whey the samples are independent of each other.\nState: Population 1 is normally distributed. Population 2 is normally distributed. Check: draw the density graph and normal quantile plot for both samples and discuss if they meet the criteria. Just as before, the t-test is fairly robust to the condition if the sample size is large. This means that if this condition isn’t met, but your sample sizes are quite large, then the results of the t-test are valid.\nState: The population variances are unknown and not assumed to be equal. The old condition is that the variances are equal. However, this condition is no longer a condition that most statisticians use. This is because it isn’t really realistic to assume that the variances are equal. So just assume the condition of the variances being unknown and not assumed to be equal is true, and it will not be checked.\n\n\n\nFind the sample statistic, test statistic, and p-value\n\nThe command using r is\nt.test(variable~factor, data=Data_Frame)\nNote: if the \\(H_a\\) is &lt;, then the command becomes\nt.test(variable~factor, data=Data_Frame, alternative=“less”)\nSimilarly for &gt; put alternative=“greater”\n\nConclusion\n\nThis is where you write reject or fail to reject \\(H_0\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\).\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n9.3.2 Confidence Interval for the Difference in Means from Two Independent Samples\nThe confidence interval for the difference in means has the same random variables and means and the same conditions as the hypothesis test for independent samples. If you have already completed the hypothesis test, then you do not need to state them again. If you haven’t completed the hypothesis test, then state the random variables and means and state and check the conditions before completing the confidence interval step.\nFind the sample statistic and confidence interval\nOn r Studio, the command is\nt.test(variable~factor, data=Data_Frame, conf.level=C) type C as a decimal\n\nStatistical Interpretation: In general this looks like, “You are C% confident that the interval contains the true mean difference.”\nReal World Interpretation: This is where you state what interval contains the true difference in means, though often you state how much more (or less) the first mean is from the second mean.\n\n\n\n9.3.3 Example: Hypothesis Test for Two Means\nThe cholesterol level of people vary for many reasons. The question is do people with diabetes have different cholesterol levels from people who do not have diabetes? Use the NHANES data frame. Test at the 5% level.\n\nnames(NHANES) #displays the names of the variables in a data frame\n\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n[13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n[17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n[21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n[25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n[29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n[33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n[37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n[41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n[45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n[49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n[53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n[57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n[61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n[65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n[69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n[73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"     \n\n\nCode book for data frame NHANES type help(“NHANES”) in the r Console.\n\n9.3.3.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = Cholesterol level of people with diabetes\n\\(x_2\\) = Cholesterol level of people without diabetes\n\\(\\mu_1\\) = mean cholesterol level of people with diabetes\n\\(\\mu_2\\) = mean cholesterol level of people without diabetes\n\nState the null and alternative hypotheses and the level of significance\n\nThe hypotheses would be\n\\(H_o: \\mu_1=\\mu_2\\)\n\\(H_a: \\mu_1 \\ne \\mu_2\\)\nlevel of significance, \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of cholesterol levels of people with diabetes is taken. A random sample of cholesterol levels of people without diabetes is taken.\nCheck: The NHANES data frame uses cluster sampling which incorporates random sampling, so the sample is probably representative. This condition has been met.\nState: The two samples are independent.\nCheck: This is because either they were dealing with people who have diabetes or not.\nState: Population of all cholesterol levels of people who have diabetes is normally distributed. Population of all cholesterol levels of people without diabetes is normally distributed.\nCheck:\n\n\nNHANES_no_NA&lt;- \n  NHANES |&gt; \n  drop_na(Diabetes) \ngf_density(~TotChol|Diabetes, data=NHANES_no_NA, title = \"Cholesterol of a person with and without Diabetes\", xlab=\"Total Cholesterol\") \n\n\n\n\n\n\n\nFigure 9.5: Density Plot of Cholesterol of a person with and without Diabetes\n\n\n\n\n\nBoth the yes group and the no group look somewhat bell shaped.\n\ngf_qq(~TotChol|Diabetes, data=NHANES_no_NA, title = \"Cholesterol of a person with and without Diabetes\")\n\n\n\n\n\n\n\nFigure 9.6: quantile Plot of Cholesterol of a person with and without Diabetes\n\n\n\n\n\nBoth the yes group and the no group look somewhat linear.\nThe population of all cholesterol levels of people who have diabetes is probably normally distributed. The population of all cholesterol levels of people who do not have diabetes is probably normally distributed.\n\nFind the sample statistic, test statistic, and p-value\n\nThe variable is cholesterol (TotChol) and separating based on if a person has diabetes or not. So the factor is Diabetes. Using r Studio the command would be\n\nt.test(TotChol~Diabetes, data=NHANES) \n\n\n    Welch Two Sample t-test\n\ndata:  TotChol by Diabetes\nt = 2.4286, df = 809.7, p-value = 0.01537\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 0.02105115 0.19851114\nsample estimates:\n mean in group No mean in group Yes \n         4.887936          4.778155 \n\n\n\nConclusion\n\nReject \\(H_o\\) since the p-value \\(&lt;\\alpha\\).\n\nInterpretation\n\nThere is enough evidence to support that people who have diabetes have different cholesterol levels on average from people who do not have diabetes.\n\n\n\n9.3.4 Example: Confidence Interval in Two Samples\nThe cholesterol level of people vary for many reasons. The question is how different is the cholesterol levels of people with diabetes from people who do not have diabetes? Use the NHANES data frame. Compute a 95% confidence interval.\n\n9.3.4.1 Solution\n\nState the random variables and the parameters in words.\n\nThese were stated in Example: Hypothesis Test for Two Means.\n\nState and check the conditions for the hypothesis test\n\nThe conditions were stated and checked in Example: Hypothesis Test for Two Means.\n\nFind the sample statistic and confidence interval\n\nThe variable is cholesterol (TotChol) and separating based on if a person has diabetes or not. So the factor is Diabetes. Using rStudio the command would be\n\nt.test(TotChol~Diabetes, data=NHANES, conf.level=0.95) \n\n\n    Welch Two Sample t-test\n\ndata:  TotChol by Diabetes\nt = 2.4286, df = 809.7, p-value = 0.01537\nalternative hypothesis: true difference in means between group No and group Yes is not equal to 0\n95 percent confidence interval:\n 0.02105115 0.19851114\nsample estimates:\n mean in group No mean in group Yes \n         4.887936          4.778155 \n\n\n\nStatistical Interpretation: You are 95% confident that the interval \\(0.02105115&lt;\\mu_1-\\mu_2&lt;0.19851114\\) contains the true difference in means.\nReal World Interpretation: The mean cholesterol level for people with diabetes is anywhere from 0.021 mmol/L to 0.199 mmol/L more than the mean cholesterol level for people without diabetes.\n\n\n\n\n9.3.5 Example: Hypothesis Test for Two Means\nThe amount of sodium in beef and poultry hot dogs was measured. (\\“SOCR 012708 id,\\” 2013). The data is in Table 9.9. Is there enough evidence to show that beef has different amounts of sodium on average than poultry hot dogs? Use a 5% level of significance.\n\nHotdog&lt;-read.csv( \"https://krkozak.github.io/MAT160/hotdog_beef_poultry.csv\") \nknitr::kable(head(Hotdog))\n\n\n\nTable 9.9: Hot dog Data\n\n\n\n\n\n\ntype\ncalories\nsodium\n\n\n\n\nBeef\n186\n495\n\n\nBeef\n181\n477\n\n\nBeef\n176\n425\n\n\nBeef\n149\n322\n\n\nBeef\n184\n482\n\n\nBeef\n190\n587\n\n\n\n\n\n\n\n\nCode book for data frame Hot dog\nDescription Results of a laboratory analysis of calories and sodium content of major hot dog brands. Researchers for Consumer Reports analyzed three types of hot dog: beef, poultry, and meat (mostly pork and beef, but up to 15% poultry meat). The meat was left off this data frame so a two-sample t-test could be performed.\nThis data frame contains the following columns:\ntype: Type of hot dog (beef or poultry)\ncalories: Calories per hot dog\nsodium: Milligrams of sodium per hot dog\nSource SOCR 012708 id data hotdogs. (2013, November 13). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_012708_ID_Data_HotDogs\nReferences SOCR Home page: http://www.socr.ucla.edu\n\n9.3.5.1 Solution\n\nState the random variables and the parameters in words.\n\n\\(x_1\\) = sodium level in beef hot dogs\n\\(x_2\\) = sodium level in poultry hot dogs\n\\(\\mu_1\\) = mean sodium level in beef hot dogs\n\\(\\mu_2\\) = mean sodium level in poultry hot dogs\n\nState the null and alternative hypotheses and the level of significance\n\nThe hypotheses would be\n\\(H_o:\\mu_1=\\mu_2\\)\n\\(H_o:\\mu_1\\ne \\mu_2\\)\nlevel of significance: \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of 20 sodium levels in beef hot dogs is taken. A random sample of 20 sodium levels in poultry hot dogs.\nCheck: The code does not state if either sample was randomly selected, but since Consumer Reports performed the test, it is safe to assume the samples were both random.\nState: The two samples are independent.\nCheck: These are different types of hot dogs so this is true.\nState; Population of all sodium levels in beef hot dogs is normally distributed. Population of all sodium levels in poultry hot dogs is normally distributed.\nCheck:\n\n\ngf_density(~sodium|type, data=Hotdog, title=\"Sodium amount in Hot Dogs facetted by Type of Meat\", xlab=\"Total Sodium Level\") \n\n\n\n\n\n\n\nFigure 9.7: Density Plot of Sodium Amount in Hot Dogs facetted by Type of Meat\n\n\n\n\n\nThe density plot for beef hot dogs looks somewhat bell shaped, but the density plot for poultry hot dogs does not look bell shaped.\n\ngf_qq(~sodium|type, data=Hotdog, title=\"Sodium amoount in Hot Dogs facetted by Type of Meat\") \n\n\n\n\n\n\n\nFigure 9.8: Quantile Plot of Sodium Amount in Hot Dogs facetted by Type of Meat\n\n\n\n\n\nThe normal quantile plot Figure 9.7 for the sodium level in beef hot dogs looks somewhat linear. The normal quantile plot Figure 9.8 for the sodium level in poultry hot dogs does not look linear. The population of all sodium levels in beef hot dogs may be normally distributed, but the population of all sodium levels in poultry hot dogs is probably not normally distributed. The sample size is not very large so the results of the test may not be valid. A larger sample would be a good idea.\n\nFind the sample statistic, test statistic, and p-value\n\nUsing rStudio the variable is sodium levels (sodium) in different types of hot dogs. So the factor is type. The command is\n\nt.test(sodium~type, data=Hotdog)\n\n\n    Welch Two Sample t-test\n\ndata:  sodium by type\nt = -1.8798, df = 34.983, p-value = 0.06848\nalternative hypothesis: true difference in means between group Beef and group Poultry is not equal to 0\n95 percent confidence interval:\n -120.325706    4.625706\nsample estimates:\n   mean in group Beef mean in group Poultry \n               401.15                459.00 \n\n\n\nConclusion: Fail to reject \\(H_o\\) since the p-value \\(\\ge \\alpha\\).\nInterpretation\n\nThis is not enough evidence to support that beef hot dogs’ sodium level is different from poultry hot dogs. (Though do realize that the population conditions is not valid, so this interpretation may be invalid.)\n\n\n\n9.3.6 Example: Confidence Interval for Two Independent Samples\nThe amount of sodium in beef and poultry hot dogs was measured. (“SOCR 012708 id,” 2013). The data is in Table 9.9. Find a 95% confidence interval for the mean difference in sodium levels between beef and poultry hot dogs.\n\n9.3.6.1 Solution\n\nState the random variables and the parameters in words.\nThese were stated in Example: Hypothesis Test for Two Means.\nState and check the conditions for the hypothesis test\n\nThe conditions were stated and checked in Example: Hypothesis Test for Two Means.\n\nFind the sample statistic and confidence interval Using r Studio the variable is sodium levels (sodium) in different types of hot dogs. So the factor is type. The command is\n\n\nt.test(sodium~type, data=Hotdog, conf.level=0.95)\n\n\n    Welch Two Sample t-test\n\ndata:  sodium by type\nt = -1.8798, df = 34.983, p-value = 0.06848\nalternative hypothesis: true difference in means between group Beef and group Poultry is not equal to 0\n95 percent confidence interval:\n -120.325706    4.625706\nsample estimates:\n   mean in group Beef mean in group Poultry \n               401.15                459.00 \n\n\n\nStatistical Interpretation: You are 95% confident that the interval \\(-120.325706&lt;\\mu_1-\\mu_2&lt;4.625706\\) contains the true difference in mean sodium level between beef and poultry hot dogs.\nReal World Interpretation: The mean sodium level of beef hot dogs is anywhere from 120.33 mg less than the mean sodium level of poultry hot dogs to 4.63 mg more. (The negative sign on the lower limit implies that the first mean is less than the second mean. The positive sign on the upper limit implies that the first mean is greater than the second mean.)\n\nDo realize that the population conditions is not valid, so this interpretation may be invalid.\n\n\n\n9.3.7 Homework for Independent Samples for Two Means Section\nIn each problem show all steps of the hypothesis test or confidence interval. If some of the conditions are not met, note that the results of the test or interval may not be correct and then continue the process of the hypothesis test or confidence interval.\n\nThe NHANES data contains many variables. One variable is the income of households derived from the middle income of different income categories. The variable is called HHIncomeMid. Is there enough evidence to show that the mean income of males is different from the mean income of females? Test at the 1% level.\n\n\nnames(NHANES)\n\n [1] \"ID\"               \"SurveyYr\"         \"Gender\"           \"Age\"             \n [5] \"AgeDecade\"        \"AgeMonths\"        \"Race1\"            \"Race3\"           \n [9] \"Education\"        \"MaritalStatus\"    \"HHIncome\"         \"HHIncomeMid\"     \n[13] \"Poverty\"          \"HomeRooms\"        \"HomeOwn\"          \"Work\"            \n[17] \"Weight\"           \"Length\"           \"HeadCirc\"         \"Height\"          \n[21] \"BMI\"              \"BMICatUnder20yrs\" \"BMI_WHO\"          \"Pulse\"           \n[25] \"BPSysAve\"         \"BPDiaAve\"         \"BPSys1\"           \"BPDia1\"          \n[29] \"BPSys2\"           \"BPDia2\"           \"BPSys3\"           \"BPDia3\"          \n[33] \"Testosterone\"     \"DirectChol\"       \"TotChol\"          \"UrineVol1\"       \n[37] \"UrineFlow1\"       \"UrineVol2\"        \"UrineFlow2\"       \"Diabetes\"        \n[41] \"DiabetesAge\"      \"HealthGen\"        \"DaysPhysHlthBad\"  \"DaysMentHlthBad\" \n[45] \"LittleInterest\"   \"Depressed\"        \"nPregnancies\"     \"nBabies\"         \n[49] \"Age1stBaby\"       \"SleepHrsNight\"    \"SleepTrouble\"     \"PhysActive\"      \n[53] \"PhysActiveDays\"   \"TVHrsDay\"         \"CompHrsDay\"       \"TVHrsDayChild\"   \n[57] \"CompHrsDayChild\"  \"Alcohol12PlusYr\"  \"AlcoholDay\"       \"AlcoholYear\"     \n[61] \"SmokeNow\"         \"Smoke100\"         \"Smoke100n\"        \"SmokeAge\"        \n[65] \"Marijuana\"        \"AgeFirstMarij\"    \"RegularMarij\"     \"AgeRegMarij\"     \n[69] \"HardDrugs\"        \"SexEver\"          \"SexAge\"           \"SexNumPartnLife\" \n[73] \"SexNumPartYear\"   \"SameSex\"          \"SexOrientation\"   \"PregnantNow\"     \n\n\n\nThe NHANES data contains many variables. One variable is the income of households derived from the middle income of different income categories. The variable is called HHIncomeMid. Estimate with 95% confidence the mean difference in incomes between males and females in the U.S.\nA study was conducted that measured the total brain volume (TBV) of patients that had schizophrenia and patients that do not have schizophrenia. Table 9.10 contains the TBV of the all patients (“SOCR data oct2009,\\” 2013). Is there enough evidence to show that the patients with schizophrenia have a different TBV on average than a patient without schizophrenia? Test at the 10% level.\n\n\nBrain &lt;- read.csv( \"https://krkozak.github.io/MAT160/brain.csv\") \nknitr::kable(head(Brain))\n\n\n\nTable 9.10: Total Brain Volume of Patients\n\n\n\n\n\n\ntype\nvolume\n\n\n\n\nn\n1663407\n\n\nn\n1583940\n\n\nn\n1299470\n\n\nn\n1535137\n\n\nn\n1431890\n\n\nn\n1578698\n\n\n\n\n\n\n\n\nCode book for data frame Brain\nDescription A study to measure the total brain volume (TBV) (in ) of patients that had schizophrenia and patients that do not have schizophrenia.\nThis data frame contains the following columns:\ntype: whether the patient had schizophrenia (s) or did not have schizophrenia (n)\nvolume: the total brain volume of a patient.(\\(mm^3\\))\nSource SOCR data Oct2009 id ni. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Oct2009_ID_NI\nReferences “SOCR data nips,” 2013\n\nA study was conducted that measured the total brain volume (TBV) of patients that had schizophrenia and patients that do not have schizophrenia. Table 9.10 contains the TBV of the all patients (“SOCR data oct2009,” 2013). Is there enough evidence to show that the patients with schizophrenia have a different TBV on average than a patient without schizophrenia? Test at the 10% level. Compute a 90% confidence interval for the difference in TBV of patients with Schizophrenia and patients without Schizophrenia.\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 3.3 (Lee, 1994). Do the data provide enough evidence to show on average that the rivers that travel to the Pacific Ocean are different length than the rivers that travel to the Tasman Sea? Use a 5% level of significance.\n\nCode book for data frame Length below Table 3.3.\n\nThe lengths (in kilometers) of rivers on the South Island of New Zealand and what body of water they flow into are listed in Table 3.3 (Lee, 1994). Estimate the difference in mean lengths of rivers between rivers in New Zealand that travel to the Pacific Ocean and ones that travel to the Tasman Sea. Use a 95% confidence level.\nA vitamin K shot is given to infants soon after birth. Nurses at Northbay Healthcare were involved in a study to see if how they handle the infants could reduce the pain the infants feel (\\“SOCR data nips,\\” 2013). The data frame is in Table 9.11. Is there enough evidence to show that infants cried a different amount on average when they are held by their mothers than if held using conventional methods? Test at the 5% level.\n\n\n9.3.7.1 Table: Crying Time of Infants Given Shots Using New Methods\n\nCrying&lt;- read.csv( \"https://krkozak.github.io/MAT160/crying.csv\") \nknitr::kable(head(Crying))\n\n\n\nTable 9.11: Crying Time of Infants Given Shots Using New Methods\n\n\n\n\n\n\nmethod\ncrying\n\n\n\n\nconvent\n63\n\n\nconvent\n0\n\n\nconvent\n2\n\n\nconvent\n46\n\n\nconvent\n33\n\n\nconvent\n33\n\n\n\n\n\n\n\n\nCode book for data frame Crying\nDescription Nurses at Northbay Healthcare were involved in a study to see if how they handle the infants could reduce the pain the infants feel. One of the measurements taken was how long, in seconds, the infant cried after being given the shot. A random sample was taken from the group that was given the shot using conventional methods, and a random sample was taken from the group that was given the shot where the mother held the infant prior to and during the shot.\nThis data frame contains the following columns:\nmethod: whether the infant was given the conventional method (convent) or the new method (new) prior to being given the vitamin K shot.\ncrying: how long the infant cried after given a vitamin K shot. (seconds)\nSource SOCR data nips infantvitK shotdata. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_NIPS_InfantVitK_ShotData\nReferences \\“SOCR data nips,\\” 2013\n\nA vitamin K shot is given to infants soon after birth. Nurses at Northbay Healthcare were involved in a study to see if how they handle the infants could reduce the pain the infants feel (\\“SOCR data nips,\\” 2013). The data frame is in Table 9.11. Calculate a 95% confidence interval for the mean difference in mean crying time after being given a vitamin K shot between infants held using conventional methods and infants held by their mothers.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Two Sample Inference.html#which-analysis-should-you-conduct",
    "href": "Two Sample Inference.html#which-analysis-should-you-conduct",
    "title": "9  Two Sample Inference",
    "section": "9.4 Which Analysis Should You Conduct?",
    "text": "9.4 Which Analysis Should You Conduct?\nOne of the most important concept that you need to understand is deciding which analysis you should conduct for a particular situation. To help you to figure out the analysis to conduct, there are a series of questions you should ask yourself.\n\nDoes the problem deal with mean or proportion?\n\nSometimes the problem states explicitly the words mean or proportion, but other times you have to figure it out based on the information you are given. If you counted number of individuals that responded in the affirmative to a question, then you are dealing with proportion. If you measured something, then you are dealing with mean.\n\nDoes the problem have one or two samples?\n\nSo look to see if one group was measured or if two groups were measured. You need to decide if the problem describes collecting data from one group or from two groups, or if you are comparing two different groups.\n\nIf you have two samples, then you need to determine if the samples are independent or dependent.\n\nIf the individuals are different for both samples, then most likely the samples are independent. If you can’t tell, then determine if a data value from the first sample influences the data value in the second sample. In other words, can you pair data values together so you can find the difference, and that difference has meaning. If the answer is yes, then the samples are paired. Otherwise, the samples are independent.\n\nDoes the situation involve a hypothesis test or a confidence interval?\n\nIf the problem talks about “do the data show”, “is there evidence of”, “test to see”, then you are doing a hypothesis test. If the problem talks about “find the value”, “estimate the” or “find the interval”, then you are doing a confidence interval.\nSo if you have a situation that has two samples, independent samples, involving the mean, and is a hypothesis test, then you have a two-sample independent t-test. Now you look up the conditions and the technology process for doing this test. Every hypothesis test involves the same six steps, and you just have to use the correct conditions and calculations. Every confidence interval has the same five steps, and again you just need to use the correct conditions and calculations. So this is why it is so important to figure out what analysis you should conduct.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Two Sample Inference</span>"
    ]
  },
  {
    "objectID": "Regression.html",
    "href": "Regression.html",
    "title": "10  Regression",
    "section": "",
    "text": "10.1 Regression\nWhen comparing different variables, two questions come to mind: “Is there a relationship between two variables?” and “How strong is that relationship?” These questions can be answered using regression and correlation. Regression answers whether there is a relationship, (again this book will explore linear relationships only) and correlation answers how strong the linear relationship is. The variable that is used to explain the change (or variability) in the other variable is called the explanatory variable or predictor variable, while the variable whose variability is being explained is called the response variable. In general, variables that help to explain the changes in a response variable are known as covariates. To introduce the concepts of regression and correlation, it is easier to look at a set of data.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#regression",
    "href": "Regression.html#regression",
    "title": "10  Regression",
    "section": "10.2 Correlation",
    "text": "10.1.1 Example: Determining if there is a Relationship\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is one, we explore a dataset of 227 beers, including their alcohol content and their calorie counts (Find Out How Many Calories in Beer?, 2019). Table 10.1 shows the first five rows of the dataset.\n\n\n\n\nTable 10.1: Alcohol and Calorie Content in Beer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nbeer\nbrewery\nlocation\nalcohol\ncalories\ncarbs\n\n\n\n\nAmerican Amber Lager\nStraub Brewery\ndomestic\n0.04\n136\n10.5\n\n\nAmerican Lager\nStraub Brewery\ndomestic\n0.04\n132\n10.5\n\n\nAmerican Light\nStraub Brewery\ndomestic\n0.03\n96\n7.6\n\n\nAnchor Porter\nAnchor\ndomestic\n0.06\n209\nNA\n\n\nAnchor Steam\nAnchor\ndomestic\n0.05\n153\n16.0\n\n\nAnheuser Busch Natural Light\nAnheuser Busch\ndomestic\n0.04\n95\n3.2\n\n\n\n\n\n\n\n\n\nClick to expand the box below to see instructions to import and view the beer_data dataset.\n\n\n\n\n\n\n Importing beer_data to Rguroo\n\n\n\n\n\n\nOpen the Data toolbox in Rguroo.\n\nFrom the Data Import dropdown, select Dataset Repository.\n\nIn the top search box, type kozak, then select the Statistics Using Technology – Kozak repository.\n\nIn the middle search box, type beer, then select the beer_data dataset that appears in the lower panel.\n\nClick the Import button to import the dataset into your Rguroo account.\nClick Close to exit the dialog.\n\nTo view the dataset, double-click the dataset name “beer_data” in the Data toolbox list.\n\n\n\n\nCode book for data frame Beer\nDescription Collection of the most popular beers from large breweries. The data is of the calories, carbs and alcohol of a specific beer. The data is shown for a 12 ounce serving. The collection includes both domestic and import beer. For the imported beers the information is per 12 oz. serving even though many imports come in pints.\nThis dataset contains the following columns:\nbeer: The name of the beer.\nbrewery: the brewery that brews the beer.\nlocation: whether the beer is brewed in the U.S. (domestic) or brewed in another country (import).\nalcohol: the alcohol content of the beer.\ncalories: the number of calories in the beer.\ncarbs: the amount of carbohydrates in the beer (g).\nSource: Find Out How Many Calories in Beer? (n.d.). Retrieved July 21, 2019, from \nReferences: (Find Out How Many Calories in Beer?, 2019)\n\n10.1.1.1 Solution\nTo aid in figuring out if there is a relationship, it helps to draw a scatterplot of the data. First, it is helpful to state the variables, and since variables in an algebra class are typically represented as \\(x\\) and \\(y\\), those labels will be used here. It helps to state which variable is \\(x\\) (explanatory or predictor) and which is \\(y\\) (response).\nState variables\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nClick to expand the box below see how to create the scatterplot of calories vs. alcohol content.\n\n\n\n\n\n\n Creating Scatterplot of Calories vs. Alcohol Content\n\n\n\n\n\nBefore you begin: Make sure you have already imported the beer_data dataset into your Rguroo account, as shown here.\n\nOpen the Plots toolbox in Rguroo.\n\nOpen the Create Plot dropdown and select Scatterplot. This opens the Scatterplot dialog.\n\nIn the Scatterplot dialog, choose the beer_data dataset from the Dataset dropdown.\n\nChoose the alcohol variable from the Predictor (x) dropdown.\n\nChoose the calories variable from the Response (y) dropdown.\n\n(Optional) In the Label section of the dialog:\n\nEnter Calories versus Alcohol Content in Beer in the Title textbox.\n\nEnter Alcohol Content in the X-Axis textbox.\n\nEnter Number of Calories in the Y-Axis textbox.\n\n\nClick the preview icon  to see a preview of the scatterplot.\nIn the  textbox, enter the name AlcoholCalorieScatter for the scatterplot. Click the button to save the scatterplot to your Rguroo account.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nScatterplot dialog showing settings for plotting alcohol content versus calories.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.1: Calories versus Alcohol Content in Beer\n\n\n\n\n\nThe scatter plot shown in Figure 10.1 looks fairly linear.\nTo find the equation for the linear relationship, the process of regression is used to determine the line that best fits the data (sometimes called the best-fitting line). The process is to draw a line through the data and measure the vertical distance from each point to the line. These distances are called the residuals. The regression line is the line that makes the sum of the square of the residuals as small as possible, which is why it is sometimes called the least squares line. The regression line on the scatter plot is displayed in Figure 10.2.\n\nClick to expand the box below to see how to add a regression line to the scatterplot.\n\n\n\n\n\n\n Adding Regression Line to a Scatterplot\n\n\n\n\n\nBefore you begin: Ensure you have created the scatterplot of calories vs. alcohol content in your Rguroo account, as shown here.\n\nOpen the Rguroo tab containing your scatterplot of calories vs. alcohol content by double-clicking the name AlcoholCalorieScatter in the Plots toolbox list.\nOpen the Basics dialog by clicking the  button.\n\nIn the Superimpose section of the dialog, select the checkbox LS Line.\n\nClick the preview icon  to see a preview of the scatterplot with the regression line.\nClick the  button to save the updated scatterplot as AlcoholCalorieScatterLS.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nScatterplot dialog for adding a least-squares line to a plot of alcohol content versus calorie counts\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.2: Scatter Plot of Beer Data with Regression Line\n\n\n\n\n\n\n10.1.2 Find the regression equation (also known as best fitting line or least squares line)\nGiven a collection of paired sample data, the regression equation is \\(\\hat{y}=mx+b\\), where the slope = \\(m\\) and \\(y\\)-intercept = \\((0,b)\\).\nAs we will show, to find the linear model using Rguroo, we use the Linear Regression function in the Analytics toolbox.\nThe residuals are the difference between the actual observed values of the response and the corresponding estimated (predicted) values, namely \\(\\text{residual} = y - \\hat{y}\\).\nThe independent variable (the \\(x\\)-value) corresponds to the explanatory or predictor variable, the variable used to predict or explain changes in another variable. The dependent variable (the \\(y\\)-value) corresponds to the response variable—the variable whose values change in relation to the predictor. In our example, alcohol content is the predictor (independent, \\(x\\)) variable, and the number of calories is the response (dependent, \\(y\\)) variable, since it makes more sense to predict calories from alcohol content rather than vice versa.\n\nAssumptions for the Validity of the Regression Equation and Line:\n\nRandomness: The set of ordered pairs is a random sample from the population of all such possible pairs.\nLinearity: For a given \\(x\\)-value, the distribution of \\(y\\)-values has a mean that lies on the least squares line.\n\nThe assumption of linearity is essential. If the relationship between the two variables is not linear, then a regression line will not fit the data well. To check linearity, we look at the scatterplot: a roughly straight-line pattern suggests the assumption is reasonable, while a curved pattern suggests it is not. We can also examine the so called residual-versus-fit plot. If the residuals are randomly scattered around the horizontal line \\(y = 0\\) on this plot, the linearity assumption is likely valid. If the residuals show a curved pattern, then the assumption of linearity is likely violated.\n\nAssumptions for the validity of inference:\n\nConstant Variance: The variance of the \\(y\\)-values should be the same across all \\(x\\)-values.\nNormality: The \\(y\\)-values for each fixed value of \\(x\\) should follow a normal distribution.\n\nThe assumptions of constant variance and normality are primarily needed for the validity of inference in regression. Inference includes procedures such as constructing confidence intervals and carrying out hypothesis tests. Just as with the confidence intervals and hypothesis tests for means and proportions that you are already familiar with, we can construct confidence intervals and tests of hypotheses for the slope and intercept of the regression line, as well as for predicted values \\(\\hat{y}\\). This we will cover in Section Section 10.3.\nTo check the constant-variance, we examine the residual-versus-fit plot: if the residuals are scattered with roughly the same vertical spread around the horizontal line \\(y = 0\\) across all fitted-values, the assumption is reasonable. A funnel-shaped pattern suggests that unequal variances.\nTo check normality, we look at a histogram or a normal probability plot of the residuals. The histogram should appear bell-shaped, and in a normal probability plot the residuals should fall roughly along a straight line; pronounced curvature or outliers indicate that the normality assumption may not be satisfied.\n\n10.1.3 Example: Find the Equation of the Regression Line\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine if there is one, we examine the sample of beer’s alcohol content and calories, portion of which is shown in Table 10.1 (Find Out How Many Calories in Beer?, 2019).\n\nFind the regression equation between alcohol content and calories.\nUse the regression equation to find the number of calories when the alcohol content is 7%.\nUse the regression equation to find the number of calories when the alcohol content is 14%.\n\n\n10.1.3.1 Solution\n\nFind the regression equation between alcohol content and calories.\n\nState random variables\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nClick to expand the box below to see how to obtain the regression equation.\n\n\n\n\n\n\n Finding the Regression Equation for the Beer Data\n\n\n\n\n\nBefore you begin: Make sure you have already imported the beer_data dataset into your Rguroo account, as shown here.\n\nOpen the Analytics toolbox in Rguroo.\n\nOpen the Analysis dropdown and select Linear Regression → Simple Regression. This opens the Simple Linear Regression dialog.\n\nFrom the Dataset dropdown, choose the beer_data dataset.\n\nFrom the Predictor (x) dropdown, choose the alcohol variable.\n\nFrom the Response (y) dropdown, choose the calories variable.\n\nClick the preview icon  to view a preview of the regression results.\nClick the  button to save the regression results as AlcoholCalorieRegress.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nSimple Linear Regression dialog in Rguroo\n\n\n\n\n\n\n\n\n\n\n\nRguroo output: Linear Regression for Beer Data\n\n\n\n\n\n\n \n\n\n\n\n\n\n\nThe following is a portion of the output from Rguroo for this regression analysis.\n\n\n\n\nTable 10.2: Data and model summary for Alcohol Content versus Calories\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis table shows various information about the regression analysis. In particular, the last row, labeled “Equation of Least Squares Line,” gives the estimates for the slope and the \\(y\\)-intercept. Note that the equation states calories in place of \\(y\\) and alcohol in place of \\(x\\).\nFrom this, rounding the values, you can see that the y-intercept is 14.5 and the slope is 2672.4. So the regression equation is \\(\\hat{y}=2672.4x+14.5\\).\nRemember, this is an estimate for the true regression equation. A different sample would produce a different estimate.\nConditions check:\n\nRandomness: A random sample of alcohol content and calories was taken.\nCheck: There is no guarantee that this was a random sample. The data was collected off of a website, and the website does not say how the data was obtained. However, it is a collection of most popular beers from large breweries, so it may be alright that it isn’t a random sample.\nLinearity: For a given \\(x\\)-value, the distribution of \\(y\\)-values has a mean that lies on the least squares line.\n\nCheck:\nWe examine two graphs to check this condition: the scatter plot of \\(y\\) versus \\(x\\) and the residual-versus-fit plot.\n\nFigure 10.1 shows the scatterplot of calories versus alcohol content. This graph is also shown as part of the default output of the Linear Regression function in Rguroo. This scatterplot looks fairly linear, so this condition appears to be satisfied.\nFigure 10.3 shows the residual-versus-fit plot, which is also part of the Linear Regression function output in Rguroo. The points on this plot are obtained by calculating the fitted values and the residuals for each data point. For each \\(x\\)-value, a predicted value for \\(y\\) is computed using the equation of the least squares line; this predicted value is called the fitted value and is denoted by \\(\\hat{y}\\). The residual is then found by subtracting the fitted value from the observed \\(y\\)-value:\n\n\\[\n\\mbox{residual} = y - \\hat{y}.\n\\] Fortunately, Rguroo calculates these automatically for you.\nClick to expand the box below to see how to obtain the fitted (predicted) values and the residuals.\n\n\n\n\n\n\n Obtaining Residuals for the Beer Data\n\n\n\n\n\nBefore you begin: Continue with the Rguroo instructions shown here for obtaining the regression equation.\n\nOpen the Rguroo tab containing your regression fit by double-clicking the name AlcoholCalorieRegress in the Analytics toolbox list.\n\nClick the  button to open the Simple Linear Regression dialog.\nSelect the checkbox Predictions & Residuals.\nClick the preview icon  to see a preview of the regression results with residuals.\nClick the  button to save the updated regression results as AlcoholCalorieResiduals.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nSimple Linear Regression dialog: Obtaining Residuals\n\n\n\n\n\n\n\n\nClick to expand the box below to see the predicted (fitted) values and the residuals for the first 20 cases of the beer data.\n\n\n\n\n\n\nResiduals for Alcohol and Calorie Content in Beer\n\n\n\n\n\n\n\n\n\nTable 10.3\n\n\n\n\n\n\nResiduals for Alcohol and Calorie Content in Beer from Rguroo\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 10.4: Residuals for Alcohol and Calorie Content in Beer\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.3: Residuals versus Fitted Values Plot for Beer Data\n\n\n\n\n\nThe points shown on the residual-versus-fit plot Figure 10.3 appear to be randomly scattered around the horizontal line \\(y = 0\\). There does not appear to be any curved pattern, so this condition appears to be satisfied.\n\nConstant Variance: The variance of the \\(y\\)-values should be the same across all \\(x\\)-values.\n\nLooking again at the residual-versus-fit plot Figure 10.3, the vertical spread of the residuals appears to be roughly the same across all fitted-values. There does not appear to be a funnel-shaped pattern, so this condition appears to be satisfied.\n\nNormality: The \\(y\\)-values for each fixed value of \\(x\\) should follow a normal distribution.\n\nTo check this assumption, we examine the normal probability plot of the residuals shown in Figure 10.4, which is also part of the Linear Regression function output in Rguroo. In this plot, the residuals fall roughly along a straight line, so this condition appears to be satisfied. More specifically, all points lie within the confidence bands ( shown as red dotted curves), and there is no pronounced curvature or outliers.\n\n\n\n\n\n\n\n\nFigure 10.4: Normal Probability Plot for Beer Data\n\n\n\n\n\n\nUse the regression equation to find the number of calories when the alcohol content is 7.00%.\n\nTo make a prediction, we substitute the given alcohol content (expressed as a decimal) into the regression equation and compute the corresponding fitted value. In this example, we want to predict the number of calories for a beer with 7.00% alcohol content. Since 7.00% expressed as a decimal is 0.07, we substitute \\(x = 0.07\\) into the regression equation: \\[\n\\hat{y} = 2672 \\times 0.07 + 14.3 = 201.34.\n\\] This value, 201.34, is the predicted number of calories for a beer whose alcohol content is 7%. So, if you are drinking a beer that has 7.00% alcohol content, then it probably has close to 200 calories.\nConsidering all the data, the mean number of calories is 154.5. However, for beers with 7% alcohol content, the predicted value of 201 calories is a more appropriate estimate because the beers in the dataset with 7% alcohol all have between 160 and 231 calories. In this case, the regression equation provides a better estimate than simply using the overall mean.\n\nUse the regression equation to find the number of calories when the alcohol content is 14%.\n\n\\[\\hat{y}=2672*0.14+14.3=388.38\\]\nYou can obtain predicted values in Rguroo. For example, to get the predicted values for alcohol contents of 7.00% and 14.00%, enter 0.07 and 0.14 (separated by a comma) in the textbox labeled Predict at new x value(s) in the Simple Linear Regression dialog in Rguroo, as shown in Figure 10.5 below.\n\n\n\n\n\n\n\n\nFigure 10.5: Predicting Calories for Alcohol Contents of 7.00% and 14.00% in Rguroo\n\n\n\n\n\nThe predicted values output from Rguroo is shown in Figure 10.6 below.\n\n\n\n\n\n\n\n\nFigure 10.6: Rguroo Output Showing Predicted Calories for Alcohol Contents of 7.00% and 14.00%\n\n\n\n\n\nIf you are drinking a beer that is 14% alcohol content, then it probably has close to 388 calories. Since, on average, a 12% alcohol beer has 330 calories, you might expect a 14% beer to have more calories than this. However, the maximum alcohol content in our dataset is 12%. The estimate for a 14% beer is therefore an example of extrapolation, because it is a prediction for an \\(x\\)-value outside the range of the data used to fit the model. It is not a good idea to predict values that fall far outside the range of the original data, because we cannot be sure that the regression relationship remains valid beyond the observed \\(x\\)-values.\nNotice that 7.00% alcohol is within the range of the original \\(x\\)-values. Predicting a value for an \\(x\\)-value inside the observed range is called interpolation. The 14.00% value is outside that range, and predicting for an \\(x\\)-value outside the original data is called extrapolation. When you interpolate, you can usually feel confident that the predicted value will be close to the true value because the model was created using data within that range and is intended to describe relationships only there. When you extrapolate, however, you cannot be sure the prediction is close to the true value, because the relationship may change for \\(x\\)-values beyond those observed when the model was created.\n\nAnother example of extrapolation is the relationship between age and height. This relationship is approximately linear only during middle childhood, roughly between ages 2 and 10. Suppose we model height (in inches) as a linear function of age (in years) using the equation\nHeight = 30 + 2.5 × Age.\nFor example, to predict the height of a child who is 6 years old (an interpolation because 6 is within the 2–10 range), we substitute Age = 6:\nHeight = 30 + 2.5 × 6 = 45 inches.\nHowever, using the same model to predict height at age 30 represents an extrapolation, since age 30 is far outside the range where the linear relationship holds. Substituting Age = 30 gives:\nHeight = 30 + 2.5 × 30 = 105 inches,\na clearly unrealistic value. This illustrates why extrapolation can lead to misleading conclusions.\nWhat does the slope mean in the context of this problem?\nThe calories increase by 26.72 for every 1% increase in alcohol content.\nThe \\(y\\)-intercept in many cases is meaningless. In this case, it means that if a drink has 0 alcohol content, then it would have 14.3 calories which maybe a reasonable value. However, consider the model for height versus age discussed above. The \\(y\\)-intercept is 30 inches, which would be the predicted height of a newborn baby (age 0). This is not a reasonable value since newborn babies are typically about 20 inches long. So in this case, the \\(y\\)-intercept is not meaningful.\n\nConsider the residuals again. According to the data, a beer with 7.0% alcohol has between 160 and 231 calories. The predicted value is 201 calories. This variation means that the actual value was between 40 calories below and 30 calories above the predicted value. That isn’t that far off. Some of the actual values differ by a large amount from the predicted value. This is due to variability in the response variable. The larger the residuals the less the model explains the variability in the response variable. There needs to be a way to calculate how well the model explains the variability in the response variable. This will be explored in the next section.\nOne last thing that you may wonder is if imported beer has a different number of calories than domestic beer. The location (imported versus domestic) is a covariate, a third variable that may affects the calorie content. A covariate that is a categorical variable is referred to as a factor. You can create a scatterplot that displays the data separately for each category of this covariate (factor).\n\nClick to expand the box below to see how to create a scatterplot of calories vs. alcohol content separated by location.\n\n\n\n\n\n\n Creating Scatterplot of Calories vs. Alcohol Content by Location\n\n\n\n\n\nBefore you begin: Make sure you have already imported the beer_data dataset into your Rguroo account, as shown here.\n\nOpen the Plots toolbox in Rguroo.\n\nOpen the Create Plot dropdown and select Scatterplot. This opens the Scatterplot dialog.\n\nIn the Scatterplot dialog, choose the beer_data dataset from the Dataset dropdown.\n\nChoose the alcohol variable from the Predictor (x) dropdown.\n\nChoose the calories variable from the Response (y) dropdown.\nChoose the location variable from the Factor dropdown.\nIn the superimpose section of the dialog, select the checkbox LS Line by Factor.\n\n(Optional) In the Label section of the dialog:\n\nEnter Calories versus Alcohol Content in Beer by Location in the Title textbox.\n\nEnter Alcohol Content in the X-Axis textbox.\n\nEnter Number of Calories in the Y-Axis textbox.\n\n\nClick the preview icon  to see a preview of the scatterplot.\nIn the  textbox, enter the name AlcoholCalorieLocationScatter for the scatterplot. Click the button to save the scatterplot to your Rguroo account.\n\n\n\n\n\n\n\nClick here to see the Rguroo dialog\n\n\n\n\n\n\n\n\nScatterplot dialog showing settings for plotting alcohol content versus calories by location, including least square lines.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10.7: Calories versus Alcohol Content of Beer separated by Location\n\n\n\n\n\nFigure 10.7 shows the scatterplot of calories versus alcohol content separated by location (imported versus domestic). The least squares lines for each location are also shown. Looking at the scatterplot, there doesn’t appear to be an affect from domestic or import. This is what is nice about scatter plots, You can visually see a possible relationships.\n\n10.1.4 Homework for Regression Section\nFor each problem, state the variables. The datasets in this section are used in the homework for sections 10.2 and 10.3 also.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected and are in Table 2.15 (“Prediction of height” 2013). Create a scatter plot and find a regression equation between the height of a person and the length of their metacarpal. Then use the regression equation to find the height of a person for a metacarpal length of 44 mm and for a metacarpal length of 55 mm. Which height that you calculated do you think is closer to the true height of the person? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name height_vs_metacarp.\nCode book for Data Frame Metacarpal is below Table 2.15.\n\n\nTable 2.16 contains the value of the house and the amount of rental income in a year that the house brings in (“Capital and rental” 2013). Create a scatter plot and find a regression equation between house value and rental income. Then use the regression equation to find the rental income of a house worth $230,000 and for a house worth $400,000. Which rental income that you calculated do you think is closer to the true rental income? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name house.\nCode book for Data Frame House is below Table 2.16.\n\n\nThe World Bank collects information on the life expectancy of a person in each country (“Life expectancy at” 2013) and the fertility rate per woman in the country (“Fertility rate” 2013). The Data Frame for countries for the year 2011 are in Table 2.17. Create a scatter plot of the Data Frame and find a linear regression equation between fertility rate and life expectancy in 2011. Then use the regression equation to find the life expectancy for a country that has a fertility rate of 2.7 and for a country with fertility rate of 8.1. Which life expectancy that you calculated do you think is closer to the true life expectancy? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name Life_exp_fert_rate.\nCode book for Data Frame Fertility is below Table 2.17.\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The Data Frame for the countries where this information is available in Table 2.18. Create a scatter plot of the Data Frame and find a regression equation between percentage spent on health expenditure and the percentage of women receiving prenatal care. Then use the regression equation to find the percent of women receiving prenatal care for a country that spends 5.0% of GDP on health expenditure and for a country that spends 12.0% of GDP. Which prenatal care percentage that you calculated do you think is closer to the true percentage? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name fertility_prenatal.\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\n\nThe height and weight of baseball players are in Table 10.5 (“MLB heightsweights” 2013). Create a scatter plot and find a regression equation between height and weight of baseball players. Then use the regression equation to find the weight of a baseball player that is 75 inches tall and for a baseball player that is 68 inches tall. Which weight that you calculated do you think is closer to the true weight? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name baseball.\n\n\n\n\nTable 10.5: Heights and Weights of Baseball Players\n\n\n\n\n\n\nplayer\nheight\nweight\n\n\n\n\n1\n65.78\n112.99\n\n\n2\n71.52\n136.49\n\n\n3\n69.40\n153.03\n\n\n4\n68.22\n142.34\n\n\n5\n67.79\n144.30\n\n\n6\n68.70\n123.30\n\n\n\n\n\n\n\n\nCode book for Data Frame Baseball\nDescription\nThe heights and weights of MLB players.\nFormat\nThis Data Frame contains the following columns:\nPlayer: Player in the sample\nheight: height of baseball player (inches)\nweight: weight of baseball player (pounds)\nSource MLB heightsweights. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights\nReferences SOCR Data Frame of MLB Heights Weights from UCLA.\n\n\nDifferent species have different body weights and brain weights are in Table 10.6. (“Brain2bodyweight” 2013). Create a scatter plot and find a regression equation between body weights and brain weights. Then use the regression equation to find the brain weight for a species that has a body weight of 62 kg and for a species that has a body weight of 180,000 kg. Which brain weight that you calculated do you think is closer to the true brain weight? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name body.\n\n\n\n\nTable 10.6: Body Weights and Brain Weights of Species\n\n\n\n\n\n\nspecies\nbodyweight\nbrainweight\nbrainbodyproportion\n\n\n\n\nNewborn_Human\n3.20\n0.3749848\n0.1171828\n\n\nAdult_Human\n73.00\n1.3499816\n0.0184929\n\n\nPithecanthropus_Man\n70.00\n0.9250109\n0.0132144\n\n\nSquirrel\n0.80\n0.0076204\n0.0095254\n\n\nHamster\n0.15\n0.0014061\n0.0093742\n\n\nChimpanzee\n50.00\n0.4199812\n0.0083996\n\n\n\n\n\n\n\n\nCode book for Data Frame Body\nDescription\nThe body weight, brain weight, and brain/body proportion of different species of animals.\nFormat\nThis Data Frame contains the following columns:\nspecies: species of animal\nbodyweight: the body weight of the species (kg)\nbrainweight: the brain weight of the species (kg)\nbrainbodyproportion: the ratio of brain weight to body weight of the species\nSource Brain2bodyweight. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Brain2BodyWeight\nReferences SOCR Data of species body weights and brain weights from UCLA.\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (“Data hotdogs” 2013) The Data Frame are in Table 9.9. Create a scatter plot and find a regression equation between amount of calories and amount of sodium. Then use the regression equation to find the amount of sodium a hot dog has if it is 170 calories and if it is 120 calories. Which sodium level that you calculated do you think is closer to the true sodium level? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name hotdog.\nCode book for data frame Hotdog is below Table 9.9.\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in Table 10.7 (“OECD economic development” 2013). Create a scatter plot and find a regression equation between percent of labor force in agriculture and per capita income. Then use the regression equation to find the per capita income in a country that has 21 percent of labor in agriculture and in a country that has 2 percent of labor in agriculture. Which per capita income that you calculated do you think is closer to the true income? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name agriculture.\n\n\n\n\nTable 10.7: Percent of Labor in Agriculture and Per Capita Income for European Countries\n\n\n\n\n\n\ncountry\npercapita\nagriculture\nindustry\nservices\n\n\n\n\nSWEEDEN\n1644\n14\n53\n33\n\n\nSWITZERLAND\n1361\n11\n56\n33\n\n\nLUXEMBOURG\n1242\n15\n51\n34\n\n\nU. KINGDOM\n1105\n4\n56\n40\n\n\nDENMARK\n1049\n18\n45\n37\n\n\nW. GERMANY\n1035\n15\n60\n25\n\n\n\n\n\n\n\n\nCode book for Data Frame Agriculture\nDescription\nThe per capita income and percent in different industries in European countries\nFormat\nThis Data Frame contains the following columns:\ncountry: country in Europe\npercapita: per captia income\nagriculture: percentage of workforce in agriculture\nindustry: percentage of workforce in industry\nservices: percentage of workforce in services\nSource OECD economic development. (2013, December 04). Retrieved from http://lib.stat.cmu.edu/DASL/Datafiles/oecdat.html\nReferences Data And Story Library\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are in the dataset cancer_1 (“Smoking and cancer” 2013). The first six rows of the data is shown in Table 10.8. Create a scatter plot and find a regression equation between number of cigarettes smoked and number of deaths from bladder cancer. Then use the regression equation to find the number of deaths from bladder cancer when the cigarette sales were 20 per capita and when the cigarette sales were 6 per capita. Which number of deaths that you calculated do you think is closer to the true number? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cancer_1.\n\n\n\n\nTable 10.8: Number of Cigarettes and Number of Bladder Cancer Deaths\n\n\n\n\n\n\nstate\ncig\nbladder\nlung\nkidney\nleukemia\n\n\n\n\nAL\n18.20\n2.90\n17.05\n1.59\n6.15\n\n\nAZ\n25.82\n3.52\n19.80\n2.75\n6.61\n\n\nAR\n18.24\n2.99\n15.98\n2.02\n6.94\n\n\nCA\n28.60\n4.46\n22.07\n2.66\n7.06\n\n\nCT\n31.10\n5.11\n22.83\n3.35\n7.20\n\n\nDE\n33.60\n4.78\n24.55\n3.36\n6.45\n\n\n\n\n\n\n\n\nCode book for Data Frame Cancer\nDescription This data frame contains the number of cigarette sales (per Capita), number of cancer Deaths (per 100 Thousand) from bladder, lung, kidney, and leukemia.\nFormat\nThis Data Frame contains the following columns:\nstate: state in US\ncig: the number of cigarette sales (per capita)\nbladder: number of deaths per 100 thousand from bladder cancer\nlung: number of deaths per 100 thousand from lung cancer\nkidney: number of deaths per 100 thousand from kidney cancer\nleukemia: number of deaths per 100 thousand from leukemia\nSource Smoking and cancer. (2013, December 04). Retrieved from http://lib.stat.cmu.edu/DASL/Datafiles/cigcancerdat.html\nReferences Data And Story Library\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and the first six rows are shown in Table 10.9 (“us auto mileage,” 2019). Create a scatter plot and find a regression equation between weight of cars and mileage. Then use the regression equation to find the mileage on a car that weighs 3800 pounds and on a car that weighs 2000 pounds. Which mileage that you calculated do you think is closer to the true mileage? Why?\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cars.\n\n\n\n\nTable 10.9: Weights and Mileages of Cars\n\n\n\n\n\n\nmake\nvol\nhp\nmpg\nsp\nwt\n\n\n\n\nGM/GeoMetroXF1\n89\n49\n65.4\n96\n17.5\n\n\nGM/GeoMetro\n92\n55\n56.0\n97\n20.0\n\n\nGM/GeoMetroLSI\n92\n55\n55.9\n97\n20.0\n\n\nSuzukiSwift\n92\n70\n49.0\n105\n20.0\n\n\nDaihatsuCharade\n92\n53\n46.5\n96\n20.0\n\n\nGM/GeoSprintTurbo\n89\n70\n46.2\n105\n20.0\n\n\n\n\n\n\n\n\nCode book for Data Frame Cars\nDescription Variation in gasoline mileage among makes and models of automobiles is influenced substantially by the weight and horsepower of the vehicles. When miles per gallon and horsepower are transformed to logarithms, the linearity of the regression is improved. A negative second order term is required to fit the logarithmic mileage to weight relation. If the logged variables are standardized, the coefficients of the first order terms indicate the standard units change in log mileage per one standard unit change in the predictor variable at the (logarithmic) mean. This change is constant in the case of mileage to horsepower, but not for mileage to weight. The coefficient of the second order weight term indicates the change in standardized slope associated with a one standard deviation increase in the logarithm of weight.\nFormat\nThis Data Frame contains the following columns:\nmake: the type of car\nvol: cubic feet of cab space\nhp: engine horsepower\nnpg: the average mileage of the car\nsp: top speed (mph)\nwt: the weight of the car (100 pounds)\nSource (n.d.). Retrieved July 21, 2019, from https://www3.nd.edu/~busiforc/handouts/Data and Stories/regression/us auto mileage/usautomileage.html\nReferences R.M. Heavenrich, J.D. Murrell, and K.H. Hellman, Light Duty Automotive Technology and Fuel Economy Trends Through 1991, U.S. Environmental Protection Agency, 1991 (EPA/AA/CTAB/91-02).\n\n** **\n\n\n10.2 Correlation\n\nA correlation exists between two variables when the values of one variable are associated with the values of the other variable. Correlation is a word used in everyday life, and many people think they understand what it means. What it really means, when stated as a number, is a measure of how strong the linear relationship is between two variables. This measure is called the correlation coefficient, or correlation for short. It is also called the Pearson correlation coefficient after Karl Pearson who developed it. There are many types of patterns (relationships) between variables that one can see in data. Common types of patterns are linear, exponential, logarithmic, or periodic. To see these patterns, you can draw a scatterplot of the data. In this chapter, and generally in this course, we mostly deal with linear relationships.\n\n10.2.1 Interpretation of the correlation coefficient\nThe correlation coefficient is a number between −1 and 1 and is usually denoted by the letter \\(r\\). A value of \\(r = 1\\) indicates a perfect positive linear relationship, and a value of \\(r = -1\\) indicates a perfect negative linear relationship. A perfect linear relationship means that all of the data points lie exactly on a straight line.\nThe strength of the linear relationship depends on how close \\(r\\) is to 1 or −1. Values of \\(r\\) close to these extremes indicate a strong linear relationship, while values closer to 0 indicate a weaker linear relationship. A correlation of \\(r = 0\\) means there is no linear relationship between the two variables.\nThe sign of \\(r\\) describes the direction of the relationship. A positive correlation means that as one variable increases, the other variable tends to increase. A negative correlation means that as one variable increases, the other variable tends to decrease.\n\n\n\n10.2.2 Example: Calculating the Linear Correlation Coefficient, \\(r\\)\nHow strong is the positive relationship between alcohol content and the number of calories in a 12-ounce beer? To answer this question, compute the correlation between alcohol content and calories using the beer dataset (“Calories in Beer,” 2011). The first six rows of this dataset are shown in Table 10.1. Use Rguroo to find the correlation coefficient and then interpret its value.\n\n10.2.2.1 Solution\nYou can obtain the correlation coefficient, \\(r\\), in Rguroo from the output of the Linear Regression function. To obtain this value for the beer example, follow the steps for creating a linear regression model here.\nFor these data the correlation coefficient between alcohol content and calories is 0.9036. This value appears in the Data and Model Summary table of the regression function output in the row labeled Pearson Correlation (r); see Table 10.2, and it indicates a strong positive linear relationship between alcohol content and calories, since the value of \\(r\\) is close to 1. Also, since \\(r\\) is positive, it means that as alcohol content increases, the number of calories tends to increase as well.\n\n\n\n\n\n10.2.3 Coefficient of Determination, \\(R^2\\)\nCorrelation is related to another measure that explains how much of the variability in the response variable is explained by the explanatory variable. The variability in a response variable can be due to many factors. Some of the variability is explained by the explanatory variable and some is due to other factors. For example, in the beer example, some of the variability in calories is explained by alcohol content and some is due to other factors, such as other ingredients in the beer. There is a way to measure how much of the variability in the response variable is explained by the explanatory variable. This measure is called the coefficient of determination, denoted by \\(R^2\\) (read R-squared), and its value is the square of the correlation.\n\nAs an example, using the beer data, we want to understand how much of the variation in calories is explained by alcohol content and how much is due to other factors, such as ingredients. For instance, two beers can have the same alcohol content but different numbers of calories because they contain different ingredients. A linear regression of calories on alcohol content explains the part of the variation in calories that is associated with alcohol content. The remaining variation is due to other factors and random differences between beers, and is not explained by the regression. Together, the explained and unexplained parts account for all of the variability in calories:\n(total variation) = (explained variation) + (unexplained variation)\nThe proportion of the variation that is explained by the model is \\(R^2=\\frac{\\text{explained variation}}{\\text{total variation}}\\).\nThis is known as the coefficient of determination.\n\n\n10.2.4 Example: Find the coefficient of determination \\(R^2\\)\nHow well does the alcohol content of a beer explain the variability in the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1.\n\n10.2.4.1 Solution\nYou can obtain the coefficient of determination, \\(R^2\\), in Rguroo from the output of the Linear Regression function. To obtain the coefficient of determination for the beer example, follow the steps for creating a linear regression model here.\nFor these data, the coefficient of determination between alcohol content and calories is 0.8164. This value appears in the Data and Model Summary table of the regression function output in the row labeled Coefficient of Determination (R-squared); see Table 10.2. This value indicates that 81.64% of the variability in calories is explained by alcohol content, while the remaining 18.36% is due to other factors.\nA high coefficient of determination (for example, above 0.8) means that only a small portion of the variability is unexplained. In this situation, alcohol content alone explains most of the variability in calories, so there may be little need to include additional explanatory variables to improve the model.\n\nThere is a relationship between correlation coefficient \\(r\\) and the coefficient of determination \\(R^2\\), namely \\(r = \\pm\\sqrt{R^2}\\). The question is which sign, plus (\\(+\\)) or minus (\\(-\\)), should be used. If the relationship is positive (that is, the slope of the regression line is positive), use the \\(+\\) sign. If the relationship is negative (the slope is negative), use the \\(-\\) sign. In other words, assign to \\(r\\) the same sign as the slope of the linear regression line.\n\n\n\n\n10.2.5 Causation\nOne common mistake people make is to assume that because there is a correlation between two variables, then one variable causes the other. This is usually not the case. That would be like saying the amount of alcohol in the beer causes it to have a certain number of calories. However, fermentation of sugars is what causes the alcohol content. The more sugars you have, the more alcohol can be made, and the more sugar, the higher the calories. It is actually the amount of sugar that causes both. Do not confuse the idea of correlation with the concept of causation. Just because two variables are correlated does not mean one causes the other to happen. However, the new theory is showing that if you have a relationship between two variables and a strong correlation, and you can show that there are no other variables that could explain the change, then you can show causation. This is how doctors have shown that smoking causes kidney cancer. Just realize that proving that one caused the other is a difficult process, and causation should not be just assumed.\n\n\n10.2.6 Example: Correlation Versus Causation\n\n\nA study showed a strong linear correlation between per capita beer consumption and teacher’s salaries. Does giving a teacher a raise cause people to buy more beer? Does buying more beer cause teachers to get a raise?\n\nThere is probably some other factor causing both of them to increase at the same time. Think about this: In a town where people have little extra money, they won’t have money for beer and they won’t give teachers raises. In another town where people have more extra money to spend it will be easier for them to buy more beer and they would be more willing to give teachers raises.\n\nA study shows that there is a correlation between people who have had a root canal and those that have cancer. Does that mean having a root canal causes cancer?\n\nJust because there is positive correlation doesn’t mean that one caused the other. It turns out that there is a positive correlation between eating carrots and cancer, but that doesn’t mean that eating carrots causes cancer. In other words, there are lots of relationships you can find between two variables, but that doesn’t mean that one caused the other.\nRemember a correlation only means a pattern exists. It does not mean that one variable causes the other variable to change.\n\n\n10.2.7 Homework for Correlation Section\nFor each problem, state the random variables.The Data Frame in this section are in section 10.1 and will be used in section 10.3.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected. The data are in the height_vs_metacarp dataset, and the first six rows of the data are shown in Table 2.15 (“Prediction of height,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name height_vs_metacarp.\nCode book for Data Frame Metacarpal is below Table 2.15.\n\n\nTable 2.16 contains the first six values of the house and the amount of rental income in a year that the house brings in (“Capital and rental,” 2013). Use the house dataset to find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name house.\nCode book for Data Frame House is below Table 2.16.\n\n\nThe World Bank collects information on the life expectancy of a person in each country (“Life expectancy at,” 2013) and the fertility rate per woman in the country (“Fertility rate,” 2013). The first siz rows of the Life_exp_fert_rate dataset for countries for the year 2011 are in Table 2.17. Use the dataset to find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name Life_exp_fert_rate.\nCode book for Data Frame Fertility is below Table 2.17.\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The data for the countries where this information is available is in the fertility_prenatal dataset and the first six rows of the data are given in Table 2.18. Find the coefficient of determination and the correlation coefficient between the percentage spent on health expenditure and the percentage of women receiving prenatal care, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name fertility_prenatal.\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\n\nThe height and weight of baseball players are in the baseball dataset and the first six rows of the data are shown in Table 10.5 (“MLB heightsweights,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name baseball.\nCode book for Data Frame Baseball is below Table 10.5.\n\n\nDifferent species have different body weights and brain weights. The dataset body contains data about these two variables, and Table 10.6 shows the first six rows of the dataset (“Brain2bodyweight,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name body.\nCode book for Data Frame Body is below Table 10.6.\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (“Data hotdogs,” 2013). The data are in hotdog dataset with the first six rows shown in Table 9.9. Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name hotdog.\nCode book for data frame Hotdog is below Table 9.9.\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in the dataset agriculture. The first six rows are shon in Table 10.7 (“OECD economic development,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name agriculture.\nCode book for Data Frame Agriculture is Table 10.7.\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are given in teh dataset cancer_1 with the first six rows showing in Table 11.15 (“Smoking and cancer,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cancer_1.\nCode book for Data Frame Cancer is below Table 11.15.\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in cars dataset. The first six rows of the data are shown in Table 10.9 (“us auto mileage,” 2019). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cars.\nCode book for Data Frame Cars is below Table 10.9.\n\n\nThere is a correlation between police expenditure and crime rate. Does this mean that spending more money on police causes the crime rate to decrease? Explain your answer.\nThere is a correlation between tobacco sales and alcohol sales. Does that mean that using tobacco causes a person to also drink alcohol? Explain your answer.\nThere is a correlation between the average temperature in a location and the mortality rate from breast cancer. Does that mean that higher temperatures cause more women to die of breast cancer? Explain your answer.\nThere is a correlation between the length of time a tableware company polishes a dish and the price of the dish. Does that mean that the time a plate is polished determines the price of the dish? Explain your answer.\n\n\n\n\n10.3 Inference for Regression and Correlation\nThe idea behind regression is to find an equation that relates the response variable to the explanatory variables, and then use that equation to predict values of the response variable from values of the explanatory variables. But how do you know how good that estimate is?\nWhen we use a regression equation to make a prediction, the predicted value is only a single number. In practice, however, there is almost always some uncertainty in that prediction. Even if we knew the true relationship between the variables, individual observations would still vary around that relationship due to natural randomness and factors not included in the model.\nBecause of this variability, it is usually more informative to give a range of plausible values for the response variable rather than a single predicted value. A prediction interval provides such a range. It gives an interval of values that is likely to contain the response value for a new observation with specified explanatory variable values, along with a stated level of confidence.\n\n\n10.3.1 Prediction Interval\nUsing the regression equation you can predict the number of calories from the alcohol content. However, you only find one value. The problem is that beers vary a bit in calories even if they have the same alcohol content. It would be nice to have a range instead of a single value. The range is called a prediction interval.\n\n\n10.3.2 Prediction Interval for an Individual \\(y\\)\nGiven the fixed value \\(x\\), a prediction interval for the response at \\(x\\) is \\(\\hat{y}\\pm E\\) where \\(\\hat{y}\\) is the predicted value from the regression equation and \\(E\\) is the margin of error for the prediction.\nRguroo will calculate the prediction interval for you in the Linear Regression function. You just have to give it the value of \\(x\\) that you want to predict for, and tell it you want a prediction interval.\n\n\n10.3.3 Example: Find the Prediction Interval\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Find a 95% prediction interval for the number of calories when the alcohol content is 7.0%.\n\n10.3.3.1 Solution\nTo obtain the prediction interval for the beer example, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, enter the value of alcohol content (0.07) in the textbox labeled Predict at new x value(s). Then click the tab labeled Prediction Interval and select the checkboxes labeled Prediction Interval. These steps are shown in Figure 10.8.\n\n\n\n\n\n\n\n\nFigure 10.8: Rguroo dialog for obtaining prediction interval for calories from alcohol content\n\n\n\n\n\nThe output containing the prediction interval is shown in Figure 10.9.\n\n\n\n\n\n\n\n\nFigure 10.9: Rguroo output showing prediction interval for calories from alcohol content\n\n\n\n\n\n\nThe value 201.592 is the model’s predicted number of calories for a beer with 7.0% alcohol. The prediction interval is shown in the columns labeled Lower Pred Limit (2.5%) and Upper Pred Limit (97.5%). This means we can be 95% confident that the actual number of calories for a beer with 7.0% alcohol lies between 166.7 and 236.5. This range is called a 95% prediction interval.\nYou can obtain prediction intervals for other alcohol contents by entering different values in the textbox labeled Predict at new x value(s) in the Linear Regression dialog box. You can also change the confidence level using the textbox labeled Confidence Level in the Confidence Interval tab.\n\n\n\n10.3.4 Hypothesis Test for Correlation or Slope:\nHow do we decide whether there is really a correlation or a linear relationship between two variables? Can we test whether such a relationship exists? The answer is yes. We can test for correlation, or we can test whether the slope of the regression line is zero.\nThese two tests are equivalent. If there is a correlation between the variables, then the slope of the regression line is not zero. Likewise, if the slope of the regression line is not zero, then there is a correlation between the variables. For this reason, testing for correlation and testing for slope lead to the same conclusion.\nThe steps for performing a hypothesis test for correlation are given below. You can use these steps to test for correlation or to test whether the slope is zero.\n\nState the variables in words.\n\n\\(x\\) = explanatory variable\n\\(y\\) = response variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nOr equivalently\n\\(H_o: \\text{the slope is zero}\\)\n\\(H_a: \\text{the slope is not zero}\\)\nAlso, state \\(\\alpha\\), the significance level for your test.\n\nState and check the conditions for the hypothesis test, as explained in the sections Assumptions for Validity of the Regression Equation and Line and Assumptions for Validity of Inference.\nFind the test statistic and p-value\n\nThis will be calculated by Rguroo.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n10.3.5 Example: Testing the Claim of a Linear Correlation\nIs there a linear relationship, or correlation, between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Test at the 5% level.\n\n10.3.5.1 Solution\n\nState the random variables in words.\n\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test were already checked in Example: Find the Equation of the Regression Line\n\nFind the test statistic and p-value\n\nTo obtain the test statistic and p-value, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, under the Test of Association tab, check the box labeled Slope, and under the method section select Theoretical t-statistic. These steps are shown in Figure 10.10. The default value for the significance level is 0.05, so you do not need to change that value. However, if you want to change it, you can do so in the textbox labeled Significance Level.\n\n\n\n\n\n\n\n\nFigure 10.10: Rguroo dialog for testing slope for calories from alcohol content\n\n\n\n\n\nThe output containing the result of the test shown in Figure 10.11.\n\n\n\n\n\n\n\n\nFigure 10.11: AlcoholCalorieRegress_test_out\n\n\n\n\n\nThe test statistic is the t value in the column labeled Obs t Stat. In this case the t-statistic is 31.634, and the p-value id \\(8.84X10^{-85}\\).\n\nConclusion\n\nReject \\(H_o\\) since the p-value is less than 0.05.\n\nInterpretation\n\nThere is enough evidence to show that there is a correlation between alcohol content and number of calories in a 12-ounce bottle of beer.\n\n\n\n10.3.6 Homework for Inference for Regression and Correlation Section\nFor each problem, state the random variables. The Data Frame in this section are in the homework for section 10.1 and were also used in section 10.2.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected. The data are in the height_vs_metacarp dataset, and the first six rows of the data are shown in Table 2.15 (“Prediction of height,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name height_vs_metacarp.\nCode book for Data Frame Metacarpal is below Table 2.15.\n\nCompute a 99% prediction interval for height of a person with a metacarpal length of 44 mm.\nTest at the 1% level for a correlation or linear relationship between length of metacarpal bone 1 and height of a person.\n\n\n\nTable 2.16 contains the first six values of the house and the amount of rental income in a year that the house brings in (“Capital and rental,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name house.\nCode book for Data Frame House is below Table 2.16.\n\n\nCompute a 95% prediction interval for the rental income on a house worth \\$230,000.\nTest at the 5% level for a correlation or linear relationship between house value and rental amount.\n\n\n\nThe World Bank collects information on the life expectancy of a person in each country (“Life expectancy at,” 2013) and the fertility rate per woman in the country (“Fertility rate,” 2013). The first siz rows of the Life_exp_fert_rate dataset for countries for the year 2011 are in Table 2.17.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name Life_exp_fert_rate.\nCode book for Data Frame Fertility is below Table 2.17.\n\n\n\nCompute a 99% prediction interval for the life expectancy for a country that has a fertility rate of 2.7.\nTest at the 1% level for a correlation or linear relationship between fertility rate and life expectancy.\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The data for the countries where this information is available is in the fertility_prenatal dataset and the first six rows of the data are given in Table 2.18.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name fertility_prenatal.\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\n\n\nCompute a 95% prediction interval for the percentage of woman receiving prenatal care for a country that spends 5.0 % of GDP on health expenditure.\nTest at the 5% level for a correlation or linear relationship between percentage spent on health expenditure and the percentage of women receiving prenatal care.\n\n\nThe height and weight of baseball players are in the baseball dataset and the first six rows of the data are shown in Table 10.5 (“MLB heightsweights,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name baseball.\nCode book for Data Frame Baseball is below Table 10.5.\n\n\n\nCompute a 95% prediction interval for the weight of a baseball player that is 75 inches tall.\nTest at the 5% level for a correlation or linear relationship between height and weight of baseball players.\n\n\nDifferent species have different body weights and brain weights. The dataset body contains data about these two variables, and Table 10.6 shows the first six rows of the dataset (“Brain2bodyweight,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name body.\nCode book for Data Frame Body is below Table 10.6.\n\n\n\nCompute a 99% prediction interval for the brain weight for a species that has a body weight of 62 kg.\nTest at the 1% level for a correlation or linear relationship between body weights and brain weights.\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (“Data hotdogs,” 2013). The data are in hotdog dataset with the first six rows shown in Table 9.9.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name hotdog.\nCode book for data frame Hotdog is below Table 9.9.\n\n\n\nCompute a 95% prediction interval for the amount of sodium a beef hot dog has if it is 170 calories.\nTest at the 5% level for a correlation or linear relationship between amount of calories and amount of sodium.\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in the dataset agriculture. The first six rows are shon in Table 10.7 (“OECD economic development,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name agriculture.\nCode book for Data Frame Agriculture is Table 10.7.\n\n\n\nCompute a 90% prediction interval for the per capita income in a country that has 21 percent of labor in agriculture.\nTest at the 5% level for a correlation or linear relationship between percent of labor force in agriculture and per capita income.\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are given in teh dataset cancer_1 with the first six rows showing in Table 11.15 (“Smoking and cancer,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cancer_1.\nCode book for Data Frame Cancer is below Table 11.15.\n\n\n\nCompute a 99% prediction interval for the number of deaths from bladder cancer when the cigarette sales were 20 per capita.\nTest at the 1% level for a correlation or linear relationship between cigarette smoking and deaths of bladder cancer.\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in cars dataset. The first six rows of the data are shown in Table 10.9 (“us auto mileage,” 2019).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cars.\nCode book for Data Frame Cars is below Table 10.9.\n\n\n\nCompute a 95% prediction interval for the mileage on a car that weighs 3800 pounds.\nTest at the 5% level for a correlation or linear relationship between the weight of cars and mileage.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#correlation",
    "href": "Regression.html#correlation",
    "title": "10  Regression",
    "section": "",
    "text": "A correlation exists between two variables when the values of one variable are associated with the values of the other variable. Correlation is a word used in everyday life, and many people think they understand what it means. What it really means, when stated as a number, is a measure of how strong the linear relationship is between two variables. This measure is called the correlation coefficient, or correlation for short. It is also called the Pearson correlation coefficient after Karl Pearson who developed it. There are many types of patterns (relationships) between variables that one can see in data. Common types of patterns are linear, exponential, logarithmic, or periodic. To see these patterns, you can draw a scatterplot of the data. In this chapter, and generally in this course, we mostly deal with linear relationships.\n\n10.2.1 Interpretation of the correlation coefficient\nThe correlation coefficient is a number between −1 and 1 and is usually denoted by the letter \\(r\\). A value of \\(r = 1\\) indicates a perfect positive linear relationship, and a value of \\(r = -1\\) indicates a perfect negative linear relationship. A perfect linear relationship means that all of the data points lie exactly on a straight line.\nThe strength of the linear relationship depends on how close \\(r\\) is to 1 or −1. Values of \\(r\\) close to these extremes indicate a strong linear relationship, while values closer to 0 indicate a weaker linear relationship. A correlation of \\(r = 0\\) means there is no linear relationship between the two variables.\nThe sign of \\(r\\) describes the direction of the relationship. A positive correlation means that as one variable increases, the other variable tends to increase. A negative correlation means that as one variable increases, the other variable tends to decrease.\n\n\n\n10.2.2 Example: Calculating the Linear Correlation Coefficient, \\(r\\)\nHow strong is the positive relationship between alcohol content and the number of calories in a 12-ounce beer? To answer this question, compute the correlation between alcohol content and calories using the beer dataset (“Calories in Beer,” 2011). The first six rows of this dataset are shown in Table 10.1. Use Rguroo to find the correlation coefficient and then interpret its value.\n\n10.2.2.1 Solution\nYou can obtain the correlation coefficient, \\(r\\), in Rguroo from the output of the Linear Regression function. To obtain this value for the beer example, follow the steps for creating a linear regression model here.\nFor these data the correlation coefficient between alcohol content and calories is 0.9036. This value appears in the Data and Model Summary table of the regression function output in the row labeled Pearson Correlation (r); see Table 10.2, and it indicates a strong positive linear relationship between alcohol content and calories, since the value of \\(r\\) is close to 1. Also, since \\(r\\) is positive, it means that as alcohol content increases, the number of calories tends to increase as well.\n\n\n\n\n\n10.2.3 Coefficient of Determination, \\(R^2\\)\nCorrelation is related to another measure that explains how much of the variability in the response variable is explained by the explanatory variable. The variability in a response variable can be due to many factors. Some of the variability is explained by the explanatory variable and some is due to other factors. For example, in the beer example, some of the variability in calories is explained by alcohol content and some is due to other factors, such as other ingredients in the beer. There is a way to measure how much of the variability in the response variable is explained by the explanatory variable. This measure is called the coefficient of determination, denoted by \\(R^2\\) (read R-squared), and its value is the square of the correlation.\n\nAs an example, using the beer data, we want to understand how much of the variation in calories is explained by alcohol content and how much is due to other factors, such as ingredients. For instance, two beers can have the same alcohol content but different numbers of calories because they contain different ingredients. A linear regression of calories on alcohol content explains the part of the variation in calories that is associated with alcohol content. The remaining variation is due to other factors and random differences between beers, and is not explained by the regression. Together, the explained and unexplained parts account for all of the variability in calories:\n(total variation) = (explained variation) + (unexplained variation)\nThe proportion of the variation that is explained by the model is \\(R^2=\\frac{\\text{explained variation}}{\\text{total variation}}\\).\nThis is known as the coefficient of determination.\n\n\n10.2.4 Example: Find the coefficient of determination \\(R^2\\)\nHow well does the alcohol content of a beer explain the variability in the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1.\n\n10.2.4.1 Solution\nYou can obtain the coefficient of determination, \\(R^2\\), in Rguroo from the output of the Linear Regression function. To obtain the coefficient of determination for the beer example, follow the steps for creating a linear regression model here.\nFor these data, the coefficient of determination between alcohol content and calories is 0.8164. This value appears in the Data and Model Summary table of the regression function output in the row labeled Coefficient of Determination (R-squared); see Table 10.2. This value indicates that 81.64% of the variability in calories is explained by alcohol content, while the remaining 18.36% is due to other factors.\nA high coefficient of determination (for example, above 0.8) means that only a small portion of the variability is unexplained. In this situation, alcohol content alone explains most of the variability in calories, so there may be little need to include additional explanatory variables to improve the model.\n\nThere is a relationship between correlation coefficient \\(r\\) and the coefficient of determination \\(R^2\\), namely \\(r = \\pm\\sqrt{R^2}\\). The question is which sign, plus (\\(+\\)) or minus (\\(-\\)), should be used. If the relationship is positive (that is, the slope of the regression line is positive), use the \\(+\\) sign. If the relationship is negative (the slope is negative), use the \\(-\\) sign. In other words, assign to \\(r\\) the same sign as the slope of the linear regression line.\n\n\n\n\n10.2.5 Causation\nOne common mistake people make is to assume that because there is a correlation between two variables, then one variable causes the other. This is usually not the case. That would be like saying the amount of alcohol in the beer causes it to have a certain number of calories. However, fermentation of sugars is what causes the alcohol content. The more sugars you have, the more alcohol can be made, and the more sugar, the higher the calories. It is actually the amount of sugar that causes both. Do not confuse the idea of correlation with the concept of causation. Just because two variables are correlated does not mean one causes the other to happen. However, the new theory is showing that if you have a relationship between two variables and a strong correlation, and you can show that there are no other variables that could explain the change, then you can show causation. This is how doctors have shown that smoking causes kidney cancer. Just realize that proving that one caused the other is a difficult process, and causation should not be just assumed.\n\n\n10.2.6 Example: Correlation Versus Causation\n\n\nA study showed a strong linear correlation between per capita beer consumption and teacher’s salaries. Does giving a teacher a raise cause people to buy more beer? Does buying more beer cause teachers to get a raise?\n\nThere is probably some other factor causing both of them to increase at the same time. Think about this: In a town where people have little extra money, they won’t have money for beer and they won’t give teachers raises. In another town where people have more extra money to spend it will be easier for them to buy more beer and they would be more willing to give teachers raises.\n\nA study shows that there is a correlation between people who have had a root canal and those that have cancer. Does that mean having a root canal causes cancer?\n\nJust because there is positive correlation doesn’t mean that one caused the other. It turns out that there is a positive correlation between eating carrots and cancer, but that doesn’t mean that eating carrots causes cancer. In other words, there are lots of relationships you can find between two variables, but that doesn’t mean that one caused the other.\nRemember a correlation only means a pattern exists. It does not mean that one variable causes the other variable to change.\n\n\n10.2.7 Homework for Correlation Section\nFor each problem, state the random variables.The Data Frame in this section are in section 10.1 and will be used in section 10.3.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected. The data are in the height_vs_metacarp dataset, and the first six rows of the data are shown in Table 2.15 (“Prediction of height,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name height_vs_metacarp.\nCode book for Data Frame Metacarpal is below Table 2.15.\n\n\nTable 2.16 contains the first six values of the house and the amount of rental income in a year that the house brings in (“Capital and rental,” 2013). Use the house dataset to find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name house.\nCode book for Data Frame House is below Table 2.16.\n\n\nThe World Bank collects information on the life expectancy of a person in each country (“Life expectancy at,” 2013) and the fertility rate per woman in the country (“Fertility rate,” 2013). The first siz rows of the Life_exp_fert_rate dataset for countries for the year 2011 are in Table 2.17. Use the dataset to find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name Life_exp_fert_rate.\nCode book for Data Frame Fertility is below Table 2.17.\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The data for the countries where this information is available is in the fertility_prenatal dataset and the first six rows of the data are given in Table 2.18. Find the coefficient of determination and the correlation coefficient between the percentage spent on health expenditure and the percentage of women receiving prenatal care, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name fertility_prenatal.\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\n\nThe height and weight of baseball players are in the baseball dataset and the first six rows of the data are shown in Table 10.5 (“MLB heightsweights,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name baseball.\nCode book for Data Frame Baseball is below Table 10.5.\n\n\nDifferent species have different body weights and brain weights. The dataset body contains data about these two variables, and Table 10.6 shows the first six rows of the dataset (“Brain2bodyweight,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name body.\nCode book for Data Frame Body is below Table 10.6.\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (“Data hotdogs,” 2013). The data are in hotdog dataset with the first six rows shown in Table 9.9. Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name hotdog.\nCode book for data frame Hotdog is below Table 9.9.\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in the dataset agriculture. The first six rows are shon in Table 10.7 (“OECD economic development,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name agriculture.\nCode book for Data Frame Agriculture is Table 10.7.\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are given in teh dataset cancer_1 with the first six rows showing in Table 11.15 (“Smoking and cancer,” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cancer_1.\nCode book for Data Frame Cancer is below Table 11.15.\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in cars dataset. The first six rows of the data are shown in Table 10.9 (“us auto mileage,” 2019). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cars.\nCode book for Data Frame Cars is below Table 10.9.\n\n\nThere is a correlation between police expenditure and crime rate. Does this mean that spending more money on police causes the crime rate to decrease? Explain your answer.\nThere is a correlation between tobacco sales and alcohol sales. Does that mean that using tobacco causes a person to also drink alcohol? Explain your answer.\nThere is a correlation between the average temperature in a location and the mortality rate from breast cancer. Does that mean that higher temperatures cause more women to die of breast cancer? Explain your answer.\nThere is a correlation between the length of time a tableware company polishes a dish and the price of the dish. Does that mean that the time a plate is polished determines the price of the dish? Explain your answer.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#inference-for-regression-and-correlation",
    "href": "Regression.html#inference-for-regression-and-correlation",
    "title": "10  Regression",
    "section": "10.3 Inference for Regression and Correlation",
    "text": "10.3 Inference for Regression and Correlation\nThe idea behind regression is to find an equation that relates the response variable to the explanatory variables, and then use that equation to predict values of the response variable from values of the explanatory variables. But how do you know how good that estimate is?\nWhen we use a regression equation to make a prediction, the predicted value is only a single number. In practice, however, there is almost always some uncertainty in that prediction. Even if we knew the true relationship between the variables, individual observations would still vary around that relationship due to natural randomness and factors not included in the model.\nBecause of this variability, it is usually more informative to give a range of plausible values for the response variable rather than a single predicted value. A prediction interval provides such a range. It gives an interval of values that is likely to contain the response value for a new observation with specified explanatory variable values, along with a stated level of confidence.\n\n\n10.3.1 Prediction Interval\nUsing the regression equation you can predict the number of calories from the alcohol content. However, you only find one value. The problem is that beers vary a bit in calories even if they have the same alcohol content. It would be nice to have a range instead of a single value. The range is called a prediction interval.\n\n\n10.3.2 Prediction Interval for an Individual \\(y\\)\nGiven the fixed value \\(x\\), a prediction interval for the response at \\(x\\) is \\(\\hat{y}\\pm E\\) where \\(\\hat{y}\\) is the predicted value from the regression equation and \\(E\\) is the margin of error for the prediction.\nRguroo will calculate the prediction interval for you in the Linear Regression function. You just have to give it the value of \\(x\\) that you want to predict for, and tell it you want a prediction interval.\n\n\n10.3.3 Example: Find the Prediction Interval\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Find a 95% prediction interval for the number of calories when the alcohol content is 7.0%.\n\n10.3.3.1 Solution\nTo obtain the prediction interval for the beer example, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, enter the value of alcohol content (0.07) in the textbox labeled Predict at new x value(s). Then click the tab labeled Prediction Interval and select the checkboxes labeled Prediction Interval. These steps are shown in Figure 10.8.\n\n\n\n\n\n\n\n\nFigure 10.8: Rguroo dialog for obtaining prediction interval for calories from alcohol content\n\n\n\n\n\nThe output containing the prediction interval is shown in Figure 10.9.\n\n\n\n\n\n\n\n\nFigure 10.9: Rguroo output showing prediction interval for calories from alcohol content\n\n\n\n\n\n\nThe value 201.592 is the model’s predicted number of calories for a beer with 7.0% alcohol. The prediction interval is shown in the columns labeled Lower Pred Limit (2.5%) and Upper Pred Limit (97.5%). This means we can be 95% confident that the actual number of calories for a beer with 7.0% alcohol lies between 166.7 and 236.5. This range is called a 95% prediction interval.\nYou can obtain prediction intervals for other alcohol contents by entering different values in the textbox labeled Predict at new x value(s) in the Linear Regression dialog box. You can also change the confidence level using the textbox labeled Confidence Level in the Confidence Interval tab.\n\n\n\n10.3.4 Hypothesis Test for Correlation or Slope:\nHow do we decide whether there is really a correlation or a linear relationship between two variables? Can we test whether such a relationship exists? The answer is yes. We can test for correlation, or we can test whether the slope of the regression line is zero.\nThese two tests are equivalent. If there is a correlation between the variables, then the slope of the regression line is not zero. Likewise, if the slope of the regression line is not zero, then there is a correlation between the variables. For this reason, testing for correlation and testing for slope lead to the same conclusion.\nThe steps for performing a hypothesis test for correlation are given below. You can use these steps to test for correlation or to test whether the slope is zero.\n\nState the variables in words.\n\n\\(x\\) = explanatory variable\n\\(y\\) = response variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nOr equivalently\n\\(H_o: \\text{the slope is zero}\\)\n\\(H_a: \\text{the slope is not zero}\\)\nAlso, state \\(\\alpha\\), the significance level for your test.\n\nState and check the conditions for the hypothesis test, as explained in the sections Assumptions for Validity of the Regression Equation and Line and Assumptions for Validity of Inference.\nFind the test statistic and p-value\n\nThis will be calculated by Rguroo.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n10.3.5 Example: Testing the Claim of a Linear Correlation\nIs there a linear relationship, or correlation, between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Test at the 5% level.\n\n10.3.5.1 Solution\n\nState the random variables in words.\n\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test were already checked in Example: Find the Equation of the Regression Line\n\nFind the test statistic and p-value\n\nTo obtain the test statistic and p-value, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, under the Test of Association tab, check the box labeled Slope, and under the method section select Theoretical t-statistic. These steps are shown in Figure 10.10. The default value for the significance level is 0.05, so you do not need to change that value. However, if you want to change it, you can do so in the textbox labeled Significance Level.\n\n\n\n\n\n\n\n\nFigure 10.10: Rguroo dialog for testing slope for calories from alcohol content\n\n\n\n\n\nThe output containing the result of the test shown in Figure 10.11.\n\n\n\n\n\n\n\n\nFigure 10.11: AlcoholCalorieRegress_test_out\n\n\n\n\n\nThe test statistic is the t value in the column labeled Obs t Stat. In this case the t-statistic is 31.634, and the p-value id \\(8.84X10^{-85}\\).\n\nConclusion\n\nReject \\(H_o\\) since the p-value is less than 0.05.\n\nInterpretation\n\nThere is enough evidence to show that there is a correlation between alcohol content and number of calories in a 12-ounce bottle of beer.\n\n\n\n10.3.6 Homework for Inference for Regression and Correlation Section\nFor each problem, state the random variables. The Data Frame in this section are in the homework for section 10.1 and were also used in section 10.2.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected. The data are in the height_vs_metacarp dataset, and the first six rows of the data are shown in Table 2.15 (“Prediction of height,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name height_vs_metacarp.\nCode book for Data Frame Metacarpal is below Table 2.15.\n\nCompute a 99% prediction interval for height of a person with a metacarpal length of 44 mm.\nTest at the 1% level for a correlation or linear relationship between length of metacarpal bone 1 and height of a person.\n\n\n\nTable 2.16 contains the first six values of the house and the amount of rental income in a year that the house brings in (“Capital and rental,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name house.\nCode book for Data Frame House is below Table 2.16.\n\n\nCompute a 95% prediction interval for the rental income on a house worth \\$230,000.\nTest at the 5% level for a correlation or linear relationship between house value and rental amount.\n\n\n\nThe World Bank collects information on the life expectancy of a person in each country (“Life expectancy at,” 2013) and the fertility rate per woman in the country (“Fertility rate,” 2013). The first siz rows of the Life_exp_fert_rate dataset for countries for the year 2011 are in Table 2.17.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name Life_exp_fert_rate.\nCode book for Data Frame Fertility is below Table 2.17.\n\n\n\nCompute a 99% prediction interval for the life expectancy for a country that has a fertility rate of 2.7.\nTest at the 1% level for a correlation or linear relationship between fertility rate and life expectancy.\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The data for the countries where this information is available is in the fertility_prenatal dataset and the first six rows of the data are given in Table 2.18.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name fertility_prenatal.\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\n\n\nCompute a 95% prediction interval for the percentage of woman receiving prenatal care for a country that spends 5.0 % of GDP on health expenditure.\nTest at the 5% level for a correlation or linear relationship between percentage spent on health expenditure and the percentage of women receiving prenatal care.\n\n\nThe height and weight of baseball players are in the baseball dataset and the first six rows of the data are shown in Table 10.5 (“MLB heightsweights,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name baseball.\nCode book for Data Frame Baseball is below Table 10.5.\n\n\n\nCompute a 95% prediction interval for the weight of a baseball player that is 75 inches tall.\nTest at the 5% level for a correlation or linear relationship between height and weight of baseball players.\n\n\nDifferent species have different body weights and brain weights. The dataset body contains data about these two variables, and Table 10.6 shows the first six rows of the dataset (“Brain2bodyweight,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name body.\nCode book for Data Frame Body is below Table 10.6.\n\n\n\nCompute a 99% prediction interval for the brain weight for a species that has a body weight of 62 kg.\nTest at the 1% level for a correlation or linear relationship between body weights and brain weights.\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (“Data hotdogs,” 2013). The data are in hotdog dataset with the first six rows shown in Table 9.9.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name hotdog.\nCode book for data frame Hotdog is below Table 9.9.\n\n\n\nCompute a 95% prediction interval for the amount of sodium a beef hot dog has if it is 170 calories.\nTest at the 5% level for a correlation or linear relationship between amount of calories and amount of sodium.\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in the dataset agriculture. The first six rows are shon in Table 10.7 (“OECD economic development,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name agriculture.\nCode book for Data Frame Agriculture is Table 10.7.\n\n\n\nCompute a 90% prediction interval for the per capita income in a country that has 21 percent of labor in agriculture.\nTest at the 5% level for a correlation or linear relationship between percent of labor force in agriculture and per capita income.\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are given in teh dataset cancer_1 with the first six rows showing in Table 11.15 (“Smoking and cancer,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cancer_1.\nCode book for Data Frame Cancer is below Table 11.15.\n\n\n\nCompute a 99% prediction interval for the number of deaths from bladder cancer when the cigarette sales were 20 per capita.\nTest at the 1% level for a correlation or linear relationship between cigarette smoking and deaths of bladder cancer.\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in cars dataset. The first six rows of the data are shown in Table 10.9 (“us auto mileage,” 2019).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cars.\nCode book for Data Frame Cars is below Table 10.9.\n\n\n\nCompute a 95% prediction interval for the mileage on a car that weighs 3800 pounds.\nTest at the 5% level for a correlation or linear relationship between the weight of cars and mileage.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Chi-Square and ANOVA Tests.html",
    "href": "Chi-Square and ANOVA Tests.html",
    "title": "11  Chi-squared and ANOVA Tests",
    "section": "",
    "text": "11.1 Chi-Square Test for Independence\nRemember, qualitative data is where you collect data on individuals that are categories or names. Then you would count how many of the individuals had particular qualities. An example is that there is a theory that there is a relationship between breastfeeding and autism. To determine if there is a relationship, researchers could collect the time period that a mother breastfed her child and if that child was diagnosed with autism. Then you would have a table containing this information. Now you want to know if each cell is independent of each other cell. Remember, independence says that one event does not affect another event. Here it means that having autism is independent of being breastfed. What you really want is to see if they are not independent. In other words, does one affect the other? If you were to do a hypothesis test, this is your alternative hypothesis and the null hypothesis is that they are independent. There is a hypothesis test for this and it is called the Chi-Square Test for Independence. Technically it should be called the Chi-Square Test for Dependence, but for historical reasons it is known as the test for independence. Just as with previous hypothesis tests, all the steps are the same except for the conditions and the test statistic.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-squared and ANOVA Tests</span>"
    ]
  },
  {
    "objectID": "Chi-Square and ANOVA Tests.html#chi-square-test-for-independence",
    "href": "Chi-Square and ANOVA Tests.html#chi-square-test-for-independence",
    "title": "11  Chi-squared and ANOVA Tests",
    "section": "",
    "text": "11.1.1 Hypothesis Test for Chi-Square Test\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) the two variables are independent (this means that the one variable is not affected by the other)\n\\(H_a:\\) the two variables are dependent (this means that the one variable is affected by the other)\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample is taken. Check: describe the process used to collect sample.\nState: Expected frequencies for each cell are greater than or equal to 5, which means \\(E\\ge5\\). Check: The expected frequencies, \\(E\\), will be calculated later.\n\n3. Find the test statistic and p-value\nFinding the test statistic involves several steps. First the data is collected and counted, and then it is organized into a table (in a table each entry is called a cell). These values are known as the observed frequencies, which the symbol for an observed frequency is \\(O\\). Each table is made up of rows and columns. Then each row is totaled to give a row total and each column is totaled to give a column total.\nThe null hypothesis is that the variables are independent. Using the multiplication rule for independent events you can calculate the probability of being one value of the first variable, \\(A\\), and one value of the second variable, \\(B\\) (the probability of a particular cell). Remember in a hypothesis test, you assume that is true, the two variables are assumed to be independent.\nNow you want to find out how many individuals you expect to be in a certain cell. To find the expected frequencies, you just need to multiply the probability of that cell times the total number of individuals. Do not round the expected frequencies.\nIf the variables are independent the expected frequencies and the observed frequencies should be the same. The test statistic here will involve looking at the difference between the expected frequency and the observed frequency for each cell. Then you want to find the “total difference” of all of these differences. The larger the total, the smaller the chances that you could find that test statistic given that the condition of independence is true. That means that the condition of independence is not true. How do you find the test statistic? First find the differences between the observed and expected frequencies. Because some of these differences will be positive and some will be negative, you need to square these differences. These squares could be large just because the frequencies are large, you need to divide by the expected frequencies to scale them. Then finally add up all of these fractional values. This is the test statistic.\nTest Statistic:\nUsing r: See Example: Hypothesis Test with Chi-Square Test for the process\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value\\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n11.1.1.1 Example: Hypothesis Test with Chi-Square Test\nIs there a relationship between autism and breastfeeding? To determine if there is, a researcher asked mothers of autistic and non-autistic children to say what time period they breastfed their children. The data is in Autism Versus Breastfeeding (Schultz, Klonoff-Cohen, Wingard, Askhoomoff, Macera, Ji & Bacher, 2006). Do the data provide enough evidence to support that breastfeeding and autism are independent? Test at the 1% level.\n\n11.1.1.1.1 Autism Versus Breastfeeding\n\nBreast Feeding\n\n\n\n\n\n\n\n\n\nAutism\nNot Breast Feed\nBreast Feed less than 2 months\nBreast Feed 2 to 6 months\nBreast Feed more than 6 months\n\n\n\n\nyes\n241\n198\n164\n215\n\n\nno\n20\n25\n27\n44\n\n\n\nTo put this data into r, use the following commands:\nIf you have the data frame instead of the summary table as in this example, you can use the tally command to create the table in r. Just save the tally command with a name.\n\n\n\n11.1.1.2 Solution\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o\\): Breastfeeding and autism are independent\n\\(H_a\\): Breastfeeding and autism are dependent\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of breastfeeding time frames and autism incidence was taken. Check: this was stated in the problem.\nState: Expected frequencies for each cell are greater than or equal to 5, \\(E\\ge 5\\). Check: See step 3. All expected frequencies are more than 5.\n\n\n\nFind the test statistic and p-value On rStudio, the command is\n\n\nchisq.test(autism_table) #calculates the test statistics and p-value \n\n\n    Pearson's Chi-squared test\n\ndata:  autism_table\nX-squared = 11.217, df = 3, p-value = 0.01061\n\nchisq.test(autism_table)$expected # shows all the expected frequencies \n\n         [,1]      [,2]      [,3]      [,4]\nyes 228.58458 195.30407 167.27837 226.83298\nno   32.41542  27.69593  23.72163  32.16702\n\n\nThe test statistic is 11.217 and the p-value is 0.01061.\n\nConclusion\n\nFail to reject \\(H_o\\) since the p-value is more than 0.01.\n\nInterpretation\n\nThere is not enough evidence to support that breastfeeding and autism are dependent. This means that you cannot say whether a child is breastfed or not will indicate if that the child will be diagnosed with autism.\n\n\n\n11.1.2 Homework for Chi-Square Test for Independence Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nThe number of people who survived the Titanic based on class and sex is in Table 11.1 (“Encyclopedia Titanica,” 2013). Is there enough evidence to show that the class and the sex of a person who survived the Titanic are independent? Test at the 5% level.\n\n\n\n\n\nTable 11.1: Surviving the Titanic\n\n\n\n\n\n\n\nfemale\nmale\n\n\n\n\nfirst\n134\n59\n\n\nsecond\n94\n25\n\n\nthird\n80\n58\n\n\n\n\n\n\n\n\n\nResearchers watched groups of dolphins off the coast of Ireland in 1998 to determine what activities the dolphins partake in at certain times of the day (“Activities of dolphin,” 2013). The numbers in Table 11.2 represent the number of groups of dolphins that were partaking in an activity at certain times of days. Is there enough evidence to show that the activity and the time period are independent for dolphins? Test at the 1% level.\n\n\nDolphin&lt;- read.csv( \"https://krkozak.github.io/MAT160/dolphins.csv\") \nDolphin_table&lt;-tally(~activity+period, data=Dolphin)\nknitr::kable(Dolphin_table)\n\n\n\nTable 11.2: Dolphin Activity\n\n\n\n\n\n\n\nAfternoon\nEvening\nMorning\nNoon\n\n\n\n\nFeed\n0\n56\n28\n4\n\n\nSocial\n9\n10\n38\n5\n\n\nTravel\n14\n13\n6\n6\n\n\n\n\n\n\n\n\n\nIs there a relationship between autism and what an infant is fed? To determine if there is, a researcher asked mothers of autistic and non-autistic children to say what they fed their infant. The data is in Table 11.3 (Schultz, Klonoff-Cohen, Wingard, Askhoomoff, Macera, Ji & Bacher, 2006). Do the data provide enough evidence to show that that what an infant is fed and autism are independent? Breast-feeding (BF), Formula with DHA/ARA (For with), and Formula without DHA/ARA (Form without)Test at the 1% level.\n\n\nFeeding&lt;- read.csv( \"https://krkozak.github.io/MAT160/Mothers.csv\") \nFeeding_table&lt;-tally(~autism+feeding, data=Feeding)\nknitr::kable(Feeding_table)\n\n\n\nTable 11.3: Autism Versus Breastfeeding\n\n\n\n\n\n\n\nbreast\nformula_with\nformula_without\n\n\n\n\nno\n6\n22\n10\n\n\nyes\n12\n39\n65\n\n\n\n\n\n\n\n\n\nStudents at multiple grade schools were asked what their personal goal (get good grades, be popular, be good at sports) was and how important good grades were to them (1 very important and 4 least important). The data is in Table 11.4 (“Popular kids datafile,” 2013). Do the data provide enough evidence to show that goal attainment and importance of grades are independent? Test at the 5% level.\n\n\nGoal&lt;- read.csv( \"https://krkozak.github.io/MAT160/Popular_Kids_clean.csv\") \nGoal_Grades_table&lt;-tally(~Goals+Grades, data=Goal)\nknitr::kable(Goal_Grades_table)\n\n\n\nTable 11.4: Personal Goal and Importance of Grades\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\nGrades\n70\n66\n55\n56\n\n\nPopular\n14\n33\n45\n49\n\n\nSports\n10\n24\n33\n23\n\n\n\n\n\n\n\n\n\nStudents at multiple grade schools were asked what their personal goal (get good grades, be popular, be good at sports) was and how important being good at sports were to them (1 very important and 4 least important). The data is in Table 11.5 (“Popular kids datafile,” 2013). Do the data provide enough evidence to show that goal attainment and importance of sports are independent? Test at the 5% level.\n\n\nGoal&lt;- read.csv( \"https://krkozak.github.io/MAT160/Popular_Kids_clean.csv\") \nGoal_Sports_table&lt;-tally(~Goals+Sports, data=Goal)\nknitr::kable(Goal_Sports_table)\n\n\n\nTable 11.5: Personal Goal and Importance of Sports\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\nGrades\n83\n81\n55\n28\n\n\nPopular\n32\n49\n43\n17\n\n\nSports\n50\n24\n14\n2\n\n\n\n\n\n\n\n\n\nStudents at multiple grade schools were asked what their personal goal (get good grades, be popular, be good at sports) was and how important having good looks were to them (1 very important and 4 least important). The data is in Table 11.6 (“Popular kids datafile,” 2013). Do the data provide enough evidence to show that goal attainment and importance of looks are independent? Test at the 5% level.\n\n\nGoal&lt;- read.csv( \"https://krkozak.github.io/MAT160/Popular_Kids_clean.csv\") \nGoal_Looks_table&lt;-tally(~Goals+Looks, data=Goal)\nknitr::kable(Goal_Looks_table)\n\n\n\nTable 11.6: Personal Goal and Importance of Looks\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\nGrades\n80\n66\n66\n35\n\n\nPopular\n81\n30\n18\n12\n\n\nSports\n24\n30\n17\n19\n\n\n\n\n\n\n\n\n\nStudents at multiple grade schools were asked what their personal goal (get good grades, be popular, be good at sports) was and how important having money were to them (1 very important and 4 least important). The data is in Table 11.7 (“Popular kids datafile,” 2013). Do the data provide enough evidence to show that goal attainment and importance of money are independent? Test at the 5% level.\n\n\nGoal&lt;- read.csv( \"https://krkozak.github.io/MAT160/Popular_Kids_clean.csv\") \nGoal_Money_table&lt;-tally(~Goals+Money, data=Goal)\nknitr::kable(Goal_Money_table)\n\n\n\nTable 11.7: Personal Goal and Importance of Money\n\n\n\n\n\n\n\n1\n2\n3\n4\n\n\n\n\nGrades\n14\n34\n71\n128\n\n\nPopular\n14\n29\n35\n63\n\n\nSports\n6\n12\n26\n46",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-squared and ANOVA Tests</span>"
    ]
  },
  {
    "objectID": "Chi-Square and ANOVA Tests.html#chi-square-goodness-of-fit",
    "href": "Chi-Square and ANOVA Tests.html#chi-square-goodness-of-fit",
    "title": "11  Chi-squared and ANOVA Tests",
    "section": "11.2 Chi-Square Goodness of Fit",
    "text": "11.2 Chi-Square Goodness of Fit\nIn probability, you calculated probabilities using both experimental and theoretical methods. There are times when it is important to determine how well the experimental values match the theoretical values. An example of this is if you wish to verify if a die is fair. To determine if observed values fit the expected values, you want to see if the difference between observed values and expected values is large enough to say that the test statistic is unlikely to happen if you assume that the observed values fit the expected values. The test statistic in this case is also the chi-square. The process is the same as for the chi-square test for independence.\n\n11.2.1 Hypothesis Test for Goodness of Fit Test\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) The data are consistent with a specific distribution\n\\(H_a:\\) The data are not consistent with a specific distribution\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample is taken. Check: State how the sample is collected.\nState: Expected frequencies for each cell are greater than or equal to 5. Check: The expected frequencies, *E*, will be calculated later.\n\n\n\nFind the test statistic and p-value\n\nUsing rStudio see example 11.2.1\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_0\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n11.2.2 Example: Goodness of Fit Test\nSuppose you have a die that you are curious if it is fair or not. If it is fair then the proportion for each value should be the same. You need to find the observed frequencies and to accomplish this you roll the die 500 times and count how often each side comes up. The data is in table Table 11.8. Do the data show that the die is fair? Test at the 5% level.\n\n\n\n\nTable 11.8: Observed Frequencies of Die for sides 1 through 6\n\n\n\n\n\n\nSides\nobserved….c.78..87..87..76..85..87.\n\n\n\n\n1\n78\n\n\n2\n87\n\n\n3\n87\n\n\n4\n76\n\n\n5\n85\n\n\n6\n87\n\n\n\n\n\n\n\n\n\n11.2.2.1 Solution\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) The observed frequencies are consistent with the distribution for fair die (the die is fair)\n\\(H_a:\\) The observed frequencies are not consistent with the distribution for fair die (the die is not fair)\n\\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample is taken. check: This is true since each throw of a die is a random event.\nState: Expected frequencies for each cell are greater than or equal to 5. Check: See step 3.\n\n\n\nFind the test statistic and p-value\n\nOn rStudio, this would be\n\nfair_die&lt;-c(1/6, 1/6, 1/6, 1/6, 1/6, 1/6) \nobserved&lt;-c(78, 87, 87, 76, 85, 87) \nchisq.test(observed, p=fair_die) \n\n\n    Chi-squared test for given probabilities\n\ndata:  observed\nX-squared = 1.504, df = 5, p-value = 0.9126\n\nchisq.test(observed, p=fair_die)$expected \n\n[1] 83.33333 83.33333 83.33333 83.33333 83.33333 83.33333\n\n\nTest Statistic: The test statistic is 1.504. The p-value is 0.9126.\n\nConclusion\n\nFail to reject \\(H_o\\) since the p-value is greater than 0.05.\n\nInterpretation\n\nThere is not enough evidence to support that the die is not consistent with the distribution for a fair die. There is not enough evidence to support that the die is not fair.\n\n\n\n11.2.3 Homework for Chi-Square Goodness of Fit Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nAccording to the M&M candy company, the expected proportion can be found in Table 11.9. In addition, the table contains the number of M&M’s of each color that were found in a case of candy (Madison, 2013). At the 5% level, do the observed frequencies support the claim of M&M?\n\n\nMandM &lt;- data.frame(\n  Color = c(\"Blue\", \"Brown\", \"Green\", \"Orange\", \"Red\", \"Yellow\"),\n  Observed = c(78, 87, 87, 76, 85, 87),\n  Expected = c(0.24, 0.13, 0.16, 0.20, 0.13, 0.14)\n)\nknitr::kable(MandM)\n\n\n\nTable 11.9: M&M Observed and Expected\n\n\n\n\n\n\nColor\nObserved\nExpected\n\n\n\n\nBlue\n78\n0.24\n\n\nBrown\n87\n0.13\n\n\nGreen\n87\n0.16\n\n\nOrange\n76\n0.20\n\n\nRed\n85\n0.13\n\n\nYellow\n87\n0.14\n\n\n\n\n\n\n\n\n\nEyeglassomatic manufactures eyeglasses for different retailers. The data frame is in Table 2.4. They test to see how many defective lenses they made the time period of January 1 to March 31. Table 11.10 gives the defect and the number of defects.\nCode book for Data Frame Defects below Table 2.4.\n\n\nDefect_table &lt;- tally(~type, data=Defects)\nknitr::kable(Defect_table)\n\n\n\nTable 11.10: Number of Defective Lenses\n\n\n\n\n\n\ntype\nFreq\n\n\n\n\naxis\n1838\n\n\nbig\n1105\n\n\nchamfer\n1596\n\n\ncracks\n1546\n\n\nflaked\n1992\n\n\nheight\n1130\n\n\nintern\n976\n\n\nlost\n976\n\n\npd\n1398\n\n\nscratch\n5865\n\n\nshape\n1485\n\n\nsmall\n4613\n\n\nspot\n1371\n\n\n\n\n\n\n\n\nDo the data support the notion that each defect type occurs in the same proportion? Test at the 5% level.\n\nOn occasion, medical studies need to model the proportion of the population that has a disease and compare that to observed frequencies of the disease actually occurring. Suppose the end-stage renal failure in south-west Wales was collected for different age groups. Do the data in Table 11.11 show that the observed frequencies are in agreement with proportion of people in each age group (Boyle, Flowerdew & Williams, 1997)? Test at the 1% level.\nTable Renal Failure Frequencies\n\nRenal &lt;- data.frame(\n  Age_group = c(\"16-29\", \"30-44\", \"45-59\", \"60-75\", \"75+\"),\n  Observed = c(32, 66, 132, 218, 91),\n  Expected = c(0.23, 0.25, 0.22, 0.21, 0.09)\n)\nknitr::kable(Renal)\n\n\n\nTable 11.11: Renal Failure Frequencies\n\n\n\n\n\n\nAge_group\nObserved\nExpected\n\n\n\n\n16-29\n32\n0.23\n\n\n30-44\n66\n0.25\n\n\n45-59\n132\n0.22\n\n\n60-75\n218\n0.21\n\n\n75+\n91\n0.09\n\n\n\n\n\n\n\n\nIn Africa in 2011, the number of deaths of a female from cardiovascular disease for different age groups are in Table 11.12 (“Global health observatory,” 2013). In addition, the proportion of deaths of females from all causes for the same age groups are also in table Deaths of Females for Different Age Groups. Do the data show that the death from cardiovascular disease are in the same proportion as all deaths for the different age groups? Test at the 5% level.\n\nDeaths &lt;- data.frame(\n  Age = c(\"5-14\", \"14-29\", \"30-49\", \"50-69\"),\n  Observed = c(8, 16, 56, 433),\n  Expected = c(0.10, 0.12, 0.226, 0.52)\n)\nknitr::kable(Deaths)\n\n\n\nTable 11.12: Deaths of Females for Different Age Groups\n\n\n\n\n\n\nAge\nObserved\nExpected\n\n\n\n\n5-14\n8\n0.100\n\n\n14-29\n16\n0.120\n\n\n30-49\n56\n0.226\n\n\n50-69\n433\n0.520\n\n\n\n\n\n\n\n\n\n\n\nIn Australia in 1995, there was a question of whether indigenous people are more likely to die in prison than non-indigenous people. To figure out, the data in Table 11.13 was collected. (“Aboriginal deaths in,” 2013). Do the data show that indigenous people die in the same proportion as non-indigenous people? Test at the 1% level.\n\nPrisoners &lt;- data.frame(\n  Died = c(\"Yes\", \"No\"),\n  Observed = c(17, 3890),\n  Expected = c(0.003, 0.997))\nknitr::kable(Prisoners)\n\n\n\nTable 11.13: Death of Indigenous Prisoners\n\n\n\n\n\n\nDied\nObserved\nExpected\n\n\n\n\nYes\n17\n0.003\n\n\nNo\n3890\n0.997\n\n\n\n\n\n\n\n\nA project conducted by the Australian Federal Office of Road Safety asked people many questions about their cars. One question was the reason that a person chooses a given car, and that data is in Table 11.14 (“Car preferences,” 2013).\n\n\nCar_table &lt;- tally(~Reason, data=Car_pref)\nknitr::kable(Car_table)\n\n\n\nTable 11.14: Reason for Choosing a Car\n\n\n\n\n\n\nReason\nFreq\n\n\n\n\ncomfort\n47\n\n\ncost\n46\n\n\nlooks\n27\n\n\nperformance\n34\n\n\nreliability\n62\n\n\nsafety\n84\n\n\n\n\n\n\n\n\nDo the data show that the frequencies observed substantiate the claim that the reasons for choosing a car are equally likely? Test at the 5% level.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-squared and ANOVA Tests</span>"
    ]
  },
  {
    "objectID": "Chi-Square and ANOVA Tests.html#analysis-of-variance-anova",
    "href": "Chi-Square and ANOVA Tests.html#analysis-of-variance-anova",
    "title": "11  Chi-squared and ANOVA Tests",
    "section": "11.3 Analysis of Variance (ANOVA)",
    "text": "11.3 Analysis of Variance (ANOVA)\nThere are times where you want to compare three or more population means. One idea is to just test different combinations of two means. The problem with that is that your chance for a type I error increases. Instead you need a process for analyzing all of them at the same time. This process is known as analysis of variance (ANOVA). The test statistic for the ANOVA is fairly complicated, you will want to use technology to find the test statistic and p-value. The test statistic is distributed as an F-distribution, which is skewed right and depends on degrees of freedom. Since you will use technology to find these, the distribution and the test statistic will not be presented. Remember, all hypothesis tests are the same process. Note that to obtain a statistically significant result there need only be a difference between any two of the \\(k\\) means.\nBefore conducting the hypothesis test, it is helpful to look at the means and standard deviations for each data set. If the sample means with consideration of the sample standard deviations are different, it may mean that some of the population means are different. However, do realize that if they are different, it doesn’t provide enough evidence to show the population means are different. Calculating the sample statistics just gives you an idea that conducting the hypothesis test is a good idea.\n\n11.3.1 Hypothesis test using ANOVA to compare \\(k\\) means\n\nState the random variables and the parameters in words\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) all the means are the same\n\\(H_a:\\) at least two of the means are different\nAlso, state your \\(\\alpha\\) level here.\n\nState and check the conditions for the hypothesis test\n\na. State: A random sample of size is taken from each population. Check: discuss how the samples were taken.\n\nState: All the samples are independent of each other. Check: Discuss how they are all independent.\nState: Each population is normally distributed. Check: Create density plots and normal quantile plot of each sample. Note: the ANOVA test is fairly robust to the condition especially if the sample sizes are fairly close to each other. Unless the populations are really not normally distributed and the sample sizes are close to each other, then this is a loose condition.\nState: The population variances are all equal. Check: See if the sample variances are close to each other. If the sample sizes are close to each other, then this is a loose condition.\n\n4. Find the test statistic and p-value\nThe test statistic is \\(F\\). To find the test statistic, use technology such r Studio.\nThe test statistic, \\(F\\), is distributed as an F-distribution, where both degrees of freedom are needed in this distribution. The p-value is also calculated r Studio.\n\nConclusion\n\nThis is where you write reject \\(H_O\\) or fail to reject \\(H_O\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\nIf you do in fact reject \\(H_o\\), then you know that at least two of the means are different. The next question you might ask is which are different? You can look at the sample means, but realize that these only give a preliminary result. To actually determine which means are different, you need to conduct other tests. Some of these tests are the range test, multiple comparison tests, Duncan test, Student-Newman-Keuls test, Tukey test, Scheffé test, Dunnett test, least significant different test, and the Bonferroni test. There is no consensus on which test to use.\n\n\n11.3.2 Example: Hypothesis Test Involving Several Means\nCancer is a terrible disease. Surviving may depend on the type of cancer the person has. To see if the mean survival time for several types of cancer are different, data was collected on the survival time in days of patients with one of these cancer in advanced stage. The data is in Table 11.15 (“Cancer survival story,” 2013). (Please realize that this data is from 1978. There have been many advances in cancer treatment, so do not use this data as an indication of survival rates from these cancers.) Do the data indicate that at least two of the mean survival time for these types of cancer are not all equal? Test at the 1% level.\n\nCancer &lt;- read.csv( \"https://krkozak.github.io/MAT160/cancer.csv\") \nknitr::kable(head(Cancer))\n\n\n\nTable 11.15: Survival Times in Days of Five Cancer Types\n\n\n\n\n\n\nsurvival\norgan\n\n\n\n\n124\nStomach\n\n\n42\nStomach\n\n\n25\nStomach\n\n\n45\nStomach\n\n\n412\nStomach\n\n\n51\nStomach\n\n\n\n\n\n\n\n\nCode book for data frame Cancer\nDescription Survival time for several types of cancer was collected.\nThis data frame contains the following columns:\nsurvival: survival times (months)\norgan: the organ that the cancer is in\nSource Cancer survival story. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/CancerSurvival.html&gt;\nReferences &lt;http://lib.stat.cmu.edu/DASL&gt;\n\n11.3.2.1 Solution\n\nState the random variables and the parameters in words\n\n\\(x_1:\\) survival time of patient with Stomach cancer\n\\(x_2:\\) survival time of patient with Bronchus (lung) cancer\n\\(x_3:\\) survival time of patient with Colon cancer\n\\(x_4:\\) survival time of patient with Ovarian cancer\n\\(x_5:\\) survival time of patient with Breast cancer\n\\(\\mu_1:\\) mean survival time of patient with Stomach cancer\n\\(\\mu_2:\\) mean survival time of patient with Bronchus (lung) cancer\n\\(\\mu_3:\\) mean survival time of patient with Colon cancer\n\\(\\mu_4:\\) mean survival time of patient with Ovarian cancer\n\\(\\mu_5:\\) mean survival time of patient with Brest cancer\nNow before conducting the hypothesis test, look at the means and standard deviations. There appears to be a difference between at least two of the means, but realize that the standard deviations are very different. The difference you see may not be significant.\nNotice the sample sizes are not the same.\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o:\\) all the means are equal\n\\(H_a:\\) some of the means are different\n\\(\\alpha=0.01\\)\n\nState and check the conditions for the hypothesis test\n\n\n\nState: A random sample of 13 survival times from stomach cancer was taken. A random sample of 17 survival times from bronchus cancer was taken. A random sample of 17 survival times from colon cancer was taken. A random sample of 6 survival times from ovarian cancer was taken. A random sample of 11 survival times from breast cancer was taken.\nCheck: These statements may not be true. This information was not shared as to whether the samples were random or not but it may be safe to assume that.\nState: The samples are all independent.\nCheck: Since the individuals have different cancers, then the samples are independent.\nState: Population of all survival times from stomach cancer is normally distributed. Population of all survival times from bronchus cancer is normally distributed. Population of all survival times from colon cancer is normally distributed. Population of all survival times from ovarian cancer is normally distributed. Population of all survival times from breast cancer is normally distributed.\nCheck: Looking at the density plots and normal quantile plots for each sample, it appears that none of the populations are normally distributed. The sample sizes are somewhat different for the problem. This condition may not be true.\n(ref:cancer-density–graphs-cap) Density Plot of Survival Times for Different Cancers\n\ngf_density(~survival|organ, data=Cancer, title=\"Survival times for Different Cancers\", xlab = \"Survival Times\")\n\n\n\n\n\n\n\nFigure 11.1: Density Plot of Survival Times for Different Cancers\n\n\n\n\n\n\ngf_qq(~survival|organ, data=Cancer, title=\"Survival times for Different Cancers\")\n\n\n\n\n\n\n\nFigure 11.2: Quantile Plot of Survival Times for Different Cancers\n\n\n\n\n\nState: The population variances are all equal.\nCheck: The sample standard deviations are approximately 346.3, 209.9, 427.2, 1098.6, and 1239.0 respectively. This condition does not appear to be met, since the sample standard deviations are very different. The sample sizes are somewhat different for the problem. This condition may not be true.\n\n4. Find the test statistic and p-value\nTo find the test statistic and p-value on r Studio, the commands would be:\n\nresults=aov(survival~organ, data=Cancer) \nsummary(results)\n\n            Df   Sum Sq Mean Sq F value   Pr(&gt;F)    \norgan        4 11535761 2883940   6.433 0.000229 ***\nResiduals   59 26448144  448274                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe test statistic is F = 6.433 and the p-value = 0.000229.\n\nConclusion\n\nReject \\(H_o:\\) since the p-value is less than 0.01.\n\nInterpretation\n\nThere is enough evidence to support that at least two of the mean survival times from different cancers are not equal.\n\ndf_stats(survival~organ, data=Cancer, mean)\n\n  response    organ      mean\n1 survival   Breast 1395.9091\n2 survival Bronchus  211.5882\n3 survival    Colon  457.4118\n4 survival    Ovary  884.3333\n5 survival  Stomach  286.0000\n\n\nBy examination of the means, it appears that the mean survival time for breast cancer is different from the mean survival times for both stomach and bronchus cancers. It may also be different for the mean survival time for colon cancer. The others may not be different enough to actually say for sure.\n\n\n\n11.3.3 Homework for Analysis of Variance (ANOVA) Section\nIn each problem show all steps of the hypothesis test. If some of the conditions are not met, note that the results of the test may not be correct and then continue the process of the hypothesis test.\n\nCuckoo birds are in the habit of laying their eggs in other birds’ nest. The other birds adopt and hatch the eggs. The lengths (in cm) of cuckoo birds’ eggs in the other species nests were measured and are in Table 11.16 (“Cuckoo eggs in,” 2013). Do the data show that the mean length of cuckoo bird’s eggs is not all the same when put into different nests? Test at the 5% level.\n\n\nEggs &lt;- read.csv( \"https://krkozak.github.io/MAT160/Birds_eggs.csv\") \nknitr::kable(head(Eggs))\n\n\n\nTable 11.16: Lengths of Cuckoo Bird Eggs in Different Species Nest\n\n\n\n\n\n\nlength\nbird\n\n\n\n\n19.65\nMeadow\n\n\n20.05\nMeadow\n\n\n20.65\nMeadow\n\n\n20.85\nMeadow\n\n\n21.65\nMeadow\n\n\n21.65\nMeadow\n\n\n\n\n\n\n\n\nCode book for data frame Eggs\nDescription Cuckoo birds are in the habit of laying their eggs in other birds’ nest. The other birds adopt and hatch the eggs. The lengths (in cm) of cuckoo birds’ eggs in the other species nests were measured\nThis data frame contains the following columns:\nlength: length of cuckoo bird’s eggs in other species nets (cm)\nbird: bids where eggs were found in their nests. The birds are Meadow Pipit, Tree Pipit, Hedge Sparrow, Robin, Pied Wagtail, and Wren\nSource Cuckoo eggs in nest of other birds. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/cuckoo.html&gt;\nReferences SOCR Home page: &lt;http://www.socr.ucla.edu&gt;\n\nLevi-Strauss Co manufactures clothing. The quality control department measures weekly values of different suppliers for the percentage difference of waste between the layout on the computer and the actual waste when the clothing is made (called run-up). The data is in Table 11.17 (“Waste run up,” 2013). Do the data show that there is a difference between some of the suppliers? Test at the 1% level.\n\n\nLevi &lt;- read.csv( \"https://krkozak.github.io/MAT160/Levi_jeans.csv\") \nknitr::kable(head(Levi))\n\n\n\nTable 11.17: Run-ups for Different Plants Making Levi Strauss Clothing\n\n\n\n\n\n\nrun_up\nplant\n\n\n\n\n1.2\nPlant_1\n\n\n10.1\nPlant_1\n\n\n-2.0\nPlant_1\n\n\n1.5\nPlant_1\n\n\n-3.0\nPlant_1\n\n\n-0.7\nPlant_1\n\n\n\n\n\n\n\n\nCode book for data frame Levi\nDescription Levi-Strauss Co manufactures clothing. The quality control department measures weekly values of different suppliers for the percentage difference of waste between the layout on the computer and the actual waste when the clothing is made (called run-up).\nThis data frame contains the following columns:\nrun_up: percentage difference of waste between the layout on the computer and the actual waste when the clothing is made. There are some negative values because sometimes the supplier is able to layout the pattern better than the computer\nplant: Which suppliers\nSource Waste run up. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/wasterunup.html&gt;\nReferences &lt;http://lib.stat.cmu.edu/DASL&gt;\n\nSeveral magazines were grouped into three categories based on what level of education of their readers the magazines are geared towards: high, medium, or low level. Then random samples of the magazines were selected to determine the number of three-plus-syllable words were in the advertising copy, and the data is in Table 11.18 (“Magazine ads readability,” 2013). Is there enough evidence to show that the mean number of three-plus-syllable words in advertising copy is different for at least two of the education levels? Test at the 5% level.\n\n\nAdvertising &lt;- read.csv( \"https://krkozak.github.io/MAT160/three_syllable_words.csv\") \nknitr::kable(head(Advertising))\n\n\n\nTable 11.18: Number of Three Plus Syllable Words in Advertising Copy\n\n\n\n\n\n\nnumber\neducation\n\n\n\n\n34\nHigh\n\n\n21\nHigh\n\n\n37\nHigh\n\n\n31\nHigh\n\n\n10\nHigh\n\n\n24\nHigh\n\n\n\n\n\n\n\n\nCode book for data frame Advertising\nDescription Several magazines were grouped into three categories based on what level of education of their readers the magazines are geared towards: high, medium, or low level. Then random samples of the magazines were selected to determine the number of three-plus-syllable words were in the advertising copy\nThis data frame contains the following columns:\nnumber: number of three=plus-syllable words in advertising copy\neducation: level of education the magazine is geared towards: high, medium, or low\nSource Magazine ads readability. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/magadsdat.html&gt;\nReferences &lt;http://lib.stat.cmu.edu/DASL&gt;\n\nA study was undertaken to see how accurate food labeling for calories on food that is considered reduced calorie. The group measured the amount of calories for each item of food and then found the percent difference between measured and labeled food. The group also looked at food that was nationally advertised, regionally distributed, or locally prepared. The data is in Table 11.19 (“Calories datafile,” 2013). Do the data indicate that at least two of the mean percent differences between the three groups are different? Test at the 5% level.\n\n\nFood &lt;- read.csv( \"https://krkozak.github.io/MAT160/Food_calories_percent_diff.csv\") \nknitr::kable(head(Food))\n\n\n\nTable 11.19: Percent Differences Between Measured and Labeled Food\n\n\n\n\n\n\npercent_diff\nfood\n\n\n\n\n2\nnational\n\n\n-28\nnational\n\n\n-6\nnational\n\n\n8\nnational\n\n\n6\nnational\n\n\n-1\nnational\n\n\n\n\n\n\n\n\nCode book for data frame Food\nDescription A study was undertaken to see how accurate food labeling for calories on food that is considered reduced calorie. The group measured the amount of calories for each item of food and then found the percent difference between measured and labeled food. The group also looked at food that was nationally advertised, regionally distributed, or locally prepared.\nThis data frame contains the following columns:\npercent_diff: percent difference between the number of calories that are measured in the food and the amount that is labeled on the food.\nfood: Where the food is created: nationally advertised, regionally distributed, or locally prepared.\nSource Calories datafile. (2013, December 07). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/Calories.html&gt;\nReferences &lt;http://lib.stat.cmu.edu/DASL&gt;\n\nThe amount of sodium (in mg) in different types of hot dogs is in Table 11.19 (“Hot dogs story,” 2013). Is there sufficient evidence to show that the mean amount of sodium in the types of hot dogs are not all equal? Test at the 5% level.\n\nCode book for data frame Food is below Table 11.19.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Chi-squared and ANOVA Tests</span>"
    ]
  },
  {
    "objectID": "References.html",
    "href": "References.html",
    "title": "12  References",
    "section": "",
    "text": "12.1 Data Sources:\n(n.d.). Retrieved July 18, 2019, from https://www.idvbook.com/teaching-aid/data-sets/the-breakfast-cereal-data-set/ The Best Kids’ Cereal. (n.d.). Retrieved July 18, 2019, from https://www.ranker.com/list/best-kids-cereal/ranker-food\n(n.d.). Retrieved from https://www3.nd.edu/~busiforc/handouts/Data and Stories/t test/Friday The Thirteenth/Friday The Thirteenth Data.html\n(n.d.). Retrieved July 21, 2019, from https://www3.nd.edu/~busiforc/handouts/Data and Stories/regression/us auto mileage/usautomileage.html\nhttp://apps.who.int/gho/athena/data/download.xsl?format=xml&target=GHO/WHOSIS_000001&profile=excel&filter=COUNTRY:;SEX:;REGION:EUR\nAboriginal deaths in custody. (2013, September 26). Retrieved from &lt;http://www.statsci.org/data/oz/custody.html\nActivities of Dolphin Groups. (n.d.). Retrieved July 12, 2019, from http://www.statsci.org/data/general/dolpacti.html\nAdvanced Solutions International, Inc. (n.d.). Retrieved July 16, 2019, from https://www.amstat.org/asa/News/ASA-Calls-Time-on-Statistically-Significant-in-Science-Research.aspx\nAnnual maximums of daily rainfall in Sydney. (2013, September 25). Retrieved from &lt;http://www.statsci.org/data/oz/sydrain.html&gt;\nAP exam scores. (2013, November 20). Retrieved from &lt;http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Dinov_030708_APExamScores&gt;\nAppliance life expectancy. (2013, November 8). Retrieved from &lt;http://www.mrappliance.com/expert/life-guide/&gt;\nAustralian Human Rights Commission, (1996). Indigenous deaths in custody 1989 - 1996. Retrieved from website: &lt;http://www.humanrights.gov.au/publications/indigenous-deaths-custody&gt;\nBhat, R., & Kushtagi, P. (2006). A re-look at the duration of human pregnancy. Singapore Med J., 47(12), 1044-8. Retrieved from &lt;http://www.ncbi.nlm.nih.gov/pubmed/17139400&gt;\nBoyle, P., Flowerdew, R., & Williams, A. (1997). Evaluating the goodness of fit in models of sparse medical data: A simulation approach. International Journal of Epidemiology, 26(3), 651-656. Retrieved from &lt;http://ije.oxfordjournals.org/content/26/3/651.full.pdf&gt; html\nBrain2bodyweight. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Brain2BodyWeight\nBuy sushi grade fish online. (2013, November 20). Retrieved from http://www.catalinaop.com/\nCancer survival story. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/CancerSurvival.html&gt;\nCalories datafile. (2013, December 07). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/Calories.html&gt;\nCapital and rental values of Auckland properties. (2013, September 26). Retrieved from http://www.statsci.org/data/oz/rentcap.html\nCar Preferences. (n.d.). Retrieved July 11, 2019, from http://www.statsci.org/data/oz/carprefs.html\nCDC-data and statistics, autism spectrum disorders - ncbdd. (2013, October 21). Retrieved from &lt;http://www.cdc.gov/ncbddd/autism/data.html&gt;\nCDC features - new data on autism spectrum disorders. (2013, November 26). Retrieved from &lt;http://www.cdc.gov/features/countingautism/&gt;\nCenter for Disease Control and Prevention, Prevalence of Autism Spectrum Disorders - Autism and Developmental Disabilities Monitoring Network. (2008). Autism and developmental disabilities monitoring network-2012. Retrieved from website: &lt;http://www.cdc.gov/ncbddd/autism/documents/ADDM-2012-Community-Report.pdf&gt;\nCO2 emissions (metric tons per capita). (n.d.). Retrieved July 18, 2019, from https://data.worldbank.org/indicator/EN.ATM.CO2E.PC\nCollege Board, SAT. (2012). Total group profile report. Retrieved from website: &lt;http://media.collegeboard.com/digitalServices/pdf/research/TotalGroup-2012.pdf&gt;\nConsumer Price Index Data from 1913 to 2019. (2019, June 12). Retrieved July 10, 2019, from https://www.usinflationcalculator.com/inflation/consumer-price-index-and-annual-percent-changes-from-1913-to-2008/\nCPS News Releases. (n.d.). Retrieved July 8, 2019, from https://www.bls.gov/cps/\nCuckoo eggs in nest of other birds. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/cuckoo.html&gt;\nCurrent health expenditure (% of GDP). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/SH.XPD.CHEX.GD.ZS\n11 little-known facts about left-handers. (2013, October 21). Retrieved from &lt;http://www.huffingtonpost.com/2012/10/29/left-handed-facts-lefties_n_2005864.html&gt;\nEducation by age datafile. (2013, December 05). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/Educationbyage.html&gt;\nEncyclopedia Titanica. (2013, November 09). Retrieved from &lt;http://www.encyclopedia-titanica.org/&gt;\nDeaths from firearms. (2013, September 26). Retrieved from http://www.statsci.org/data/oz/firearms.html\nFlanagan, R., Rooney, C., & Griffiths, C. (2005). Fatal poisoning in childhood, england & wales 1968-2000. Forensic Science International, 148:121-129, Retrieved from &lt;http://www.cdc.gov/nchs/data/ice/fatal_poisoning_child.pdf&gt;\nFederal Trade Commission, (2008). Consumer fraud and identity theft complaint data: January-December 2007. Retrieved from website: &lt;http://www.ftc.gov/opa/2008/02/fraud.pdf&gt;\nFertility rate, total (births per woman). (n.d.). Retrieved July 8, 2019, from https://data.worldbank.org/indicator/SP.DYN.TFRT.IN\nFind Out How Many Calories in Beer? (n.d.). Retrieved July 21, 2019, from https://www.beer100.com/beer-calories/\nGettler, L. T., McDade, T. W., Feranil, A. B., & Kuzawa, C. W. (2011). Longitudinal evidence that fatherhood decreases testosterone in human males. The Proceedings of the National Academy of Sciences, PNAS 2011, doi: 10.1073/pnas.1105403108\nGlobal health observatory data respository. (2013, October 09). Retrieved from [http://apps.who.int/gho/athena/data/download.xsl?format=xml&target=GHO/MORT\\\\\\_400&profile=excel&filter=AGEGROUP:YEARS05-14;AGEGROUP:YEARS15-29;AGEGROUP:YEARS30-49;AGEGROUP:YEARS50-69;AGEGROUP:YEARS70](http://apps.who.int/gho/athena/data/download.xsl?format=xml&target=GHO/MORT\\_400&profile=excel&filter=AGEGROUP:YEARS05-14;AGEGROUP:YEARS15-29;AGEGROUP:YEARS30-49;AGEGROUP:YEARS50-69;AGEGROUP:YEARS70){.uri} ;MGHEREG:REG6_AFR;GHECAUSES:\\;SEX:\\\nHealth Insurance Market Place Retrieved from website: http://aspe.hhs.gov/health/reports/2013/marketplacepremiums/ib_premiumslandscape.pdf\nHelmet Sizes for New Zealand Airforce. (n.d.). Retrieved July 20, 2019, from http://www.statsci.org/data/oz/nzhelmet.html\nHo, P. M., Bryson, C. L., & Rumsfeld, J. S. (2009). Medication adherence. Circulation, 119(23), 3028-3035. Retrieved from &lt;http://circ.ahajournals.org/content/119/23/3028&gt;\nHouseholds by age of householder and size of household: 1990 to 2010. (2013, October 19). Retrieved from &lt;http://www.census.gov/compendia/statab/2012/tables/12s0062.pdf&gt;\nJanssen, P. A., Thiessen, P., Klein, M. C., Whitfield, M. F., MacNab, Y. C., & Cullis-Kuhl, S. C. (2007). Standards for the measurement of birth weight, length and head circumference at term in neonates of european, chinese and south asian ancestry. Open Medicine, 1(2), e74-e88. Retrieved from &lt;http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2802014/&gt;\nJohn Matic provided the data from a company he worked with. The company’s name is fictitious, but the data is from an actual company.\nKiama blowhole eruptions. (2013, September 25). Retrieved from &lt;http://www.statsci.org/data/oz/kiama.html&gt;\nKozak K (2019). Survey results form surveys collected in statistics class at Coconino Community College.\nKuulasmaa, K., Hense, H., & Tolonen, H. World Health Organization (WHO), WHO Monica Project. (1998). Quality assessment of data on blood pressure in the who monica project (ISSN 2242-1246). Retrieved from WHO MONICA Project e-publications website: &lt;http://www.thl.fi/publications/monica/bp/bpqa.htm&gt;\nLabor force participation rate, female (% of female population ages 15 ) (modeled ILO estimate). (n.d.). Retrieved July 20, 2019, from https://data.worldbank.org/indicator/SL.TLF.CACT.FE.ZS\nLange TL, Royals HE, Connor LL (1993) Influence of water chemistry on mercury concentration in largemouth bass from Florida lakes. Trans Am Fish Soc 122:74-84. Michael K. Saiki, Darell G. Slotton, Thomas W. May, Shaun M. Ayers, and Charles N. Alpers (2000) Summary of Total Mercury Concentrations in Fillets of Selected Sport Fishes Collected during 2000–2003 from Lake Natoma, Sacramento County, California (Raw data is included in appendix), U.S. Geological Survey Data Series 103, 1-21. NISER 081107 ID Data. (n.d.). Retrieved July 18, 2019, from http://wiki.stat.ucla.edu/socr/index.php/NISER_081107_ID_Data\nLawes, C., Hoorn, S., Law, M., & Rodgers, A. (2004). High cholesterol. In M. Ezzati, A. Lopez, A. Rodgers & C. Murray (Eds.), Comparative Quantification of Health Risks (1 ed., Vol. 1, pp. 391-496). Retrieved from &lt;http://www.who.int/publications/cra/chapters/volume1/0391-0496.pdf&gt;\nLee, A. (1994). Data analysis: An introduction based on r. Auckland. Retrieved from &lt;http://www.statsci.org/data/oz/nzrivers.html&gt;\nLife expectancy at birth. (2013, October 14). Retrieved from http://data.worldbank.org/indicator/SP.DYN.LE00.IN\nLife expectancy in southeast Asia. (2013, September 23). Retrieved from &lt;http://apps.who.int/gho/data/node.main.688&gt;\nMadison, J. (2013, October 15). M&M’s color distribution analysis. Retrieved from &lt;http://joshmadison.com/2007/12/02/mms-color-distribution-analysis/&gt;\nMagazine ads readability. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/magadsdat.html&gt;\nM&M’s Color Distribution Analysis. (n.d.). Retrieved July 11, 2019, from https://joshmadison.com/2007/12/02/mms-color-distribution-analysis/\nMaintaining Balance while Concentrating. (n.d.). Retrieved July 19, 2019, from http://www.statsci.org/data/general/balaconc.html\nMLB heightsweights. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_MLB_HeightsWeights\nOECD economic development. (2013, December 04). Retrieved from http://lib.stat.cmu.edu/DASL/Datafiles/oecdat.html\nOlson, K., & Hanson, J. (1997). Using reiki to manage pain: a preliminary report. Cancer Prev Control, 1(2), 108-13. Retrieved from &lt;http://www.ncbi.nlm.nih.gov/pubmed/9765732&gt;\nOvegard, M., Berndt, K., & Lunneryd, S. (2012). Condition indices of Atlantic cod (gadus morhua) biased by capturing method. ICES Journal of Marine Science, doi: 10.1093/icesjms/fss145\nPopular kids datafile. (2013, December 05). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Datafiles/PopularKids.html&gt;\nPopulation density (people per sq. km of land area). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/EN.POP.DNST\nPopulation reference bureau. (2013, October 8). Retrieved from &lt;http://www.prb.org/DataFinder/Topic/Rankings.aspx?ind=25&gt;\nPrediction of Height from Metacarpal Bone Length. (n.d.). Retrieved July 9, 2019, from http://www.statsci.org/data/general/stature.html\nPregnant women receiving prenatal care (%). (n.d.). Retrieved July 9, 2019, from https://data.worldbank.org/indicator/SH.STA.ANVC.ZS\nPulse rates before and after exercise. (2013, September 25). Retrieved from &lt;http://www.statsci.org/data/oz/ms212.html&gt;\nReserve Bank of Australia. (2019, May 13). Statistical Tables. Retrieved July 10, 2019, from https://www.rba.gov.au/statistics/tables/\nRyan, B. F., Joiner, B. L., & Ryan, Jr, T. A. (1985). Cholesterol levels after heart attack. Retrieved from &lt;http://www.statsci.org/data/general/cholest.html&gt;\nSanchez, Y. W. (2016, October 20). Poll: Arizona voters still favor legalizing marijuana. Retrieved from https://www.azcentral.com/story/news/politics/elections/2016/10/20/poll-arizona-marijuana-legalization-proposition-205/92417690/\nSchultz, S. T., Klonoff-Cohen, H. S., Wingard, D. L., Askhoomoff, N. A., Macera, C. A., Ji, M., & Bacher, C. (2006). Breastfeeding, infant formula supplementation, and autistic disorder: the results of a parent survey. International Breastfeeding Journal, 1(16), doi: 10.1186/1746-4358-1-16\nSeafood online. (2013, November 20). Retrieved from http://www.allfreshseafood.com/\nSmoking and cancer. (2013, December 04). Retrieved from http://lib.stat.cmu.edu/DASL/Datafiles/cigcancerdat.html\nSOCR 012708 id data hotdogs. (2013, November 13). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_012708_ID_Data_HotDogs\nSOCR Data 2008 World CountriesRankings. (n.d.). Retrieved July 19, 2019, from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_2008_World_CountriesRankings#SOCR_Data_-_Ranking_of_the_top_100_Countries_based_on_Political.2C_Economic.2C_Health.2C_and_Quality-of-Life_Factors\nSOCR data Oct2009 id ni. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_Oct2009_ID_NI\nSOCR data nips infantvitK shotdata. (2013, November 16). Retrieved from http://wiki.stat.ucla.edu/socr/index.php/SOCR_Data_NIPS_InfantVitK_ShotData\nStaff nurse - RN salary. (2013, November 08). Retrieved from &lt;http://www1.salary.com/Staff-Nurse-RN-salary.html&gt;\nTime between nerve pulses. (2019, July 3). Retrieved from &lt;http://www.statsci.org/data/general/nerve.html&gt;\nTime of passages of play in rugby. (2013, September 25). Retrieved from &lt;http://www.statsci.org/data/oz/rugby.html&gt;\nTuition and Fees, 1998-99 Through 2018-19. (2018, December 31). Retrieved from https://www.chronicle.com/interactives/tuition-and-fees\nU.S. Census Bureau, Current Population Survey, Annual Social and Economic Supplements.\nUS Department of Agriculture, Agricultural Research Service. (2012). What we eat in America. Retrieved from website: &lt;http://www.ars.usda.gov/Services/docs.htm?docid=18349&gt;\nUS Department of Commerce, & Noaa. (2016, November 15). 1950 Oklahoma Tornadoes. Retrieved from https://www.weather.gov/oun/tornadodata-ok-1950\nUV radiation: Burden of disease by country. (2013, September 4). Retrieved from &lt;http://apps.who.int/gho/data/node.main.165?lang=en&gt;\nWaste run up. (2013, December 04). Retrieved from &lt;http://lib.stat.cmu.edu/DASL/Stories/wasterunup.html&gt;\nWhat percentage of people have green eyes?. (2013, October 21). Retrieved from &lt;http://www.ask.com/question/what-percentage-of-people-have-green-eyes&gt;",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>References</span>"
    ]
  },
  {
    "objectID": "Regression.html#correlation-and-coefficient-of-determination",
    "href": "Regression.html#correlation-and-coefficient-of-determination",
    "title": "10  Regression",
    "section": "",
    "text": "A correlation exists between two variables when the values of one variable are associated with the values of the other variable. Correlation is a word used in everyday life, and many people think they understand what it means. What it really means, when stated as a number, is a measure of how strong the linear relationship is between two variables. This measure is called the correlation coefficient, or correlation for short. It is also called the Pearson correlation coefficient after Karl Pearson who developed it. There are many types of patterns (relationships) between variables that one can see in data. Common types of patterns are linear, exponential, logarithmic, or periodic. To see these patterns, you can draw a scatterplot of the data. In this chapter, and generally in this course, we mostly deal with linear relationships.\n\n10.2.1 Interpretation of the correlation coefficient\nThe correlation coefficient is a number between −1 and 1 and is usually denoted by the letter \\(r\\). A value of \\(r = 1\\) indicates a perfect positive linear relationship, and a value of \\(r = -1\\) indicates a perfect negative linear relationship. A perfect linear relationship means that all of the data points lie exactly on a straight line.\nThe strength of the linear relationship depends on how close \\(r\\) is to 1 or −1. Values of \\(r\\) close to these extremes indicate a strong linear relationship, while values closer to 0 indicate a weaker linear relationship. A correlation of \\(r = 0\\) means there is no linear relationship between the two variables.\nThe sign of \\(r\\) describes the direction of the relationship. A positive correlation means that as one variable increases, the other variable tends to increase. A negative correlation means that as one variable increases, the other variable tends to decrease.\n\n\n\n10.2.2 Example: Calculating the Linear Correlation Coefficient, \\(r\\)\nHow strong is the positive relationship between alcohol content and the number of calories in a 12-ounce beer? To answer this question, compute the correlation between alcohol content and calories using the beer dataset (“Calories in Beer,” 2011). The first six rows of this dataset are shown in Table 10.1. Use Rguroo to find the correlation coefficient and then interpret its value.\n\n10.2.2.1 Solution\nYou can obtain the correlation coefficient, \\(r\\), in Rguroo from the output of the Linear Regression function. To obtain this value for the beer example, follow the steps for creating a linear regression model here.\nFor these data the correlation coefficient between alcohol content and calories is 0.9036. This value appears in the Data and Model Summary table of the regression function output in the row labeled Pearson Correlation (r); see Table 10.2, and it indicates a strong positive linear relationship between alcohol content and calories, since the value of \\(r\\) is close to 1. Also, since \\(r\\) is positive, it means that as alcohol content increases, the number of calories tends to increase as well.\n\n\nCorrelation is related to another measure that explains how much of the variability in the response variable is explained by the explanatory variable. The variability in a response variable can be due to many factors. Some of the variability is explained by the explanatory variable and some is due to other factors. For example, in the beer example, some of the variability in calories is explained by alcohol content and some is due to other factors, such as other ingredients in the beer. There is a way to measure how much of the variability in the response variable is explained by the explanatory variable. This measure is called the coefficient of determination, denoted by \\(R^2\\) (read R-squared), and its value is the square of the correlation.\n\nAs an example, using the beer data, we want to understand how much of the variation in calories is explained by alcohol content and how much is due to other factors, such as ingredients. For instance, two beers can have the same alcohol content but different numbers of calories because they contain different ingredients. A linear regression of calories on alcohol content explains the part of the variation in calories that is associated with alcohol content. The remaining variation is due to other factors and random differences between beers, and is not explained by the regression. Together, the explained and unexplained parts account for all of the variability in calories:\n(total variation) = (explained variation) + (unexplained variation)\nThe proportion of the variation that is explained by the model is \\(R^2=\\frac{\\text{explained variation}}{\\text{total variation}}\\).\nThis is known as the coefficient of determination.\n\n\n\n10.2.3 Example: Find the coefficient of determination \\(R^2\\)\nHow well does the alcohol content of a beer explain the variability in the number of calories in 12-ounce beer? To determine this, use a sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019), is in Table 10.1.\n\n10.2.3.1 Solution\nYou can obtain the coefficient of determination, \\(R^2\\), in Rguroo from the output of the Linear Regression function. To obtain the coefficient of determination for the beer example, follow the steps for creating a linear regression model here.\nFor these data, the coefficient of determination between alcohol content and calories is 0.8164. This value appears in the Data and Model Summary table of the regression function output in the row labeled Coefficient of Determination (R-squared); see Table 10.2. This value indicates that 81.64% of the variability in calories is explained by alcohol content, while the remaining 18.36% is due to other factors.\nA high coefficient of determination (for example, above 0.8) means that only a small portion of the variability is unexplained. In this situation, alcohol content alone explains most of the variability in calories, so there may be little need to include additional explanatory variables to improve the model.\n\nThis is all you have to do to find how much of the variability in the response variable is explained by the linear model.\nThe symbol for the correlation coefficient is \\(r\\). As noted earlier, you can find \\(r\\) in the output of the Linear Regression function in Rguroo. Another way to obtain \\(r\\) is to take the square root of \\(R^2\\), so \\(r = \\pm\\sqrt{R^2}\\).\nThe question is which sign, plus (\\(+\\)) or minus (\\(-\\)), should be used. If the relationship is positive (that is, the slope of the regression line is positive), use the \\(+\\) sign. If the relationship is negative (the slope is negative), use the \\(-\\) sign. In other words, assign to \\(r\\) the same sign as the slope of the linear regression line.\n\n\n\n\n10.2.4 Causation\nOne common mistake people make is to assume that because there is a correlation, then one variable causes the other. This is usually not the case. That would be like saying the amount of alcohol in the beer causes it to have a certain number of calories. However, fermentation of sugars is what causes the alcohol content. The more sugars you have, the more alcohol can be made, and the more sugar, the higher the calories. It is actually the amount of sugar that causes both. Do not confuse the idea of correlation with the concept of causation. Just because two variables are correlated does not mean one causes the other to happen. However, the new theory is showing that if you have a relationship between two variables and a strong correlation, and you can show that there are no other variables that could explain the change, then you can show causation. This is how doctors have shown that smoking causes kidney cancer. Just realize that proving that one caused the other is a difficult process, and causation should not be just assumed.\n\n\n10.2.5 Example: Correlation Versus Causation\n\nA study showed a strong linear correlation between per capita beer consumption and teacher’s salaries. Does giving a teacher a raise cause people to buy more beer? Does buying more beer cause teachers to get a raise?\nA study shows that there is a correlation between people who have had a root canal and those that have cancer. Does that mean having a root canal causes cancer?\n\n\n10.2.5.1 Solution\n\nA study showed a strong linear correlation between per capita beer consumption and teacher’s salaries. Does giving a teacher a raise cause people to buy more beer? Does buying more beer cause teachers to get a raise?\nThere is probably some other factor causing both of them to increase at the same time. Think about this: In a town where people have little extra money, they won’t have money for beer and they won’t give teachers raises. In another town where people have more extra money to spend it will be easier for them to buy more beer and they would be more willing to give teachers raises.\n\n\n\nA study shows that there is a correlation between people who have had a root canal and those that have cancer. Does that mean having a root canal causes cancer?\nJust because there is positive correlation doesn’t mean that one caused the other. It turns out that there is a positive correlation between eating carrots and cancer, but that doesn’t mean that eating carrots causes cancer. In other words, there are lots of relationships you can find between two variables, but that doesn’t mean that one caused the other.\n\nRemember a correlation only means a pattern exists. It does not mean that one variable causes the other variable to change.\n\n\n\n10.2.6 Homework for Correlation Section\nFor each problem, state the random variables.The Data Frame in this section are in section 10.1 and will be used in section 10.3.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected and are in Table 2.15 (\\“Prediction of height,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Metacarpal is below Table 2.15.\n\nTable 2.16 contains the value of the house and the amount of rental income in a year that the house brings in (\\“Capital and rental,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame House is below Table 2.16.\n\nThe World Bank collects information on the life expectancy of a person in each country (\\“Life expectancy at,\\” 2013) and the fertility rate per woman in the country (\\“Fertility rate,\\” 2013). The Data Frame for countries for the year 2011 are in Table 2.17. Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Fertility is below Table 2.17.\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The Data Frame for the countries where this information is available in Table 2.18. Find the coefficient of determination and the correlation coefficient between the percentage spent on health expenditure and the percentage of women receiving prenatal care, then interpret both.\n\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\nThe height and weight of baseball players are in Table 10.5 (\\“MLB heightsweights,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Baseball is below Table 10.5.\n\nDifferent species have different body weights and brain weights are in Table 10.6 (\\“Brain2bodyweight,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Body is below Table 10.6.\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (\\“Data hotdogs,\\” 2013) The Data Frame are in Table 9.9. Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for data frame Hotdog is below Table 9.9.\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in Table 10.7 (\\“OECD economic development,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Agriculture is Table 10.7.\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are in Table 11.15 (\\“Smoking and cancer,\\” 2013). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Cancer is below Table 11.15.\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in Table 10.9 (\\“us auto mileage,\\” 2019). Find the coefficient of determination and the correlation coefficient, then interpret both.\n\nCode book for Data Frame Cars is below Table 10.9.\n\nThere is a correlation between police expenditure and crime rate. Does this mean that spending more money on police causes the crime rate to decrease? Explain your answer.\nThere is a correlation between tobacco sales and alcohol sales. Does that mean that using tobacco causes a person to also drink alcohol? Explain your answer.\nThere is a correlation between the average temperature in a location and the mortality rate from breast cancer. Does that mean that higher temperatures cause more women to die of breast cancer? Explain your answer.\nThere is a correlation between the length of time a tableware company polishes a dish and the price of the dish. Does that mean that the time a plate is polished determines the price of the dish? Explain your answer.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#inference-regression",
    "href": "Regression.html#inference-regression",
    "title": "10  Regression",
    "section": "10.3 Inference for Regression and Correlation",
    "text": "10.3 Inference for Regression and Correlation\nThe idea behind regression is to find an equation that relates the response variable to the explanatory variables, and then use that equation to predict values of the response variable from values of the explanatory variables. But how do you know how good that estimate is?\nWhen we use a regression equation to make a prediction, the predicted value is only a single number. In practice, however, there is almost always some uncertainty in that prediction. Even if we knew the true relationship between the variables, individual observations would still vary around that relationship due to natural randomness and factors not included in the model.\nBecause of this variability, it is usually more informative to give a range of plausible values for the response variable rather than a single predicted value. A prediction interval provides such a range. It gives an interval of values that is likely to contain the response value for a new observation with specified explanatory variable values, along with a stated level of confidence.\n\n\n10.3.1 Prediction Interval\nUsing the regression equation you can predict the number of calories from the alcohol content. However, you only find one value. The problem is that beers vary a bit in calories even if they have the same alcohol content. It would be nice to have a range instead of a single value. The range is called a prediction interval.\n\n\n10.3.2 Prediction Interval for an Individual \\(y\\)\nGiven the fixed value \\(x\\), a prediction interval for the response at \\(x\\) is \\(\\hat{y}\\pm E\\) where \\(\\hat{y}\\) is the predicted value from the regression equation and \\(E\\) is the margin of error for the prediction.\nRguroo will calculate the prediction interval for you in the Linear Regression function. You just have to give it the value of \\(x\\) that you want to predict for, and tell it you want a prediction interval.\n\n\n10.3.3 Example: Find the Prediction Interval\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Find a 95% prediction interval for the number of calories when the alcohol content is 7.0%.\n\n10.3.3.1 Solution\nTo obtain the prediction interval for the beer example, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, enter the value of alcohol content (0.07) in the textbox labeled Predict at new x value(s). Then click the tab labeled Prediction Interval and select the checkboxes labeled Prediction Interval. These steps are shown in Figure 10.8.\n\n\n\n\n\n\n\n\nFigure 10.8: Rguroo dialog for obtaining prediction interval for calories from alcohol content\n\n\n\n\n\nThe output containing the prediction interval is shown in Figure 10.9.\n\n\n\n\n\n\n\n\nFigure 10.9: Rguroo output showing prediction interval for calories from alcohol content\n\n\n\n\n\n\nThe value 201.592 is the model’s predicted number of calories for a beer with 7.0% alcohol. The prediction interval is shown in the columns labeled Lower Pred Limit (2.5%) and Upper Pred Limit (97.5%). This means we can be 95% confident that the actual number of calories for a beer with 7.0% alcohol lies between 166.7 and 236.5. This range is called a 95% prediction interval.\nYou can obtain prediction intervals for other alcohol contents by entering different values in the textbox labeled Predict at new x value(s) in the Linear Regression dialog box. You can also change the confidence level using the textbox labeled Confidence Level in the Confidence Interval tab.\n\n\n\n10.3.4 Hypothesis Test for Correlation or Slope:\nHow do we decide whether there is really a correlation or a linear relationship between two variables? Can we test whether such a relationship exists? The answer is yes. We can test for correlation, or we can test whether the slope of the regression line is zero.\nThese two tests are equivalent. If there is a correlation between the variables, then the slope of the regression line is not zero. Likewise, if the slope of the regression line is not zero, then there is a correlation between the variables. For this reason, testing for correlation and testing for slope lead to the same conclusion.\nThe steps for performing a hypothesis test for correlation are given below. You can use these steps to test for correlation or to test whether the slope is zero.\n\nState the variables in words.\n\n\\(x\\) = explanatory variable\n\\(y\\) = response variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nOr equivalently\n\\(H_o: \\text{the slope is zero}\\)\n\\(H_a: \\text{the slope is not zero}\\)\nAlso, state \\(\\alpha\\), the significance level for your test.\n\nState and check the conditions for the hypothesis test, as explained in the sections Assumptions for Validity of the Regression Equation and Line and Assumptions for Validity of Inference.\nFind the test statistic and p-value\n\nThis will be calculated by Rguroo.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n10.3.5 Example: Testing the Claim of a Linear Correlation\nIs there a linear relationship, or correlation, between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Test at the 5% level.\n\n10.3.5.1 Solution\n\nState the random variables in words.\n\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test were already checked in Example: Find the Equation of the Regression Line\n\nFind the test statistic and p-value\n\nTo obtain the test statistic and p-value, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, under the Test of Association tab, check the box labeled Slope, and under the method section select Theoretical t-statistic. These steps are shown in Figure 10.10. The default value for the significance level is 0.05, so you do not need to change that value. However, if you want to change it, you can do so in the textbox labeled Significance Level.\n\n\n\n\n\n\n\n\nFigure 10.10: Rguroo dialog for testing slope for calories from alcohol content\n\n\n\n\n\nThe output containing the result of the test shown in Figure 10.11.\n\n\n\n\n\n\n\n\nFigure 10.11: AlcoholCalorieRegress_test_out\n\n\n\n\n\nThe test statistic is the t value in the column labeled Obs t Stat. In this case the t-statistic is 31.634, and the p-value id \\(8.84X10^{-85}\\).\n\nConclusion\n\nReject \\(H_o\\) since the p-value is less than 0.05.\n\nInterpretation\n\nThere is enough evidence to show that there is a correlation between alcohol content and number of calories in a 12-ounce bottle of beer.\n\n\n\n10.3.6 Homework for Inference for Regression and Correlation Section\nFor each problem, state the random variables. The Data Frame in this section are in the homework for section 10.1 and were also used in section 10.2.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected. The data are in the height_vs_metacarp dataset, and the first six rows of the data are shown in Table 2.15 (“Prediction of height,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name height_vs_metacarp.\nCode book for Data Frame Metacarpal is below Table 2.15.\n\nCompute a 99% prediction interval for height of a person with a metacarpal length of 44 mm.\nTest at the 1% level for a correlation or linear relationship between length of metacarpal bone 1 and height of a person.\n\n\n\nTable 2.16 contains the first six values of the house and the amount of rental income in a year that the house brings in (“Capital and rental,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name house.\nCode book for Data Frame House is below Table 2.16.\n\n\nCompute a 95% prediction interval for the rental income on a house worth \\$230,000.\nTest at the 5% level for a correlation or linear relationship between house value and rental amount.\n\n\n\nThe World Bank collects information on the life expectancy of a person in each country (“Life expectancy at,” 2013) and the fertility rate per woman in the country (“Fertility rate,” 2013). The first siz rows of the Life_exp_fert_rate dataset for countries for the year 2011 are in Table 2.17.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name Life_exp_fert_rate.\nCode book for Data Frame Fertility is below Table 2.17.\n\n\n\nCompute a 99% prediction interval for the life expectancy for a country that has a fertility rate of 2.7.\nTest at the 1% level for a correlation or linear relationship between fertility rate and life expectancy.\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The data for the countries where this information is available is in the fertility_prenatal dataset and the first six rows of the data are given in Table 2.18.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name fertility_prenatal.\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\n\n\nCompute a 95% prediction interval for the percentage of woman receiving prenatal care for a country that spends 5.0 % of GDP on health expenditure.\nTest at the 5% level for a correlation or linear relationship between percentage spent on health expenditure and the percentage of women receiving prenatal care.\n\n\nThe height and weight of baseball players are in the baseball dataset and the first six rows of the data are shown in Table 10.5 (“MLB heightsweights,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name baseball.\nCode book for Data Frame Baseball is below Table 10.5.\n\n\n\nCompute a 95% prediction interval for the weight of a baseball player that is 75 inches tall.\nTest at the 5% level for a correlation or linear relationship between height and weight of baseball players.\n\n\nDifferent species have different body weights and brain weights. The dataset body contains data about these two variables, and Table 10.6 shows the first six rows of the dataset (“Brain2bodyweight,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name body.\nCode book for Data Frame Body is below Table 10.6.\n\n\n\nCompute a 99% prediction interval for the brain weight for a species that has a body weight of 62 kg.\nTest at the 1% level for a correlation or linear relationship between body weights and brain weights.\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (“Data hotdogs,” 2013). The data are in hotdog dataset with the first six rows shown in Table 9.9.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name hotdog.\nCode book for data frame Hotdog is below Table 9.9.\n\n\n\nCompute a 95% prediction interval for the amount of sodium a beef hot dog has if it is 170 calories.\nTest at the 5% level for a correlation or linear relationship between amount of calories and amount of sodium.\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in the dataset agriculture. The first six rows are shon in Table 10.7 (“OECD economic development,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name agriculture.\nCode book for Data Frame Agriculture is Table 10.7.\n\n\n\nCompute a 90% prediction interval for the per capita income in a country that has 21 percent of labor in agriculture.\nTest at the 5% level for a correlation or linear relationship between percent of labor force in agriculture and per capita income.\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are given in teh dataset cancer_1 with the first six rows showing in Table 11.15 (“Smoking and cancer,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cancer_1.\nCode book for Data Frame Cancer is below Table 11.15.\n\n\n\nCompute a 99% prediction interval for the number of deaths from bladder cancer when the cigarette sales were 20 per capita.\nTest at the 1% level for a correlation or linear relationship between cigarette smoking and deaths of bladder cancer.\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in cars dataset. The first six rows of the data are shown in Table 10.9 (“us auto mileage,” 2019).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cars.\nCode book for Data Frame Cars is below Table 10.9.\n\n\n\nCompute a 95% prediction interval for the mileage on a car that weighs 3800 pounds.\nTest at the 5% level for a correlation or linear relationship between the weight of cars and mileage.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#sec-inference-regression",
    "href": "Regression.html#sec-inference-regression",
    "title": "10  Regression",
    "section": "10.3 Inference for Regression and Correlation",
    "text": "10.3 Inference for Regression and Correlation\nThe idea behind regression is to find an equation that relates the response variable to the explanatory variables, and then use that equation to predict values of the response variable from values of the explanatory variables. But how do you know how good that estimate is?\nWhen we use a regression equation to make a prediction, the predicted value is only a single number. In practice, however, there is almost always some uncertainty in that prediction. Even if we knew the true relationship between the variables, individual observations would still vary around that relationship due to natural randomness and factors not included in the model.\nBecause of this variability, it is usually more informative to give a range of plausible values for the response variable rather than a single predicted value. A prediction interval provides such a range. It gives an interval of values that is likely to contain the response value for a new observation with specified explanatory variable values, along with a stated level of confidence.\n\n\n10.3.1 Prediction Interval\nUsing the regression equation you can predict the number of calories from the alcohol content. However, you only find one value. The problem is that beers vary a bit in calories even if they have the same alcohol content. It would be nice to have a range instead of a single value. The range is called a prediction interval.\n\n\n10.3.2 Prediction Interval for an Individual \\(y\\)\nGiven the fixed value \\(x\\), a prediction interval for the response at \\(x\\) is \\(\\hat{y}\\pm E\\) where \\(\\hat{y}\\) is the predicted value from the regression equation and \\(E\\) is the margin of error for the prediction.\nRguroo will calculate the prediction interval for you in the Linear Regression function. You just have to give it the value of \\(x\\) that you want to predict for, and tell it you want a prediction interval.\n\n\n10.3.3 Example: Find the Prediction Interval\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Find a 95% prediction interval for the number of calories when the alcohol content is 7.0%.\n\n10.3.3.1 Solution\nTo obtain the prediction interval for the beer example, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, enter the value of alcohol content (0.07) in the textbox labeled Predict at new x value(s). Then click the tab labeled Prediction Interval and select the checkboxes labeled Prediction Interval. These steps are shown in Figure 10.8.\n\n\n\n\n\n\n\n\nFigure 10.8: Rguroo dialog for obtaining prediction interval for calories from alcohol content\n\n\n\n\n\nThe output containing the prediction interval is shown in Figure 10.9.\n\n\n\n\n\n\n\n\nFigure 10.9: Rguroo output showing prediction interval for calories from alcohol content\n\n\n\n\n\n\nThe value 201.592 is the model’s predicted number of calories for a beer with 7.0% alcohol. The prediction interval is shown in the columns labeled Lower Pred Limit (2.5%) and Upper Pred Limit (97.5%). This means we can be 95% confident that the actual number of calories for a beer with 7.0% alcohol lies between 166.7 and 236.5. This range is called a 95% prediction interval.\nYou can obtain prediction intervals for other alcohol contents by entering different values in the textbox labeled Predict at new x value(s) in the Linear Regression dialog box. You can also change the confidence level using the textbox labeled Confidence Level in the Confidence Interval tab.\n\n\n\n10.3.4 Hypothesis Test for Correlation or Slope:\nHow do we decide whether there is really a correlation or a linear relationship between two variables? Can we test whether such a relationship exists? The answer is yes. We can test for correlation, or we can test whether the slope of the regression line is zero.\nThese two tests are equivalent. If there is a correlation between the variables, then the slope of the regression line is not zero. Likewise, if the slope of the regression line is not zero, then there is a correlation between the variables. For this reason, testing for correlation and testing for slope lead to the same conclusion.\nThe steps for performing a hypothesis test for correlation are given below. You can use these steps to test for correlation or to test whether the slope is zero.\n\nState the variables in words.\n\n\\(x\\) = explanatory variable\n\\(y\\) = response variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nOr equivalently\n\\(H_o: \\text{the slope is zero}\\)\n\\(H_a: \\text{the slope is not zero}\\)\nAlso, state \\(\\alpha\\), the significance level for your test.\n\nState and check the conditions for the hypothesis test, as explained in the sections Assumptions for Validity of the Regression Equation and Line and Assumptions for Validity of Inference.\nFind the test statistic and p-value\n\nThis will be calculated by Rguroo.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n10.3.5 Example: Testing the Claim of a Linear Correlation\nIs there a linear relationship, or correlation, between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Test at the 5% level.\n\n10.3.5.1 Solution\n\nState the random variables in words.\n\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test were already checked in Example: Find the Equation of the Regression Line\n\nFind the test statistic and p-value\n\nTo obtain the test statistic and p-value, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, under the Test of Association tab, check the box labeled Slope, and under the method section select Theoretical t-statistic. These steps are shown in Figure 10.10. The default value for the significance level is 0.05, so you do not need to change that value. However, if you want to change it, you can do so in the textbox labeled Significance Level.\n\n\n\n\n\n\n\n\nFigure 10.10: Rguroo dialog for testing slope for calories from alcohol content\n\n\n\n\n\nThe output containing the result of the test shown in Figure 10.11.\n\n\n\n\n\n\n\n\nFigure 10.11: AlcoholCalorieRegress_test_out\n\n\n\n\n\nThe test statistic is the t value in the column labeled Obs t Stat. In this case the t-statistic is 31.634, and the p-value id \\(8.84X10^{-85}\\).\n\nConclusion\n\nReject \\(H_o\\) since the p-value is less than 0.05.\n\nInterpretation\n\nThere is enough evidence to show that there is a correlation between alcohol content and number of calories in a 12-ounce bottle of beer.\n\n\n\n10.3.6 Homework for Inference for Regression and Correlation Section\nFor each problem, state the random variables. The Data Frame in this section are in the homework for section 10.1 and were also used in section 10.2.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected. The data are in the height_vs_metacarp dataset, and the first six rows of the data are shown in Table 2.15 (“Prediction of height,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name height_vs_metacarp.\nCode book for Data Frame Metacarpal is below Table 2.15.\n\nCompute a 99% prediction interval for height of a person with a metacarpal length of 44 mm.\nTest at the 1% level for a correlation or linear relationship between length of metacarpal bone 1 and height of a person.\n\n\n\nTable 2.16 contains the first six values of the house and the amount of rental income in a year that the house brings in (“Capital and rental,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name house.\nCode book for Data Frame House is below Table 2.16.\n\n\nCompute a 95% prediction interval for the rental income on a house worth \\$230,000.\nTest at the 5% level for a correlation or linear relationship between house value and rental amount.\n\n\n\nThe World Bank collects information on the life expectancy of a person in each country (“Life expectancy at,” 2013) and the fertility rate per woman in the country (“Fertility rate,” 2013). The first siz rows of the Life_exp_fert_rate dataset for countries for the year 2011 are in Table 2.17.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name Life_exp_fert_rate.\nCode book for Data Frame Fertility is below Table 2.17.\n\n\n\nCompute a 99% prediction interval for the life expectancy for a country that has a fertility rate of 2.7.\nTest at the 1% level for a correlation or linear relationship between fertility rate and life expectancy.\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The data for the countries where this information is available is in the fertility_prenatal dataset and the first six rows of the data are given in Table 2.18.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name fertility_prenatal.\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\n\n\nCompute a 95% prediction interval for the percentage of woman receiving prenatal care for a country that spends 5.0 % of GDP on health expenditure.\nTest at the 5% level for a correlation or linear relationship between percentage spent on health expenditure and the percentage of women receiving prenatal care.\n\n\nThe height and weight of baseball players are in the baseball dataset and the first six rows of the data are shown in Table 10.5 (“MLB heightsweights,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name baseball.\nCode book for Data Frame Baseball is below Table 10.5.\n\n\n\nCompute a 95% prediction interval for the weight of a baseball player that is 75 inches tall.\nTest at the 5% level for a correlation or linear relationship between height and weight of baseball players.\n\n\nDifferent species have different body weights and brain weights. The dataset body contains data about these two variables, and Table 10.6 shows the first six rows of the dataset (“Brain2bodyweight,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name body.\nCode book for Data Frame Body is below Table 10.6.\n\n\n\nCompute a 99% prediction interval for the brain weight for a species that has a body weight of 62 kg.\nTest at the 1% level for a correlation or linear relationship between body weights and brain weights.\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (“Data hotdogs,” 2013). The data are in hotdog dataset with the first six rows shown in Table 9.9.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name hotdog.\nCode book for data frame Hotdog is below Table 9.9.\n\n\n\nCompute a 95% prediction interval for the amount of sodium a beef hot dog has if it is 170 calories.\nTest at the 5% level for a correlation or linear relationship between amount of calories and amount of sodium.\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in the dataset agriculture. The first six rows are shon in Table 10.7 (“OECD economic development,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name agriculture.\nCode book for Data Frame Agriculture is Table 10.7.\n\n\n\nCompute a 90% prediction interval for the per capita income in a country that has 21 percent of labor in agriculture.\nTest at the 5% level for a correlation or linear relationship between percent of labor force in agriculture and per capita income.\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are given in teh dataset cancer_1 with the first six rows showing in Table 11.15 (“Smoking and cancer,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cancer_1.\nCode book for Data Frame Cancer is below Table 11.15.\n\n\n\nCompute a 99% prediction interval for the number of deaths from bladder cancer when the cigarette sales were 20 per capita.\nTest at the 1% level for a correlation or linear relationship between cigarette smoking and deaths of bladder cancer.\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in cars dataset. The first six rows of the data are shown in Table 10.9 (“us auto mileage,” 2019).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cars.\nCode book for Data Frame Cars is below Table 10.9.\n\n\n\nCompute a 95% prediction interval for the mileage on a car that weighs 3800 pounds.\nTest at the 5% level for a correlation or linear relationship between the weight of cars and mileage.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression</span>"
    ]
  },
  {
    "objectID": "Regression.html#sec:inference-regression",
    "href": "Regression.html#sec:inference-regression",
    "title": "10  Regression",
    "section": "10.3 Inference for Regression and Correlation",
    "text": "10.3 Inference for Regression and Correlation\nThe idea behind regression is to find an equation that relates the response variable to the explanatory variables, and then use that equation to predict values of the response variable from values of the explanatory variables. But how do you know how good that estimate is?\nWhen we use a regression equation to make a prediction, the predicted value is only a single number. In practice, however, there is almost always some uncertainty in that prediction. Even if we knew the true relationship between the variables, individual observations would still vary around that relationship due to natural randomness and factors not included in the model.\nBecause of this variability, it is usually more informative to give a range of plausible values for the response variable rather than a single predicted value. A prediction interval provides such a range. It gives an interval of values that is likely to contain the response value for a new observation with specified explanatory variable values, along with a stated level of confidence.\n\n\n10.3.1 Prediction Interval\nUsing the regression equation you can predict the number of calories from the alcohol content. However, you only find one value. The problem is that beers vary a bit in calories even if they have the same alcohol content. It would be nice to have a range instead of a single value. The range is called a prediction interval.\n\n\n10.3.2 Prediction Interval for an Individual \\(y\\)\nGiven the fixed value \\(x\\), a prediction interval for the response at \\(x\\) is \\(\\hat{y}\\pm E\\) where \\(\\hat{y}\\) is the predicted value from the regression equation and \\(E\\) is the margin of error for the prediction.\nRguroo will calculate the prediction interval for you in the Linear Regression function. You just have to give it the value of \\(x\\) that you want to predict for, and tell it you want a prediction interval.\n\n\n10.3.3 Example: Find the Prediction Interval\nIs there a relationship between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Find a 95% prediction interval for the number of calories when the alcohol content is 7.0%.\n\n10.3.3.1 Solution\nTo obtain the prediction interval for the beer example, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, enter the value of alcohol content (0.07) in the textbox labeled Predict at new x value(s). Then click the tab labeled Prediction Interval and select the checkboxes labeled Prediction Interval. These steps are shown in Figure 10.8.\n\n\n\n\n\n\n\n\nFigure 10.8: Rguroo dialog for obtaining prediction interval for calories from alcohol content\n\n\n\n\n\nThe output containing the prediction interval is shown in Figure 10.9.\n\n\n\n\n\n\n\n\nFigure 10.9: Rguroo output showing prediction interval for calories from alcohol content\n\n\n\n\n\n\nThe value 201.592 is the model’s predicted number of calories for a beer with 7.0% alcohol. The prediction interval is shown in the columns labeled Lower Pred Limit (2.5%) and Upper Pred Limit (97.5%). This means we can be 95% confident that the actual number of calories for a beer with 7.0% alcohol lies between 166.7 and 236.5. This range is called a 95% prediction interval.\nYou can obtain prediction intervals for other alcohol contents by entering different values in the textbox labeled Predict at new x value(s) in the Linear Regression dialog box. You can also change the confidence level using the textbox labeled Confidence Level in the Confidence Interval tab.\n\n\n\n10.3.4 Hypothesis Test for Correlation or Slope:\nHow do we decide whether there is really a correlation or a linear relationship between two variables? Can we test whether such a relationship exists? The answer is yes. We can test for correlation, or we can test whether the slope of the regression line is zero.\nThese two tests are equivalent. If there is a correlation between the variables, then the slope of the regression line is not zero. Likewise, if the slope of the regression line is not zero, then there is a correlation between the variables. For this reason, testing for correlation and testing for slope lead to the same conclusion.\nThe steps for performing a hypothesis test for correlation are given below. You can use these steps to test for correlation or to test whether the slope is zero.\n\nState the variables in words.\n\n\\(x\\) = explanatory variable\n\\(y\\) = response variable\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nOr equivalently\n\\(H_o: \\text{the slope is zero}\\)\n\\(H_a: \\text{the slope is not zero}\\)\nAlso, state \\(\\alpha\\), the significance level for your test.\n\nState and check the conditions for the hypothesis test, as explained in the sections Assumptions for Validity of the Regression Equation and Line and Assumptions for Validity of Inference.\nFind the test statistic and p-value\n\nThis will be calculated by Rguroo.\n\nConclusion\n\nThis is where you write reject \\(H_o\\) or fail to reject \\(H_o\\). The rule is: if the p-value \\(&lt;\\alpha\\), then reject \\(H_o\\). If the p-value \\(\\ge \\alpha\\), then fail to reject \\(H_o\\)\n\nInterpretation\n\nThis is where you interpret in real world terms the conclusion to the test. The conclusion for a hypothesis test is that you either have enough evidence to support \\(H_a\\), or you do not have enough evidence to support \\(H_a\\).\n\n\n10.3.5 Example: Testing the Claim of a Linear Correlation\nIs there a linear relationship, or correlation, between the alcohol content and the number of calories in 12-ounce beer? To determine this, use the sample of beer’s alcohol content and calories (Find Out How Many Calories in Beer?, 2019) in the beer dataset. The first six rows of this dataset is shown in Table 10.1. Test at the 5% level.\n\n10.3.5.1 Solution\n\nState the random variables in words.\n\n\\(x\\) = alcohol content in the beer\n\\(y\\) = calories in 12 ounce beer\n\nState the null and alternative hypotheses and the level of significance\n\n\\(H_o: \\text{there is not a correlation}\\)\n\\(H_a: \\text{there is a correlation}\\)\nlevel of significance \\(\\alpha=0.05\\)\n\nState and check the conditions for the hypothesis test\n\nThe conditions for the hypothesis test were already checked in Example: Find the Equation of the Regression Line\n\nFind the test statistic and p-value\n\nTo obtain the test statistic and p-value, follow the steps for creating a linear regression model here. Additionally, in the linear Regression dialog box, under the Test of Association tab, check the box labeled Slope, and under the method section select Theoretical t-statistic. These steps are shown in Figure 10.10. The default value for the significance level is 0.05, so you do not need to change that value. However, if you want to change it, you can do so in the textbox labeled Significance Level.\n\n\n\n\n\n\n\n\nFigure 10.10: Rguroo dialog for testing slope for calories from alcohol content\n\n\n\n\n\nThe output containing the result of the test shown in Figure 10.11.\n\n\n\n\n\n\n\n\nFigure 10.11: AlcoholCalorieRegress_test_out\n\n\n\n\n\nThe test statistic is the t value in the column labeled Obs t Stat. In this case the t-statistic is 31.634, and the p-value id \\(8.84X10^{-85}\\).\n\nConclusion\n\nReject \\(H_o\\) since the p-value is less than 0.05.\n\nInterpretation\n\nThere is enough evidence to show that there is a correlation between alcohol content and number of calories in a 12-ounce bottle of beer.\n\n\n\n10.3.6 Homework for Inference for Regression and Correlation Section\nFor each problem, state the random variables. The Data Frame in this section are in the homework for section 10.1 and were also used in section 10.2.\n\nWhen an anthropologist finds skeletal remains, they need to figure out the height of the person. The height of a person (in cm) and the length of their metacarpal bone 1 (in mm) were collected. The data are in the height_vs_metacarp dataset, and the first six rows of the data are shown in Table 2.15 (“Prediction of height,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name height_vs_metacarp.\nCode book for Data Frame Metacarpal is below Table 2.15.\n\nCompute a 99% prediction interval for height of a person with a metacarpal length of 44 mm.\nTest at the 1% level for a correlation or linear relationship between length of metacarpal bone 1 and height of a person.\n\n\n\nTable 2.16 contains the first six values of the house and the amount of rental income in a year that the house brings in (“Capital and rental,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name house.\nCode book for Data Frame House is below Table 2.16.\n\n\nCompute a 95% prediction interval for the rental income on a house worth \\$230,000.\nTest at the 5% level for a correlation or linear relationship between house value and rental amount.\n\n\n\nThe World Bank collects information on the life expectancy of a person in each country (“Life expectancy at,” 2013) and the fertility rate per woman in the country (“Fertility rate,” 2013). The first siz rows of the Life_exp_fert_rate dataset for countries for the year 2011 are in Table 2.17.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name Life_exp_fert_rate.\nCode book for Data Frame Fertility is below Table 2.17.\n\n\n\nCompute a 99% prediction interval for the life expectancy for a country that has a fertility rate of 2.7.\nTest at the 1% level for a correlation or linear relationship between fertility rate and life expectancy.\n\n\nThe World Bank collected data on the percentage of gross domestic product (GDP) that a country spends on health expenditures (Current health expenditure (% of GDP), 2019), the fertility rate of the country (Fertility rate, total (births per woman), 2019), and the percentage of woman receiving prenatal care (Pregnant women receiving prenatal care (%), 2019). The data for the countries where this information is available is in the fertility_prenatal dataset and the first six rows of the data are given in Table 2.18.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name fertility_prenatal.\nCode book for Data Frame Fert_prenatal is below Table 2.18.\n\n\n\nCompute a 95% prediction interval for the percentage of woman receiving prenatal care for a country that spends 5.0 % of GDP on health expenditure.\nTest at the 5% level for a correlation or linear relationship between percentage spent on health expenditure and the percentage of women receiving prenatal care.\n\n\nThe height and weight of baseball players are in the baseball dataset and the first six rows of the data are shown in Table 10.5 (“MLB heightsweights,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name baseball.\nCode book for Data Frame Baseball is below Table 10.5.\n\n\n\nCompute a 95% prediction interval for the weight of a baseball player that is 75 inches tall.\nTest at the 5% level for a correlation or linear relationship between height and weight of baseball players.\n\n\nDifferent species have different body weights and brain weights. The dataset body contains data about these two variables, and Table 10.6 shows the first six rows of the dataset (“Brain2bodyweight,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name body.\nCode book for Data Frame Body is below Table 10.6.\n\n\n\nCompute a 99% prediction interval for the brain weight for a species that has a body weight of 62 kg.\nTest at the 1% level for a correlation or linear relationship between body weights and brain weights.\n\n\nA sample of hot dogs was taken and the amount of sodium (in mg) and calories were measured. (“Data hotdogs,” 2013). The data are in hotdog dataset with the first six rows shown in Table 9.9.\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name hotdog.\nCode book for data frame Hotdog is below Table 9.9.\n\n\n\nCompute a 95% prediction interval for the amount of sodium a beef hot dog has if it is 170 calories.\nTest at the 5% level for a correlation or linear relationship between amount of calories and amount of sodium.\n\n\nPer capita income in 1960 dollars for European countries and the percent of the labor force that works in agriculture in 1960 are in the dataset agriculture. The first six rows are shon in Table 10.7 (“OECD economic development,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name agriculture.\nCode book for Data Frame Agriculture is Table 10.7.\n\n\n\nCompute a 90% prediction interval for the per capita income in a country that has 21 percent of labor in agriculture.\nTest at the 5% level for a correlation or linear relationship between percent of labor force in agriculture and per capita income.\n\n\nCigarette smoking and cancer have been linked. The number of deaths per one hundred thousand from bladder cancer and the number of cigarettes sold per capita in 1960 are given in teh dataset cancer_1 with the first six rows showing in Table 11.15 (“Smoking and cancer,” 2013).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cancer_1.\nCode book for Data Frame Cancer is below Table 11.15.\n\n\n\nCompute a 99% prediction interval for the number of deaths from bladder cancer when the cigarette sales were 20 per capita.\nTest at the 1% level for a correlation or linear relationship between cigarette smoking and deaths of bladder cancer.\n\n\nThe weight of a car can influence the mileage that the car can obtain. A random sample of cars’ weights and mileage was collected and are in cars dataset. The first six rows of the data are shown in Table 10.9 (“us auto mileage,” 2019).\n\n\n The dataset for this exercise is available in the Rguroo dataset repository Kozak, with the dataset name cars.\nCode book for Data Frame Cars is below Table 10.9.\n\n\n\nCompute a 95% prediction interval for the mileage on a car that weighs 3800 pounds.\nTest at the 5% level for a correlation or linear relationship between the weight of cars and mileage.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regression</span>"
    ]
  }
]